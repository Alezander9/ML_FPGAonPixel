{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 13:56:54.742211: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 13:56:54.835392: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 13:56:54.836318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 13:56:57.063957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.13.1\n",
      "keras version: 2.13.1\n",
      "qkeras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "# Machine Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, ReLU, Dropout, BatchNormalization, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow_model_optimization\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "import keras\n",
    "print(\"keras version:\",keras.__version__)\n",
    "import qkeras\n",
    "from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm\n",
    "from qkeras import quantized_relu, quantized_bits\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "print(\"qkeras version:\",keras.__version__)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Display and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Data management\n",
    "import psutil\n",
    "import h5py\n",
    "# Memory management\n",
    "import gc\n",
    "# Notifications\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def send_email_notification(subject, content):\n",
    "    sender_email = os.getenv('EMAIL_USER')\n",
    "    receiver_email = \"alexander.j.yue@gmail.com\"\n",
    "    password = os.getenv('EMAIL_PASS')\n",
    "\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = receiver_email\n",
    "    message[\"Subject\"] = subject\n",
    "    body = content\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message.as_string())\n",
    "\n",
    "# Memory monitoring functions\n",
    "def print_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory usage percentage: {memory.percent}%\")\n",
    "\n",
    "def print_cpu_usage():\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pixel cluster to transverse momentum dataset into the input_data and target_data\n",
    "def load_combine_shuffle_data_optimized_hdf5():\n",
    "    # Load the dataset from Kenny's computer\n",
    "    with h5py.File('/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/fl32_data_v3.hdf5', 'r') as h5f:\n",
    "        combined_input = None\n",
    "        combined_target = None\n",
    "\n",
    "        for data_type in ['sig', 'bkg']:\n",
    "            # Construct dataset names\n",
    "            input_dataset_name = f'{data_type}_input'\n",
    "            target_dataset_name = f'{data_type}_target'\n",
    "\n",
    "            # Check if the dataset exists and load data sequentially\n",
    "            if input_dataset_name in h5f and target_dataset_name in h5f:\n",
    "                input_data = h5f[input_dataset_name][:].astype(np.float32)\n",
    "                target_data = h5f[target_dataset_name][:].astype(np.float32)\n",
    "\n",
    "                if combined_input is None:\n",
    "                    combined_input = input_data\n",
    "                    combined_target = target_data\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "                else:\n",
    "                    print_memory_usage()\n",
    "                    combined_input = np.vstack((combined_input, input_data))\n",
    "                    combined_target = np.vstack((combined_target, target_data))\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "            else:\n",
    "                print(f\"Dataset {input_dataset_name} or {target_dataset_name} not found.\")\n",
    "\n",
    "        # Shuffling\n",
    "        indices = np.arange(combined_input.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        combined_input = combined_input[indices]\n",
    "        combined_target = combined_target[indices]\n",
    "\n",
    "        return combined_input, combined_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load dataset into memory\n",
    "    input_data, target_data = load_combine_shuffle_data_optimized_hdf5()\n",
    "    # Format the dataset into a 20x13x21 tensor (time, y, x)\n",
    "    input_data = input_data.reshape(input_data.shape[0],20,13,21)\n",
    "    return input_data, target_data\n",
    "\n",
    "def process_dataset(input_data, target_data, hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    TRAIN_PT_THRESHOLD = hyperparams[\"TRAIN_PT_THRESHOLD\"]\n",
    "    TEST_PT_THRESHOLD = hyperparams[\"TEST_PT_THRESHOLD\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "\n",
    "    # Split 80% of data into training data, 10% for validation data and 10% for testing data\n",
    "    input_train_data, input_temp, target_train_data, target_temp = \\\n",
    "    train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "    del input_data\n",
    "    del target_data\n",
    "    gc.collect()\n",
    "    input_validate_data, input_test_data, target_validate_data, target_test_data = \\\n",
    "    train_test_split(input_temp, target_temp, test_size=0.5, random_state=42)\n",
    "    del input_temp\n",
    "    del target_temp\n",
    "    gc.collect()\n",
    "\n",
    "    # Save some data for displaying\n",
    "    input_data_example = input_test_data[0:100,:]\n",
    "    target_data_example = target_test_data[0:100,:]\n",
    "\n",
    "    # Fit the scalers on the training data to it all scales the exact same\n",
    "    input_scaler = StandardScaler()\n",
    "    input_scaler.fit(input_train_data[:, :NUM_TIME_SLICES, :, :].reshape(-1,8*13))\n",
    "    y0_scaler = StandardScaler()\n",
    "    y0_scaler.fit(target_train_data[:,7].reshape(-1, 1))\n",
    "\n",
    "    # Process the data into input shape and labels for training\n",
    "    def process_data(input_data, target_data, pt_threshold):\n",
    "        if input_data.shape[1:] == (20, 13, 21) and target_data.shape[1:] == (13, ):\n",
    "\n",
    "            # Truncate down to first time slices\n",
    "            input_data = input_data[:, :NUM_TIME_SLICES, :, :]\n",
    "\n",
    "            # sum over the x axis to turn the input data into a 2D NUM_TIME_SLICES x 13 tensor (time, y)\n",
    "            input_data = np.sum(input_data, axis=3)\n",
    "\n",
    "            if OUTPUT == \"SOFTMAX\" or OUTPUT == \"LINEAR\":\n",
    "                # Encode the target data into one_hot encoding\n",
    "                one_hot = np.zeros((target_data.shape[0], 3))\n",
    "                # Assign 1 for p_t > pt_threshold in GeV, for low p_t put 1 in slot 2 for negative and a 1 in slot 3 for positive\n",
    "                one_hot[np.abs(target_data[:, 8]) >= pt_threshold, 0] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] > 0), 1] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] < 0), 2] = 1\n",
    "                label_data = one_hot\n",
    "            # elif OUTPUT == \"ARGMAX\": # DOES NOT WORK \n",
    "            #     label_data = np.argmax(one_hot, axis=1).astype(np.int64)\n",
    "            #     print(\"one hot is \", one_hot)\n",
    "            elif OUTPUT == \"SINGLE\":\n",
    "            # Binary labels for 0th category\n",
    "                label_data = (np.abs(target_data[:, 8]) >= pt_threshold).astype(np.int64)\n",
    "\n",
    "            # Flatten the input data\n",
    "            input_data = input_data.reshape(-1,NUM_TIME_SLICES*13)\n",
    "\n",
    "            # Normalize the input data to have mean| 0 and std 1\n",
    "            \n",
    "            input_data = input_scaler.transform(input_data)\n",
    "            # # Replace all values < 1 with 1 so they log to 0\n",
    "            # input_data = np.where(np.abs(input_data) < 1.0, 1.0, input_data)\n",
    "            # # Apply logarithmic scaling\n",
    "            # input_data = np.log(np.abs(input_data)) * np.sign(input_data)\n",
    "            # # Min-max normalization (global)\n",
    "            # min_val = np.min(input_data)\n",
    "            # max_val = np.max(input_data)\n",
    "            # print(f\"max of log of data is {max_val} and min is {min_val}\")\n",
    "            # input_data = (input_data) / np.max([max_val,min_val])\n",
    "\n",
    "            # Get the y_0 data\n",
    "            y0_data = target_data[:,7].reshape(-1, 1)\n",
    "            y0_data = y0_scaler.transform(y0_data)\n",
    "            # Combine with input data\n",
    "            if (MODEL_TYPE == \"DNN\"):\n",
    "                # For DNN we concatenate in the y_0 data\n",
    "                input_data_combined = np.hstack((input_data, y0_data))\n",
    "\n",
    "                \n",
    "            elif (MODEL_TYPE == \"CNN\"):\n",
    "                # Reshape data into a matrix for the convolutions\n",
    "                input_data= input_data.reshape(-1, NUM_TIME_SLICES, 13)\n",
    "                # Package with the y_0 data to be added later\n",
    "                input_data_combined = [input_data, y0_data]\n",
    "\n",
    "            return input_data_combined, label_data\n",
    "        else:\n",
    "            raise ValueError(\"Wrong array shape!\")\n",
    "\n",
    "    # Apply data processing to our datasets\n",
    "    input_train_data_combined, target_train_data_coded = process_data(input_train_data, target_train_data, TRAIN_PT_THRESHOLD)\n",
    "    input_validate_data_combined, target_validate_data_coded = process_data(input_validate_data, target_validate_data, TRAIN_PT_THRESHOLD)\n",
    "    input_test_data_combined, target_test_data_coded = process_data(input_test_data, target_test_data, TEST_PT_THRESHOLD)\n",
    "\n",
    "    # Save some data for displaying\n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        input_data_combined_example = input_test_data_combined[0:100,:]\n",
    "        if OUTPUT == \"ARGMAX\" or OUTPUT == \"SINGLE\":\n",
    "            target_data_coded_example = target_test_data_coded[0:100]\n",
    "        else:\n",
    "            target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    elif MODEL_TYPE == \"CNN\":\n",
    "        input_data_combined_example = np.hstack((input_test_data_combined[0][0:100,:].reshape(100, -1), input_test_data_combined[1][0:100,:]))\n",
    "        target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    processed_dataset = {\n",
    "        \"input_train_data_combined\": input_train_data_combined,\n",
    "        \"target_train_data_coded\": target_train_data_coded,\n",
    "        \"input_validate_data_combined\": input_validate_data_combined,\n",
    "        \"target_validate_data_coded\": target_validate_data_coded,\n",
    "        \"input_test_data_combined\": input_test_data_combined,\n",
    "        \"target_test_data_coded\": target_test_data_coded,\n",
    "\n",
    "        \"input_data_example\": input_data_example,\n",
    "        \"target_data_example\": target_data_example,\n",
    "        \"input_data_combined_example\": input_data_combined_example,\n",
    "        \"target_data_coded_example\": target_data_coded_example,\n",
    "    }\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class ArgmaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ArgmaxLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(tf.argmax(inputs, axis=-1), dtype=tf.int64)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ArgmaxLayer, self).get_config()\n",
    "        return config\n",
    "    \n",
    "def qDNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    DNN_LAYERS = hyperparams[\"DNN_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    y_timed_input = Input(shape=(NUM_TIME_SLICES*13 + 1,), name='y_timed_input')\n",
    "    layer = y_timed_input\n",
    "    \n",
    "    for i, size in enumerate(DNN_LAYERS):\n",
    "        layer = QDense(size, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name=f'dense{i+1}')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS))(layer)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = ArgmaxLayer(name='output_argmax')(output)\n",
    "        print(f\"Argmax output dtype: {output.dtype}\")\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        output = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS), \n",
    "                bias_quantizer=quantized_bits(BIAS_BITS), \n",
    "                # activation='sigmoid', \n",
    "                name='dense_output')(layer)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported output type\")\n",
    "   \n",
    "    model = Model(inputs=y_timed_input, outputs=output)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "              loss='mse', \n",
    "              metrics=[Precision()])\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def qCNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    CONV_LAYER_DEPTHS = hyperparams[\"CONV_LAYER_DEPTHS\"]\n",
    "    CONV_LAYER_KERNELS = hyperparams[\"CONV_LAYER_KERNELS\"]\n",
    "    CONV_LAYER_STRIDES = hyperparams[\"CONV_LAYER_STRIDES\"]\n",
    "    MAX_POOLING_SIZE = hyperparams[\"MAX_POOLING_SIZE\"]\n",
    "    FLATTENED_LAYERS = hyperparams[\"FLATTENED_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    INTEGER_BITS = hyperparams[\"INTEGER_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "\n",
    "    y_profile_input = Input(shape=(NUM_TIME_SLICES, 13, 1), name='y_profile_input')  # Adjust the shape based on your input\n",
    "    layer = y_profile_input\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(len(CONV_LAYER_DEPTHS)):\n",
    "        layer = QConv2D(\n",
    "        CONV_LAYER_DEPTHS[i],\n",
    "        kernel_size=CONV_LAYER_KERNELS[i],\n",
    "        strides=CONV_LAYER_STRIDES[i],\n",
    "        kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        padding='same',\n",
    "        use_bias=True,\n",
    "        name=f'conv{i+1}'\n",
    "        )(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS), name=f'relu{i+1}')(layer)\n",
    "        layer = MaxPooling2D(pool_size=MAX_POOLING_SIZE, name=f'maxpool{i+1}')(layer)\n",
    "\n",
    "    # Flatten the output to feed into a dense layer\n",
    "    layer = Flatten(name='flattened')(layer)\n",
    "\n",
    "    # Flatten and concatenate with y0 input\n",
    "    y0_input = Input(shape=(1,), name='y0_input')\n",
    "    layer = Concatenate(name='concat')([layer, y0_input])\n",
    "\n",
    "    # Post-flattening dense layers\n",
    "    for i in range(len(FLATTENED_LAYERS)):\n",
    "        layer = QDense(FLATTENED_LAYERS[i], kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name=f'dense{i+1}')(layer)\n",
    "        layer = QActivation(quantized_relu(10), name=f'relu{len(CONV_LAYER_DEPTHS)+i+1}')(layer)\n",
    "\n",
    "    # Output layer (adjust based on your classification problem)\n",
    "    output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    # layer = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0), \n",
    "    #                bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name='output_dense')(layer)\n",
    "    # output = Activation(\"sigmoid\", name='output_sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=[y_profile_input, y0_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy']) # loss='binary_crossentropy'\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Pruning the model\n",
    "def pruneFunction(layer, train_data_size, hyperparams):\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    FINAL_SPARSITY = hyperparams[\"FINAL_SPARSITY\"]\n",
    "    PRUNE_START_EPOCH = hyperparams[\"PRUNE_START_EPOCH\"]\n",
    "    NUM_PRUNE_EPOCHS = hyperparams[\"NUM_PRUNE_EPOCHS\"]\n",
    "\n",
    "    steps_per_epoch = train_data_size // BATCH_SIZE #input_train_data_combined.shape[0]\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.0,\n",
    "            final_sparsity=FINAL_SPARSITY,\n",
    "            begin_step=steps_per_epoch * PRUNE_START_EPOCH,\n",
    "            end_step=steps_per_epoch * (PRUNE_START_EPOCH + NUM_PRUNE_EPOCHS),\n",
    "            frequency=steps_per_epoch # prune after every epoch\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "    if isinstance(layer, QDense):\n",
    "        if layer.name != 'output_softmax' and layer.name != 'dense2':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        elif layer.name != 'output_softmax' and layer.name != 'dense1':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        else:\n",
    "            print(f\"cannot prune layer {layer.name}\")\n",
    "            return layer\n",
    "\n",
    "    else:\n",
    "        print(f\"cannot prune layer {layer.name}\")\n",
    "        return layer\n",
    "    \n",
    "def pruneFunctionWrapper(train_data_size, hyperparams):\n",
    "    def wrapper(layer):\n",
    "        return pruneFunction(layer, train_data_size, hyperparams)\n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights = layer.get_weights()[0]\n",
    "            total_params += weights.size\n",
    "            zero_params += np.sum(weights == 0)\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, hyperparams):\n",
    "    input_train_data_combined = data[\"input_train_data_combined\"]\n",
    "    target_train_data_coded = data[\"target_train_data_coded\"]\n",
    "    input_validate_data_combined = data[\"input_validate_data_combined\"]\n",
    "    target_validate_data_coded = data[\"target_validate_data_coded\"]\n",
    "\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    PATIENCE = hyperparams[\"PATIENCE\"]\n",
    "    EPOCHS = hyperparams[\"EPOCHS\"]\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    POST_PRUNE_EPOCHS = hyperparams[\"POST_PRUNE_EPOCHS\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define the model\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        model = qDNNmodel(hyperparams)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        model = qCNNmodel(hyperparams)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported model type\")\n",
    "\n",
    "    model.summary()\n",
    "    print_qmodel_summary(model)\n",
    "    print(f\"Initial Sparsity: {calculate_sparsity(model) * 100:.2f}%\")\n",
    "\n",
    "    train_metrics = {}\n",
    "\n",
    "    # Train the model\n",
    "    earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=PATIENCE, restore_best_weights=True)\n",
    "    print(\"shape 12323 is \", target_train_data_coded.shape, \"data is like\", target_validate_data_coded[1:5])\n",
    "    history = model.fit(\n",
    "        input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "        validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[earlyStop_callback]\n",
    "    )\n",
    "    # Best at this step val_loss 0.7085\n",
    "    train_metrics[\"val_loss\"] = history.history['val_loss'][-1]\n",
    "\n",
    "    # Prune the DNN model \n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        model_pruned = keras.models.clone_model(model, clone_function=pruneFunctionWrapper(input_train_data_combined.shape[0], hyperparams))\n",
    "        model_pruned.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Re-train the pruned model\n",
    "        history = model_pruned.fit(\n",
    "            input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "            validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "            epochs=POST_PRUNE_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks = [pruning_callbacks.UpdatePruningStep()]\n",
    "        ) \n",
    "        # best at this step val_loss 0.4314\n",
    "\n",
    "        model = strip_pruning(model_pruned)\n",
    "        # train_metrics[\"pruned_sparsity\"] = calculate_sparsity(model)\n",
    "\n",
    "    try:\n",
    "        train_metrics[\"pruned_val_loss\"] = history.history['val_loss'][-1]\n",
    "    except:\n",
    "        print(\"Error: no post-pruning val_loss found\")\n",
    "        train_metrics[\"pruned_val_loss\"] = train_metrics[\"val_loss\"]\n",
    "\n",
    "    return model, train_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data, model, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    input_test_data_combined = data[\"input_test_data_combined\"]\n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "\n",
    "    \n",
    "    # Test the model at threshold 0.5\n",
    "    predictions = model.predict(input_test_data_combined)\n",
    "    print(predictions[:10, :])\n",
    "    predictions_prob = predictions[:,0]\n",
    "    predictions_labels = (predictions_prob >= 0.5).astype(int).flatten()\n",
    "\n",
    "    # Test the model at different thresholds\n",
    "    thresholds = np.linspace(0.0, 1.0, 1000)\n",
    "    signal_efficiencies = []\n",
    "    background_rejections = []\n",
    "    max_sum_se = 0\n",
    "    max_sum_br = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # predicted_class = ((predictions_prob[:, 0] + threshold > predictions_prob[:, 1]) & (predictions_prob[:, 0] + threshold > predictions_prob[:, 2])).astype(int)\n",
    "        predicted_class = (predictions_prob > threshold).astype(int)\n",
    "        # Compute confusion matrix\n",
    "        if OUTPUT == \"SINGLE\":\n",
    "            cm = confusion_matrix(target_test_data_coded[:], predicted_class)\n",
    "        else:\n",
    "            cm = confusion_matrix(target_test_data_coded[:, 0], predicted_class)\n",
    "\n",
    "        # Calculate signal efficiency and background rejection\n",
    "        signal_efficiency = cm[1, 1] / np.sum(cm[1, :])\n",
    "        background_rejection = cm[0, 0] / np.sum(cm[0, :])\n",
    "\n",
    "        # Store metrics\n",
    "        signal_efficiencies.append(signal_efficiency)\n",
    "        background_rejections.append(background_rejection)\n",
    "\n",
    "        # get maximum added score\n",
    "        if signal_efficiency + background_rejection > max_sum_se + max_sum_br:\n",
    "            max_sum_se = signal_efficiency\n",
    "            max_sum_br = background_rejection\n",
    "    \n",
    "    test_results = {\n",
    "        \"predictions_prob\": predictions_prob,\n",
    "        \"predictions_labels\": predictions_labels,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"signal_efficiencies\": signal_efficiencies,\n",
    "        \"background_rejections\": background_rejections,\n",
    "        \"max_sum_se\": max_sum_se,\n",
    "        \"max_sum_br\": max_sum_br,\n",
    "    }\n",
    "\n",
    "    return test_results\n",
    "\n",
    "def ShowConfusionMatrix(data, test_results, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "    predictions_labels = test_results[\"predictions_labels\"]\n",
    "\n",
    "    if OUTPUT == \"SINGLE\":\n",
    "        cm = confusion_matrix(target_test_data_coded[:], predictions_labels)\n",
    "    else:\n",
    "        cm = confusion_matrix(target_test_data_coded[:, 0], predictions_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='YlGnBu')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def showMetricsByThreshold(test_results):\n",
    "    thresholds = test_results[\"thresholds\"]\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, signal_efficiencies, label='Signal Efficiency')\n",
    "    plt.plot(thresholds, background_rejections, label='Background Rejection')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Effect of Threshold on Signal Efficiency and Background Rejection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def showEfficiencyVSRejection(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(signal_efficiencies, background_rejections, marker='o')\n",
    "    plt.xlabel('Signal Efficiency')\n",
    "    plt.ylabel('Background Rejection')\n",
    "    plt.title('Background Rejection vs. Signal Efficiency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def find_closest(sorted_array, value):\n",
    "    # Ensure the array is a NumPy array\n",
    "    sorted_array = np.array(sorted_array)\n",
    "    # Compute the absolute difference\n",
    "    abs_diff = np.abs(sorted_array - value)\n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = np.argmin(abs_diff)\n",
    "    return closest_index\n",
    "\n",
    "def getTargetMetrics(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    target_efficiencies = [0.873, 0.90, 0.93, 0.96, 0.98, 0.99, 0.995, 0.999]\n",
    "    metrics = []\n",
    "    for target in target_efficiencies:\n",
    "        index = find_closest(signal_efficiencies, target)\n",
    "        metrics.append((signal_efficiencies[index], background_rejections[index]))\n",
    "        # print(f\"Signal Efficiency: {signal_efficiencies[index]*100:.1f}%,\",f\"Background Rejections: {background_rejections[index]*100:.1f}%\")\n",
    "    return metrics\n",
    "\n",
    "def displayPerformance(data, test_results, metrics, hyperparams):\n",
    "    ShowConfusionMatrix(data, test_results, hyperparams)\n",
    "    showMetricsByThreshold(test_results)\n",
    "    showEfficiencyVSRejection(test_results)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(metrics):\n",
    "    # Convert metrics list of tuples to a formatted string\n",
    "    return \", \".join([f\"({m1:.2f}, {m2:.2f})\" for m1, m2 in metrics])\n",
    "\n",
    "def hyperparameter_search(data, base_hyperparams, param_grid, result_file='hyperparameter_results.json'):\n",
    "    best_metric = 0.0\n",
    "\n",
    "    # Load existing results from file if it exists\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        hyperparams = dict(zip(keys, v))\n",
    "        # Update base hyperparameters with the current set\n",
    "        current_hyperparams = base_hyperparams.copy()\n",
    "        current_hyperparams.update(hyperparams)\n",
    "\n",
    "        # Convert hyperparameters to a string for use as a dictionary key\n",
    "        hyperparams_str = json.dumps(current_hyperparams, sort_keys=True)\n",
    "\n",
    "        # Check if these hyperparameters have been tried before\n",
    "        if hyperparams_str in all_results:\n",
    "            print(f\"Skipping already tested hyperparameters: {current_hyperparams}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Testing hyperparameters: {current_hyperparams}\")\n",
    "\n",
    "        # Train the model\n",
    "        model, train_metrics = train_model(data, current_hyperparams)\n",
    "\n",
    "        # Test the model\n",
    "        test_results = test_model(data, model, current_hyperparams)\n",
    "        metrics = getTargetMetrics(test_results)\n",
    "        \n",
    "        test_scores = {\n",
    "            \"max_sum_se\": test_results[\"max_sum_se\"],\n",
    "            \"max_sum_br\": test_results[\"max_sum_br\"],\n",
    "            \"metrics\": format_metrics(metrics),\n",
    "        }\n",
    "        # Add all keys and values from train_metrics into test_scores\n",
    "        test_scores.update(train_metrics)\n",
    "\n",
    "        # Save the results to the file\n",
    "        all_results[hyperparams_str] = test_scores\n",
    "        with open(result_file, 'w') as file:\n",
    "            json.dump(all_results, file, indent=4)\n",
    "\n",
    "\n",
    "        # If new best found, email alex\n",
    "        if metrics[0][1] > best_metric:\n",
    "            best_metric = metrics[0][1]\n",
    "            print(f\"########### FOUND NEW BEST METRIC {best_metric} ##############\")\n",
    "            model.save(f'./hyperparam_search_recent_best.h5')\n",
    "\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def find_min_pruned_val_loss(result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        print(f\"No results found in {result_file}\")\n",
    "        return None\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    min_hyperparams = None\n",
    "\n",
    "    # Iterate through the results to find the minimum pruned_val_loss\n",
    "    for hyperparams_str, results in all_results.items():\n",
    "        if \"pruned_val_loss\" in results:\n",
    "            pruned_val_loss = results[\"pruned_val_loss\"]\n",
    "            if pruned_val_loss < min_loss:\n",
    "                min_loss = pruned_val_loss\n",
    "                min_hyperparams = hyperparams_str\n",
    "\n",
    "    # Print the hyperparameters with the minimum pruned_val_loss\n",
    "    if min_hyperparams is not None:\n",
    "        print(f\"Hyperparameters with minimum pruned_val_loss: {min_hyperparams}\")\n",
    "        print(f\"Minimum pruned_val_loss: {min_loss}\")\n",
    "    else:\n",
    "        print(\"No entry with pruned_val_loss found\")\n",
    "\n",
    "    return min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found in L2_S24_10_15_results.json\n"
     ]
    }
   ],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    # Model Type\n",
    "    \"MODEL_TYPE\": \"DNN\",  # DNN or CNN\n",
    "    # Input format\n",
    "    \"NUM_TIME_SLICES\": 8,\n",
    "    \"TRAIN_PT_THRESHOLD\": 2,  # in GeV\n",
    "    \"TEST_PT_THRESHOLD\": 2,  # in GeV\n",
    "    # DNN model mormat\n",
    "    \"DNN_LAYERS\": [24, 12],\n",
    "    # CNN model format\n",
    "    \"CONV_LAYER_DEPTHS\": [4, 7],\n",
    "    \"CONV_LAYER_KERNELS\": [(3, 3), (3, 3)],\n",
    "    \"CONV_LAYER_STRIDES\": [(1, 1), (1, 1)],\n",
    "    \"FLATTENED_LAYERS\": [7],\n",
    "    \"MAX_POOLING_SIZE\": (2, 2),\n",
    "    # Output function\n",
    "    \"OUTPUT\": \"SINGLE\", # SOFTMAX or ARGMAX (not working) or LINEAR or SINGLE\n",
    "    # Model quantization\n",
    "    \"WEIGHTS_BITS\": 10,\n",
    "    \"BIAS_BITS\": 10,\n",
    "    \"ACTIVATION_BITS\": 15,\n",
    "    \"INTEGER_BITS\": 2,\n",
    "    # Training\n",
    "    \"LEARNING_RATE\": 0.002,\n",
    "    \"BATCH_SIZE\": 1024,  # Number of samples per gradient update\n",
    "    \"EPOCHS\": 150,  # Number of epochs to train\n",
    "    \"PATIENCE\": 20,  # Stop after this number of epochs without improvement\n",
    "    # Pruning\n",
    "    \"PRUNE_START_EPOCH\": 0,  # Number of epochs before pruning\n",
    "    \"NUM_PRUNE_EPOCHS\": 10,\n",
    "    \"FINAL_SPARSITY\": 0.0,\n",
    "    \"POST_PRUNE_EPOCHS\": 50,\n",
    "}\n",
    "\n",
    "SAVE_FILE = \"L2_S24_10_15_results.json\"\n",
    "\n",
    "param_grid = {\n",
    "    \"FINAL_SPARSITY\": [0.0, 0.15, 0.3], \n",
    "    \"LEARNING_RATE\": [0.002, 0.001, 0.0005],\n",
    "    \"EPOCHS\": [150, 200]\n",
    "}\n",
    "\n",
    "find_min_pruned_val_loss(result_file=(SAVE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 376.23 GB\n",
      "Available memory: 306.88 GB\n",
      "Used memory: 58.85 GB\n",
      "Memory usage percentage: 18.4%\n",
      "Total memory: 376.23 GB\n",
      "Available memory: 295.54 GB\n",
      "Used memory: 70.20 GB\n",
      "Memory usage percentage: 21.4%\n"
     ]
    }
   ],
   "source": [
    "input_data, target_data = load_dataset()\n",
    "data = process_dataset(input_data, target_data, HYPERPARAMETERS) # Depends only on: NUM_TIME_SLICES MODEL_TYPE TRAIN_PT_THRESHOLD TEST_PT_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.002, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 24)                96        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 24)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 12)                48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 12)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_2 is normal keras bn layer\n",
      "q_activation_2       quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_3 is normal keras bn layer\n",
      "q_activation_3       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 1.1908 - accuracy: 0.5003 - val_loss: 0.7064 - val_accuracy: 0.4999\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7012 - accuracy: 0.5019 - val_loss: 0.6997 - val_accuracy: 0.5002\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6975 - accuracy: 0.5034 - val_loss: 0.6969 - val_accuracy: 0.5023\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.5051 - val_loss: 0.6956 - val_accuracy: 0.5020\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6947 - accuracy: 0.5077 - val_loss: 0.6948 - val_accuracy: 0.5054\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5086 - val_loss: 0.6945 - val_accuracy: 0.5067\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5100 - val_loss: 0.6951 - val_accuracy: 0.5007\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6934 - accuracy: 0.5097 - val_loss: 0.6942 - val_accuracy: 0.5071\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5102 - val_loss: 0.6938 - val_accuracy: 0.5083\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.5076\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5120 - val_loss: 0.6938 - val_accuracy: 0.5098\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5134 - val_loss: 0.6935 - val_accuracy: 0.5106\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5123 - val_loss: 0.6936 - val_accuracy: 0.5114\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5145 - val_loss: 0.6934 - val_accuracy: 0.5090\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6945 - val_accuracy: 0.5001\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6932 - val_accuracy: 0.5093\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5155 - val_loss: 0.6931 - val_accuracy: 0.5152\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5168 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5157 - val_loss: 0.6934 - val_accuracy: 0.5071\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5173 - val_loss: 0.6932 - val_accuracy: 0.5132\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5183 - val_loss: 0.6932 - val_accuracy: 0.5154\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5206 - val_loss: 0.6923 - val_accuracy: 0.5235\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5220 - val_loss: 0.6919 - val_accuracy: 0.5238\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5296 - val_loss: 0.6908 - val_accuracy: 0.5239\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5358 - val_loss: 0.6887 - val_accuracy: 0.5315\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5411 - val_loss: 0.6840 - val_accuracy: 0.5463\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5481 - val_loss: 0.6860 - val_accuracy: 0.5359\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5566 - val_loss: 0.6770 - val_accuracy: 0.5649\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6764 - accuracy: 0.5635 - val_loss: 0.6793 - val_accuracy: 0.5554\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6752 - accuracy: 0.5658 - val_loss: 0.6798 - val_accuracy: 0.5592\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5248 - val_loss: 0.6950 - val_accuracy: 0.5009\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6934 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5027\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6929 - val_accuracy: 0.5172\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5215 - val_loss: 0.6912 - val_accuracy: 0.5240\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5336 - val_loss: 0.6900 - val_accuracy: 0.5306\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5480 - val_loss: 0.6805 - val_accuracy: 0.5585\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6803 - accuracy: 0.5564 - val_loss: 0.6969 - val_accuracy: 0.5263\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5563 - val_loss: 0.6862 - val_accuracy: 0.5422\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5629 - val_loss: 0.6733 - val_accuracy: 0.5691\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5636 - val_loss: 0.6743 - val_accuracy: 0.5697\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6701 - accuracy: 0.5734 - val_loss: 0.6716 - val_accuracy: 0.5677\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6726 - accuracy: 0.5676 - val_loss: 0.6656 - val_accuracy: 0.5855\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5386 - val_loss: 0.6949 - val_accuracy: 0.5056\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5091 - val_loss: 0.6928 - val_accuracy: 0.5154\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5237 - val_loss: 0.6906 - val_accuracy: 0.5307\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6777 - accuracy: 0.5570 - val_loss: 0.6992 - val_accuracy: 0.5104\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5208 - val_loss: 0.6931 - val_accuracy: 0.5071\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5151 - val_loss: 0.6921 - val_accuracy: 0.5171\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5319 - val_loss: 0.6900 - val_accuracy: 0.5315\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6741 - accuracy: 0.5651 - val_loss: 0.6686 - val_accuracy: 0.5726\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6674 - accuracy: 0.5772 - val_loss: 0.6641 - val_accuracy: 0.5807\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6643 - accuracy: 0.5819 - val_loss: 0.6715 - val_accuracy: 0.5734\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6766 - accuracy: 0.5613 - val_loss: 0.6892 - val_accuracy: 0.5158\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5604 - val_loss: 0.7061 - val_accuracy: 0.5171\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6807 - accuracy: 0.5533 - val_loss: 0.7050 - val_accuracy: 0.5226\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6714 - accuracy: 0.5696 - val_loss: 0.6630 - val_accuracy: 0.5809\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6610 - accuracy: 0.5899 - val_loss: 0.6536 - val_accuracy: 0.5949\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6652 - accuracy: 0.5845 - val_loss: 0.7082 - val_accuracy: 0.5020\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5058 - val_loss: 0.6927 - val_accuracy: 0.5071\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5115 - val_loss: 0.6925 - val_accuracy: 0.5160\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5162 - val_loss: 0.6915 - val_accuracy: 0.5171\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5317 - val_loss: 0.6860 - val_accuracy: 0.5472\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5615 - val_loss: 0.6728 - val_accuracy: 0.5708\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5375 - val_loss: 0.6939 - val_accuracy: 0.5034\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5056\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5115 - val_loss: 0.6926 - val_accuracy: 0.5093\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5175 - val_loss: 0.6918 - val_accuracy: 0.5182\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5238 - val_loss: 0.6904 - val_accuracy: 0.5219\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5147 - val_loss: 0.6945 - val_accuracy: 0.5030\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5087 - val_loss: 0.6932 - val_accuracy: 0.5045\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6930 - val_accuracy: 0.5069\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5118\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5117 - val_loss: 0.6929 - val_accuracy: 0.5090\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5109 - val_loss: 0.6928 - val_accuracy: 0.5090\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5087\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_2\n",
      "cannot prune layer q_activation_2\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_3\n",
      "cannot prune layer q_activation_3\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 8s 7ms/step - loss: 0.6639 - accuracy: 0.5833 - val_loss: 0.6685 - val_accuracy: 0.5784\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5674 - val_loss: 0.6658 - val_accuracy: 0.5821\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6650 - accuracy: 0.5802 - val_loss: 0.6847 - val_accuracy: 0.5583\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6839 - accuracy: 0.5359 - val_loss: 0.6946 - val_accuracy: 0.5017\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5081 - val_loss: 0.6933 - val_accuracy: 0.5052\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5108 - val_loss: 0.6926 - val_accuracy: 0.5103\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5159 - val_loss: 0.6916 - val_accuracy: 0.5215\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6870 - accuracy: 0.5363 - val_loss: 0.6843 - val_accuracy: 0.5479\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6747 - accuracy: 0.5646 - val_loss: 0.6727 - val_accuracy: 0.5685\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6835 - accuracy: 0.5441 - val_loss: 0.6792 - val_accuracy: 0.5552\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6707 - accuracy: 0.5684 - val_loss: 0.6665 - val_accuracy: 0.5831\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6649 - accuracy: 0.5783 - val_loss: 0.6664 - val_accuracy: 0.5763\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6620 - accuracy: 0.5832 - val_loss: 0.6565 - val_accuracy: 0.5906\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6851 - accuracy: 0.5326 - val_loss: 0.6921 - val_accuracy: 0.5147\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6845 - accuracy: 0.5377 - val_loss: 0.6795 - val_accuracy: 0.5547\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6689 - accuracy: 0.5722 - val_loss: 0.6689 - val_accuracy: 0.5707\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6812 - accuracy: 0.5521 - val_loss: 0.7026 - val_accuracy: 0.5089\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6977 - accuracy: 0.5039 - val_loss: 0.6941 - val_accuracy: 0.5055\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6935 - accuracy: 0.5069 - val_loss: 0.6936 - val_accuracy: 0.5077\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5081 - val_loss: 0.6930 - val_accuracy: 0.5064\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5081 - val_loss: 0.6930 - val_accuracy: 0.5096\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6930 - val_accuracy: 0.5101\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6929 - val_accuracy: 0.5101\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6929 - val_accuracy: 0.5118\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5116 - val_loss: 0.6929 - val_accuracy: 0.5081\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5110 - val_loss: 0.6930 - val_accuracy: 0.5095\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5127 - val_loss: 0.6927 - val_accuracy: 0.5125\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5128 - val_loss: 0.6925 - val_accuracy: 0.5085\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5144\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6918 - val_accuracy: 0.5216\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6883 - accuracy: 0.5330 - val_loss: 0.6861 - val_accuracy: 0.5466\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6811 - accuracy: 0.5531 - val_loss: 0.6785 - val_accuracy: 0.5605\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6882 - accuracy: 0.5302 - val_loss: 0.6910 - val_accuracy: 0.5153\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5269 - val_loss: 0.6870 - val_accuracy: 0.5403\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6788 - accuracy: 0.5593 - val_loss: 0.6774 - val_accuracy: 0.5595\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6756 - accuracy: 0.5610 - val_loss: 0.6752 - val_accuracy: 0.5577\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6670 - accuracy: 0.5755 - val_loss: 0.6634 - val_accuracy: 0.5817\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6655 - accuracy: 0.5794 - val_loss: 0.6626 - val_accuracy: 0.5816\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6621 - accuracy: 0.5818 - val_loss: 0.6635 - val_accuracy: 0.5814\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6590 - accuracy: 0.5883 - val_loss: 0.6679 - val_accuracy: 0.5783\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6714 - accuracy: 0.5646 - val_loss: 0.6938 - val_accuracy: 0.5128\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6900 - accuracy: 0.5221 - val_loss: 0.6889 - val_accuracy: 0.5232\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6803 - accuracy: 0.5514 - val_loss: 0.6792 - val_accuracy: 0.5636\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5718 - val_loss: 0.6664 - val_accuracy: 0.5771\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6652 - accuracy: 0.5782 - val_loss: 0.6645 - val_accuracy: 0.5803\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6564 - accuracy: 0.5942 - val_loss: 0.6524 - val_accuracy: 0.6023\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6606 - accuracy: 0.5868 - val_loss: 0.6646 - val_accuracy: 0.5875\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6852 - accuracy: 0.5324 - val_loss: 0.6927 - val_accuracy: 0.5088\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5181 - val_loss: 0.6908 - val_accuracy: 0.5193\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6881 - accuracy: 0.5322 - val_loss: 0.6893 - val_accuracy: 0.5272\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.49674785]\n",
      " [0.5451554 ]\n",
      " [0.5389303 ]\n",
      " [0.5399257 ]\n",
      " [0.5302012 ]\n",
      " [0.5436375 ]\n",
      " [0.5428493 ]\n",
      " [0.49215496]\n",
      " [0.44253922]\n",
      " [0.5521493 ]]\n",
      "########### FOUND NEW BEST METRIC 0.18306886009608386 ##############\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.002, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 24)                96        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 24)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12)                48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_5 (QActivatio  (None, 12)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_4 is normal keras bn layer\n",
      "q_activation_4       quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_5 is normal keras bn layer\n",
      "q_activation_5       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.8564 - accuracy: 0.4995 - val_loss: 0.7167 - val_accuracy: 0.5007\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7095 - accuracy: 0.5014 - val_loss: 0.7050 - val_accuracy: 0.5030\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7024 - accuracy: 0.5032 - val_loss: 0.7003 - val_accuracy: 0.5052\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5039 - val_loss: 0.6976 - val_accuracy: 0.5052\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6970 - accuracy: 0.5067 - val_loss: 0.6966 - val_accuracy: 0.5045\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6960 - accuracy: 0.5068 - val_loss: 0.6962 - val_accuracy: 0.5050\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5076 - val_loss: 0.6952 - val_accuracy: 0.5043\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6944 - accuracy: 0.5093 - val_loss: 0.6958 - val_accuracy: 0.5066\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6940 - accuracy: 0.5100 - val_loss: 0.6948 - val_accuracy: 0.5045\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5118 - val_loss: 0.6943 - val_accuracy: 0.5081\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5119 - val_loss: 0.6944 - val_accuracy: 0.5107\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5133 - val_loss: 0.6942 - val_accuracy: 0.5048\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5135 - val_loss: 0.6942 - val_accuracy: 0.5117\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5144 - val_loss: 0.6941 - val_accuracy: 0.5060\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5150 - val_loss: 0.6936 - val_accuracy: 0.5048\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5160 - val_loss: 0.6939 - val_accuracy: 0.5102\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5151 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5152 - val_loss: 0.6938 - val_accuracy: 0.5078\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.6933 - val_accuracy: 0.5071\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5186 - val_loss: 0.6933 - val_accuracy: 0.5122\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5193 - val_loss: 0.6927 - val_accuracy: 0.5172\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5217 - val_loss: 0.6928 - val_accuracy: 0.5164\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5233 - val_loss: 0.6929 - val_accuracy: 0.5152\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5275 - val_loss: 0.6938 - val_accuracy: 0.5194\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6896 - accuracy: 0.5298 - val_loss: 0.6916 - val_accuracy: 0.5236\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6887 - accuracy: 0.5345 - val_loss: 0.6885 - val_accuracy: 0.5299\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6866 - accuracy: 0.5407 - val_loss: 0.6896 - val_accuracy: 0.5275\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6849 - accuracy: 0.5459 - val_loss: 0.6838 - val_accuracy: 0.5476\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6814 - accuracy: 0.5536 - val_loss: 0.6815 - val_accuracy: 0.5519\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6782 - accuracy: 0.5611 - val_loss: 0.6849 - val_accuracy: 0.5500\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.5651 - val_loss: 0.6766 - val_accuracy: 0.5663\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6729 - accuracy: 0.5720 - val_loss: 0.6812 - val_accuracy: 0.5574\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5737 - val_loss: 0.6806 - val_accuracy: 0.5568\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6689 - accuracy: 0.5775 - val_loss: 0.6829 - val_accuracy: 0.5552\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6672 - accuracy: 0.5805 - val_loss: 0.6674 - val_accuracy: 0.5768\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6653 - accuracy: 0.5827 - val_loss: 0.6667 - val_accuracy: 0.5819\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6632 - accuracy: 0.5871 - val_loss: 0.6611 - val_accuracy: 0.5948\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5592 - val_loss: 0.6974 - val_accuracy: 0.5140\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5118 - val_loss: 0.6924 - val_accuracy: 0.5146\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6881 - accuracy: 0.5339 - val_loss: 0.7008 - val_accuracy: 0.5048\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6732 - accuracy: 0.5700 - val_loss: 0.6768 - val_accuracy: 0.5691\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6661 - accuracy: 0.5838 - val_loss: 0.6601 - val_accuracy: 0.5978\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6622 - accuracy: 0.5893 - val_loss: 0.6598 - val_accuracy: 0.5933\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6620 - accuracy: 0.5904 - val_loss: 0.6587 - val_accuracy: 0.5986\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6594 - accuracy: 0.5935 - val_loss: 0.6778 - val_accuracy: 0.5740\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6574 - accuracy: 0.5968 - val_loss: 0.6627 - val_accuracy: 0.5848\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6570 - accuracy: 0.5963 - val_loss: 0.6511 - val_accuracy: 0.6047\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6568 - accuracy: 0.5968 - val_loss: 0.6892 - val_accuracy: 0.5563\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6576 - accuracy: 0.5955 - val_loss: 0.6932 - val_accuracy: 0.5667\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6759 - accuracy: 0.5612 - val_loss: 0.6868 - val_accuracy: 0.5422\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6753 - accuracy: 0.5638 - val_loss: 0.6682 - val_accuracy: 0.5761\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5822 - val_loss: 0.6728 - val_accuracy: 0.5698\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6594 - accuracy: 0.5906 - val_loss: 0.6549 - val_accuracy: 0.5960\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6757 - accuracy: 0.5598 - val_loss: 0.6918 - val_accuracy: 0.5173\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6766 - accuracy: 0.5576 - val_loss: 0.6654 - val_accuracy: 0.5850\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6610 - accuracy: 0.5897 - val_loss: 0.6563 - val_accuracy: 0.5919\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5500 - val_loss: 0.6982 - val_accuracy: 0.5035\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5038 - val_loss: 0.6938 - val_accuracy: 0.5059\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5052 - val_loss: 0.6932 - val_accuracy: 0.5077\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5079 - val_loss: 0.6931 - val_accuracy: 0.5079\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5093 - val_loss: 0.6930 - val_accuracy: 0.5055\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5115 - val_loss: 0.6930 - val_accuracy: 0.5089\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5147 - val_loss: 0.6928 - val_accuracy: 0.5128\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5225 - val_loss: 0.6905 - val_accuracy: 0.5277\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6826 - accuracy: 0.5492 - val_loss: 0.7367 - val_accuracy: 0.4971\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5717 - val_loss: 0.6797 - val_accuracy: 0.5573\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6749 - accuracy: 0.5685 - val_loss: 0.6838 - val_accuracy: 0.5339\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_4\n",
      "cannot prune layer q_activation_4\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_5\n",
      "cannot prune layer q_activation_5\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6562 - accuracy: 0.5982 - val_loss: 0.6585 - val_accuracy: 0.5927\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6550 - accuracy: 0.5996 - val_loss: 0.6496 - val_accuracy: 0.6108\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6018 - val_loss: 0.6556 - val_accuracy: 0.6025\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6527 - accuracy: 0.6032 - val_loss: 0.6482 - val_accuracy: 0.6085\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6545 - accuracy: 0.6002 - val_loss: 0.6508 - val_accuracy: 0.6086\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6598 - accuracy: 0.5944 - val_loss: 0.6518 - val_accuracy: 0.6074\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6626 - accuracy: 0.5875 - val_loss: 0.6753 - val_accuracy: 0.5614\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6563 - accuracy: 0.5949 - val_loss: 0.6539 - val_accuracy: 0.6026\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6645 - accuracy: 0.5859 - val_loss: 0.7037 - val_accuracy: 0.5152\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6811 - accuracy: 0.5535 - val_loss: 0.6709 - val_accuracy: 0.5800\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6780 - accuracy: 0.5575 - val_loss: 0.6830 - val_accuracy: 0.5447\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6670 - accuracy: 0.5812 - val_loss: 0.6607 - val_accuracy: 0.5954\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6571 - accuracy: 0.5972 - val_loss: 0.6619 - val_accuracy: 0.5913\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.6051 - val_loss: 0.6710 - val_accuracy: 0.5851\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6499 - accuracy: 0.6076 - val_loss: 0.6469 - val_accuracy: 0.6113\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6494 - accuracy: 0.6075 - val_loss: 0.6671 - val_accuracy: 0.5725\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6487 - accuracy: 0.6083 - val_loss: 0.6489 - val_accuracy: 0.6055\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6481 - accuracy: 0.6100 - val_loss: 0.6438 - val_accuracy: 0.6119\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6472 - accuracy: 0.6104 - val_loss: 0.6616 - val_accuracy: 0.5934\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6457 - accuracy: 0.6121 - val_loss: 0.6426 - val_accuracy: 0.6186\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6450 - accuracy: 0.6129 - val_loss: 0.6466 - val_accuracy: 0.6158\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6445 - accuracy: 0.6131 - val_loss: 0.6384 - val_accuracy: 0.6225\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6443 - accuracy: 0.6151 - val_loss: 0.6563 - val_accuracy: 0.5940\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6448 - accuracy: 0.6132 - val_loss: 0.6429 - val_accuracy: 0.6140\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6439 - accuracy: 0.6134 - val_loss: 0.6406 - val_accuracy: 0.6142\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6422 - accuracy: 0.6159 - val_loss: 0.6611 - val_accuracy: 0.5904\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6454 - accuracy: 0.6122 - val_loss: 0.6583 - val_accuracy: 0.5899\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6640 - accuracy: 0.5779 - val_loss: 0.6973 - val_accuracy: 0.5076\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6819 - accuracy: 0.5446 - val_loss: 0.6808 - val_accuracy: 0.5521\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6532 - accuracy: 0.6043 - val_loss: 0.6582 - val_accuracy: 0.6057\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6491 - accuracy: 0.6087 - val_loss: 0.6371 - val_accuracy: 0.6284\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6424 - accuracy: 0.6170 - val_loss: 0.6520 - val_accuracy: 0.6003\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6444 - accuracy: 0.6147 - val_loss: 0.7349 - val_accuracy: 0.5143\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6497 - accuracy: 0.6069 - val_loss: 0.6452 - val_accuracy: 0.6077\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6441 - accuracy: 0.6134 - val_loss: 0.6471 - val_accuracy: 0.6047\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6950 - accuracy: 0.5260 - val_loss: 0.6975 - val_accuracy: 0.5053\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6950 - accuracy: 0.5095 - val_loss: 0.6938 - val_accuracy: 0.5122\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5141\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6918 - val_accuracy: 0.5198\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6910 - val_accuracy: 0.5257\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5252 - val_loss: 0.6898 - val_accuracy: 0.5307\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6888 - accuracy: 0.5329 - val_loss: 0.6882 - val_accuracy: 0.5355\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6825 - accuracy: 0.5516 - val_loss: 0.6756 - val_accuracy: 0.5732\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6697 - accuracy: 0.5739 - val_loss: 0.6674 - val_accuracy: 0.5796\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6613 - accuracy: 0.5864 - val_loss: 0.6552 - val_accuracy: 0.5937\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6572 - accuracy: 0.5928 - val_loss: 0.6592 - val_accuracy: 0.5862\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.6004 - val_loss: 0.6418 - val_accuracy: 0.6193\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6469 - accuracy: 0.6075 - val_loss: 0.6547 - val_accuracy: 0.6007\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6479 - accuracy: 0.6079 - val_loss: 0.6567 - val_accuracy: 0.5982\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6652 - accuracy: 0.5848 - val_loss: 0.6452 - val_accuracy: 0.6134\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.54780424]\n",
      " [0.33243984]\n",
      " [0.5780505 ]\n",
      " [0.45871645]\n",
      " [0.5290667 ]\n",
      " [0.48191267]\n",
      " [0.58797646]\n",
      " [0.22693619]\n",
      " [0.63463795]\n",
      " [0.35054937]]\n",
      "########### FOUND NEW BEST METRIC 0.3466661813946717 ##############\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 24)                96        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_6 (QActivatio  (None, 24)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 12)                48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_7 (QActivatio  (None, 12)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_6 is normal keras bn layer\n",
      "q_activation_6       quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_7 is normal keras bn layer\n",
      "q_activation_7       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 3.0707 - accuracy: 0.5032 - val_loss: 1.8643 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 1.3399 - accuracy: 0.4998 - val_loss: 1.1815 - val_accuracy: 0.4974\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.8584 - accuracy: 0.4990 - val_loss: 0.7986 - val_accuracy: 0.4984\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7749 - accuracy: 0.5000 - val_loss: 0.7576 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7470 - accuracy: 0.5009 - val_loss: 0.7360 - val_accuracy: 0.5022\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7302 - accuracy: 0.5008 - val_loss: 0.7250 - val_accuracy: 0.5035\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7180 - accuracy: 0.5019 - val_loss: 0.7120 - val_accuracy: 0.5023\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7220 - accuracy: 0.5006 - val_loss: 0.7092 - val_accuracy: 0.5019\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7080 - accuracy: 0.5009 - val_loss: 0.7047 - val_accuracy: 0.5032\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7042 - accuracy: 0.5022 - val_loss: 0.7023 - val_accuracy: 0.5009\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7018 - accuracy: 0.5028 - val_loss: 0.7021 - val_accuracy: 0.5030\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7004 - accuracy: 0.5032 - val_loss: 0.6997 - val_accuracy: 0.5025\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.5042 - val_loss: 0.6986 - val_accuracy: 0.5019\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6979 - accuracy: 0.5059 - val_loss: 0.6977 - val_accuracy: 0.5025\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6971 - accuracy: 0.5057 - val_loss: 0.6971 - val_accuracy: 0.5028\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6964 - accuracy: 0.5056 - val_loss: 0.6965 - val_accuracy: 0.5043\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5067 - val_loss: 0.6962 - val_accuracy: 0.5024\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5071 - val_loss: 0.6959 - val_accuracy: 0.4996\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5086 - val_loss: 0.6954 - val_accuracy: 0.5041\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5088 - val_loss: 0.6951 - val_accuracy: 0.5038\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5076 - val_loss: 0.7014 - val_accuracy: 0.5028\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5074 - val_loss: 0.6946 - val_accuracy: 0.5034\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5085 - val_loss: 0.6944 - val_accuracy: 0.5056\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5101 - val_loss: 0.6939 - val_accuracy: 0.5069\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5102 - val_loss: 0.6938 - val_accuracy: 0.5058\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5110 - val_loss: 0.6940 - val_accuracy: 0.5099\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5111 - val_loss: 0.6938 - val_accuracy: 0.5074\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.5141\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5131 - val_loss: 0.6932 - val_accuracy: 0.5107\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5136 - val_loss: 0.6935 - val_accuracy: 0.5117\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5145 - val_loss: 0.6930 - val_accuracy: 0.5134\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5158 - val_loss: 0.6931 - val_accuracy: 0.5119\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5165 - val_loss: 0.6931 - val_accuracy: 0.5113\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5179 - val_loss: 0.6928 - val_accuracy: 0.5094\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5192 - val_loss: 0.6922 - val_accuracy: 0.5190\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5207 - val_loss: 0.6923 - val_accuracy: 0.5245\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5219 - val_loss: 0.6915 - val_accuracy: 0.5213\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5192\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5266 - val_loss: 0.6904 - val_accuracy: 0.5269\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5303 - val_loss: 0.6904 - val_accuracy: 0.5268\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5345 - val_loss: 0.6922 - val_accuracy: 0.5234\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6866 - accuracy: 0.5377 - val_loss: 0.6880 - val_accuracy: 0.5340\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6857 - accuracy: 0.5407 - val_loss: 0.6871 - val_accuracy: 0.5337\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5435 - val_loss: 0.6856 - val_accuracy: 0.5394\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6831 - accuracy: 0.5467 - val_loss: 0.6843 - val_accuracy: 0.5511\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.5510 - val_loss: 0.6819 - val_accuracy: 0.5539\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.6848 - val_accuracy: 0.5451\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6795 - accuracy: 0.5564 - val_loss: 0.6819 - val_accuracy: 0.5485\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6790 - accuracy: 0.5570 - val_loss: 0.6825 - val_accuracy: 0.5518\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6778 - accuracy: 0.5612 - val_loss: 0.6988 - val_accuracy: 0.5253\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6768 - accuracy: 0.5637 - val_loss: 0.6790 - val_accuracy: 0.5610\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6767 - accuracy: 0.5642 - val_loss: 0.6785 - val_accuracy: 0.5623\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5695 - val_loss: 0.6829 - val_accuracy: 0.5490\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6829 - accuracy: 0.5523 - val_loss: 0.7011 - val_accuracy: 0.5184\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5499 - val_loss: 0.6821 - val_accuracy: 0.5558\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6726 - accuracy: 0.5728 - val_loss: 0.6743 - val_accuracy: 0.5654\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5767 - val_loss: 0.6732 - val_accuracy: 0.5718\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6697 - accuracy: 0.5776 - val_loss: 0.6689 - val_accuracy: 0.5796\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5654 - val_loss: 0.6796 - val_accuracy: 0.5544\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6723 - accuracy: 0.5714 - val_loss: 0.6836 - val_accuracy: 0.5527\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6667 - accuracy: 0.5815 - val_loss: 0.6641 - val_accuracy: 0.5877\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5737 - val_loss: 0.6746 - val_accuracy: 0.5719\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6657 - accuracy: 0.5839 - val_loss: 0.6649 - val_accuracy: 0.5855\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6621 - accuracy: 0.5892 - val_loss: 0.6609 - val_accuracy: 0.5904\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6606 - accuracy: 0.5903 - val_loss: 0.6600 - val_accuracy: 0.5909\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6602 - accuracy: 0.5907 - val_loss: 0.6656 - val_accuracy: 0.5731\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6613 - accuracy: 0.5900 - val_loss: 0.6690 - val_accuracy: 0.5812\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.5949 - val_loss: 0.6631 - val_accuracy: 0.5794\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6565 - accuracy: 0.5950 - val_loss: 0.6581 - val_accuracy: 0.5919\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6548 - accuracy: 0.5986 - val_loss: 0.6509 - val_accuracy: 0.6055\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.5990 - val_loss: 0.6538 - val_accuracy: 0.5926\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6021 - val_loss: 0.6643 - val_accuracy: 0.5884\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6000 - val_loss: 0.6534 - val_accuracy: 0.5930\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6524 - accuracy: 0.6015 - val_loss: 0.6551 - val_accuracy: 0.5973\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6507 - accuracy: 0.6039 - val_loss: 0.6548 - val_accuracy: 0.6036\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6495 - accuracy: 0.6066 - val_loss: 0.6466 - val_accuracy: 0.6122\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6502 - accuracy: 0.6060 - val_loss: 0.6478 - val_accuracy: 0.6107\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6485 - accuracy: 0.6080 - val_loss: 0.6530 - val_accuracy: 0.6081\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6636 - accuracy: 0.5896 - val_loss: 0.6560 - val_accuracy: 0.5984\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6480 - accuracy: 0.6074 - val_loss: 0.6533 - val_accuracy: 0.6043\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6477 - accuracy: 0.6083 - val_loss: 0.6441 - val_accuracy: 0.6132\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6009 - val_loss: 0.6527 - val_accuracy: 0.6004\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6475 - accuracy: 0.6082 - val_loss: 0.6428 - val_accuracy: 0.6143\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6453 - accuracy: 0.6120 - val_loss: 0.6531 - val_accuracy: 0.5927\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6456 - accuracy: 0.6114 - val_loss: 0.6633 - val_accuracy: 0.6000\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6444 - accuracy: 0.6129 - val_loss: 0.6376 - val_accuracy: 0.6201\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6438 - accuracy: 0.6129 - val_loss: 0.6417 - val_accuracy: 0.6157\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6432 - accuracy: 0.6146 - val_loss: 0.6572 - val_accuracy: 0.6053\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6414 - accuracy: 0.6170 - val_loss: 0.6565 - val_accuracy: 0.6125\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6430 - accuracy: 0.6157 - val_loss: 0.6497 - val_accuracy: 0.6077\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6403 - accuracy: 0.6189 - val_loss: 0.6364 - val_accuracy: 0.6201\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6428 - accuracy: 0.6142 - val_loss: 0.6352 - val_accuracy: 0.6233\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6389 - accuracy: 0.6196 - val_loss: 0.6751 - val_accuracy: 0.5887\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6378 - accuracy: 0.6197 - val_loss: 0.6356 - val_accuracy: 0.6244\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6378 - accuracy: 0.6209 - val_loss: 0.6322 - val_accuracy: 0.6311\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6407 - accuracy: 0.6190 - val_loss: 0.8515 - val_accuracy: 0.5006\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.6020 - val_loss: 0.6371 - val_accuracy: 0.6248\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6373 - accuracy: 0.6209 - val_loss: 0.6303 - val_accuracy: 0.6332\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6358 - accuracy: 0.6227 - val_loss: 0.6362 - val_accuracy: 0.6219\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6656 - accuracy: 0.5870 - val_loss: 0.6430 - val_accuracy: 0.6168\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_6\n",
      "cannot prune layer q_activation_6\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_7\n",
      "cannot prune layer q_activation_7\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6407 - accuracy: 0.6169 - val_loss: 0.6507 - val_accuracy: 0.6006\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6380 - accuracy: 0.6202 - val_loss: 0.6327 - val_accuracy: 0.6252\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6266 - val_loss: 0.6322 - val_accuracy: 0.6276\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6323 - accuracy: 0.6263 - val_loss: 0.6265 - val_accuracy: 0.6329\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6316 - accuracy: 0.6269 - val_loss: 0.6522 - val_accuracy: 0.6103\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6446 - accuracy: 0.6140 - val_loss: 0.7225 - val_accuracy: 0.5209\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5709 - val_loss: 0.6587 - val_accuracy: 0.5992\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6397 - accuracy: 0.6180 - val_loss: 0.6315 - val_accuracy: 0.6274\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6323 - accuracy: 0.6264 - val_loss: 0.6252 - val_accuracy: 0.6389\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6312 - accuracy: 0.6274 - val_loss: 0.6241 - val_accuracy: 0.6388\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6310 - accuracy: 0.6275 - val_loss: 0.6340 - val_accuracy: 0.6290\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6295 - val_loss: 0.6239 - val_accuracy: 0.6389\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6297 - accuracy: 0.6289 - val_loss: 0.6244 - val_accuracy: 0.6363\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6600 - accuracy: 0.5903 - val_loss: 0.6543 - val_accuracy: 0.5994\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6420 - accuracy: 0.6144 - val_loss: 0.6342 - val_accuracy: 0.6264\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6374 - accuracy: 0.6201 - val_loss: 0.6402 - val_accuracy: 0.6178\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6337 - accuracy: 0.6249 - val_loss: 0.6298 - val_accuracy: 0.6293\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6402 - accuracy: 0.6179 - val_loss: 0.6305 - val_accuracy: 0.6301\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6309 - accuracy: 0.6283 - val_loss: 0.6417 - val_accuracy: 0.6198\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6290 - accuracy: 0.6299 - val_loss: 0.6242 - val_accuracy: 0.6382\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6280 - accuracy: 0.6308 - val_loss: 0.6213 - val_accuracy: 0.6421\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6635 - accuracy: 0.5805 - val_loss: 0.7056 - val_accuracy: 0.5022\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6979 - accuracy: 0.5033 - val_loss: 0.6960 - val_accuracy: 0.5022\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6953 - accuracy: 0.5049 - val_loss: 0.6948 - val_accuracy: 0.5060\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6944 - accuracy: 0.5062 - val_loss: 0.6942 - val_accuracy: 0.5061\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6938 - accuracy: 0.5085 - val_loss: 0.6938 - val_accuracy: 0.5073\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6934 - accuracy: 0.5100 - val_loss: 0.6934 - val_accuracy: 0.5098\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5124 - val_loss: 0.6933 - val_accuracy: 0.5133\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5128 - val_loss: 0.6933 - val_accuracy: 0.5124\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5154 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5176 - val_loss: 0.6925 - val_accuracy: 0.5171\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5216\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5247 - val_loss: 0.6905 - val_accuracy: 0.5228\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5326 - val_loss: 0.6894 - val_accuracy: 0.5279\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6833 - accuracy: 0.5498 - val_loss: 0.6841 - val_accuracy: 0.5512\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6699 - accuracy: 0.5784 - val_loss: 0.6669 - val_accuracy: 0.5847\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.5995 - val_loss: 0.6485 - val_accuracy: 0.6075\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6478 - accuracy: 0.6085 - val_loss: 0.6722 - val_accuracy: 0.5945\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6451 - accuracy: 0.6126 - val_loss: 0.6334 - val_accuracy: 0.6287\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6406 - accuracy: 0.6188 - val_loss: 0.6500 - val_accuracy: 0.6121\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6390 - accuracy: 0.6194 - val_loss: 0.6545 - val_accuracy: 0.6180\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6345 - accuracy: 0.6244 - val_loss: 0.6306 - val_accuracy: 0.6306\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6314 - accuracy: 0.6281 - val_loss: 0.6517 - val_accuracy: 0.6242\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6351 - accuracy: 0.6243 - val_loss: 0.6280 - val_accuracy: 0.6311\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6314 - accuracy: 0.6270 - val_loss: 0.6219 - val_accuracy: 0.6414\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6365 - accuracy: 0.6216 - val_loss: 0.6321 - val_accuracy: 0.6288\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6285 - val_loss: 0.6376 - val_accuracy: 0.6195\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6290 - accuracy: 0.6301 - val_loss: 0.6199 - val_accuracy: 0.6428\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6298 - accuracy: 0.6286 - val_loss: 0.6324 - val_accuracy: 0.6278\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6356 - accuracy: 0.6236 - val_loss: 0.6366 - val_accuracy: 0.6221\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "[[0.3988336 ]\n",
      " [0.6234314 ]\n",
      " [0.6092339 ]\n",
      " [0.36644515]\n",
      " [0.6229558 ]\n",
      " [0.5189848 ]\n",
      " [0.57746744]\n",
      " [0.4263329 ]\n",
      " [0.56217635]\n",
      " [0.58859956]]\n",
      "########### FOUND NEW BEST METRIC 0.36180666763721064 ##############\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 24)                96        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_8 (QActivatio  (None, 24)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 12)                48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_9 (QActivatio  (None, 12)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_8 is normal keras bn layer\n",
      "q_activation_8       quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_9 is normal keras bn layer\n",
      "q_activation_9       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 1.9973 - accuracy: 0.5016 - val_loss: 0.8726 - val_accuracy: 0.4993\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.8022 - accuracy: 0.4999 - val_loss: 0.7714 - val_accuracy: 0.4992\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7358 - accuracy: 0.5007 - val_loss: 0.7211 - val_accuracy: 0.5019\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7192 - accuracy: 0.5005 - val_loss: 0.7139 - val_accuracy: 0.5006\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7140 - accuracy: 0.5007 - val_loss: 0.7081 - val_accuracy: 0.5007\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7087 - accuracy: 0.5008 - val_loss: 0.7048 - val_accuracy: 0.5009\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7153 - accuracy: 0.5018 - val_loss: 0.7072 - val_accuracy: 0.5058\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7224 - accuracy: 0.5010 - val_loss: 0.7189 - val_accuracy: 0.5045\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7239 - accuracy: 0.5001 - val_loss: 0.7219 - val_accuracy: 0.5044\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7188 - accuracy: 0.4991 - val_loss: 0.7079 - val_accuracy: 0.4989\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7131 - accuracy: 0.5018 - val_loss: 0.7027 - val_accuracy: 0.4977\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7086 - accuracy: 0.5004 - val_loss: 0.7530 - val_accuracy: 0.5031\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7029 - accuracy: 0.5025 - val_loss: 0.7764 - val_accuracy: 0.5042\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6969 - accuracy: 0.5050 - val_loss: 0.6946 - val_accuracy: 0.5061\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5065 - val_loss: 0.6943 - val_accuracy: 0.5069\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5075 - val_loss: 0.6939 - val_accuracy: 0.5089\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5091 - val_loss: 0.6939 - val_accuracy: 0.5057\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5090 - val_loss: 0.6935 - val_accuracy: 0.5085\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5108 - val_loss: 0.6938 - val_accuracy: 0.5082\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5098 - val_loss: 0.6933 - val_accuracy: 0.5077\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5108 - val_loss: 0.6933 - val_accuracy: 0.5056\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5110 - val_loss: 0.6936 - val_accuracy: 0.5075\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6929 - val_accuracy: 0.5125\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5124 - val_loss: 0.6933 - val_accuracy: 0.5118\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5139 - val_loss: 0.6929 - val_accuracy: 0.5154\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6929 - val_accuracy: 0.5113\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5150 - val_loss: 0.6930 - val_accuracy: 0.5139\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5157 - val_loss: 0.6925 - val_accuracy: 0.5133\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5162 - val_loss: 0.6931 - val_accuracy: 0.5121\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5172 - val_loss: 0.6926 - val_accuracy: 0.5130\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5191 - val_loss: 0.6927 - val_accuracy: 0.5106\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5199 - val_loss: 0.6917 - val_accuracy: 0.5211\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5218 - val_loss: 0.6917 - val_accuracy: 0.5181\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5234 - val_loss: 0.6913 - val_accuracy: 0.5236\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5258 - val_loss: 0.6907 - val_accuracy: 0.5241\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5287 - val_loss: 0.6906 - val_accuracy: 0.5280\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5298 - val_loss: 0.6897 - val_accuracy: 0.5315\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5335 - val_loss: 0.6889 - val_accuracy: 0.5275\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5376 - val_loss: 0.6869 - val_accuracy: 0.5387\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5386 - val_loss: 0.6869 - val_accuracy: 0.5393\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5441 - val_loss: 0.6855 - val_accuracy: 0.5397\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5511 - val_loss: 0.6823 - val_accuracy: 0.5537\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6807 - accuracy: 0.5554 - val_loss: 0.6804 - val_accuracy: 0.5506\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5611 - val_loss: 0.6779 - val_accuracy: 0.5610\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6765 - accuracy: 0.5660 - val_loss: 0.6781 - val_accuracy: 0.5501\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6743 - accuracy: 0.5704 - val_loss: 0.6747 - val_accuracy: 0.5680\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5707 - val_loss: 0.6795 - val_accuracy: 0.5748\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5797 - val_loss: 0.6704 - val_accuracy: 0.5768\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6716 - accuracy: 0.5776 - val_loss: 0.6715 - val_accuracy: 0.5799\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6667 - accuracy: 0.5866 - val_loss: 0.6653 - val_accuracy: 0.5915\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6637 - accuracy: 0.5925 - val_loss: 0.6623 - val_accuracy: 0.5960\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6620 - accuracy: 0.5935 - val_loss: 0.6695 - val_accuracy: 0.5684\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5970 - val_loss: 0.6698 - val_accuracy: 0.5724\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.5548 - val_loss: 0.6849 - val_accuracy: 0.5415\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6681 - accuracy: 0.5851 - val_loss: 0.6639 - val_accuracy: 0.5948\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6583 - accuracy: 0.5998 - val_loss: 0.7534 - val_accuracy: 0.5158\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6701 - accuracy: 0.5777 - val_loss: 0.6699 - val_accuracy: 0.5788\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6551 - accuracy: 0.6031 - val_loss: 0.6532 - val_accuracy: 0.6061\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6525 - accuracy: 0.6068 - val_loss: 0.6504 - val_accuracy: 0.6130\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6068 - val_loss: 0.6533 - val_accuracy: 0.6050\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6078 - val_loss: 0.6535 - val_accuracy: 0.6085\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6540 - accuracy: 0.6063 - val_loss: 0.6491 - val_accuracy: 0.6116\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6057 - val_loss: 0.6530 - val_accuracy: 0.6064\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6509 - accuracy: 0.6110 - val_loss: 0.6553 - val_accuracy: 0.6046\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6498 - accuracy: 0.6127 - val_loss: 0.6510 - val_accuracy: 0.6141\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6494 - accuracy: 0.6131 - val_loss: 0.6470 - val_accuracy: 0.6183\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6499 - accuracy: 0.6122 - val_loss: 0.6470 - val_accuracy: 0.6160\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6071 - val_loss: 0.6483 - val_accuracy: 0.6138\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6658 - accuracy: 0.5842 - val_loss: 0.6820 - val_accuracy: 0.5674\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6492 - accuracy: 0.6124 - val_loss: 0.6520 - val_accuracy: 0.6108\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6491 - accuracy: 0.6116 - val_loss: 0.6551 - val_accuracy: 0.5978\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6456 - accuracy: 0.6164 - val_loss: 0.6477 - val_accuracy: 0.6139\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6470 - accuracy: 0.6141 - val_loss: 0.6529 - val_accuracy: 0.6097\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6555 - accuracy: 0.6042 - val_loss: 0.6492 - val_accuracy: 0.6145\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6723 - accuracy: 0.5778 - val_loss: 0.6623 - val_accuracy: 0.6023\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6501 - accuracy: 0.6124 - val_loss: 0.6428 - val_accuracy: 0.6218\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6451 - accuracy: 0.6186 - val_loss: 0.6581 - val_accuracy: 0.6052\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5872 - val_loss: 0.6632 - val_accuracy: 0.6031\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6624 - accuracy: 0.5921 - val_loss: 0.6756 - val_accuracy: 0.5643\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6491 - accuracy: 0.6117 - val_loss: 0.6390 - val_accuracy: 0.6326\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6461 - accuracy: 0.6166 - val_loss: 0.6607 - val_accuracy: 0.6060\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6417 - accuracy: 0.6208 - val_loss: 0.6395 - val_accuracy: 0.6275\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6607 - accuracy: 0.5924 - val_loss: 0.6970 - val_accuracy: 0.5227\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5456 - val_loss: 0.6775 - val_accuracy: 0.5614\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6058 - val_loss: 0.6475 - val_accuracy: 0.6139\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6448 - accuracy: 0.6170 - val_loss: 0.6363 - val_accuracy: 0.6307\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6431 - accuracy: 0.6190 - val_loss: 0.6344 - val_accuracy: 0.6302\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6375 - accuracy: 0.6257 - val_loss: 0.6411 - val_accuracy: 0.6156\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6450 - accuracy: 0.6152 - val_loss: 0.6410 - val_accuracy: 0.6206\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6371 - accuracy: 0.6253 - val_loss: 0.6335 - val_accuracy: 0.6314\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6743 - accuracy: 0.5712 - val_loss: 0.7060 - val_accuracy: 0.5462\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6093 - val_loss: 0.6365 - val_accuracy: 0.6267\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6685 - accuracy: 0.5833 - val_loss: 0.7010 - val_accuracy: 0.5237\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.5526 - val_loss: 0.6739 - val_accuracy: 0.5637\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6037 - val_loss: 0.6534 - val_accuracy: 0.6097\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6411 - accuracy: 0.6211 - val_loss: 0.6440 - val_accuracy: 0.6167\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6356 - accuracy: 0.6270 - val_loss: 0.6554 - val_accuracy: 0.6090\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5649 - val_loss: 0.6909 - val_accuracy: 0.5310\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5489 - val_loss: 0.6775 - val_accuracy: 0.5682\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6664 - accuracy: 0.5857 - val_loss: 0.6550 - val_accuracy: 0.6017\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6540 - accuracy: 0.6041 - val_loss: 0.6445 - val_accuracy: 0.6170\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6434 - accuracy: 0.6173 - val_loss: 0.6435 - val_accuracy: 0.6143\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7111 - accuracy: 0.5036 - val_loss: 0.7048 - val_accuracy: 0.5090\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7018 - accuracy: 0.5044 - val_loss: 0.6983 - val_accuracy: 0.5028\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6998 - accuracy: 0.5073 - val_loss: 0.7013 - val_accuracy: 0.5130\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6981 - accuracy: 0.5102 - val_loss: 0.7021 - val_accuracy: 0.5157\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5107 - val_loss: 0.6950 - val_accuracy: 0.5140\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5116 - val_loss: 0.6935 - val_accuracy: 0.5154\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5150 - val_loss: 0.6934 - val_accuracy: 0.5151\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5181 - val_loss: 0.6971 - val_accuracy: 0.5259\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_8\n",
      "cannot prune layer q_activation_8\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_9\n",
      "cannot prune layer q_activation_9\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6405 - accuracy: 0.6221 - val_loss: 0.6396 - val_accuracy: 0.6277\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6346 - accuracy: 0.6288 - val_loss: 0.6305 - val_accuracy: 0.6367\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6381 - accuracy: 0.6244 - val_loss: 0.6441 - val_accuracy: 0.6187\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6503 - accuracy: 0.6077 - val_loss: 0.6461 - val_accuracy: 0.6174\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6352 - accuracy: 0.6268 - val_loss: 0.6319 - val_accuracy: 0.6332\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6370 - accuracy: 0.6258 - val_loss: 0.6366 - val_accuracy: 0.6261\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6334 - accuracy: 0.6294 - val_loss: 0.6365 - val_accuracy: 0.6273\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.6029 - val_loss: 0.6824 - val_accuracy: 0.5623\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6652 - accuracy: 0.5889 - val_loss: 0.6519 - val_accuracy: 0.6137\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6452 - accuracy: 0.6162 - val_loss: 0.6505 - val_accuracy: 0.6041\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6402 - accuracy: 0.6228 - val_loss: 0.6325 - val_accuracy: 0.6304\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6355 - accuracy: 0.6274 - val_loss: 0.6451 - val_accuracy: 0.6100\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6390 - accuracy: 0.6214 - val_loss: 0.6875 - val_accuracy: 0.5554\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6362 - accuracy: 0.6265 - val_loss: 0.6317 - val_accuracy: 0.6325\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6310 - accuracy: 0.6326 - val_loss: 0.6330 - val_accuracy: 0.6296\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6312 - accuracy: 0.6330 - val_loss: 0.6293 - val_accuracy: 0.6403\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6389 - accuracy: 0.6231 - val_loss: 0.6292 - val_accuracy: 0.6378\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6420 - accuracy: 0.6200 - val_loss: 0.6458 - val_accuracy: 0.6132\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6340 - val_loss: 0.6252 - val_accuracy: 0.6413\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6637 - accuracy: 0.5851 - val_loss: 0.6762 - val_accuracy: 0.5682\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6365 - accuracy: 0.6263 - val_loss: 0.6324 - val_accuracy: 0.6331\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6299 - accuracy: 0.6341 - val_loss: 0.6233 - val_accuracy: 0.6444\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6314 - accuracy: 0.6320 - val_loss: 0.6260 - val_accuracy: 0.6421\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6376 - accuracy: 0.6246 - val_loss: 0.6400 - val_accuracy: 0.6189\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6356 - accuracy: 0.6269 - val_loss: 0.6304 - val_accuracy: 0.6374\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6339 - accuracy: 0.6294 - val_loss: 0.6257 - val_accuracy: 0.6433\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6290 - accuracy: 0.6356 - val_loss: 0.6251 - val_accuracy: 0.6399\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6285 - accuracy: 0.6349 - val_loss: 0.6262 - val_accuracy: 0.6395\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6359 - accuracy: 0.6268 - val_loss: 0.6278 - val_accuracy: 0.6361\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6854 - accuracy: 0.5437 - val_loss: 0.6878 - val_accuracy: 0.5350\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6634 - accuracy: 0.5879 - val_loss: 0.6600 - val_accuracy: 0.5932\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6358 - accuracy: 0.6277 - val_loss: 0.6347 - val_accuracy: 0.6266\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6389 - accuracy: 0.6245 - val_loss: 0.6376 - val_accuracy: 0.6224\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6312 - accuracy: 0.6331 - val_loss: 0.6291 - val_accuracy: 0.6347\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6284 - accuracy: 0.6360 - val_loss: 0.6292 - val_accuracy: 0.6370\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6264 - accuracy: 0.6376 - val_loss: 0.6323 - val_accuracy: 0.6318\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6451 - accuracy: 0.6142 - val_loss: 0.6316 - val_accuracy: 0.6317\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6270 - accuracy: 0.6370 - val_loss: 0.6229 - val_accuracy: 0.6435\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6289 - accuracy: 0.6352 - val_loss: 0.6324 - val_accuracy: 0.6302\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6279 - accuracy: 0.6367 - val_loss: 0.6285 - val_accuracy: 0.6324\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6387 - accuracy: 0.6230 - val_loss: 0.6336 - val_accuracy: 0.6310\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6306 - accuracy: 0.6327 - val_loss: 0.6448 - val_accuracy: 0.6189\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6273 - accuracy: 0.6374 - val_loss: 0.6242 - val_accuracy: 0.6414\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6321 - val_loss: 0.6392 - val_accuracy: 0.6210\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6248 - accuracy: 0.6403 - val_loss: 0.6283 - val_accuracy: 0.6398\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6292 - accuracy: 0.6356 - val_loss: 0.6244 - val_accuracy: 0.6364\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6442 - accuracy: 0.6159 - val_loss: 0.6304 - val_accuracy: 0.6389\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6283 - accuracy: 0.6362 - val_loss: 0.6214 - val_accuracy: 0.6474\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6252 - accuracy: 0.6403 - val_loss: 0.6237 - val_accuracy: 0.6418\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6486 - accuracy: 0.6085 - val_loss: 0.6991 - val_accuracy: 0.5275\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.54579234]\n",
      " [0.4348504 ]\n",
      " [0.5169034 ]\n",
      " [0.5122197 ]\n",
      " [0.33744186]\n",
      " [0.43942976]\n",
      " [0.52986157]\n",
      " [0.24951267]\n",
      " [0.5816762 ]\n",
      " [0.62328565]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_10 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_11 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_10 is normal keras bn layer\n",
      "q_activation_10      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_11 is normal keras bn layer\n",
      "q_activation_11      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 1.4552 - accuracy: 0.4988 - val_loss: 0.9414 - val_accuracy: 0.4993\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.8509 - accuracy: 0.4972 - val_loss: 0.8127 - val_accuracy: 0.5012\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7865 - accuracy: 0.4965 - val_loss: 0.7730 - val_accuracy: 0.5001\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7624 - accuracy: 0.4976 - val_loss: 0.7624 - val_accuracy: 0.5012\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7464 - accuracy: 0.4989 - val_loss: 0.7390 - val_accuracy: 0.5029\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7394 - accuracy: 0.4992 - val_loss: 0.7339 - val_accuracy: 0.5037\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7282 - accuracy: 0.4991 - val_loss: 0.7262 - val_accuracy: 0.5025\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7243 - accuracy: 0.4995 - val_loss: 0.7219 - val_accuracy: 0.5016\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7170 - accuracy: 0.5010 - val_loss: 0.7155 - val_accuracy: 0.5034\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7136 - accuracy: 0.4998 - val_loss: 0.7105 - val_accuracy: 0.5039\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7092 - accuracy: 0.5011 - val_loss: 0.7073 - val_accuracy: 0.5043\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7064 - accuracy: 0.5025 - val_loss: 0.7070 - val_accuracy: 0.5059\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7043 - accuracy: 0.5037 - val_loss: 0.7028 - val_accuracy: 0.5067\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7018 - accuracy: 0.5041 - val_loss: 0.7025 - val_accuracy: 0.5059\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7002 - accuracy: 0.5042 - val_loss: 0.7010 - val_accuracy: 0.5054\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6990 - accuracy: 0.5045 - val_loss: 0.6989 - val_accuracy: 0.5081\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6981 - accuracy: 0.5060 - val_loss: 0.6981 - val_accuracy: 0.5069\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6973 - accuracy: 0.5065 - val_loss: 0.6992 - val_accuracy: 0.5045\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6966 - accuracy: 0.5076 - val_loss: 0.6966 - val_accuracy: 0.5067\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6966 - accuracy: 0.5079 - val_loss: 0.6974 - val_accuracy: 0.5063\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5071 - val_loss: 0.6955 - val_accuracy: 0.5081\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5095 - val_loss: 0.6953 - val_accuracy: 0.5081\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5093 - val_loss: 0.6948 - val_accuracy: 0.5086\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5097 - val_loss: 0.6945 - val_accuracy: 0.5092\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5105 - val_loss: 0.6947 - val_accuracy: 0.5049\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6937 - accuracy: 0.5113 - val_loss: 0.6943 - val_accuracy: 0.5099\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5115 - val_loss: 0.6942 - val_accuracy: 0.5106\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5125 - val_loss: 0.6940 - val_accuracy: 0.5126\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5133 - val_loss: 0.6938 - val_accuracy: 0.5087\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5139 - val_loss: 0.6938 - val_accuracy: 0.5092\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5144 - val_loss: 0.6940 - val_accuracy: 0.5090\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5156 - val_loss: 0.6936 - val_accuracy: 0.5148\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5164 - val_loss: 0.6937 - val_accuracy: 0.5132\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5163 - val_loss: 0.6932 - val_accuracy: 0.5115\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5176 - val_loss: 0.6931 - val_accuracy: 0.5109\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5181 - val_loss: 0.6929 - val_accuracy: 0.5144\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5187 - val_loss: 0.6929 - val_accuracy: 0.5159\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5174 - val_loss: 0.6931 - val_accuracy: 0.5112\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5201 - val_loss: 0.6930 - val_accuracy: 0.5223\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5198 - val_loss: 0.6929 - val_accuracy: 0.5128\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6929 - val_accuracy: 0.5186\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5206 - val_loss: 0.6927 - val_accuracy: 0.5223\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5218 - val_loss: 0.6925 - val_accuracy: 0.5199\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5212 - val_loss: 0.6924 - val_accuracy: 0.5200\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5209 - val_loss: 0.6927 - val_accuracy: 0.5234\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6923 - val_accuracy: 0.5233\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5222 - val_loss: 0.6925 - val_accuracy: 0.5197\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5241 - val_loss: 0.6923 - val_accuracy: 0.5242\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5240 - val_loss: 0.6922 - val_accuracy: 0.5204\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5253 - val_loss: 0.6925 - val_accuracy: 0.5155\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5264 - val_loss: 0.6921 - val_accuracy: 0.5241\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5266 - val_loss: 0.6919 - val_accuracy: 0.5261\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5278 - val_loss: 0.6917 - val_accuracy: 0.5290\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5303 - val_loss: 0.6922 - val_accuracy: 0.5207\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5314 - val_loss: 0.6912 - val_accuracy: 0.5266\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6896 - accuracy: 0.5330 - val_loss: 0.6921 - val_accuracy: 0.5124\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5333 - val_loss: 0.6911 - val_accuracy: 0.5307\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5354 - val_loss: 0.6905 - val_accuracy: 0.5339\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6888 - accuracy: 0.5360 - val_loss: 0.6906 - val_accuracy: 0.5329\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5373 - val_loss: 0.6902 - val_accuracy: 0.5294\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5386 - val_loss: 0.6902 - val_accuracy: 0.5325\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5395 - val_loss: 0.6892 - val_accuracy: 0.5391\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6874 - accuracy: 0.5408 - val_loss: 0.6887 - val_accuracy: 0.5447\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5424 - val_loss: 0.6884 - val_accuracy: 0.5396\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6862 - accuracy: 0.5434 - val_loss: 0.6885 - val_accuracy: 0.5365\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5456 - val_loss: 0.6869 - val_accuracy: 0.5454\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5474 - val_loss: 0.6864 - val_accuracy: 0.5453\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5487 - val_loss: 0.6854 - val_accuracy: 0.5460\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6834 - accuracy: 0.5500 - val_loss: 0.6861 - val_accuracy: 0.5381\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5519 - val_loss: 0.6845 - val_accuracy: 0.5457\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6820 - accuracy: 0.5526 - val_loss: 0.6837 - val_accuracy: 0.5468\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5540 - val_loss: 0.6841 - val_accuracy: 0.5506\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6809 - accuracy: 0.5551 - val_loss: 0.6830 - val_accuracy: 0.5486\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6800 - accuracy: 0.5566 - val_loss: 0.6833 - val_accuracy: 0.5499\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5574 - val_loss: 0.6813 - val_accuracy: 0.5555\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6788 - accuracy: 0.5583 - val_loss: 0.6824 - val_accuracy: 0.5531\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6783 - accuracy: 0.5585 - val_loss: 0.6806 - val_accuracy: 0.5549\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6783 - accuracy: 0.5586 - val_loss: 0.6808 - val_accuracy: 0.5529\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5597 - val_loss: 0.6834 - val_accuracy: 0.5496\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5608 - val_loss: 0.6798 - val_accuracy: 0.5556\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6757 - accuracy: 0.5623 - val_loss: 0.6777 - val_accuracy: 0.5638\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6776 - accuracy: 0.5593 - val_loss: 0.6799 - val_accuracy: 0.5548\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5644 - val_loss: 0.6862 - val_accuracy: 0.5394\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5652 - val_loss: 0.6777 - val_accuracy: 0.5610\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6732 - accuracy: 0.5655 - val_loss: 0.6826 - val_accuracy: 0.5480\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6726 - accuracy: 0.5664 - val_loss: 0.6757 - val_accuracy: 0.5628\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6718 - accuracy: 0.5677 - val_loss: 0.6740 - val_accuracy: 0.5653\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5688 - val_loss: 0.6724 - val_accuracy: 0.5713\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.5653 - val_loss: 0.6743 - val_accuracy: 0.5634\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6712 - accuracy: 0.5686 - val_loss: 0.6748 - val_accuracy: 0.5661\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5686 - val_loss: 0.6714 - val_accuracy: 0.5702\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5678 - val_loss: 0.6733 - val_accuracy: 0.5729\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6700 - accuracy: 0.5711 - val_loss: 0.6904 - val_accuracy: 0.5304\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6700 - accuracy: 0.5696 - val_loss: 0.6696 - val_accuracy: 0.5740\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6691 - accuracy: 0.5725 - val_loss: 0.6702 - val_accuracy: 0.5753\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.5729 - val_loss: 0.6728 - val_accuracy: 0.5726\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6750 - accuracy: 0.5640 - val_loss: 0.6764 - val_accuracy: 0.5528\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6720 - accuracy: 0.5673 - val_loss: 0.6799 - val_accuracy: 0.5661\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5648 - val_loss: 0.6766 - val_accuracy: 0.5707\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_10\n",
      "cannot prune layer q_activation_10\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_11\n",
      "cannot prune layer q_activation_11\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6742 - accuracy: 0.5655 - val_loss: 0.6729 - val_accuracy: 0.5732\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6691 - accuracy: 0.5731 - val_loss: 0.6824 - val_accuracy: 0.5531\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5432 - val_loss: 0.6898 - val_accuracy: 0.5431\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6859 - accuracy: 0.5452 - val_loss: 0.6813 - val_accuracy: 0.5595\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6784 - accuracy: 0.5590 - val_loss: 0.6770 - val_accuracy: 0.5687\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6743 - accuracy: 0.5653 - val_loss: 0.6798 - val_accuracy: 0.5515\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6724 - accuracy: 0.5690 - val_loss: 0.6717 - val_accuracy: 0.5758\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6701 - accuracy: 0.5717 - val_loss: 0.6779 - val_accuracy: 0.5531\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6690 - accuracy: 0.5735 - val_loss: 0.6682 - val_accuracy: 0.5784\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6707 - accuracy: 0.5709 - val_loss: 0.6799 - val_accuracy: 0.5435\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6689 - accuracy: 0.5739 - val_loss: 0.6685 - val_accuracy: 0.5803\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6701 - accuracy: 0.5733 - val_loss: 0.6683 - val_accuracy: 0.5777\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6666 - accuracy: 0.5782 - val_loss: 0.6646 - val_accuracy: 0.5849\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6669 - accuracy: 0.5769 - val_loss: 0.6661 - val_accuracy: 0.5781\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6662 - accuracy: 0.5790 - val_loss: 0.6670 - val_accuracy: 0.5761\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6657 - accuracy: 0.5781 - val_loss: 0.6666 - val_accuracy: 0.5799\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6641 - accuracy: 0.5816 - val_loss: 0.6939 - val_accuracy: 0.5440\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6642 - accuracy: 0.5821 - val_loss: 0.6641 - val_accuracy: 0.5838\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6642 - accuracy: 0.5807 - val_loss: 0.6783 - val_accuracy: 0.5560\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6641 - accuracy: 0.5802 - val_loss: 0.6659 - val_accuracy: 0.5790\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6635 - accuracy: 0.5817 - val_loss: 0.6640 - val_accuracy: 0.5867\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6651 - accuracy: 0.5799 - val_loss: 0.6884 - val_accuracy: 0.5531\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6643 - accuracy: 0.5807 - val_loss: 0.6645 - val_accuracy: 0.5866\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6618 - accuracy: 0.5840 - val_loss: 0.6681 - val_accuracy: 0.5840\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6714 - accuracy: 0.5715 - val_loss: 0.7017 - val_accuracy: 0.5263\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6893 - accuracy: 0.5350 - val_loss: 0.6862 - val_accuracy: 0.5396\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6790 - accuracy: 0.5556 - val_loss: 0.6795 - val_accuracy: 0.5524\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6744 - accuracy: 0.5658 - val_loss: 0.6734 - val_accuracy: 0.5668\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6677 - accuracy: 0.5779 - val_loss: 0.6688 - val_accuracy: 0.5782\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6638 - accuracy: 0.5833 - val_loss: 0.6631 - val_accuracy: 0.5847\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6647 - accuracy: 0.5822 - val_loss: 0.6875 - val_accuracy: 0.5542\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6818 - accuracy: 0.5594 - val_loss: 0.7136 - val_accuracy: 0.5150\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6963 - accuracy: 0.5215 - val_loss: 0.6923 - val_accuracy: 0.5257\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6891 - accuracy: 0.5341 - val_loss: 0.6886 - val_accuracy: 0.5381\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6861 - accuracy: 0.5423 - val_loss: 0.6859 - val_accuracy: 0.5465\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6833 - accuracy: 0.5516 - val_loss: 0.6829 - val_accuracy: 0.5535\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6802 - accuracy: 0.5581 - val_loss: 0.6794 - val_accuracy: 0.5620\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6779 - accuracy: 0.5639 - val_loss: 0.6776 - val_accuracy: 0.5638\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5674 - val_loss: 0.6758 - val_accuracy: 0.5675\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.5710 - val_loss: 0.6761 - val_accuracy: 0.5648\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6711 - accuracy: 0.5743 - val_loss: 0.6883 - val_accuracy: 0.5326\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6692 - accuracy: 0.5766 - val_loss: 0.6675 - val_accuracy: 0.5852\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6713 - accuracy: 0.5740 - val_loss: 0.6820 - val_accuracy: 0.5573\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6658 - accuracy: 0.5817 - val_loss: 0.6613 - val_accuracy: 0.5931\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6622 - accuracy: 0.5864 - val_loss: 0.6605 - val_accuracy: 0.5957\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6601 - accuracy: 0.5910 - val_loss: 0.6591 - val_accuracy: 0.6006\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6602 - accuracy: 0.5907 - val_loss: 0.6623 - val_accuracy: 0.5887\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6583 - accuracy: 0.5927 - val_loss: 0.6588 - val_accuracy: 0.5979\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6572 - accuracy: 0.5954 - val_loss: 0.6611 - val_accuracy: 0.5850\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6563 - accuracy: 0.5969 - val_loss: 0.6569 - val_accuracy: 0.6056\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.27307528]\n",
      " [0.6358222 ]\n",
      " [0.5243123 ]\n",
      " [0.5187469 ]\n",
      " [0.5577334 ]\n",
      " [0.4231012 ]\n",
      " [0.54711086]\n",
      " [0.5295411 ]\n",
      " [0.56466675]\n",
      " [0.4248014 ]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_12 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_13 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_12 is normal keras bn layer\n",
      "q_activation_12      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_13 is normal keras bn layer\n",
      "q_activation_13      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 1.9584 - accuracy: 0.5000 - val_loss: 1.1441 - val_accuracy: 0.5006\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0288 - accuracy: 0.4977 - val_loss: 0.9168 - val_accuracy: 0.4994\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9161 - accuracy: 0.4979 - val_loss: 1.0121 - val_accuracy: 0.4994\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.8543 - accuracy: 0.4983 - val_loss: 0.8380 - val_accuracy: 0.5003\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7848 - accuracy: 0.4997 - val_loss: 0.7659 - val_accuracy: 0.5027\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7679 - accuracy: 0.5002 - val_loss: 0.8133 - val_accuracy: 0.5005\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7570 - accuracy: 0.5004 - val_loss: 0.7455 - val_accuracy: 0.5033\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7439 - accuracy: 0.5001 - val_loss: 0.7372 - val_accuracy: 0.5043\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7344 - accuracy: 0.4998 - val_loss: 0.7280 - val_accuracy: 0.5015\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7277 - accuracy: 0.5001 - val_loss: 0.7242 - val_accuracy: 0.5021\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7229 - accuracy: 0.5003 - val_loss: 0.7194 - val_accuracy: 0.5023\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7189 - accuracy: 0.5008 - val_loss: 0.7162 - val_accuracy: 0.5019\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7152 - accuracy: 0.5015 - val_loss: 0.7129 - val_accuracy: 0.5006\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7123 - accuracy: 0.5023 - val_loss: 0.7106 - val_accuracy: 0.4999\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7093 - accuracy: 0.5031 - val_loss: 0.7080 - val_accuracy: 0.5003\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7067 - accuracy: 0.5028 - val_loss: 0.7058 - val_accuracy: 0.4998\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7047 - accuracy: 0.5033 - val_loss: 0.7038 - val_accuracy: 0.5003\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7029 - accuracy: 0.5042 - val_loss: 0.7025 - val_accuracy: 0.5023\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7012 - accuracy: 0.5044 - val_loss: 0.7011 - val_accuracy: 0.5025\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6998 - accuracy: 0.5054 - val_loss: 0.6996 - val_accuracy: 0.5045\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6986 - accuracy: 0.5054 - val_loss: 0.6988 - val_accuracy: 0.5047\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5066 - val_loss: 0.6977 - val_accuracy: 0.5064\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5064 - val_loss: 0.6973 - val_accuracy: 0.5037\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5076 - val_loss: 0.6959 - val_accuracy: 0.5071\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5076 - val_loss: 0.6972 - val_accuracy: 0.5041\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5092 - val_loss: 0.6951 - val_accuracy: 0.5095\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5095 - val_loss: 0.6948 - val_accuracy: 0.5082\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5108 - val_loss: 0.6942 - val_accuracy: 0.5110\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5113 - val_loss: 0.6941 - val_accuracy: 0.5112\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5128 - val_loss: 0.6938 - val_accuracy: 0.5100\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5136 - val_loss: 0.6935 - val_accuracy: 0.5143\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5138 - val_loss: 0.6935 - val_accuracy: 0.5108\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5156 - val_loss: 0.6932 - val_accuracy: 0.5142\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5149 - val_loss: 0.6933 - val_accuracy: 0.5138\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5162 - val_loss: 0.6930 - val_accuracy: 0.5131\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5164 - val_loss: 0.6932 - val_accuracy: 0.5126\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5171 - val_loss: 0.6933 - val_accuracy: 0.5102\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5159\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.5188 - val_loss: 0.6929 - val_accuracy: 0.5099\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5197 - val_loss: 0.6928 - val_accuracy: 0.5081\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5198 - val_loss: 0.6925 - val_accuracy: 0.5189\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5197 - val_loss: 0.6924 - val_accuracy: 0.5180\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5219 - val_loss: 0.6925 - val_accuracy: 0.5186\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5225 - val_loss: 0.6923 - val_accuracy: 0.5168\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5179\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6933 - val_accuracy: 0.5057\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5246 - val_loss: 0.6922 - val_accuracy: 0.5225\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5265 - val_loss: 0.6923 - val_accuracy: 0.5162\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6900 - accuracy: 0.5276 - val_loss: 0.6913 - val_accuracy: 0.5230\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5302 - val_loss: 0.6915 - val_accuracy: 0.5272\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5320 - val_loss: 0.6906 - val_accuracy: 0.5293\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5353 - val_loss: 0.6899 - val_accuracy: 0.5344\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5386 - val_loss: 0.6899 - val_accuracy: 0.5324\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5406 - val_loss: 0.6893 - val_accuracy: 0.5310\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5445 - val_loss: 0.6879 - val_accuracy: 0.5417\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6851 - accuracy: 0.5471 - val_loss: 0.6878 - val_accuracy: 0.5421\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5491 - val_loss: 0.6861 - val_accuracy: 0.5448\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5522 - val_loss: 0.6841 - val_accuracy: 0.5508\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5556 - val_loss: 0.6828 - val_accuracy: 0.5561\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6803 - accuracy: 0.5583 - val_loss: 0.6813 - val_accuracy: 0.5536\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5612 - val_loss: 0.6833 - val_accuracy: 0.5498\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5636 - val_loss: 0.6776 - val_accuracy: 0.5635\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.5662 - val_loss: 0.6776 - val_accuracy: 0.5627\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6771 - accuracy: 0.5674 - val_loss: 0.6769 - val_accuracy: 0.5644\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6759 - accuracy: 0.5694 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6744 - accuracy: 0.5725 - val_loss: 0.6731 - val_accuracy: 0.5761\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6753 - accuracy: 0.5713 - val_loss: 0.6727 - val_accuracy: 0.5788\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6748 - accuracy: 0.5728 - val_loss: 0.6734 - val_accuracy: 0.5697\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.5730 - val_loss: 0.6763 - val_accuracy: 0.5728\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6730 - accuracy: 0.5762 - val_loss: 0.6711 - val_accuracy: 0.5787\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5762 - val_loss: 0.6764 - val_accuracy: 0.5795\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5784 - val_loss: 0.6682 - val_accuracy: 0.5840\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5801 - val_loss: 0.6673 - val_accuracy: 0.5894\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6710 - accuracy: 0.5786 - val_loss: 0.6696 - val_accuracy: 0.5744\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6682 - accuracy: 0.5830 - val_loss: 0.6692 - val_accuracy: 0.5813\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6652 - accuracy: 0.5861 - val_loss: 0.6671 - val_accuracy: 0.5889\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6659 - accuracy: 0.5858 - val_loss: 0.6723 - val_accuracy: 0.5801\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6755 - accuracy: 0.5709 - val_loss: 0.6739 - val_accuracy: 0.5788\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.5800 - val_loss: 0.6667 - val_accuracy: 0.5915\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6681 - accuracy: 0.5841 - val_loss: 0.6713 - val_accuracy: 0.5830\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6673 - accuracy: 0.5862 - val_loss: 0.6646 - val_accuracy: 0.5943\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6651 - accuracy: 0.5890 - val_loss: 0.6658 - val_accuracy: 0.5797\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6638 - accuracy: 0.5910 - val_loss: 0.6690 - val_accuracy: 0.5841\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6666 - accuracy: 0.5872 - val_loss: 0.6690 - val_accuracy: 0.5797\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6636 - accuracy: 0.5896 - val_loss: 0.6610 - val_accuracy: 0.5988\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6751 - accuracy: 0.5739 - val_loss: 0.6849 - val_accuracy: 0.5521\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5646 - val_loss: 0.6719 - val_accuracy: 0.5726\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6652 - accuracy: 0.5845 - val_loss: 0.6664 - val_accuracy: 0.5895\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6610 - accuracy: 0.5931 - val_loss: 0.6599 - val_accuracy: 0.5960\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.5939 - val_loss: 0.6573 - val_accuracy: 0.6043\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6593 - accuracy: 0.5965 - val_loss: 0.6581 - val_accuracy: 0.5972\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6603 - accuracy: 0.5964 - val_loss: 0.6844 - val_accuracy: 0.5672\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6665 - accuracy: 0.5866 - val_loss: 0.6882 - val_accuracy: 0.5615\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6655 - accuracy: 0.5881 - val_loss: 0.6588 - val_accuracy: 0.5984\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6613 - accuracy: 0.5946 - val_loss: 0.6943 - val_accuracy: 0.5563\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6588 - accuracy: 0.5985 - val_loss: 0.6603 - val_accuracy: 0.6026\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6588 - accuracy: 0.5967 - val_loss: 0.6557 - val_accuracy: 0.6060\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6583 - accuracy: 0.5983 - val_loss: 0.6604 - val_accuracy: 0.5883\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6660 - accuracy: 0.5886 - val_loss: 0.8308 - val_accuracy: 0.5107\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5396 - val_loss: 0.6844 - val_accuracy: 0.5449\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6626 - accuracy: 0.5934 - val_loss: 0.6601 - val_accuracy: 0.5966\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6594 - accuracy: 0.5969 - val_loss: 0.6579 - val_accuracy: 0.6046\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6586 - accuracy: 0.5977 - val_loss: 0.6563 - val_accuracy: 0.6044\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.5978 - val_loss: 0.6569 - val_accuracy: 0.6043\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6568 - accuracy: 0.6007 - val_loss: 0.6713 - val_accuracy: 0.5783\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6762 - accuracy: 0.5697 - val_loss: 0.6936 - val_accuracy: 0.5420\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6815 - accuracy: 0.5605 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6660 - accuracy: 0.5852 - val_loss: 0.6584 - val_accuracy: 0.6032\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6578 - accuracy: 0.5980 - val_loss: 0.6673 - val_accuracy: 0.5775\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6558 - accuracy: 0.6014 - val_loss: 0.6533 - val_accuracy: 0.6094\n",
      "Epoch 111/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6558 - accuracy: 0.6012 - val_loss: 0.6546 - val_accuracy: 0.5977\n",
      "Epoch 112/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6582 - accuracy: 0.5976 - val_loss: 0.6523 - val_accuracy: 0.6140\n",
      "Epoch 113/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6546 - accuracy: 0.6029 - val_loss: 0.6643 - val_accuracy: 0.5833\n",
      "Epoch 114/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6048 - val_loss: 0.6528 - val_accuracy: 0.6017\n",
      "Epoch 115/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.6036 - val_loss: 0.6673 - val_accuracy: 0.5808\n",
      "Epoch 116/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6035 - val_loss: 0.6523 - val_accuracy: 0.6048\n",
      "Epoch 117/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6551 - accuracy: 0.6022 - val_loss: 0.6508 - val_accuracy: 0.6126\n",
      "Epoch 118/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6512 - accuracy: 0.6075 - val_loss: 0.6640 - val_accuracy: 0.5845\n",
      "Epoch 119/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6510 - accuracy: 0.6081 - val_loss: 0.6473 - val_accuracy: 0.6142\n",
      "Epoch 120/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6500 - accuracy: 0.6086 - val_loss: 0.6658 - val_accuracy: 0.5864\n",
      "Epoch 121/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6084 - val_loss: 0.6466 - val_accuracy: 0.6170\n",
      "Epoch 122/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6821 - accuracy: 0.5638 - val_loss: 0.7073 - val_accuracy: 0.5180\n",
      "Epoch 123/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7028 - accuracy: 0.5166 - val_loss: 0.6991 - val_accuracy: 0.5192\n",
      "Epoch 124/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6973 - accuracy: 0.5209 - val_loss: 0.6952 - val_accuracy: 0.5233\n",
      "Epoch 125/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5244 - val_loss: 0.6928 - val_accuracy: 0.5264\n",
      "Epoch 126/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5276 - val_loss: 0.6912 - val_accuracy: 0.5300\n",
      "Epoch 127/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5306 - val_loss: 0.6896 - val_accuracy: 0.5331\n",
      "Epoch 128/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5351 - val_loss: 0.6874 - val_accuracy: 0.5406\n",
      "Epoch 129/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5415 - val_loss: 0.6846 - val_accuracy: 0.5478\n",
      "Epoch 130/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5525 - val_loss: 0.6789 - val_accuracy: 0.5631\n",
      "Epoch 131/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5716 - val_loss: 0.6702 - val_accuracy: 0.5808\n",
      "Epoch 132/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5835 - val_loss: 0.6655 - val_accuracy: 0.5908\n",
      "Epoch 133/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6628 - accuracy: 0.5910 - val_loss: 0.6596 - val_accuracy: 0.5971\n",
      "Epoch 134/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6585 - accuracy: 0.5970 - val_loss: 0.6554 - val_accuracy: 0.6007\n",
      "Epoch 135/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6553 - accuracy: 0.6010 - val_loss: 0.6534 - val_accuracy: 0.6039\n",
      "Epoch 136/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6502 - accuracy: 0.6081 - val_loss: 0.6491 - val_accuracy: 0.6092\n",
      "Epoch 137/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6480 - accuracy: 0.6113 - val_loss: 0.6489 - val_accuracy: 0.6117\n",
      "Epoch 138/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6467 - accuracy: 0.6126 - val_loss: 0.6471 - val_accuracy: 0.6116\n",
      "Epoch 139/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6463 - accuracy: 0.6131 - val_loss: 0.6453 - val_accuracy: 0.6187\n",
      "Epoch 140/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6474 - accuracy: 0.6124 - val_loss: 0.6461 - val_accuracy: 0.6163\n",
      "Epoch 141/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6700 - accuracy: 0.5788 - val_loss: 0.6979 - val_accuracy: 0.5379\n",
      "Epoch 142/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6640 - accuracy: 0.5882 - val_loss: 0.6503 - val_accuracy: 0.6107\n",
      "Epoch 143/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6485 - accuracy: 0.6106 - val_loss: 0.6452 - val_accuracy: 0.6163\n",
      "Epoch 144/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.6074 - val_loss: 0.6524 - val_accuracy: 0.6027\n",
      "Epoch 145/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6491 - accuracy: 0.6100 - val_loss: 0.6460 - val_accuracy: 0.6189\n",
      "Epoch 146/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6445 - accuracy: 0.6158 - val_loss: 0.6432 - val_accuracy: 0.6178\n",
      "Epoch 147/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6458 - accuracy: 0.6131 - val_loss: 0.6440 - val_accuracy: 0.6197\n",
      "Epoch 148/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6426 - accuracy: 0.6181 - val_loss: 0.6456 - val_accuracy: 0.6122\n",
      "Epoch 149/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6428 - accuracy: 0.6176 - val_loss: 0.6395 - val_accuracy: 0.6229\n",
      "Epoch 150/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6429 - accuracy: 0.6178 - val_loss: 0.6421 - val_accuracy: 0.6181\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_12\n",
      "cannot prune layer q_activation_12\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_13\n",
      "cannot prune layer q_activation_13\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6416 - accuracy: 0.6192 - val_loss: 0.6511 - val_accuracy: 0.6077\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6413 - accuracy: 0.6194 - val_loss: 0.6400 - val_accuracy: 0.6236\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6437 - accuracy: 0.6172 - val_loss: 0.6395 - val_accuracy: 0.6231\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6443 - accuracy: 0.6158 - val_loss: 0.6417 - val_accuracy: 0.6217\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6415 - accuracy: 0.6191 - val_loss: 0.6361 - val_accuracy: 0.6282\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6384 - accuracy: 0.6238 - val_loss: 0.6380 - val_accuracy: 0.6257\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6400 - accuracy: 0.6214 - val_loss: 0.6381 - val_accuracy: 0.6246\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6403 - accuracy: 0.6213 - val_loss: 0.6458 - val_accuracy: 0.6144\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6376 - accuracy: 0.6237 - val_loss: 0.6409 - val_accuracy: 0.6186\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6374 - accuracy: 0.6248 - val_loss: 0.6424 - val_accuracy: 0.6214\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6363 - accuracy: 0.6264 - val_loss: 0.6354 - val_accuracy: 0.6290\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6702 - accuracy: 0.5841 - val_loss: 0.6700 - val_accuracy: 0.5756\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6469 - accuracy: 0.6122 - val_loss: 0.6397 - val_accuracy: 0.6213\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6982 - accuracy: 0.5273 - val_loss: 0.6943 - val_accuracy: 0.5203\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6902 - accuracy: 0.5306 - val_loss: 0.6885 - val_accuracy: 0.5359\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6847 - accuracy: 0.5441 - val_loss: 0.6817 - val_accuracy: 0.5568\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6714 - accuracy: 0.5762 - val_loss: 0.6655 - val_accuracy: 0.5894\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6571 - accuracy: 0.6000 - val_loss: 0.6506 - val_accuracy: 0.6116\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6510 - accuracy: 0.6076 - val_loss: 0.6469 - val_accuracy: 0.6140\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6464 - accuracy: 0.6145 - val_loss: 0.6425 - val_accuracy: 0.6209\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6434 - accuracy: 0.6179 - val_loss: 0.6391 - val_accuracy: 0.6264\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6411 - accuracy: 0.6210 - val_loss: 0.6359 - val_accuracy: 0.6299\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6394 - accuracy: 0.6234 - val_loss: 0.6352 - val_accuracy: 0.6293\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6386 - accuracy: 0.6238 - val_loss: 0.6448 - val_accuracy: 0.6185\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6378 - accuracy: 0.6246 - val_loss: 0.6336 - val_accuracy: 0.6305\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6351 - accuracy: 0.6277 - val_loss: 0.6350 - val_accuracy: 0.6292\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6076 - val_loss: 0.6889 - val_accuracy: 0.5610\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6582 - accuracy: 0.6027 - val_loss: 0.6481 - val_accuracy: 0.6195\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6377 - accuracy: 0.6275 - val_loss: 0.6320 - val_accuracy: 0.6316\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6342 - accuracy: 0.6291 - val_loss: 0.6336 - val_accuracy: 0.6274\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6358 - accuracy: 0.6272 - val_loss: 0.6358 - val_accuracy: 0.6231\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6384 - accuracy: 0.6237 - val_loss: 0.6461 - val_accuracy: 0.6153\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6359 - accuracy: 0.6264 - val_loss: 0.6320 - val_accuracy: 0.6317\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6328 - accuracy: 0.6303 - val_loss: 0.6275 - val_accuracy: 0.6371\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6323 - accuracy: 0.6307 - val_loss: 0.6287 - val_accuracy: 0.6331\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6391 - accuracy: 0.6238 - val_loss: 0.6495 - val_accuracy: 0.6108\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6354 - accuracy: 0.6277 - val_loss: 0.6265 - val_accuracy: 0.6386\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6317 - val_loss: 0.6269 - val_accuracy: 0.6361\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6324 - accuracy: 0.6302 - val_loss: 0.6289 - val_accuracy: 0.6339\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6331 - accuracy: 0.6299 - val_loss: 0.6466 - val_accuracy: 0.6170\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6370 - accuracy: 0.6266 - val_loss: 0.6290 - val_accuracy: 0.6389\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6324 - val_loss: 0.6255 - val_accuracy: 0.6391\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6343 - accuracy: 0.6292 - val_loss: 0.6286 - val_accuracy: 0.6364\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6321 - accuracy: 0.6316 - val_loss: 0.6256 - val_accuracy: 0.6400\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6370 - accuracy: 0.6253 - val_loss: 0.6358 - val_accuracy: 0.6286\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6309 - accuracy: 0.6322 - val_loss: 0.6260 - val_accuracy: 0.6417\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6287 - accuracy: 0.6354 - val_loss: 0.6230 - val_accuracy: 0.6414\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6297 - accuracy: 0.6340 - val_loss: 0.6230 - val_accuracy: 0.6430\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6338 - val_loss: 0.6241 - val_accuracy: 0.6418\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6281 - accuracy: 0.6359 - val_loss: 0.6282 - val_accuracy: 0.6366\n",
      "1714/1714 [==============================] - 4s 1ms/step\n",
      "[[0.5646422 ]\n",
      " [0.6856685 ]\n",
      " [0.5818889 ]\n",
      " [0.62254953]\n",
      " [0.5955776 ]\n",
      " [0.45701313]\n",
      " [0.68934625]\n",
      " [0.61444587]\n",
      " [0.6912729 ]\n",
      " [0.5918588 ]]\n",
      "########### FOUND NEW BEST METRIC 0.3889212403552191 ##############\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.002, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.15, 'POST_PRUNE_EPOCHS': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_14 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_15 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_14 is normal keras bn layer\n",
      "q_activation_14      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_15 is normal keras bn layer\n",
      "q_activation_15      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 1.5097 - accuracy: 0.5017 - val_loss: 0.7125 - val_accuracy: 0.5040\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7041 - accuracy: 0.5042 - val_loss: 0.6994 - val_accuracy: 0.5030\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6973 - accuracy: 0.5052 - val_loss: 0.6963 - val_accuracy: 0.5085\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6951 - accuracy: 0.5072 - val_loss: 0.6949 - val_accuracy: 0.5090\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6941 - accuracy: 0.5083 - val_loss: 0.6940 - val_accuracy: 0.5097\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5091 - val_loss: 0.6939 - val_accuracy: 0.5101\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5097 - val_loss: 0.6935 - val_accuracy: 0.5124\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6933 - val_accuracy: 0.5094\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5111 - val_loss: 0.6929 - val_accuracy: 0.5133\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.5134\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5145\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5158 - val_loss: 0.6927 - val_accuracy: 0.5157\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6931 - val_accuracy: 0.5071\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5167 - val_loss: 0.6925 - val_accuracy: 0.5165\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5182 - val_loss: 0.6922 - val_accuracy: 0.5157\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5189 - val_loss: 0.6924 - val_accuracy: 0.5162\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5207 - val_loss: 0.6921 - val_accuracy: 0.5212\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5248 - val_loss: 0.6911 - val_accuracy: 0.5204\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5317 - val_loss: 0.6897 - val_accuracy: 0.5326\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5383 - val_loss: 0.6871 - val_accuracy: 0.5415\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5429 - val_loss: 0.6883 - val_accuracy: 0.5359\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6825 - accuracy: 0.5475 - val_loss: 0.6871 - val_accuracy: 0.5348\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6799 - accuracy: 0.5544 - val_loss: 0.6755 - val_accuracy: 0.5660\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6777 - accuracy: 0.5579 - val_loss: 0.6722 - val_accuracy: 0.5700\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5694 - val_loss: 0.6720 - val_accuracy: 0.5733\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5303 - val_loss: 0.6888 - val_accuracy: 0.5342\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6737 - accuracy: 0.5657 - val_loss: 0.6898 - val_accuracy: 0.5336\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5295 - val_loss: 0.6942 - val_accuracy: 0.5041\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5137 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5194 - val_loss: 0.6915 - val_accuracy: 0.5215\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5244 - val_loss: 0.6918 - val_accuracy: 0.5133\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6896 - accuracy: 0.5282 - val_loss: 0.6911 - val_accuracy: 0.5148\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5343 - val_loss: 0.6879 - val_accuracy: 0.5313\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5464 - val_loss: 0.6846 - val_accuracy: 0.5432\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6800 - accuracy: 0.5572 - val_loss: 0.6797 - val_accuracy: 0.5556\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.5657 - val_loss: 0.6754 - val_accuracy: 0.5668\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5697 - val_loss: 0.6685 - val_accuracy: 0.5790\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.5513 - val_loss: 0.7011 - val_accuracy: 0.5094\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5113 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6923 - val_accuracy: 0.5113\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5184 - val_loss: 0.6915 - val_accuracy: 0.5178\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5217 - val_loss: 0.6902 - val_accuracy: 0.5257\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6862 - accuracy: 0.5392 - val_loss: 0.6859 - val_accuracy: 0.5431\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5369 - val_loss: 0.6948 - val_accuracy: 0.5125\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5222 - val_loss: 0.6895 - val_accuracy: 0.5264\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6829 - accuracy: 0.5481 - val_loss: 0.6888 - val_accuracy: 0.5303\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6725 - accuracy: 0.5682 - val_loss: 0.6695 - val_accuracy: 0.5734\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6824 - accuracy: 0.5422 - val_loss: 0.6884 - val_accuracy: 0.5251\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6777 - accuracy: 0.5556 - val_loss: 0.6942 - val_accuracy: 0.5298\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6707 - accuracy: 0.5722 - val_loss: 0.6830 - val_accuracy: 0.5605\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6661 - accuracy: 0.5787 - val_loss: 0.6658 - val_accuracy: 0.5753\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.5871 - val_loss: 0.6585 - val_accuracy: 0.5925\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6582 - accuracy: 0.5913 - val_loss: 0.6630 - val_accuracy: 0.5813\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6627 - accuracy: 0.5830 - val_loss: 0.7390 - val_accuracy: 0.5145\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6774 - accuracy: 0.5546 - val_loss: 0.6695 - val_accuracy: 0.5753\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6597 - accuracy: 0.5902 - val_loss: 0.7133 - val_accuracy: 0.5292\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5868 - val_loss: 0.6519 - val_accuracy: 0.6004\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6604 - accuracy: 0.5870 - val_loss: 0.6560 - val_accuracy: 0.5991\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.6005 - val_loss: 0.6514 - val_accuracy: 0.6016\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6492 - accuracy: 0.6030 - val_loss: 0.6588 - val_accuracy: 0.5864\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6469 - accuracy: 0.6064 - val_loss: 0.6481 - val_accuracy: 0.6061\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6484 - accuracy: 0.6045 - val_loss: 0.6512 - val_accuracy: 0.6041\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6446 - accuracy: 0.6101 - val_loss: 0.6411 - val_accuracy: 0.6142\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6603 - accuracy: 0.5860 - val_loss: 0.6923 - val_accuracy: 0.5372\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5754 - val_loss: 0.6570 - val_accuracy: 0.5938\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6536 - accuracy: 0.5974 - val_loss: 0.6689 - val_accuracy: 0.5821\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6457 - accuracy: 0.6110 - val_loss: 0.6398 - val_accuracy: 0.6197\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6472 - accuracy: 0.6081 - val_loss: 0.6409 - val_accuracy: 0.6175\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6483 - accuracy: 0.6069 - val_loss: 0.6438 - val_accuracy: 0.6118\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6452 - accuracy: 0.6103 - val_loss: 0.6488 - val_accuracy: 0.6031\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6430 - accuracy: 0.6127 - val_loss: 0.6561 - val_accuracy: 0.6085\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6421 - accuracy: 0.6136 - val_loss: 0.6369 - val_accuracy: 0.6228\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6441 - accuracy: 0.6118 - val_loss: 0.6463 - val_accuracy: 0.6122\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6397 - accuracy: 0.6169 - val_loss: 0.6369 - val_accuracy: 0.6253\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6388 - accuracy: 0.6190 - val_loss: 0.6450 - val_accuracy: 0.6185\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6392 - accuracy: 0.6178 - val_loss: 0.6363 - val_accuracy: 0.6228\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6889 - accuracy: 0.5212 - val_loss: 0.6941 - val_accuracy: 0.5024\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5144 - val_loss: 0.6927 - val_accuracy: 0.5133\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5182 - val_loss: 0.6920 - val_accuracy: 0.5188\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5241 - val_loss: 0.6893 - val_accuracy: 0.5268\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6737 - accuracy: 0.5672 - val_loss: 0.6777 - val_accuracy: 0.5709\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.5997 - val_loss: 0.6574 - val_accuracy: 0.5953\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6498 - accuracy: 0.6041 - val_loss: 0.6606 - val_accuracy: 0.5760\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6431 - accuracy: 0.6120 - val_loss: 0.6578 - val_accuracy: 0.5927\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6424 - accuracy: 0.6140 - val_loss: 0.6467 - val_accuracy: 0.6083\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6423 - accuracy: 0.6144 - val_loss: 0.6402 - val_accuracy: 0.6195\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6582 - accuracy: 0.5899 - val_loss: 0.6591 - val_accuracy: 0.5897\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6423 - accuracy: 0.6137 - val_loss: 0.6453 - val_accuracy: 0.6074\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.5987 - val_loss: 0.6403 - val_accuracy: 0.6180\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6419 - accuracy: 0.6151 - val_loss: 0.6362 - val_accuracy: 0.6192\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6412 - accuracy: 0.6161 - val_loss: 0.6361 - val_accuracy: 0.6241\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6362 - accuracy: 0.6209 - val_loss: 0.6520 - val_accuracy: 0.6094\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6425 - accuracy: 0.6134 - val_loss: 0.6353 - val_accuracy: 0.6228\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6359 - accuracy: 0.6211 - val_loss: 0.6328 - val_accuracy: 0.6296\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6392 - accuracy: 0.6188 - val_loss: 0.6370 - val_accuracy: 0.6231\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6586 - accuracy: 0.5824 - val_loss: 0.6962 - val_accuracy: 0.5035\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5083 - val_loss: 0.6935 - val_accuracy: 0.5088\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_14\n",
      "cannot prune layer q_activation_14\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_15\n",
      "cannot prune layer q_activation_15\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6786 - accuracy: 0.5503 - val_loss: 0.6843 - val_accuracy: 0.5517\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6569 - accuracy: 0.5929 - val_loss: 0.6844 - val_accuracy: 0.5681\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6431 - accuracy: 0.6126 - val_loss: 0.6363 - val_accuracy: 0.6183\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6394 - accuracy: 0.6160 - val_loss: 0.6322 - val_accuracy: 0.6286\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6377 - accuracy: 0.6182 - val_loss: 0.6419 - val_accuracy: 0.6129\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6356 - accuracy: 0.6216 - val_loss: 0.6261 - val_accuracy: 0.6338\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6402 - accuracy: 0.6173 - val_loss: 0.6328 - val_accuracy: 0.6237\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6338 - accuracy: 0.6251 - val_loss: 0.6373 - val_accuracy: 0.6199\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6366 - accuracy: 0.6215 - val_loss: 0.6505 - val_accuracy: 0.5993\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6373 - accuracy: 0.6225 - val_loss: 0.6256 - val_accuracy: 0.6374\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6349 - accuracy: 0.6242 - val_loss: 0.6356 - val_accuracy: 0.6199\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6313 - accuracy: 0.6277 - val_loss: 0.6259 - val_accuracy: 0.6360\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6319 - accuracy: 0.6263 - val_loss: 0.6376 - val_accuracy: 0.6234\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6303 - accuracy: 0.6286 - val_loss: 0.6271 - val_accuracy: 0.6338\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6285 - val_loss: 0.6352 - val_accuracy: 0.6257\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6296 - val_loss: 0.6240 - val_accuracy: 0.6354\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6315 - accuracy: 0.6273 - val_loss: 0.6280 - val_accuracy: 0.6367\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6880 - accuracy: 0.5424 - val_loss: 0.6863 - val_accuracy: 0.5466\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6707 - accuracy: 0.5757 - val_loss: 0.6736 - val_accuracy: 0.5735\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6529 - accuracy: 0.6004 - val_loss: 0.6643 - val_accuracy: 0.5894\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6487 - accuracy: 0.6055 - val_loss: 0.6453 - val_accuracy: 0.6081\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6413 - accuracy: 0.6143 - val_loss: 0.6316 - val_accuracy: 0.6331\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6353 - accuracy: 0.6223 - val_loss: 0.6557 - val_accuracy: 0.6086\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6330 - accuracy: 0.6258 - val_loss: 0.6251 - val_accuracy: 0.6377\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6311 - accuracy: 0.6281 - val_loss: 0.6326 - val_accuracy: 0.6268\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6308 - accuracy: 0.6280 - val_loss: 0.6252 - val_accuracy: 0.6386\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6294 - accuracy: 0.6294 - val_loss: 0.6308 - val_accuracy: 0.6312\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6331 - accuracy: 0.6263 - val_loss: 0.6449 - val_accuracy: 0.6094\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6453 - accuracy: 0.6109 - val_loss: 0.6314 - val_accuracy: 0.6300\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6329 - accuracy: 0.6250 - val_loss: 0.6222 - val_accuracy: 0.6377\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6289 - accuracy: 0.6301 - val_loss: 0.6617 - val_accuracy: 0.5980\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6296 - accuracy: 0.6302 - val_loss: 0.6263 - val_accuracy: 0.6343\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6617 - accuracy: 0.5820 - val_loss: 0.6928 - val_accuracy: 0.5257\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6774 - accuracy: 0.5692 - val_loss: 0.6738 - val_accuracy: 0.5772\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6618 - accuracy: 0.5915 - val_loss: 0.7328 - val_accuracy: 0.5129\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6881 - accuracy: 0.5376 - val_loss: 0.6809 - val_accuracy: 0.5547\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6627 - accuracy: 0.5845 - val_loss: 0.6514 - val_accuracy: 0.6049\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6517 - accuracy: 0.6025 - val_loss: 0.6421 - val_accuracy: 0.6178\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6458 - accuracy: 0.6105 - val_loss: 0.6386 - val_accuracy: 0.6180\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6821 - accuracy: 0.5422 - val_loss: 0.6945 - val_accuracy: 0.5078\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6938 - accuracy: 0.5073 - val_loss: 0.6934 - val_accuracy: 0.5090\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5096 - val_loss: 0.6931 - val_accuracy: 0.5114\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6937 - accuracy: 0.5103 - val_loss: 0.6927 - val_accuracy: 0.5125\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5169 - val_loss: 0.6910 - val_accuracy: 0.5273\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6884 - accuracy: 0.5351 - val_loss: 0.6875 - val_accuracy: 0.5426\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6777 - accuracy: 0.5590 - val_loss: 0.6766 - val_accuracy: 0.5608\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6661 - accuracy: 0.5786 - val_loss: 0.6736 - val_accuracy: 0.5668\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6877 - accuracy: 0.5309 - val_loss: 0.6950 - val_accuracy: 0.5034\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5139 - val_loss: 0.6917 - val_accuracy: 0.5177\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5275 - val_loss: 0.6946 - val_accuracy: 0.4959\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.57860184]\n",
      " [0.4681773 ]\n",
      " [0.50782657]\n",
      " [0.35732043]\n",
      " [0.5037447 ]\n",
      " [0.44601753]\n",
      " [0.5309791 ]\n",
      " [0.407016  ]\n",
      " [0.44573385]\n",
      " [0.42590088]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.002, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.15, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_16 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_17 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_16 is normal keras bn layer\n",
      "q_activation_16      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_17 is normal keras bn layer\n",
      "q_activation_17      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.7413 - accuracy: 0.5034 - val_loss: 0.6978 - val_accuracy: 0.5070\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5070 - val_loss: 0.6949 - val_accuracy: 0.5107\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5098 - val_loss: 0.6945 - val_accuracy: 0.5095\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5113 - val_loss: 0.6939 - val_accuracy: 0.5100\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5120 - val_loss: 0.6933 - val_accuracy: 0.5120\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5120 - val_loss: 0.6934 - val_accuracy: 0.5058\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5116 - val_loss: 0.6927 - val_accuracy: 0.5131\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5138 - val_loss: 0.6927 - val_accuracy: 0.5130\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5158 - val_loss: 0.6925 - val_accuracy: 0.5153\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5162 - val_loss: 0.6926 - val_accuracy: 0.5150\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.5195 - val_loss: 0.6922 - val_accuracy: 0.5185\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5210 - val_loss: 0.6923 - val_accuracy: 0.5113\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5243 - val_loss: 0.6908 - val_accuracy: 0.5258\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5289 - val_loss: 0.6887 - val_accuracy: 0.5324\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6878 - accuracy: 0.5358 - val_loss: 0.6876 - val_accuracy: 0.5333\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5450 - val_loss: 0.6872 - val_accuracy: 0.5352\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6807 - accuracy: 0.5536 - val_loss: 0.6771 - val_accuracy: 0.5638\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5615 - val_loss: 0.6802 - val_accuracy: 0.5544\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6726 - accuracy: 0.5682 - val_loss: 0.6700 - val_accuracy: 0.5755\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6691 - accuracy: 0.5738 - val_loss: 0.6676 - val_accuracy: 0.5776\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6658 - accuracy: 0.5803 - val_loss: 0.6651 - val_accuracy: 0.5788\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6642 - accuracy: 0.5820 - val_loss: 0.6685 - val_accuracy: 0.5741\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6618 - accuracy: 0.5868 - val_loss: 0.6655 - val_accuracy: 0.5776\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6614 - accuracy: 0.5869 - val_loss: 0.6617 - val_accuracy: 0.5864\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.5923 - val_loss: 0.6587 - val_accuracy: 0.5929\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.5938 - val_loss: 0.6489 - val_accuracy: 0.6079\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6575 - accuracy: 0.5963 - val_loss: 0.6526 - val_accuracy: 0.6046\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6583 - accuracy: 0.5970 - val_loss: 0.6643 - val_accuracy: 0.5836\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6568 - accuracy: 0.5991 - val_loss: 0.6500 - val_accuracy: 0.6078\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6566 - accuracy: 0.5992 - val_loss: 0.6488 - val_accuracy: 0.6106\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5230 - val_loss: 0.6937 - val_accuracy: 0.5036\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5132 - val_loss: 0.6929 - val_accuracy: 0.5120\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5166 - val_loss: 0.6921 - val_accuracy: 0.5157\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5214 - val_loss: 0.6914 - val_accuracy: 0.5208\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6898 - accuracy: 0.5308 - val_loss: 0.6892 - val_accuracy: 0.5344\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5470 - val_loss: 0.6909 - val_accuracy: 0.5087\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5666 - val_loss: 0.6768 - val_accuracy: 0.5668\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6649 - accuracy: 0.5819 - val_loss: 0.6660 - val_accuracy: 0.5866\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5870 - val_loss: 0.6642 - val_accuracy: 0.5790\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6611 - accuracy: 0.5877 - val_loss: 0.6599 - val_accuracy: 0.5863\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.5870 - val_loss: 0.6469 - val_accuracy: 0.6059\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6514 - accuracy: 0.5997 - val_loss: 0.6486 - val_accuracy: 0.6009\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6471 - accuracy: 0.6046 - val_loss: 0.6424 - val_accuracy: 0.6151\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6464 - accuracy: 0.6076 - val_loss: 0.6502 - val_accuracy: 0.6067\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6454 - accuracy: 0.6081 - val_loss: 0.6405 - val_accuracy: 0.6166\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6471 - accuracy: 0.6062 - val_loss: 0.6519 - val_accuracy: 0.5982\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6457 - accuracy: 0.6082 - val_loss: 0.6415 - val_accuracy: 0.6184\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6410 - accuracy: 0.6146 - val_loss: 0.6343 - val_accuracy: 0.6278\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6577 - accuracy: 0.5918 - val_loss: 0.7327 - val_accuracy: 0.4982\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5072 - val_loss: 0.6931 - val_accuracy: 0.5132\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6929 - val_accuracy: 0.5111\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5144 - val_loss: 0.6921 - val_accuracy: 0.5142\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5175 - val_loss: 0.6916 - val_accuracy: 0.5191\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5217 - val_loss: 0.6909 - val_accuracy: 0.5239\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6891 - accuracy: 0.5304 - val_loss: 0.6887 - val_accuracy: 0.5319\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6834 - accuracy: 0.5453 - val_loss: 0.6853 - val_accuracy: 0.5301\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6757 - accuracy: 0.5637 - val_loss: 0.6687 - val_accuracy: 0.5756\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5780 - val_loss: 0.6842 - val_accuracy: 0.5482\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6564 - accuracy: 0.5929 - val_loss: 0.6458 - val_accuracy: 0.6066\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6478 - accuracy: 0.6057 - val_loss: 0.6511 - val_accuracy: 0.5950\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6443 - accuracy: 0.6101 - val_loss: 0.6378 - val_accuracy: 0.6206\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6414 - accuracy: 0.6140 - val_loss: 0.6417 - val_accuracy: 0.6093\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6418 - accuracy: 0.6135 - val_loss: 0.6379 - val_accuracy: 0.6163\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6490 - accuracy: 0.6037 - val_loss: 0.6359 - val_accuracy: 0.6222\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6395 - accuracy: 0.6164 - val_loss: 0.6308 - val_accuracy: 0.6257\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6395 - accuracy: 0.6174 - val_loss: 0.6554 - val_accuracy: 0.6037\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6385 - accuracy: 0.6180 - val_loss: 0.6311 - val_accuracy: 0.6277\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6458 - accuracy: 0.6086 - val_loss: 0.7012 - val_accuracy: 0.5083\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5296 - val_loss: 0.6872 - val_accuracy: 0.5397\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.5944 - val_loss: 0.6483 - val_accuracy: 0.6131\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.6076 - val_loss: 0.6680 - val_accuracy: 0.5722\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6821 - accuracy: 0.5533 - val_loss: 0.6825 - val_accuracy: 0.5497\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6634 - accuracy: 0.5866 - val_loss: 0.6665 - val_accuracy: 0.5809\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.5966 - val_loss: 0.6501 - val_accuracy: 0.6052\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6666 - accuracy: 0.5744 - val_loss: 0.6994 - val_accuracy: 0.5002\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6903 - val_accuracy: 0.5173\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6866 - accuracy: 0.5290 - val_loss: 0.6839 - val_accuracy: 0.5354\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6781 - accuracy: 0.5541 - val_loss: 0.6742 - val_accuracy: 0.5683\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5774 - val_loss: 0.6578 - val_accuracy: 0.5920\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6568 - accuracy: 0.5926 - val_loss: 0.6454 - val_accuracy: 0.6086\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.5991 - val_loss: 0.6693 - val_accuracy: 0.5896\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6505 - accuracy: 0.6043 - val_loss: 0.6618 - val_accuracy: 0.5903\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6457 - accuracy: 0.6110 - val_loss: 0.6408 - val_accuracy: 0.6171\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6442 - accuracy: 0.6146 - val_loss: 0.6340 - val_accuracy: 0.6286\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5395 - val_loss: 0.6844 - val_accuracy: 0.5532\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_16\n",
      "cannot prune layer q_activation_16\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_17\n",
      "cannot prune layer q_activation_17\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6388 - accuracy: 0.6187 - val_loss: 0.6367 - val_accuracy: 0.6193\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6361 - accuracy: 0.6218 - val_loss: 0.6340 - val_accuracy: 0.6254\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6362 - accuracy: 0.6217 - val_loss: 0.6314 - val_accuracy: 0.6273\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6347 - accuracy: 0.6236 - val_loss: 0.6433 - val_accuracy: 0.6122\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6349 - accuracy: 0.6236 - val_loss: 0.6316 - val_accuracy: 0.6209\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6637 - accuracy: 0.5744 - val_loss: 0.6949 - val_accuracy: 0.5022\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5203 - val_loss: 0.6906 - val_accuracy: 0.5265\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6836 - accuracy: 0.5481 - val_loss: 0.6760 - val_accuracy: 0.5696\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6809 - accuracy: 0.5529 - val_loss: 0.6891 - val_accuracy: 0.5236\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6784 - accuracy: 0.5598 - val_loss: 0.6721 - val_accuracy: 0.5735\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5841 - val_loss: 0.6639 - val_accuracy: 0.5869\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6601 - accuracy: 0.5948 - val_loss: 0.6733 - val_accuracy: 0.5761\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6636 - accuracy: 0.5938 - val_loss: 0.6612 - val_accuracy: 0.5852\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6721 - accuracy: 0.5730 - val_loss: 0.6719 - val_accuracy: 0.5776\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.5566 - val_loss: 0.6947 - val_accuracy: 0.5055\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6935 - accuracy: 0.5059 - val_loss: 0.6930 - val_accuracy: 0.5069\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6926 - val_accuracy: 0.5072\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6922 - val_accuracy: 0.5178\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6917 - val_accuracy: 0.5159\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5217 - val_loss: 0.6894 - val_accuracy: 0.5287\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6817 - accuracy: 0.5497 - val_loss: 0.6797 - val_accuracy: 0.5556\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6643 - accuracy: 0.5878 - val_loss: 0.6692 - val_accuracy: 0.5758\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6811 - accuracy: 0.5575 - val_loss: 0.6702 - val_accuracy: 0.5773\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.5982 - val_loss: 0.7316 - val_accuracy: 0.5261\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6693 - accuracy: 0.5749 - val_loss: 0.6472 - val_accuracy: 0.6160\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6622 - accuracy: 0.5927 - val_loss: 0.6654 - val_accuracy: 0.5941\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6809 - accuracy: 0.5586 - val_loss: 0.6986 - val_accuracy: 0.5148\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6586 - accuracy: 0.5987 - val_loss: 0.7398 - val_accuracy: 0.5118\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5229 - val_loss: 0.6871 - val_accuracy: 0.5340\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6620 - accuracy: 0.5860 - val_loss: 0.6635 - val_accuracy: 0.5909\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.5965 - val_loss: 0.6538 - val_accuracy: 0.6011\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6662 - accuracy: 0.5812 - val_loss: 0.6965 - val_accuracy: 0.5164\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6881 - accuracy: 0.5308 - val_loss: 0.6849 - val_accuracy: 0.5453\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6789 - accuracy: 0.5578 - val_loss: 0.6984 - val_accuracy: 0.5057\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6938 - accuracy: 0.5119 - val_loss: 0.6926 - val_accuracy: 0.5145\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5213 - val_loss: 0.6905 - val_accuracy: 0.5214\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6879 - accuracy: 0.5362 - val_loss: 0.6884 - val_accuracy: 0.5346\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6780 - accuracy: 0.5599 - val_loss: 0.6791 - val_accuracy: 0.5557\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6692 - accuracy: 0.5753 - val_loss: 0.6682 - val_accuracy: 0.5677\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6717 - accuracy: 0.5709 - val_loss: 0.6614 - val_accuracy: 0.5952\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6589 - accuracy: 0.5938 - val_loss: 0.6634 - val_accuracy: 0.5948\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6531 - accuracy: 0.6020 - val_loss: 0.6448 - val_accuracy: 0.6138\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6939 - accuracy: 0.5195 - val_loss: 0.6919 - val_accuracy: 0.5231\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5297 - val_loss: 0.6891 - val_accuracy: 0.5342\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6759 - accuracy: 0.5623 - val_loss: 0.6717 - val_accuracy: 0.5771\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5646 - val_loss: 0.6939 - val_accuracy: 0.5096\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5149 - val_loss: 0.6911 - val_accuracy: 0.5204\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6783 - accuracy: 0.5543 - val_loss: 0.6844 - val_accuracy: 0.5511\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6610 - accuracy: 0.5913 - val_loss: 0.6550 - val_accuracy: 0.6019\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6565 - accuracy: 0.5972 - val_loss: 0.6521 - val_accuracy: 0.6043\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.52129084]\n",
      " [0.6688272 ]\n",
      " [0.5304559 ]\n",
      " [0.60305125]\n",
      " [0.5601564 ]\n",
      " [0.5109614 ]\n",
      " [0.6675839 ]\n",
      " [0.28996265]\n",
      " [0.6721605 ]\n",
      " [0.4360184 ]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.15, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_18 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_19 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_18 is normal keras bn layer\n",
      "q_activation_18      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_19 is normal keras bn layer\n",
      "q_activation_19      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 3.2540 - accuracy: 0.5018 - val_loss: 1.1143 - val_accuracy: 0.4962\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7801 - accuracy: 0.5008 - val_loss: 0.7196 - val_accuracy: 0.5032\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7114 - accuracy: 0.5026 - val_loss: 0.7057 - val_accuracy: 0.5038\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7038 - accuracy: 0.5036 - val_loss: 0.7014 - val_accuracy: 0.5033\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7005 - accuracy: 0.5034 - val_loss: 0.6993 - val_accuracy: 0.5024\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6987 - accuracy: 0.5042 - val_loss: 0.6980 - val_accuracy: 0.5048\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6975 - accuracy: 0.5049 - val_loss: 0.6970 - val_accuracy: 0.5053\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6966 - accuracy: 0.5044 - val_loss: 0.6962 - val_accuracy: 0.5060\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5043 - val_loss: 0.6958 - val_accuracy: 0.5090\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6952 - accuracy: 0.5059 - val_loss: 0.6950 - val_accuracy: 0.5063\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5070 - val_loss: 0.6945 - val_accuracy: 0.5086\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5077 - val_loss: 0.6941 - val_accuracy: 0.5090\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6937 - accuracy: 0.5092 - val_loss: 0.6938 - val_accuracy: 0.5092\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6934 - accuracy: 0.5100 - val_loss: 0.6937 - val_accuracy: 0.5097\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6936 - val_accuracy: 0.5080\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5101 - val_loss: 0.6936 - val_accuracy: 0.5096\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6933 - val_accuracy: 0.5115\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5127 - val_loss: 0.6931 - val_accuracy: 0.5154\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5101\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5138 - val_loss: 0.6928 - val_accuracy: 0.5133\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6925 - val_accuracy: 0.5133\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5173 - val_loss: 0.6925 - val_accuracy: 0.5160\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5229 - val_loss: 0.6916 - val_accuracy: 0.5264\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5270 - val_loss: 0.6905 - val_accuracy: 0.5317\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6896 - accuracy: 0.5300 - val_loss: 0.6904 - val_accuracy: 0.5303\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5343 - val_loss: 0.6884 - val_accuracy: 0.5354\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6870 - accuracy: 0.5400 - val_loss: 0.6867 - val_accuracy: 0.5418\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6850 - accuracy: 0.5463 - val_loss: 0.6839 - val_accuracy: 0.5523\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.5518 - val_loss: 0.6819 - val_accuracy: 0.5584\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6800 - accuracy: 0.5591 - val_loss: 0.6808 - val_accuracy: 0.5555\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6781 - accuracy: 0.5634 - val_loss: 0.6789 - val_accuracy: 0.5613\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5677 - val_loss: 0.6743 - val_accuracy: 0.5727\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6738 - accuracy: 0.5722 - val_loss: 0.6742 - val_accuracy: 0.5685\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6714 - accuracy: 0.5769 - val_loss: 0.6720 - val_accuracy: 0.5751\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6700 - accuracy: 0.5783 - val_loss: 0.6715 - val_accuracy: 0.5781\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6674 - accuracy: 0.5831 - val_loss: 0.6680 - val_accuracy: 0.5799\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5859 - val_loss: 0.6672 - val_accuracy: 0.5818\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6645 - accuracy: 0.5876 - val_loss: 0.6615 - val_accuracy: 0.5982\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6636 - accuracy: 0.5888 - val_loss: 0.6609 - val_accuracy: 0.5952\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6619 - accuracy: 0.5920 - val_loss: 0.6616 - val_accuracy: 0.5928\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6597 - accuracy: 0.5951 - val_loss: 0.6707 - val_accuracy: 0.5812\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.5971 - val_loss: 0.6722 - val_accuracy: 0.5749\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6560 - accuracy: 0.6003 - val_loss: 0.6587 - val_accuracy: 0.6030\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6554 - accuracy: 0.6017 - val_loss: 0.6594 - val_accuracy: 0.5993\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6550 - accuracy: 0.6008 - val_loss: 0.6563 - val_accuracy: 0.6054\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6617 - accuracy: 0.5947 - val_loss: 0.6608 - val_accuracy: 0.5968\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.6006 - val_loss: 0.6569 - val_accuracy: 0.5980\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6607 - accuracy: 0.5980 - val_loss: 0.6649 - val_accuracy: 0.5873\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6590 - accuracy: 0.5995 - val_loss: 0.6682 - val_accuracy: 0.5953\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6834 - accuracy: 0.5591 - val_loss: 0.6759 - val_accuracy: 0.5671\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6590 - accuracy: 0.5992 - val_loss: 0.6818 - val_accuracy: 0.5948\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6809 - accuracy: 0.5536 - val_loss: 0.6955 - val_accuracy: 0.5162\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5465 - val_loss: 0.6787 - val_accuracy: 0.5653\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6682 - accuracy: 0.5841 - val_loss: 0.6656 - val_accuracy: 0.5852\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6585 - accuracy: 0.6028 - val_loss: 0.6531 - val_accuracy: 0.6064\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6555 - accuracy: 0.6066 - val_loss: 0.7182 - val_accuracy: 0.5443\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6606 - accuracy: 0.5974 - val_loss: 0.6506 - val_accuracy: 0.6151\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6575 - accuracy: 0.6039 - val_loss: 0.6518 - val_accuracy: 0.6116\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6638 - accuracy: 0.5910 - val_loss: 0.6693 - val_accuracy: 0.5802\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.5992 - val_loss: 0.7037 - val_accuracy: 0.5442\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5920 - val_loss: 0.6592 - val_accuracy: 0.5997\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.6023 - val_loss: 0.6609 - val_accuracy: 0.5932\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6644 - accuracy: 0.5915 - val_loss: 0.6586 - val_accuracy: 0.6016\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5924 - val_loss: 0.6655 - val_accuracy: 0.5910\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6586 - accuracy: 0.6011 - val_loss: 0.6536 - val_accuracy: 0.6103\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6557 - accuracy: 0.6047 - val_loss: 0.6495 - val_accuracy: 0.6178\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6498 - accuracy: 0.6150 - val_loss: 0.6485 - val_accuracy: 0.6162\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6523 - accuracy: 0.6119 - val_loss: 0.6576 - val_accuracy: 0.6045\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6494 - accuracy: 0.6136 - val_loss: 0.6475 - val_accuracy: 0.6179\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6455 - accuracy: 0.6176 - val_loss: 0.6464 - val_accuracy: 0.6209\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6455 - accuracy: 0.6178 - val_loss: 0.6446 - val_accuracy: 0.6216\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6444 - accuracy: 0.6192 - val_loss: 0.6537 - val_accuracy: 0.6097\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6456 - accuracy: 0.6165 - val_loss: 0.6485 - val_accuracy: 0.6202\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6556 - accuracy: 0.6010 - val_loss: 0.6953 - val_accuracy: 0.5387\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.5988 - val_loss: 0.6433 - val_accuracy: 0.6215\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6508 - accuracy: 0.6090 - val_loss: 0.6672 - val_accuracy: 0.5854\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6483 - accuracy: 0.6124 - val_loss: 0.6416 - val_accuracy: 0.6230\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6390 - accuracy: 0.6232 - val_loss: 0.6337 - val_accuracy: 0.6310\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6375 - accuracy: 0.6253 - val_loss: 0.6386 - val_accuracy: 0.6253\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6392 - accuracy: 0.6242 - val_loss: 0.6381 - val_accuracy: 0.6264\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6368 - accuracy: 0.6260 - val_loss: 0.6431 - val_accuracy: 0.6300\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6387 - accuracy: 0.6255 - val_loss: 0.6362 - val_accuracy: 0.6282\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6436 - accuracy: 0.6193 - val_loss: 0.6450 - val_accuracy: 0.6187\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6376 - accuracy: 0.6251 - val_loss: 0.6328 - val_accuracy: 0.6321\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6341 - accuracy: 0.6288 - val_loss: 0.6315 - val_accuracy: 0.6366\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6343 - accuracy: 0.6292 - val_loss: 0.6331 - val_accuracy: 0.6332\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6332 - accuracy: 0.6302 - val_loss: 0.6310 - val_accuracy: 0.6322\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6334 - accuracy: 0.6298 - val_loss: 0.6463 - val_accuracy: 0.6117\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6425 - accuracy: 0.6181 - val_loss: 0.7318 - val_accuracy: 0.4949\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5145 - val_loss: 0.6921 - val_accuracy: 0.5222\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5406 - val_loss: 0.6845 - val_accuracy: 0.5459\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.5951 - val_loss: 0.6519 - val_accuracy: 0.6082\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5892 - val_loss: 0.6796 - val_accuracy: 0.5571\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6080 - val_loss: 0.6454 - val_accuracy: 0.6216\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6445 - accuracy: 0.6198 - val_loss: 0.6489 - val_accuracy: 0.6185\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6414 - accuracy: 0.6251 - val_loss: 0.6431 - val_accuracy: 0.6192\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5360 - val_loss: 0.6939 - val_accuracy: 0.5065\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6912 - val_accuracy: 0.5199\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5345 - val_loss: 0.6874 - val_accuracy: 0.5356\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_18\n",
      "cannot prune layer q_activation_18\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_19\n",
      "cannot prune layer q_activation_19\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6638 - accuracy: 0.5889 - val_loss: 0.6604 - val_accuracy: 0.5938\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6490 - accuracy: 0.6147 - val_loss: 0.6438 - val_accuracy: 0.6282\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6474 - accuracy: 0.6149 - val_loss: 0.6697 - val_accuracy: 0.5714\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6451 - accuracy: 0.6177 - val_loss: 0.6467 - val_accuracy: 0.6177\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6401 - accuracy: 0.6241 - val_loss: 0.6398 - val_accuracy: 0.6310\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6374 - accuracy: 0.6278 - val_loss: 0.6385 - val_accuracy: 0.6297\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6349 - accuracy: 0.6313 - val_loss: 0.6501 - val_accuracy: 0.6151\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6369 - accuracy: 0.6293 - val_loss: 0.6317 - val_accuracy: 0.6403\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6390 - accuracy: 0.6272 - val_loss: 0.6333 - val_accuracy: 0.6328\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6471 - accuracy: 0.6160 - val_loss: 0.6367 - val_accuracy: 0.6344\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6501 - accuracy: 0.6080 - val_loss: 0.6917 - val_accuracy: 0.5314\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6782 - accuracy: 0.5591 - val_loss: 0.6962 - val_accuracy: 0.5112\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5297 - val_loss: 0.6876 - val_accuracy: 0.5392\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6731 - accuracy: 0.5720 - val_loss: 0.6653 - val_accuracy: 0.5903\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.6041 - val_loss: 0.7338 - val_accuracy: 0.5356\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6812 - accuracy: 0.5568 - val_loss: 0.6719 - val_accuracy: 0.5674\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6604 - accuracy: 0.5868 - val_loss: 0.6562 - val_accuracy: 0.5951\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6679 - accuracy: 0.5760 - val_loss: 0.6805 - val_accuracy: 0.5489\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6575 - accuracy: 0.5979 - val_loss: 0.6610 - val_accuracy: 0.5885\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6459 - accuracy: 0.6170 - val_loss: 0.6568 - val_accuracy: 0.5978\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6422 - accuracy: 0.6213 - val_loss: 0.6359 - val_accuracy: 0.6272\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6383 - accuracy: 0.6261 - val_loss: 0.6400 - val_accuracy: 0.6314\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6405 - accuracy: 0.6230 - val_loss: 0.6318 - val_accuracy: 0.6301\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6359 - accuracy: 0.6293 - val_loss: 0.6301 - val_accuracy: 0.6360\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6349 - accuracy: 0.6297 - val_loss: 0.6292 - val_accuracy: 0.6323\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6330 - accuracy: 0.6312 - val_loss: 0.6290 - val_accuracy: 0.6338\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6319 - accuracy: 0.6323 - val_loss: 0.6240 - val_accuracy: 0.6413\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6328 - accuracy: 0.6313 - val_loss: 0.6317 - val_accuracy: 0.6316\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6823 - accuracy: 0.5429 - val_loss: 0.6967 - val_accuracy: 0.5060\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5131 - val_loss: 0.6933 - val_accuracy: 0.5123\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6911 - accuracy: 0.5264 - val_loss: 0.6911 - val_accuracy: 0.5266\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6892 - accuracy: 0.5365 - val_loss: 0.6888 - val_accuracy: 0.5404\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6861 - accuracy: 0.5499 - val_loss: 0.6845 - val_accuracy: 0.5530\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.5666 - val_loss: 0.6700 - val_accuracy: 0.5775\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6597 - accuracy: 0.5931 - val_loss: 0.6535 - val_accuracy: 0.6010\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6497 - accuracy: 0.6087 - val_loss: 0.6497 - val_accuracy: 0.6141\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6442 - accuracy: 0.6167 - val_loss: 0.6396 - val_accuracy: 0.6243\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6391 - accuracy: 0.6239 - val_loss: 0.6323 - val_accuracy: 0.6342\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6384 - accuracy: 0.6260 - val_loss: 0.6951 - val_accuracy: 0.5684\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6426 - accuracy: 0.6213 - val_loss: 0.6536 - val_accuracy: 0.6114\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6385 - accuracy: 0.6271 - val_loss: 0.6266 - val_accuracy: 0.6387\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6347 - val_loss: 0.6469 - val_accuracy: 0.6086\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6296 - accuracy: 0.6345 - val_loss: 0.6280 - val_accuracy: 0.6374\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6290 - accuracy: 0.6354 - val_loss: 0.6235 - val_accuracy: 0.6448\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6292 - accuracy: 0.6354 - val_loss: 0.6212 - val_accuracy: 0.6442\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6272 - accuracy: 0.6367 - val_loss: 0.6210 - val_accuracy: 0.6437\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6374 - accuracy: 0.6247 - val_loss: 0.6452 - val_accuracy: 0.6172\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6317 - accuracy: 0.6346 - val_loss: 0.6317 - val_accuracy: 0.6298\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6355 - accuracy: 0.6298 - val_loss: 0.6992 - val_accuracy: 0.5542\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6506 - accuracy: 0.6147 - val_loss: 0.6388 - val_accuracy: 0.6291\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.562911  ]\n",
      " [0.6092394 ]\n",
      " [0.6045996 ]\n",
      " [0.46003988]\n",
      " [0.4284698 ]\n",
      " [0.4491281 ]\n",
      " [0.5178977 ]\n",
      " [0.53560656]\n",
      " [0.5712998 ]\n",
      " [0.34540135]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.15, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_20 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_21 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_20 is normal keras bn layer\n",
      "q_activation_20      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_21 is normal keras bn layer\n",
      "q_activation_21      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 2.6639 - accuracy: 0.5011 - val_loss: 1.4314 - val_accuracy: 0.4994\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.8836 - accuracy: 0.5015 - val_loss: 0.7197 - val_accuracy: 0.5023\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7131 - accuracy: 0.5019 - val_loss: 0.7094 - val_accuracy: 0.5019\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7058 - accuracy: 0.5022 - val_loss: 0.7039 - val_accuracy: 0.5024\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7021 - accuracy: 0.5024 - val_loss: 0.7012 - val_accuracy: 0.5033\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6998 - accuracy: 0.5029 - val_loss: 0.6993 - val_accuracy: 0.5019\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6981 - accuracy: 0.5039 - val_loss: 0.6978 - val_accuracy: 0.5023\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6969 - accuracy: 0.5046 - val_loss: 0.6967 - val_accuracy: 0.5043\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6960 - accuracy: 0.5046 - val_loss: 0.6958 - val_accuracy: 0.5039\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6953 - accuracy: 0.5060 - val_loss: 0.6952 - val_accuracy: 0.5047\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5072 - val_loss: 0.6948 - val_accuracy: 0.5061\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6942 - accuracy: 0.5078 - val_loss: 0.6946 - val_accuracy: 0.5065\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5079 - val_loss: 0.6943 - val_accuracy: 0.5068\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5086 - val_loss: 0.6940 - val_accuracy: 0.5068\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6934 - accuracy: 0.5095 - val_loss: 0.6939 - val_accuracy: 0.5083\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5093 - val_loss: 0.6937 - val_accuracy: 0.5060\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6937 - val_accuracy: 0.5059\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5109 - val_loss: 0.6937 - val_accuracy: 0.5046\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6935 - val_accuracy: 0.5078\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5115 - val_loss: 0.6933 - val_accuracy: 0.5087\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6932 - val_accuracy: 0.5080\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5123 - val_loss: 0.6933 - val_accuracy: 0.5074\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5112\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5139 - val_loss: 0.6931 - val_accuracy: 0.5119\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5147 - val_loss: 0.6928 - val_accuracy: 0.5100\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5150 - val_loss: 0.6928 - val_accuracy: 0.5139\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6928 - val_accuracy: 0.5133\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.7164 - val_accuracy: 0.4981\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5173 - val_loss: 0.6924 - val_accuracy: 0.5135\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5173 - val_loss: 0.6924 - val_accuracy: 0.5162\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5180 - val_loss: 0.6926 - val_accuracy: 0.5123\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5179 - val_loss: 0.7112 - val_accuracy: 0.4980\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5175 - val_loss: 0.6923 - val_accuracy: 0.5160\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5186 - val_loss: 0.6920 - val_accuracy: 0.5198\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5197 - val_loss: 0.6915 - val_accuracy: 0.5231\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5238 - val_loss: 0.6912 - val_accuracy: 0.5221\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5306 - val_loss: 0.6901 - val_accuracy: 0.5243\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6877 - accuracy: 0.5357 - val_loss: 0.6879 - val_accuracy: 0.5360\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5425 - val_loss: 0.6882 - val_accuracy: 0.5373\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5483 - val_loss: 0.6878 - val_accuracy: 0.5358\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6821 - accuracy: 0.5549 - val_loss: 0.6822 - val_accuracy: 0.5560\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5604 - val_loss: 0.6795 - val_accuracy: 0.5662\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6772 - accuracy: 0.5665 - val_loss: 0.6998 - val_accuracy: 0.5317\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6752 - accuracy: 0.5698 - val_loss: 0.6855 - val_accuracy: 0.5424\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6729 - accuracy: 0.5751 - val_loss: 0.6720 - val_accuracy: 0.5799\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6704 - accuracy: 0.5800 - val_loss: 0.6680 - val_accuracy: 0.5870\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6688 - accuracy: 0.5838 - val_loss: 0.6883 - val_accuracy: 0.5778\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6809 - accuracy: 0.5602 - val_loss: 0.6812 - val_accuracy: 0.5547\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6726 - accuracy: 0.5779 - val_loss: 0.6828 - val_accuracy: 0.5522\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6733 - accuracy: 0.5754 - val_loss: 0.6706 - val_accuracy: 0.5725\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6654 - accuracy: 0.5883 - val_loss: 0.6638 - val_accuracy: 0.5957\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6661 - accuracy: 0.5892 - val_loss: 0.6672 - val_accuracy: 0.5915\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6639 - accuracy: 0.5922 - val_loss: 0.6560 - val_accuracy: 0.6024\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5968 - val_loss: 0.6562 - val_accuracy: 0.6006\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6605 - accuracy: 0.5967 - val_loss: 0.6530 - val_accuracy: 0.6088\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6618 - accuracy: 0.5944 - val_loss: 0.6808 - val_accuracy: 0.5568\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6566 - accuracy: 0.6005 - val_loss: 0.6510 - val_accuracy: 0.6086\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6520 - accuracy: 0.6065 - val_loss: 0.6477 - val_accuracy: 0.6131\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6506 - accuracy: 0.6072 - val_loss: 0.6464 - val_accuracy: 0.6127\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.6083 - val_loss: 0.6471 - val_accuracy: 0.6108\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6474 - accuracy: 0.6110 - val_loss: 0.6424 - val_accuracy: 0.6199\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6468 - accuracy: 0.6118 - val_loss: 0.6480 - val_accuracy: 0.6116\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6463 - accuracy: 0.6123 - val_loss: 0.6435 - val_accuracy: 0.6134\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6717 - accuracy: 0.5759 - val_loss: 0.6888 - val_accuracy: 0.5199\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6014 - val_loss: 0.6458 - val_accuracy: 0.6125\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6452 - accuracy: 0.6135 - val_loss: 0.6446 - val_accuracy: 0.6174\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6433 - accuracy: 0.6155 - val_loss: 0.6421 - val_accuracy: 0.6181\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6419 - accuracy: 0.6177 - val_loss: 0.6365 - val_accuracy: 0.6266\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6426 - accuracy: 0.6169 - val_loss: 0.6524 - val_accuracy: 0.6046\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6410 - accuracy: 0.6189 - val_loss: 0.6359 - val_accuracy: 0.6264\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6429 - accuracy: 0.6160 - val_loss: 0.6378 - val_accuracy: 0.6191\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6415 - accuracy: 0.6186 - val_loss: 0.6402 - val_accuracy: 0.6199\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6393 - accuracy: 0.6215 - val_loss: 0.6632 - val_accuracy: 0.5911\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6385 - accuracy: 0.6226 - val_loss: 0.6342 - val_accuracy: 0.6282\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6388 - accuracy: 0.6219 - val_loss: 0.6417 - val_accuracy: 0.6185\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6396 - accuracy: 0.6229 - val_loss: 0.6413 - val_accuracy: 0.6198\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6379 - accuracy: 0.6226 - val_loss: 0.6330 - val_accuracy: 0.6272\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6387 - accuracy: 0.6225 - val_loss: 0.6376 - val_accuracy: 0.6249\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6355 - accuracy: 0.6271 - val_loss: 0.6369 - val_accuracy: 0.6257\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6470 - accuracy: 0.6137 - val_loss: 0.6439 - val_accuracy: 0.6177\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6617 - accuracy: 0.5946 - val_loss: 0.6623 - val_accuracy: 0.5907\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6399 - accuracy: 0.6210 - val_loss: 0.6412 - val_accuracy: 0.6170\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6516 - accuracy: 0.6074 - val_loss: 0.6426 - val_accuracy: 0.6211\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6344 - accuracy: 0.6282 - val_loss: 0.6420 - val_accuracy: 0.6182\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6337 - accuracy: 0.6287 - val_loss: 0.6447 - val_accuracy: 0.6170\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6773 - accuracy: 0.5716 - val_loss: 0.6721 - val_accuracy: 0.5741\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6416 - accuracy: 0.6186 - val_loss: 0.6447 - val_accuracy: 0.6101\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6341 - accuracy: 0.6274 - val_loss: 0.6300 - val_accuracy: 0.6353\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6347 - accuracy: 0.6279 - val_loss: 0.6454 - val_accuracy: 0.6202\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6361 - accuracy: 0.6270 - val_loss: 0.6410 - val_accuracy: 0.6221\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6349 - accuracy: 0.6275 - val_loss: 0.6313 - val_accuracy: 0.6333\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6317 - accuracy: 0.6304 - val_loss: 0.6369 - val_accuracy: 0.6286\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6303 - accuracy: 0.6330 - val_loss: 0.6270 - val_accuracy: 0.6347\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6312 - accuracy: 0.6308 - val_loss: 0.6286 - val_accuracy: 0.6360\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6320 - accuracy: 0.6311 - val_loss: 0.6391 - val_accuracy: 0.6203\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6843 - accuracy: 0.5641 - val_loss: 0.7046 - val_accuracy: 0.5039\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6959 - accuracy: 0.5153 - val_loss: 0.6940 - val_accuracy: 0.5209\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6900 - accuracy: 0.5298 - val_loss: 0.6899 - val_accuracy: 0.5302\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5442 - val_loss: 0.6831 - val_accuracy: 0.5490\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6805 - accuracy: 0.5535 - val_loss: 0.6782 - val_accuracy: 0.5616\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5659 - val_loss: 0.6747 - val_accuracy: 0.5656\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5729 - val_loss: 0.6730 - val_accuracy: 0.5779\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6658 - accuracy: 0.5848 - val_loss: 0.6603 - val_accuracy: 0.5915\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6575 - accuracy: 0.5972 - val_loss: 0.6469 - val_accuracy: 0.6094\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6468 - accuracy: 0.6101 - val_loss: 0.6421 - val_accuracy: 0.6138\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6534 - accuracy: 0.6018 - val_loss: 0.6900 - val_accuracy: 0.5501\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6630 - accuracy: 0.5854 - val_loss: 0.6451 - val_accuracy: 0.6137\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6427 - accuracy: 0.6157 - val_loss: 0.6375 - val_accuracy: 0.6274\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6389 - accuracy: 0.6205 - val_loss: 0.6333 - val_accuracy: 0.6305\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6980 - accuracy: 0.5209 - val_loss: 0.6948 - val_accuracy: 0.4946\n",
      "Epoch 111/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.5666 - val_loss: 0.6655 - val_accuracy: 0.5914\n",
      "Epoch 112/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6463 - accuracy: 0.6114 - val_loss: 0.6371 - val_accuracy: 0.6268\n",
      "Epoch 113/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6378 - accuracy: 0.6221 - val_loss: 0.6373 - val_accuracy: 0.6260\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_20\n",
      "cannot prune layer q_activation_20\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_21\n",
      "cannot prune layer q_activation_21\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6340 - accuracy: 0.6290 - val_loss: 0.6356 - val_accuracy: 0.6223\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6330 - accuracy: 0.6295 - val_loss: 0.6428 - val_accuracy: 0.6208\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6296 - accuracy: 0.6339 - val_loss: 0.6357 - val_accuracy: 0.6259\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6316 - accuracy: 0.6318 - val_loss: 0.6273 - val_accuracy: 0.6384\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6289 - accuracy: 0.6354 - val_loss: 0.6323 - val_accuracy: 0.6300\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6313 - val_loss: 0.6256 - val_accuracy: 0.6411\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6293 - accuracy: 0.6344 - val_loss: 0.6271 - val_accuracy: 0.6383\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6324 - accuracy: 0.6301 - val_loss: 0.6248 - val_accuracy: 0.6380\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6404 - accuracy: 0.6230 - val_loss: 0.6278 - val_accuracy: 0.6355\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6294 - accuracy: 0.6344 - val_loss: 0.6397 - val_accuracy: 0.6253\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.6368 - val_loss: 0.6230 - val_accuracy: 0.6425\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6481 - accuracy: 0.6162 - val_loss: 0.6604 - val_accuracy: 0.6022\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6351 - accuracy: 0.6299 - val_loss: 0.6283 - val_accuracy: 0.6371\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6287 - accuracy: 0.6359 - val_loss: 0.6210 - val_accuracy: 0.6439\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6962 - accuracy: 0.5176 - val_loss: 0.6928 - val_accuracy: 0.5117\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5239 - val_loss: 0.6874 - val_accuracy: 0.5332\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.5517 - val_loss: 0.6796 - val_accuracy: 0.5624\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6631 - accuracy: 0.5988 - val_loss: 0.6516 - val_accuracy: 0.6162\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6393 - accuracy: 0.6276 - val_loss: 0.6310 - val_accuracy: 0.6359\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6562 - accuracy: 0.6037 - val_loss: 0.6485 - val_accuracy: 0.6147\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6403 - accuracy: 0.6205 - val_loss: 0.6310 - val_accuracy: 0.6352\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6306 - accuracy: 0.6328 - val_loss: 0.6247 - val_accuracy: 0.6419\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6352 - accuracy: 0.6277 - val_loss: 0.6255 - val_accuracy: 0.6405\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6277 - accuracy: 0.6356 - val_loss: 0.6214 - val_accuracy: 0.6467\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6274 - accuracy: 0.6361 - val_loss: 0.6238 - val_accuracy: 0.6434\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6303 - accuracy: 0.6329 - val_loss: 0.6277 - val_accuracy: 0.6338\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6273 - accuracy: 0.6357 - val_loss: 0.6228 - val_accuracy: 0.6418\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6423 - accuracy: 0.6193 - val_loss: 0.6606 - val_accuracy: 0.5904\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6317 - accuracy: 0.6283 - val_loss: 0.6218 - val_accuracy: 0.6473\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6366 - accuracy: 0.6292 - val_loss: 0.6444 - val_accuracy: 0.6143\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6266 - accuracy: 0.6382 - val_loss: 0.6254 - val_accuracy: 0.6464\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6413 - accuracy: 0.6224 - val_loss: 0.6420 - val_accuracy: 0.6213\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6279 - accuracy: 0.6370 - val_loss: 0.6238 - val_accuracy: 0.6410\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6240 - accuracy: 0.6407 - val_loss: 0.6288 - val_accuracy: 0.6398\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6244 - accuracy: 0.6401 - val_loss: 0.6240 - val_accuracy: 0.6398\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6252 - accuracy: 0.6394 - val_loss: 0.6301 - val_accuracy: 0.6360\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6240 - accuracy: 0.6404 - val_loss: 0.6217 - val_accuracy: 0.6432\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6651 - accuracy: 0.5881 - val_loss: 0.6941 - val_accuracy: 0.5397\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6786 - accuracy: 0.5671 - val_loss: 0.6711 - val_accuracy: 0.5841\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6522 - accuracy: 0.6110 - val_loss: 0.6336 - val_accuracy: 0.6312\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6446 - accuracy: 0.6189 - val_loss: 0.6368 - val_accuracy: 0.6254\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6325 - accuracy: 0.6310 - val_loss: 0.6664 - val_accuracy: 0.5861\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6302 - accuracy: 0.6340 - val_loss: 0.6234 - val_accuracy: 0.6414\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6283 - accuracy: 0.6364 - val_loss: 0.6472 - val_accuracy: 0.6157\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6287 - accuracy: 0.6356 - val_loss: 0.6293 - val_accuracy: 0.6376\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6499 - accuracy: 0.6112 - val_loss: 0.6419 - val_accuracy: 0.6182\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6319 - accuracy: 0.6310 - val_loss: 0.6232 - val_accuracy: 0.6429\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6264 - accuracy: 0.6384 - val_loss: 0.6231 - val_accuracy: 0.6461\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5817 - val_loss: 0.6701 - val_accuracy: 0.5780\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6431 - accuracy: 0.6182 - val_loss: 0.6316 - val_accuracy: 0.6353\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.509837  ]\n",
      " [0.74998677]\n",
      " [0.6697436 ]\n",
      " [0.573895  ]\n",
      " [0.5419041 ]\n",
      " [0.559422  ]\n",
      " [0.68276083]\n",
      " [0.6199392 ]\n",
      " [0.68034345]\n",
      " [0.27082103]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.15, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_22 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_23 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_22 is normal keras bn layer\n",
      "q_activation_22      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_23 is normal keras bn layer\n",
      "q_activation_23      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 2.1997 - accuracy: 0.5024 - val_loss: 1.0680 - val_accuracy: 0.5024\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.8694 - accuracy: 0.5018 - val_loss: 0.8014 - val_accuracy: 0.5020\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7787 - accuracy: 0.5018 - val_loss: 0.7570 - val_accuracy: 0.5024\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7505 - accuracy: 0.5026 - val_loss: 0.7696 - val_accuracy: 0.5013\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7391 - accuracy: 0.5020 - val_loss: 0.7322 - val_accuracy: 0.5015\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7256 - accuracy: 0.5028 - val_loss: 0.7216 - val_accuracy: 0.4997\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7168 - accuracy: 0.5024 - val_loss: 0.7143 - val_accuracy: 0.4992\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7128 - accuracy: 0.5025 - val_loss: 0.7131 - val_accuracy: 0.4999\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7086 - accuracy: 0.5034 - val_loss: 0.7070 - val_accuracy: 0.5022\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7052 - accuracy: 0.5035 - val_loss: 0.7039 - val_accuracy: 0.5015\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7030 - accuracy: 0.5043 - val_loss: 0.7023 - val_accuracy: 0.5034\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7012 - accuracy: 0.5043 - val_loss: 0.7003 - val_accuracy: 0.5038\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6994 - accuracy: 0.5041 - val_loss: 0.6993 - val_accuracy: 0.5027\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6981 - accuracy: 0.5049 - val_loss: 0.6984 - val_accuracy: 0.5043\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6978 - accuracy: 0.5066 - val_loss: 0.6980 - val_accuracy: 0.5051\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6969 - accuracy: 0.5064 - val_loss: 0.6971 - val_accuracy: 0.5036\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5056 - val_loss: 0.6975 - val_accuracy: 0.5056\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6975 - accuracy: 0.5063 - val_loss: 0.6961 - val_accuracy: 0.5064\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6990 - accuracy: 0.5072 - val_loss: 0.6970 - val_accuracy: 0.5071\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7008 - accuracy: 0.5073 - val_loss: 0.6967 - val_accuracy: 0.5046\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7018 - accuracy: 0.5077 - val_loss: 0.6986 - val_accuracy: 0.5059\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7032 - accuracy: 0.5055 - val_loss: 0.6996 - val_accuracy: 0.5084\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7029 - accuracy: 0.5055 - val_loss: 0.6977 - val_accuracy: 0.5079\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7029 - accuracy: 0.5052 - val_loss: 0.7089 - val_accuracy: 0.4989\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7024 - accuracy: 0.5066 - val_loss: 0.7001 - val_accuracy: 0.5084\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7023 - accuracy: 0.5064 - val_loss: 0.7054 - val_accuracy: 0.4987\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7012 - accuracy: 0.5067 - val_loss: 0.7036 - val_accuracy: 0.5018\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7006 - accuracy: 0.5080 - val_loss: 0.7017 - val_accuracy: 0.5003\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7000 - accuracy: 0.5087 - val_loss: 0.6993 - val_accuracy: 0.5105\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6996 - accuracy: 0.5076 - val_loss: 0.7002 - val_accuracy: 0.5116\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6988 - accuracy: 0.5087 - val_loss: 0.7006 - val_accuracy: 0.5069\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6983 - accuracy: 0.5107 - val_loss: 0.6998 - val_accuracy: 0.5030\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6983 - accuracy: 0.5091 - val_loss: 0.6992 - val_accuracy: 0.5111\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5102 - val_loss: 0.7004 - val_accuracy: 0.5097\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6972 - accuracy: 0.5115 - val_loss: 0.6976 - val_accuracy: 0.5048\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6969 - accuracy: 0.5119 - val_loss: 0.6984 - val_accuracy: 0.5030\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5118 - val_loss: 0.6970 - val_accuracy: 0.5136\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.5125 - val_loss: 0.6962 - val_accuracy: 0.5038\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_22\n",
      "cannot prune layer q_activation_22\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_23\n",
      "cannot prune layer q_activation_23\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6989 - accuracy: 0.5073 - val_loss: 0.6965 - val_accuracy: 0.5049\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7000 - accuracy: 0.5059 - val_loss: 0.6976 - val_accuracy: 0.5084\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7014 - accuracy: 0.5065 - val_loss: 0.6962 - val_accuracy: 0.5078\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7014 - accuracy: 0.5059 - val_loss: 0.7097 - val_accuracy: 0.4997\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7007 - accuracy: 0.5070 - val_loss: 0.7116 - val_accuracy: 0.4999\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7004 - accuracy: 0.5073 - val_loss: 0.6975 - val_accuracy: 0.5126\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7003 - accuracy: 0.5066 - val_loss: 0.7038 - val_accuracy: 0.4985\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6996 - accuracy: 0.5092 - val_loss: 0.7006 - val_accuracy: 0.5081\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6993 - accuracy: 0.5084 - val_loss: 0.7011 - val_accuracy: 0.5002\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6990 - accuracy: 0.5089 - val_loss: 0.6987 - val_accuracy: 0.5045\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6985 - accuracy: 0.5094 - val_loss: 0.6994 - val_accuracy: 0.5014\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6981 - accuracy: 0.5098 - val_loss: 0.6993 - val_accuracy: 0.5033\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6982 - accuracy: 0.5100 - val_loss: 0.6997 - val_accuracy: 0.5115\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6974 - accuracy: 0.5106 - val_loss: 0.6972 - val_accuracy: 0.5035\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6969 - accuracy: 0.5110 - val_loss: 0.6990 - val_accuracy: 0.5153\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6967 - accuracy: 0.5109 - val_loss: 0.6961 - val_accuracy: 0.5079\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6963 - accuracy: 0.5115 - val_loss: 0.6983 - val_accuracy: 0.5160\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6963 - accuracy: 0.5131 - val_loss: 0.6968 - val_accuracy: 0.5035\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6958 - accuracy: 0.5130 - val_loss: 0.6967 - val_accuracy: 0.5034\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6956 - accuracy: 0.5129 - val_loss: 0.6969 - val_accuracy: 0.5147\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6951 - accuracy: 0.5144 - val_loss: 0.6957 - val_accuracy: 0.5075\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5147 - val_loss: 0.6971 - val_accuracy: 0.5137\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5148 - val_loss: 0.6947 - val_accuracy: 0.5135\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6942 - accuracy: 0.5167 - val_loss: 0.6943 - val_accuracy: 0.5105\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6939 - accuracy: 0.5161 - val_loss: 0.6976 - val_accuracy: 0.5164\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5164 - val_loss: 0.6932 - val_accuracy: 0.5126\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5182 - val_loss: 0.6964 - val_accuracy: 0.5169\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5183 - val_loss: 0.6940 - val_accuracy: 0.5087\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5191 - val_loss: 0.6930 - val_accuracy: 0.5208\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5218 - val_loss: 0.6958 - val_accuracy: 0.5055\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5234 - val_loss: 0.6918 - val_accuracy: 0.5219\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5254 - val_loss: 0.6913 - val_accuracy: 0.5266\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5268 - val_loss: 0.6916 - val_accuracy: 0.5270\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5277 - val_loss: 0.6909 - val_accuracy: 0.5322\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6897 - accuracy: 0.5307 - val_loss: 0.6907 - val_accuracy: 0.5311\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5329 - val_loss: 0.6900 - val_accuracy: 0.5349\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6888 - accuracy: 0.5342 - val_loss: 0.6902 - val_accuracy: 0.5285\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6880 - accuracy: 0.5371 - val_loss: 0.6887 - val_accuracy: 0.5398\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6873 - accuracy: 0.5386 - val_loss: 0.6880 - val_accuracy: 0.5385\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6868 - accuracy: 0.5415 - val_loss: 0.6870 - val_accuracy: 0.5439\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6859 - accuracy: 0.5446 - val_loss: 0.6869 - val_accuracy: 0.5418\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6852 - accuracy: 0.5459 - val_loss: 0.6858 - val_accuracy: 0.5523\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6845 - val_accuracy: 0.5518\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6834 - accuracy: 0.5515 - val_loss: 0.6843 - val_accuracy: 0.5554\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.5546 - val_loss: 0.6830 - val_accuracy: 0.5575\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6820 - accuracy: 0.5561 - val_loss: 0.6820 - val_accuracy: 0.5599\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6810 - accuracy: 0.5583 - val_loss: 0.6820 - val_accuracy: 0.5574\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6802 - accuracy: 0.5607 - val_loss: 0.6799 - val_accuracy: 0.5663\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6793 - accuracy: 0.5631 - val_loss: 0.6794 - val_accuracy: 0.5654\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6788 - accuracy: 0.5644 - val_loss: 0.6788 - val_accuracy: 0.5673\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.4950577 ]\n",
      " [0.45231962]\n",
      " [0.5806295 ]\n",
      " [0.5060985 ]\n",
      " [0.37364888]\n",
      " [0.3149808 ]\n",
      " [0.56620824]\n",
      " [0.4354608 ]\n",
      " [0.58136827]\n",
      " [0.5255733 ]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.15, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_24 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_25 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_24 is normal keras bn layer\n",
      "q_activation_24      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_25 is normal keras bn layer\n",
      "q_activation_25      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 3.7707 - accuracy: 0.5001 - val_loss: 1.7927 - val_accuracy: 0.4986\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9982 - accuracy: 0.4998 - val_loss: 0.7444 - val_accuracy: 0.5007\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7290 - accuracy: 0.5001 - val_loss: 0.7189 - val_accuracy: 0.5025\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7145 - accuracy: 0.5016 - val_loss: 0.7108 - val_accuracy: 0.5026\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7094 - accuracy: 0.5021 - val_loss: 0.7072 - val_accuracy: 0.5035\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7065 - accuracy: 0.5026 - val_loss: 0.7049 - val_accuracy: 0.5016\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7041 - accuracy: 0.5028 - val_loss: 0.7030 - val_accuracy: 0.5032\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7022 - accuracy: 0.5035 - val_loss: 0.7016 - val_accuracy: 0.5040\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7006 - accuracy: 0.5040 - val_loss: 0.7003 - val_accuracy: 0.5030\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6991 - accuracy: 0.5046 - val_loss: 0.6991 - val_accuracy: 0.5034\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6980 - accuracy: 0.5051 - val_loss: 0.6982 - val_accuracy: 0.5039\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6971 - accuracy: 0.5057 - val_loss: 0.6973 - val_accuracy: 0.5033\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5059 - val_loss: 0.6967 - val_accuracy: 0.5036\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6956 - accuracy: 0.5079 - val_loss: 0.6962 - val_accuracy: 0.5049\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5084 - val_loss: 0.6956 - val_accuracy: 0.5046\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5080 - val_loss: 0.6956 - val_accuracy: 0.5030\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6940 - accuracy: 0.5093 - val_loss: 0.6949 - val_accuracy: 0.5048\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5089 - val_loss: 0.6944 - val_accuracy: 0.5034\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5111 - val_loss: 0.6944 - val_accuracy: 0.5060\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5116 - val_loss: 0.6940 - val_accuracy: 0.5058\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5120 - val_loss: 0.6939 - val_accuracy: 0.5039\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5120 - val_loss: 0.6938 - val_accuracy: 0.5082\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6935 - val_accuracy: 0.5068\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5133 - val_loss: 0.6934 - val_accuracy: 0.5081\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5128 - val_loss: 0.6989 - val_accuracy: 0.5010\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5119 - val_loss: 0.6982 - val_accuracy: 0.5001\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5130 - val_loss: 0.6934 - val_accuracy: 0.5095\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5129 - val_loss: 0.6942 - val_accuracy: 0.5075\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5118 - val_loss: 0.6935 - val_accuracy: 0.5093\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5142 - val_loss: 0.6967 - val_accuracy: 0.5035\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5130 - val_loss: 0.6934 - val_accuracy: 0.5112\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5149 - val_loss: 0.6935 - val_accuracy: 0.5121\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5161 - val_loss: 0.6931 - val_accuracy: 0.5125\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5156 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5161 - val_loss: 0.6926 - val_accuracy: 0.5132\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5171 - val_loss: 0.6966 - val_accuracy: 0.5026\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5191 - val_loss: 0.6927 - val_accuracy: 0.5149\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5172\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.5215 - val_loss: 0.6919 - val_accuracy: 0.5222\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5238 - val_loss: 0.6917 - val_accuracy: 0.5227\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5280 - val_loss: 0.6916 - val_accuracy: 0.5259\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5318 - val_loss: 0.6905 - val_accuracy: 0.5328\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5351 - val_loss: 0.6901 - val_accuracy: 0.5341\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5364 - val_loss: 0.6899 - val_accuracy: 0.5368\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6884 - accuracy: 0.5396 - val_loss: 0.6886 - val_accuracy: 0.5416\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6872 - accuracy: 0.5446 - val_loss: 0.6880 - val_accuracy: 0.5410\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5483 - val_loss: 0.6875 - val_accuracy: 0.5441\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6846 - accuracy: 0.5522 - val_loss: 0.6870 - val_accuracy: 0.5468\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6831 - accuracy: 0.5576 - val_loss: 0.6845 - val_accuracy: 0.5516\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6812 - accuracy: 0.5616 - val_loss: 0.6811 - val_accuracy: 0.5643\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6795 - accuracy: 0.5654 - val_loss: 0.6809 - val_accuracy: 0.5634\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6775 - accuracy: 0.5683 - val_loss: 0.6789 - val_accuracy: 0.5693\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6754 - accuracy: 0.5730 - val_loss: 0.6771 - val_accuracy: 0.5688\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6735 - accuracy: 0.5765 - val_loss: 0.6740 - val_accuracy: 0.5751\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5806 - val_loss: 0.6728 - val_accuracy: 0.5772\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6696 - accuracy: 0.5837 - val_loss: 0.6719 - val_accuracy: 0.5831\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6682 - accuracy: 0.5859 - val_loss: 0.6692 - val_accuracy: 0.5839\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5881 - val_loss: 0.6692 - val_accuracy: 0.5820\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6658 - accuracy: 0.5891 - val_loss: 0.6657 - val_accuracy: 0.5909\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6653 - accuracy: 0.5903 - val_loss: 0.6684 - val_accuracy: 0.5844\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5928 - val_loss: 0.6652 - val_accuracy: 0.5900\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6633 - accuracy: 0.5934 - val_loss: 0.6643 - val_accuracy: 0.5938\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6618 - accuracy: 0.5947 - val_loss: 0.6688 - val_accuracy: 0.5763\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6606 - accuracy: 0.5974 - val_loss: 0.6631 - val_accuracy: 0.5971\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5993 - val_loss: 0.6610 - val_accuracy: 0.5986\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6585 - accuracy: 0.6000 - val_loss: 0.6712 - val_accuracy: 0.5821\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6579 - accuracy: 0.6014 - val_loss: 0.6635 - val_accuracy: 0.5942\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6569 - accuracy: 0.6027 - val_loss: 0.6582 - val_accuracy: 0.5991\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6564 - accuracy: 0.6028 - val_loss: 0.6653 - val_accuracy: 0.5881\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6553 - accuracy: 0.6053 - val_loss: 0.6584 - val_accuracy: 0.5978\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6545 - accuracy: 0.6068 - val_loss: 0.6597 - val_accuracy: 0.6014\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6076 - val_loss: 0.6533 - val_accuracy: 0.6111\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6085 - val_loss: 0.6547 - val_accuracy: 0.6094\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6560 - accuracy: 0.6064 - val_loss: 0.6546 - val_accuracy: 0.6122\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6570 - accuracy: 0.6049 - val_loss: 0.6549 - val_accuracy: 0.6085\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6084 - val_loss: 0.6625 - val_accuracy: 0.6068\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6559 - accuracy: 0.6078 - val_loss: 0.6549 - val_accuracy: 0.6050\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6090 - val_loss: 0.6557 - val_accuracy: 0.6092\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6529 - accuracy: 0.6098 - val_loss: 0.6550 - val_accuracy: 0.6040\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6094 - val_loss: 0.6504 - val_accuracy: 0.6137\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6520 - accuracy: 0.6110 - val_loss: 0.6629 - val_accuracy: 0.5951\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.6034 - val_loss: 0.6626 - val_accuracy: 0.5964\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6568 - accuracy: 0.6044 - val_loss: 0.6539 - val_accuracy: 0.6071\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6523 - accuracy: 0.6105 - val_loss: 0.6475 - val_accuracy: 0.6200\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6511 - accuracy: 0.6124 - val_loss: 0.6498 - val_accuracy: 0.6121\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6632 - accuracy: 0.5948 - val_loss: 0.6621 - val_accuracy: 0.5955\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6559 - accuracy: 0.6046 - val_loss: 0.6501 - val_accuracy: 0.6123\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6094 - val_loss: 0.6473 - val_accuracy: 0.6174\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6517 - accuracy: 0.6104 - val_loss: 0.6501 - val_accuracy: 0.6137\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.6134 - val_loss: 0.6484 - val_accuracy: 0.6162\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6501 - accuracy: 0.6136 - val_loss: 0.7555 - val_accuracy: 0.5087\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6853 - accuracy: 0.5571 - val_loss: 0.6686 - val_accuracy: 0.5898\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.5999 - val_loss: 0.6539 - val_accuracy: 0.6114\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6532 - accuracy: 0.6090 - val_loss: 0.6657 - val_accuracy: 0.5887\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6099 - val_loss: 0.6510 - val_accuracy: 0.6107\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6494 - accuracy: 0.6138 - val_loss: 0.6473 - val_accuracy: 0.6202\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6474 - accuracy: 0.6175 - val_loss: 0.6514 - val_accuracy: 0.6130\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5757 - val_loss: 0.6700 - val_accuracy: 0.5856\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6618 - accuracy: 0.5961 - val_loss: 0.6560 - val_accuracy: 0.6039\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6528 - accuracy: 0.6103 - val_loss: 0.6479 - val_accuracy: 0.6204\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5892 - val_loss: 0.6708 - val_accuracy: 0.5803\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6584 - accuracy: 0.6004 - val_loss: 0.6533 - val_accuracy: 0.6108\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6490 - accuracy: 0.6144 - val_loss: 0.6530 - val_accuracy: 0.6094\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6489 - accuracy: 0.6161 - val_loss: 0.6451 - val_accuracy: 0.6183\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6471 - accuracy: 0.6179 - val_loss: 0.6445 - val_accuracy: 0.6263\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6461 - accuracy: 0.6192 - val_loss: 0.6457 - val_accuracy: 0.6244\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6470 - accuracy: 0.6169 - val_loss: 0.6461 - val_accuracy: 0.6219\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6458 - accuracy: 0.6201 - val_loss: 0.6411 - val_accuracy: 0.6282\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6541 - accuracy: 0.6072 - val_loss: 0.6517 - val_accuracy: 0.6097\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5942 - val_loss: 0.7438 - val_accuracy: 0.5172\n",
      "Epoch 111/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5351 - val_loss: 0.6795 - val_accuracy: 0.5603\n",
      "Epoch 112/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6684 - accuracy: 0.5851 - val_loss: 0.6616 - val_accuracy: 0.6017\n",
      "Epoch 113/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6548 - accuracy: 0.6073 - val_loss: 0.6510 - val_accuracy: 0.6151\n",
      "Epoch 114/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6492 - accuracy: 0.6151 - val_loss: 0.6471 - val_accuracy: 0.6221\n",
      "Epoch 115/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6456 - accuracy: 0.6187 - val_loss: 0.6452 - val_accuracy: 0.6168\n",
      "Epoch 116/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6459 - accuracy: 0.6196 - val_loss: 0.6415 - val_accuracy: 0.6275\n",
      "Epoch 117/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6439 - accuracy: 0.6212 - val_loss: 0.6456 - val_accuracy: 0.6160\n",
      "Epoch 118/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6420 - accuracy: 0.6240 - val_loss: 0.6370 - val_accuracy: 0.6314\n",
      "Epoch 119/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6394 - accuracy: 0.6251 - val_loss: 0.6447 - val_accuracy: 0.6267\n",
      "Epoch 120/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6390 - accuracy: 0.6259 - val_loss: 0.6408 - val_accuracy: 0.6266\n",
      "Epoch 121/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6380 - accuracy: 0.6271 - val_loss: 0.6349 - val_accuracy: 0.6330\n",
      "Epoch 122/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6370 - accuracy: 0.6282 - val_loss: 0.6412 - val_accuracy: 0.6250\n",
      "Epoch 123/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6384 - accuracy: 0.6257 - val_loss: 0.6339 - val_accuracy: 0.6345\n",
      "Epoch 124/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6370 - accuracy: 0.6274 - val_loss: 0.6406 - val_accuracy: 0.6249\n",
      "Epoch 125/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6373 - accuracy: 0.6269 - val_loss: 0.6376 - val_accuracy: 0.6300\n",
      "Epoch 126/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6363 - accuracy: 0.6280 - val_loss: 0.6332 - val_accuracy: 0.6310\n",
      "Epoch 127/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6348 - accuracy: 0.6304 - val_loss: 0.6378 - val_accuracy: 0.6294\n",
      "Epoch 128/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6363 - accuracy: 0.6285 - val_loss: 0.6473 - val_accuracy: 0.6157\n",
      "Epoch 129/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6352 - accuracy: 0.6295 - val_loss: 0.6358 - val_accuracy: 0.6292\n",
      "Epoch 130/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6370 - accuracy: 0.6272 - val_loss: 0.6334 - val_accuracy: 0.6318\n",
      "Epoch 131/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6371 - accuracy: 0.6278 - val_loss: 0.6390 - val_accuracy: 0.6272\n",
      "Epoch 132/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6341 - accuracy: 0.6302 - val_loss: 0.6378 - val_accuracy: 0.6260\n",
      "Epoch 133/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6340 - accuracy: 0.6303 - val_loss: 0.6357 - val_accuracy: 0.6313\n",
      "Epoch 134/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6331 - accuracy: 0.6318 - val_loss: 0.6308 - val_accuracy: 0.6371\n",
      "Epoch 135/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6386 - accuracy: 0.6241 - val_loss: 0.6296 - val_accuracy: 0.6361\n",
      "Epoch 136/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6346 - accuracy: 0.6305 - val_loss: 0.6307 - val_accuracy: 0.6359\n",
      "Epoch 137/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6325 - accuracy: 0.6319 - val_loss: 0.6295 - val_accuracy: 0.6360\n",
      "Epoch 138/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6522 - accuracy: 0.6079 - val_loss: 0.6864 - val_accuracy: 0.5564\n",
      "Epoch 139/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6582 - accuracy: 0.5993 - val_loss: 0.6444 - val_accuracy: 0.6212\n",
      "Epoch 140/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6370 - accuracy: 0.6269 - val_loss: 0.6364 - val_accuracy: 0.6286\n",
      "Epoch 141/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6325 - val_loss: 0.6334 - val_accuracy: 0.6302\n",
      "Epoch 142/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6316 - accuracy: 0.6327 - val_loss: 0.6322 - val_accuracy: 0.6357\n",
      "Epoch 143/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6302 - accuracy: 0.6343 - val_loss: 0.6353 - val_accuracy: 0.6291\n",
      "Epoch 144/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6309 - accuracy: 0.6328 - val_loss: 0.6329 - val_accuracy: 0.6342\n",
      "Epoch 145/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6304 - accuracy: 0.6351 - val_loss: 0.6352 - val_accuracy: 0.6296\n",
      "Epoch 146/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6295 - accuracy: 0.6366 - val_loss: 0.6370 - val_accuracy: 0.6273\n",
      "Epoch 147/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6379 - accuracy: 0.6266 - val_loss: 0.6389 - val_accuracy: 0.6276\n",
      "Epoch 148/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6299 - accuracy: 0.6353 - val_loss: 0.6312 - val_accuracy: 0.6329\n",
      "Epoch 149/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7101 - accuracy: 0.5123 - val_loss: 0.6978 - val_accuracy: 0.5093\n",
      "Epoch 150/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5230 - val_loss: 0.6896 - val_accuracy: 0.5250\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_24\n",
      "cannot prune layer q_activation_24\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_25\n",
      "cannot prune layer q_activation_25\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6548 - accuracy: 0.6007 - val_loss: 0.6523 - val_accuracy: 0.6102\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6344 - accuracy: 0.6303 - val_loss: 0.6393 - val_accuracy: 0.6215\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6331 - accuracy: 0.6321 - val_loss: 0.6465 - val_accuracy: 0.6137\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6713 - accuracy: 0.5825 - val_loss: 0.6578 - val_accuracy: 0.5916\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6360 - accuracy: 0.6296 - val_loss: 0.6378 - val_accuracy: 0.6257\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6304 - accuracy: 0.6348 - val_loss: 0.6416 - val_accuracy: 0.6205\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6304 - accuracy: 0.6354 - val_loss: 0.6276 - val_accuracy: 0.6382\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6286 - accuracy: 0.6372 - val_loss: 0.6274 - val_accuracy: 0.6423\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6288 - accuracy: 0.6368 - val_loss: 0.6316 - val_accuracy: 0.6374\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6288 - accuracy: 0.6370 - val_loss: 0.6256 - val_accuracy: 0.6419\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6274 - accuracy: 0.6390 - val_loss: 0.6252 - val_accuracy: 0.6419\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5848 - val_loss: 0.6984 - val_accuracy: 0.5389\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5473 - val_loss: 0.6851 - val_accuracy: 0.5526\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6829 - accuracy: 0.5571 - val_loss: 0.6804 - val_accuracy: 0.5635\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6783 - accuracy: 0.5659 - val_loss: 0.6766 - val_accuracy: 0.5716\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6743 - accuracy: 0.5739 - val_loss: 0.6723 - val_accuracy: 0.5813\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6690 - accuracy: 0.5843 - val_loss: 0.6660 - val_accuracy: 0.5906\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6626 - accuracy: 0.5935 - val_loss: 0.6598 - val_accuracy: 0.5981\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6569 - accuracy: 0.6027 - val_loss: 0.6537 - val_accuracy: 0.6061\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6531 - accuracy: 0.6080 - val_loss: 0.6496 - val_accuracy: 0.6128\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6490 - accuracy: 0.6137 - val_loss: 0.6498 - val_accuracy: 0.6139\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6454 - accuracy: 0.6181 - val_loss: 0.6452 - val_accuracy: 0.6182\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6418 - accuracy: 0.6230 - val_loss: 0.6402 - val_accuracy: 0.6261\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6381 - accuracy: 0.6265 - val_loss: 0.6367 - val_accuracy: 0.6305\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6353 - accuracy: 0.6303 - val_loss: 0.6353 - val_accuracy: 0.6331\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6333 - accuracy: 0.6316 - val_loss: 0.6303 - val_accuracy: 0.6394\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6318 - accuracy: 0.6339 - val_loss: 0.6292 - val_accuracy: 0.6374\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6772 - accuracy: 0.5956 - val_loss: 0.7385 - val_accuracy: 0.5254\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7094 - accuracy: 0.5379 - val_loss: 0.6961 - val_accuracy: 0.5495\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5550 - val_loss: 0.6850 - val_accuracy: 0.5614\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6752 - accuracy: 0.5730 - val_loss: 0.6663 - val_accuracy: 0.5869\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6531 - accuracy: 0.6061 - val_loss: 0.6453 - val_accuracy: 0.6201\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6418 - accuracy: 0.6211 - val_loss: 0.6359 - val_accuracy: 0.6299\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6363 - accuracy: 0.6278 - val_loss: 0.6320 - val_accuracy: 0.6371\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6486 - accuracy: 0.6118 - val_loss: 0.6481 - val_accuracy: 0.6185\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6403 - accuracy: 0.6239 - val_loss: 0.6360 - val_accuracy: 0.6299\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6332 - accuracy: 0.6329 - val_loss: 0.6308 - val_accuracy: 0.6389\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6313 - accuracy: 0.6348 - val_loss: 0.6312 - val_accuracy: 0.6333\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6327 - accuracy: 0.6335 - val_loss: 0.6341 - val_accuracy: 0.6312\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6300 - accuracy: 0.6359 - val_loss: 0.6269 - val_accuracy: 0.6442\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6294 - accuracy: 0.6365 - val_loss: 0.6244 - val_accuracy: 0.6444\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6295 - accuracy: 0.6361 - val_loss: 0.7389 - val_accuracy: 0.5463\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6367 - accuracy: 0.6286 - val_loss: 0.6268 - val_accuracy: 0.6417\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6290 - accuracy: 0.6372 - val_loss: 0.6275 - val_accuracy: 0.6408\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6274 - accuracy: 0.6388 - val_loss: 0.6330 - val_accuracy: 0.6326\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.6395 - val_loss: 0.6273 - val_accuracy: 0.6459\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.6404 - val_loss: 0.6293 - val_accuracy: 0.6367\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6260 - accuracy: 0.6402 - val_loss: 0.6263 - val_accuracy: 0.6404\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6279 - accuracy: 0.6380 - val_loss: 0.6253 - val_accuracy: 0.6448\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6266 - accuracy: 0.6391 - val_loss: 0.6279 - val_accuracy: 0.6392\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.423079  ]\n",
      " [0.60368705]\n",
      " [0.54961866]\n",
      " [0.52334636]\n",
      " [0.14553446]\n",
      " [0.4507345 ]\n",
      " [0.6248934 ]\n",
      " [0.63589364]\n",
      " [0.57060176]\n",
      " [0.5671425 ]]\n",
      "########### FOUND NEW BEST METRIC 0.3960911340806522 ##############\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.002, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_26 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_27 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_26 is normal keras bn layer\n",
      "q_activation_26      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_27 is normal keras bn layer\n",
      "q_activation_27      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 1.5038 - accuracy: 0.4998 - val_loss: 0.9148 - val_accuracy: 0.5018\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7377 - accuracy: 0.5005 - val_loss: 0.7292 - val_accuracy: 0.5028\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7195 - accuracy: 0.5012 - val_loss: 0.7099 - val_accuracy: 0.5032\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7082 - accuracy: 0.5005 - val_loss: 0.7051 - val_accuracy: 0.5022\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7045 - accuracy: 0.5014 - val_loss: 0.7038 - val_accuracy: 0.5032\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7020 - accuracy: 0.5022 - val_loss: 0.7012 - val_accuracy: 0.5046\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7001 - accuracy: 0.5034 - val_loss: 0.6997 - val_accuracy: 0.5042\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6988 - accuracy: 0.5040 - val_loss: 0.6986 - val_accuracy: 0.5044\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6978 - accuracy: 0.5046 - val_loss: 0.6978 - val_accuracy: 0.5030\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6970 - accuracy: 0.5036 - val_loss: 0.6971 - val_accuracy: 0.5042\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5051 - val_loss: 0.6966 - val_accuracy: 0.5034\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5054 - val_loss: 0.6960 - val_accuracy: 0.5030\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5057 - val_loss: 0.6956 - val_accuracy: 0.5055\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5059 - val_loss: 0.6951 - val_accuracy: 0.5050\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5065 - val_loss: 0.6951 - val_accuracy: 0.5053\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5045 - val_loss: 0.6949 - val_accuracy: 0.5080\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5061 - val_loss: 0.6948 - val_accuracy: 0.5063\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5060 - val_loss: 0.6947 - val_accuracy: 0.5056\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5072 - val_loss: 0.6945 - val_accuracy: 0.5077\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6938 - accuracy: 0.5072 - val_loss: 0.6942 - val_accuracy: 0.5079\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6938 - val_accuracy: 0.5072\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5066 - val_loss: 0.6944 - val_accuracy: 0.5013\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5085 - val_loss: 0.6939 - val_accuracy: 0.5081\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5092 - val_loss: 0.6939 - val_accuracy: 0.5075\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5091 - val_loss: 0.6935 - val_accuracy: 0.5046\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5084 - val_loss: 0.6936 - val_accuracy: 0.5070\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5093 - val_loss: 0.6938 - val_accuracy: 0.5076\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5078 - val_loss: 0.6935 - val_accuracy: 0.5093\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5091 - val_loss: 0.6936 - val_accuracy: 0.5072\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6941 - val_accuracy: 0.5063\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6933 - val_accuracy: 0.5016\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6938 - val_accuracy: 0.5048\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5116 - val_loss: 0.6929 - val_accuracy: 0.5126\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5108 - val_loss: 0.6934 - val_accuracy: 0.5075\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5116 - val_loss: 0.6931 - val_accuracy: 0.5097\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5132 - val_loss: 0.6932 - val_accuracy: 0.5087\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5162 - val_loss: 0.6931 - val_accuracy: 0.5142\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5162 - val_loss: 0.6928 - val_accuracy: 0.5180\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.5189 - val_loss: 0.6923 - val_accuracy: 0.5243\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5225 - val_loss: 0.6918 - val_accuracy: 0.5234\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5267 - val_loss: 0.6916 - val_accuracy: 0.5227\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5320 - val_loss: 0.6912 - val_accuracy: 0.5304\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5386 - val_loss: 0.6880 - val_accuracy: 0.5404\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5442 - val_loss: 0.6859 - val_accuracy: 0.5456\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5481 - val_loss: 0.6831 - val_accuracy: 0.5490\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6816 - accuracy: 0.5532 - val_loss: 0.6803 - val_accuracy: 0.5537\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5574 - val_loss: 0.6937 - val_accuracy: 0.5390\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5602 - val_loss: 0.6779 - val_accuracy: 0.5601\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5660 - val_loss: 0.6746 - val_accuracy: 0.5699\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5690 - val_loss: 0.6732 - val_accuracy: 0.5665\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6712 - accuracy: 0.5710 - val_loss: 0.6975 - val_accuracy: 0.5330\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5728 - val_loss: 0.6915 - val_accuracy: 0.5476\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.5711 - val_loss: 0.6686 - val_accuracy: 0.5783\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5725 - val_loss: 0.6697 - val_accuracy: 0.5758\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.5761 - val_loss: 0.6633 - val_accuracy: 0.5844\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6654 - accuracy: 0.5807 - val_loss: 0.6613 - val_accuracy: 0.5872\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5633 - val_loss: 0.6989 - val_accuracy: 0.5086\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5048 - val_loss: 0.6932 - val_accuracy: 0.5035\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5158 - val_loss: 0.6931 - val_accuracy: 0.5247\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5307 - val_loss: 0.6903 - val_accuracy: 0.5255\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6770 - accuracy: 0.5610 - val_loss: 0.6848 - val_accuracy: 0.5395\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5726 - val_loss: 0.6719 - val_accuracy: 0.5723\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6678 - accuracy: 0.5803 - val_loss: 0.6662 - val_accuracy: 0.5803\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6731 - accuracy: 0.5675 - val_loss: 0.6803 - val_accuracy: 0.5537\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6672 - accuracy: 0.5802 - val_loss: 0.6708 - val_accuracy: 0.5733\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6698 - accuracy: 0.5718 - val_loss: 0.6968 - val_accuracy: 0.5176\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6726 - accuracy: 0.5687 - val_loss: 0.6728 - val_accuracy: 0.5711\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5409 - val_loss: 0.6955 - val_accuracy: 0.5040\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5050 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5077 - val_loss: 0.6931 - val_accuracy: 0.5091\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5091\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5111 - val_loss: 0.6926 - val_accuracy: 0.5133\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5121 - val_loss: 0.6925 - val_accuracy: 0.5111\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5126 - val_loss: 0.6925 - val_accuracy: 0.5129\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5147 - val_loss: 0.6923 - val_accuracy: 0.5145\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5168 - val_loss: 0.6922 - val_accuracy: 0.5168\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_26\n",
      "cannot prune layer q_activation_26\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_27\n",
      "cannot prune layer q_activation_27\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6729 - accuracy: 0.5695 - val_loss: 0.6783 - val_accuracy: 0.5581\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6624 - accuracy: 0.5851 - val_loss: 0.6659 - val_accuracy: 0.5840\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6603 - accuracy: 0.5889 - val_loss: 0.6645 - val_accuracy: 0.5815\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6597 - accuracy: 0.5897 - val_loss: 0.6600 - val_accuracy: 0.5868\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6869 - accuracy: 0.5434 - val_loss: 0.7024 - val_accuracy: 0.5001\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6959 - accuracy: 0.5030 - val_loss: 0.6946 - val_accuracy: 0.5067\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6939 - accuracy: 0.5072 - val_loss: 0.6942 - val_accuracy: 0.5071\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6936 - val_accuracy: 0.5092\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5114 - val_loss: 0.6931 - val_accuracy: 0.5124\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5136 - val_loss: 0.6930 - val_accuracy: 0.5136\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5155 - val_loss: 0.6928 - val_accuracy: 0.5136\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5189 - val_loss: 0.6922 - val_accuracy: 0.5195\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5218 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5258 - val_loss: 0.6910 - val_accuracy: 0.5232\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6899 - accuracy: 0.5298 - val_loss: 0.6894 - val_accuracy: 0.5314\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5335 - val_loss: 0.6876 - val_accuracy: 0.5386\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6868 - accuracy: 0.5390 - val_loss: 0.6866 - val_accuracy: 0.5369\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6846 - accuracy: 0.5465 - val_loss: 0.6859 - val_accuracy: 0.5440\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6815 - accuracy: 0.5546 - val_loss: 0.6793 - val_accuracy: 0.5594\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6787 - accuracy: 0.5605 - val_loss: 0.6793 - val_accuracy: 0.5560\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6761 - accuracy: 0.5659 - val_loss: 0.6746 - val_accuracy: 0.5739\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.5719 - val_loss: 0.6735 - val_accuracy: 0.5705\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6787 - accuracy: 0.5594 - val_loss: 0.6804 - val_accuracy: 0.5568\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6737 - accuracy: 0.5679 - val_loss: 0.6695 - val_accuracy: 0.5744\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6732 - accuracy: 0.5696 - val_loss: 0.6836 - val_accuracy: 0.5473\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6720 - accuracy: 0.5732 - val_loss: 0.6671 - val_accuracy: 0.5843\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6672 - accuracy: 0.5833 - val_loss: 0.6712 - val_accuracy: 0.5718\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6667 - accuracy: 0.5834 - val_loss: 0.6657 - val_accuracy: 0.5827\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6639 - accuracy: 0.5868 - val_loss: 0.6603 - val_accuracy: 0.5916\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6639 - accuracy: 0.5888 - val_loss: 0.6629 - val_accuracy: 0.5870\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6626 - accuracy: 0.5898 - val_loss: 0.6599 - val_accuracy: 0.5937\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6615 - accuracy: 0.5910 - val_loss: 0.6649 - val_accuracy: 0.5835\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6609 - accuracy: 0.5915 - val_loss: 0.6610 - val_accuracy: 0.5917\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6599 - accuracy: 0.5933 - val_loss: 0.6570 - val_accuracy: 0.5978\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6602 - accuracy: 0.5931 - val_loss: 0.6570 - val_accuracy: 0.5985\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6591 - accuracy: 0.5949 - val_loss: 0.6589 - val_accuracy: 0.5947\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6606 - accuracy: 0.5924 - val_loss: 0.6582 - val_accuracy: 0.5972\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6584 - accuracy: 0.5956 - val_loss: 0.6556 - val_accuracy: 0.6023\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6629 - accuracy: 0.5917 - val_loss: 0.6625 - val_accuracy: 0.5868\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6676 - accuracy: 0.5835 - val_loss: 0.6989 - val_accuracy: 0.5165\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6748 - accuracy: 0.5650 - val_loss: 0.6737 - val_accuracy: 0.5779\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6612 - accuracy: 0.5919 - val_loss: 0.6693 - val_accuracy: 0.5783\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6585 - accuracy: 0.5955 - val_loss: 0.6560 - val_accuracy: 0.5989\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6751 - accuracy: 0.5732 - val_loss: 0.6725 - val_accuracy: 0.5751\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6640 - accuracy: 0.5892 - val_loss: 0.6579 - val_accuracy: 0.5994\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6587 - accuracy: 0.5966 - val_loss: 0.6568 - val_accuracy: 0.5970\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.5965 - val_loss: 0.6604 - val_accuracy: 0.5935\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6571 - accuracy: 0.5984 - val_loss: 0.6560 - val_accuracy: 0.5988\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6577 - accuracy: 0.5971 - val_loss: 0.6572 - val_accuracy: 0.5972\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6552 - accuracy: 0.6010 - val_loss: 0.6560 - val_accuracy: 0.5988\n",
      "1714/1714 [==============================] - 4s 1ms/step\n",
      "[[0.48235464]\n",
      " [0.58553773]\n",
      " [0.5742767 ]\n",
      " [0.5329846 ]\n",
      " [0.62263685]\n",
      " [0.540028  ]\n",
      " [0.5881012 ]\n",
      " [0.54043263]\n",
      " [0.5953689 ]\n",
      " [0.4096765 ]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.002, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_28 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_29 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_28 is normal keras bn layer\n",
      "q_activation_28      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_29 is normal keras bn layer\n",
      "q_activation_29      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.7367 - accuracy: 0.4993 - val_loss: 0.7015 - val_accuracy: 0.5023\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6982 - accuracy: 0.5040 - val_loss: 0.6962 - val_accuracy: 0.5094\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6954 - accuracy: 0.5074 - val_loss: 0.6951 - val_accuracy: 0.5106\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6942 - accuracy: 0.5091 - val_loss: 0.6942 - val_accuracy: 0.5055\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5092 - val_loss: 0.6940 - val_accuracy: 0.5059\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5115 - val_loss: 0.6941 - val_accuracy: 0.5072\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5115 - val_loss: 0.6940 - val_accuracy: 0.5104\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5117 - val_loss: 0.6937 - val_accuracy: 0.5120\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6937 - val_accuracy: 0.5102\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6930 - val_accuracy: 0.5126\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5157 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5155 - val_loss: 0.6928 - val_accuracy: 0.5115\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5145 - val_loss: 0.6973 - val_accuracy: 0.5114\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5155 - val_loss: 0.6925 - val_accuracy: 0.5175\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5206 - val_loss: 0.6927 - val_accuracy: 0.5136\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5241 - val_loss: 0.6919 - val_accuracy: 0.5214\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5287 - val_loss: 0.6904 - val_accuracy: 0.5337\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5334 - val_loss: 0.6900 - val_accuracy: 0.5337\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5399 - val_loss: 0.6892 - val_accuracy: 0.5216\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6863 - accuracy: 0.5439 - val_loss: 0.6866 - val_accuracy: 0.5475\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5491 - val_loss: 0.6827 - val_accuracy: 0.5537\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6802 - accuracy: 0.5579 - val_loss: 0.6782 - val_accuracy: 0.5585\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5658 - val_loss: 0.6781 - val_accuracy: 0.5653\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6741 - accuracy: 0.5690 - val_loss: 0.6802 - val_accuracy: 0.5544\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6726 - accuracy: 0.5707 - val_loss: 0.6723 - val_accuracy: 0.5724\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6720 - accuracy: 0.5720 - val_loss: 0.6773 - val_accuracy: 0.5574\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6654 - accuracy: 0.5816 - val_loss: 0.6640 - val_accuracy: 0.5824\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6613 - accuracy: 0.5865 - val_loss: 0.6633 - val_accuracy: 0.5840\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6602 - accuracy: 0.5896 - val_loss: 0.6692 - val_accuracy: 0.5804\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6578 - accuracy: 0.5914 - val_loss: 0.6582 - val_accuracy: 0.5906\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6551 - accuracy: 0.5947 - val_loss: 0.6504 - val_accuracy: 0.6018\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5248 - val_loss: 0.6947 - val_accuracy: 0.5036\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5141 - val_loss: 0.6927 - val_accuracy: 0.5191\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5234 - val_loss: 0.6902 - val_accuracy: 0.5243\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6840 - accuracy: 0.5467 - val_loss: 0.6805 - val_accuracy: 0.5576\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6674 - accuracy: 0.5797 - val_loss: 0.6840 - val_accuracy: 0.5505\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6548 - accuracy: 0.5970 - val_loss: 0.6668 - val_accuracy: 0.5887\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.5988 - val_loss: 0.6593 - val_accuracy: 0.5897\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6611 - accuracy: 0.5871 - val_loss: 0.6631 - val_accuracy: 0.5882\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.5982 - val_loss: 0.6761 - val_accuracy: 0.5827\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6492 - accuracy: 0.6037 - val_loss: 0.6581 - val_accuracy: 0.5896\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6499 - accuracy: 0.6022 - val_loss: 0.6493 - val_accuracy: 0.6077\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6490 - accuracy: 0.6046 - val_loss: 0.7102 - val_accuracy: 0.5396\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6685 - accuracy: 0.5738 - val_loss: 0.6944 - val_accuracy: 0.5214\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6806 - accuracy: 0.5534 - val_loss: 0.6711 - val_accuracy: 0.5793\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6595 - accuracy: 0.5899 - val_loss: 0.6552 - val_accuracy: 0.5982\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6547 - accuracy: 0.5951 - val_loss: 0.6525 - val_accuracy: 0.6007\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6520 - accuracy: 0.5989 - val_loss: 0.6489 - val_accuracy: 0.5967\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6506 - accuracy: 0.6027 - val_loss: 0.6598 - val_accuracy: 0.5842\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6476 - accuracy: 0.6050 - val_loss: 0.6420 - val_accuracy: 0.6193\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6469 - accuracy: 0.6070 - val_loss: 0.6404 - val_accuracy: 0.6210\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6446 - accuracy: 0.6104 - val_loss: 0.6488 - val_accuracy: 0.6012\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6646 - accuracy: 0.5765 - val_loss: 0.6969 - val_accuracy: 0.5104\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5208 - val_loss: 0.6906 - val_accuracy: 0.5315\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5444 - val_loss: 0.6985 - val_accuracy: 0.5217\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6554 - accuracy: 0.5971 - val_loss: 0.6468 - val_accuracy: 0.6146\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6500 - accuracy: 0.6050 - val_loss: 0.6461 - val_accuracy: 0.6116\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6483 - accuracy: 0.6062 - val_loss: 0.6463 - val_accuracy: 0.6134\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6733 - accuracy: 0.5626 - val_loss: 0.6976 - val_accuracy: 0.5013\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5104\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5119 - val_loss: 0.6926 - val_accuracy: 0.5118\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6922 - val_accuracy: 0.5156\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5182 - val_loss: 0.6915 - val_accuracy: 0.5183\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5226 - val_loss: 0.6906 - val_accuracy: 0.5225\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6896 - accuracy: 0.5289 - val_loss: 0.6903 - val_accuracy: 0.5267\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5356 - val_loss: 0.6875 - val_accuracy: 0.5332\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5430 - val_loss: 0.6836 - val_accuracy: 0.5463\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6802 - accuracy: 0.5526 - val_loss: 0.6777 - val_accuracy: 0.5530\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6712 - accuracy: 0.5706 - val_loss: 0.6791 - val_accuracy: 0.5612\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6624 - accuracy: 0.5861 - val_loss: 0.6636 - val_accuracy: 0.5833\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_28\n",
      "cannot prune layer q_activation_28\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_29\n",
      "cannot prune layer q_activation_29\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6502 - accuracy: 0.6038 - val_loss: 0.6750 - val_accuracy: 0.5726\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6437 - accuracy: 0.6117 - val_loss: 0.6411 - val_accuracy: 0.6124\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6425 - accuracy: 0.6136 - val_loss: 0.6799 - val_accuracy: 0.5740\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6419 - accuracy: 0.6151 - val_loss: 0.6658 - val_accuracy: 0.5818\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6424 - accuracy: 0.6139 - val_loss: 0.6583 - val_accuracy: 0.6003\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6406 - accuracy: 0.6154 - val_loss: 0.6390 - val_accuracy: 0.6175\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6397 - accuracy: 0.6177 - val_loss: 0.6445 - val_accuracy: 0.6107\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6383 - accuracy: 0.6196 - val_loss: 0.6327 - val_accuracy: 0.6278\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6374 - accuracy: 0.6205 - val_loss: 0.6303 - val_accuracy: 0.6318\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6373 - accuracy: 0.6210 - val_loss: 0.6723 - val_accuracy: 0.5765\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6381 - accuracy: 0.6202 - val_loss: 0.6701 - val_accuracy: 0.5751\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6416 - accuracy: 0.6162 - val_loss: 0.6388 - val_accuracy: 0.6240\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6368 - accuracy: 0.6212 - val_loss: 0.6348 - val_accuracy: 0.6216\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6394 - accuracy: 0.6203 - val_loss: 0.6359 - val_accuracy: 0.6268\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6541 - accuracy: 0.6017 - val_loss: 0.6632 - val_accuracy: 0.5965\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.5973 - val_loss: 0.6598 - val_accuracy: 0.6003\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6399 - accuracy: 0.6180 - val_loss: 0.6339 - val_accuracy: 0.6236\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6359 - accuracy: 0.6217 - val_loss: 0.6315 - val_accuracy: 0.6293\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6711 - accuracy: 0.5781 - val_loss: 0.6551 - val_accuracy: 0.6043\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6377 - accuracy: 0.6197 - val_loss: 0.6349 - val_accuracy: 0.6214\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6741 - accuracy: 0.5653 - val_loss: 0.6793 - val_accuracy: 0.5579\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.6008 - val_loss: 0.6756 - val_accuracy: 0.5684\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6430 - accuracy: 0.6131 - val_loss: 0.6393 - val_accuracy: 0.6096\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6379 - accuracy: 0.6190 - val_loss: 0.6740 - val_accuracy: 0.5790\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6693 - accuracy: 0.5776 - val_loss: 0.6942 - val_accuracy: 0.5305\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6675 - accuracy: 0.5817 - val_loss: 0.6650 - val_accuracy: 0.5855\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6410 - accuracy: 0.6169 - val_loss: 0.6348 - val_accuracy: 0.6231\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6358 - accuracy: 0.6221 - val_loss: 0.6668 - val_accuracy: 0.5776\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6361 - accuracy: 0.6220 - val_loss: 0.6267 - val_accuracy: 0.6346\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6345 - accuracy: 0.6248 - val_loss: 0.6347 - val_accuracy: 0.6272\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6464 - accuracy: 0.6106 - val_loss: 0.6345 - val_accuracy: 0.6268\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6341 - accuracy: 0.6234 - val_loss: 0.6310 - val_accuracy: 0.6275\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6428 - accuracy: 0.6147 - val_loss: 0.6387 - val_accuracy: 0.6129\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6370 - accuracy: 0.6200 - val_loss: 0.6323 - val_accuracy: 0.6256\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6329 - accuracy: 0.6263 - val_loss: 0.6266 - val_accuracy: 0.6299\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6735 - accuracy: 0.5619 - val_loss: 0.6955 - val_accuracy: 0.5137\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5199 - val_loss: 0.6919 - val_accuracy: 0.5178\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6863 - accuracy: 0.5453 - val_loss: 0.6940 - val_accuracy: 0.5261\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6595 - accuracy: 0.5925 - val_loss: 0.6500 - val_accuracy: 0.6104\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6438 - accuracy: 0.6124 - val_loss: 0.6450 - val_accuracy: 0.6174\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6368 - accuracy: 0.6214 - val_loss: 0.6345 - val_accuracy: 0.6183\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6437 - accuracy: 0.6114 - val_loss: 0.6864 - val_accuracy: 0.5538\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6381 - accuracy: 0.6201 - val_loss: 0.6294 - val_accuracy: 0.6355\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6339 - accuracy: 0.6265 - val_loss: 0.6362 - val_accuracy: 0.6194\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6338 - accuracy: 0.6253 - val_loss: 0.6366 - val_accuracy: 0.6224\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6275 - val_loss: 0.6280 - val_accuracy: 0.6310\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6342 - accuracy: 0.6252 - val_loss: 0.6278 - val_accuracy: 0.6383\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6341 - accuracy: 0.6261 - val_loss: 0.6257 - val_accuracy: 0.6382\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6339 - accuracy: 0.6262 - val_loss: 0.6262 - val_accuracy: 0.6346\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6027 - val_loss: 0.6348 - val_accuracy: 0.6285\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5875566 ]\n",
      " [0.69737273]\n",
      " [0.65545154]\n",
      " [0.48506677]\n",
      " [0.41130918]\n",
      " [0.5828571 ]\n",
      " [0.58446896]\n",
      " [0.5691482 ]\n",
      " [0.69341147]\n",
      " [0.5410796 ]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_30 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_31 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_30 is normal keras bn layer\n",
      "q_activation_30      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_31 is normal keras bn layer\n",
      "q_activation_31      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 1.2090 - accuracy: 0.4997 - val_loss: 0.8801 - val_accuracy: 0.5056\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7838 - accuracy: 0.5005 - val_loss: 0.7439 - val_accuracy: 0.5028\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7359 - accuracy: 0.5011 - val_loss: 0.7239 - val_accuracy: 0.5037\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7229 - accuracy: 0.5011 - val_loss: 0.7116 - val_accuracy: 0.5046\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7271 - accuracy: 0.5022 - val_loss: 0.7286 - val_accuracy: 0.5034\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7186 - accuracy: 0.5024 - val_loss: 0.7156 - val_accuracy: 0.5052\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7106 - accuracy: 0.5026 - val_loss: 0.7036 - val_accuracy: 0.5048\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7037 - accuracy: 0.5037 - val_loss: 0.7012 - val_accuracy: 0.5061\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7014 - accuracy: 0.5038 - val_loss: 0.6993 - val_accuracy: 0.5053\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6996 - accuracy: 0.5042 - val_loss: 0.6975 - val_accuracy: 0.5080\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6980 - accuracy: 0.5047 - val_loss: 0.6967 - val_accuracy: 0.5070\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6970 - accuracy: 0.5061 - val_loss: 0.6958 - val_accuracy: 0.5068\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5057 - val_loss: 0.6951 - val_accuracy: 0.5084\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5059 - val_loss: 0.6948 - val_accuracy: 0.5086\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5069 - val_loss: 0.6945 - val_accuracy: 0.5083\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5064 - val_loss: 0.6943 - val_accuracy: 0.5104\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6945 - accuracy: 0.5084 - val_loss: 0.6942 - val_accuracy: 0.5103\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5076 - val_loss: 0.6939 - val_accuracy: 0.5123\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5093 - val_loss: 0.6940 - val_accuracy: 0.5108\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5091 - val_loss: 0.6940 - val_accuracy: 0.5103\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5063\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5099 - val_loss: 0.6937 - val_accuracy: 0.5123\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5107 - val_loss: 0.6943 - val_accuracy: 0.5040\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5124 - val_loss: 0.6932 - val_accuracy: 0.5118\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5103 - val_loss: 0.6934 - val_accuracy: 0.5136\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5122 - val_loss: 0.6929 - val_accuracy: 0.5103\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5118 - val_loss: 0.6931 - val_accuracy: 0.5114\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5127 - val_loss: 0.6930 - val_accuracy: 0.5099\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6930 - val_accuracy: 0.5078\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5139 - val_loss: 0.6929 - val_accuracy: 0.5123\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6953 - val_accuracy: 0.5030\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5144 - val_loss: 0.6928 - val_accuracy: 0.5137\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5153 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5157 - val_loss: 0.6931 - val_accuracy: 0.5125\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6929 - val_accuracy: 0.5169\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5168 - val_loss: 0.6928 - val_accuracy: 0.5121\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5184 - val_loss: 0.6925 - val_accuracy: 0.5209\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5183 - val_loss: 0.6937 - val_accuracy: 0.5139\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5217 - val_loss: 0.6936 - val_accuracy: 0.5158\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5238 - val_loss: 0.6944 - val_accuracy: 0.5077\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5268 - val_loss: 0.6915 - val_accuracy: 0.5245\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5298 - val_loss: 0.6907 - val_accuracy: 0.5334\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5319 - val_loss: 0.6903 - val_accuracy: 0.5305\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5339 - val_loss: 0.6895 - val_accuracy: 0.5333\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5352 - val_loss: 0.6888 - val_accuracy: 0.5351\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5368 - val_loss: 0.6896 - val_accuracy: 0.5357\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6878 - accuracy: 0.5387 - val_loss: 0.6892 - val_accuracy: 0.5351\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5401 - val_loss: 0.6870 - val_accuracy: 0.5441\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5420 - val_loss: 0.6898 - val_accuracy: 0.5371\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5441 - val_loss: 0.6884 - val_accuracy: 0.5374\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5454 - val_loss: 0.6854 - val_accuracy: 0.5475\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5478 - val_loss: 0.6851 - val_accuracy: 0.5466\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5478 - val_loss: 0.6858 - val_accuracy: 0.5436\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5505 - val_loss: 0.6837 - val_accuracy: 0.5518\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5523 - val_loss: 0.6854 - val_accuracy: 0.5432\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6824 - accuracy: 0.5536 - val_loss: 0.6839 - val_accuracy: 0.5473\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6819 - accuracy: 0.5543 - val_loss: 0.6821 - val_accuracy: 0.5532\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6814 - accuracy: 0.5553 - val_loss: 0.6828 - val_accuracy: 0.5484\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.5570 - val_loss: 0.6824 - val_accuracy: 0.5528\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5598 - val_loss: 0.6815 - val_accuracy: 0.5510\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6789 - accuracy: 0.5592 - val_loss: 0.6768 - val_accuracy: 0.5660\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6773 - accuracy: 0.5622 - val_loss: 0.6791 - val_accuracy: 0.5608\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6760 - accuracy: 0.5636 - val_loss: 0.6764 - val_accuracy: 0.5634\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6763 - accuracy: 0.5634 - val_loss: 0.6838 - val_accuracy: 0.5376\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6747 - accuracy: 0.5645 - val_loss: 0.7184 - val_accuracy: 0.5117\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6747 - accuracy: 0.5644 - val_loss: 0.6933 - val_accuracy: 0.5130\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6723 - accuracy: 0.5689 - val_loss: 0.6695 - val_accuracy: 0.5742\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6691 - accuracy: 0.5742 - val_loss: 0.6719 - val_accuracy: 0.5629\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7016 - accuracy: 0.5134 - val_loss: 0.6997 - val_accuracy: 0.5069\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7005 - accuracy: 0.5072 - val_loss: 0.6989 - val_accuracy: 0.5075\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6999 - accuracy: 0.5072 - val_loss: 0.6998 - val_accuracy: 0.5037\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.5075 - val_loss: 0.7001 - val_accuracy: 0.5063\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6992 - accuracy: 0.5074 - val_loss: 0.6999 - val_accuracy: 0.5113\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6986 - accuracy: 0.5084 - val_loss: 0.6996 - val_accuracy: 0.5061\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6979 - accuracy: 0.5107 - val_loss: 0.6952 - val_accuracy: 0.5180\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.5130 - val_loss: 0.6947 - val_accuracy: 0.5198\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6966 - accuracy: 0.5125 - val_loss: 0.7008 - val_accuracy: 0.5149\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5172 - val_loss: 0.6931 - val_accuracy: 0.5140\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5236 - val_loss: 0.6916 - val_accuracy: 0.5130\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5463 - val_loss: 0.6856 - val_accuracy: 0.5401\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5629 - val_loss: 0.6878 - val_accuracy: 0.5576\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6789 - accuracy: 0.5649 - val_loss: 0.6874 - val_accuracy: 0.5422\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6734 - accuracy: 0.5726 - val_loss: 0.6784 - val_accuracy: 0.5686\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5763 - val_loss: 0.7192 - val_accuracy: 0.5148\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5721 - val_loss: 0.6639 - val_accuracy: 0.5845\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5802 - val_loss: 0.6675 - val_accuracy: 0.5805\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5816 - val_loss: 0.6638 - val_accuracy: 0.5907\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6631 - accuracy: 0.5851 - val_loss: 0.6674 - val_accuracy: 0.5760\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6601 - accuracy: 0.5889 - val_loss: 0.6621 - val_accuracy: 0.5884\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6588 - accuracy: 0.5921 - val_loss: 0.6556 - val_accuracy: 0.5940\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6598 - accuracy: 0.5894 - val_loss: 0.6610 - val_accuracy: 0.5910\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6618 - accuracy: 0.5878 - val_loss: 0.6791 - val_accuracy: 0.5652\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.5904 - val_loss: 0.6534 - val_accuracy: 0.6033\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6556 - accuracy: 0.5959 - val_loss: 0.6516 - val_accuracy: 0.6009\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6567 - accuracy: 0.5937 - val_loss: 0.6642 - val_accuracy: 0.5875\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5690 - val_loss: 0.6574 - val_accuracy: 0.5919\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6555 - accuracy: 0.5957 - val_loss: 0.6570 - val_accuracy: 0.5934\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.5970 - val_loss: 0.6488 - val_accuracy: 0.6058\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6819 - accuracy: 0.5469 - val_loss: 0.6965 - val_accuracy: 0.5096\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_30\n",
      "cannot prune layer q_activation_30\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_31\n",
      "cannot prune layer q_activation_31\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 8s 7ms/step - loss: 0.6816 - accuracy: 0.5459 - val_loss: 0.6884 - val_accuracy: 0.5373\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6630 - accuracy: 0.5819 - val_loss: 0.6881 - val_accuracy: 0.5294\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6613 - accuracy: 0.5860 - val_loss: 0.6741 - val_accuracy: 0.5654\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6584 - accuracy: 0.5900 - val_loss: 0.6867 - val_accuracy: 0.5706\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6581 - accuracy: 0.5903 - val_loss: 0.7160 - val_accuracy: 0.5282\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6711 - accuracy: 0.5716 - val_loss: 0.6866 - val_accuracy: 0.5587\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6602 - accuracy: 0.5871 - val_loss: 0.7054 - val_accuracy: 0.5544\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6558 - accuracy: 0.5942 - val_loss: 0.6518 - val_accuracy: 0.5996\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6546 - accuracy: 0.5960 - val_loss: 0.6530 - val_accuracy: 0.5976\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.5966 - val_loss: 0.6494 - val_accuracy: 0.6026\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5311 - val_loss: 0.6823 - val_accuracy: 0.5466\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6626 - accuracy: 0.5850 - val_loss: 0.6539 - val_accuracy: 0.5990\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6533 - accuracy: 0.5978 - val_loss: 0.6518 - val_accuracy: 0.5983\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6521 - accuracy: 0.5990 - val_loss: 0.6543 - val_accuracy: 0.5928\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.6001 - val_loss: 0.6487 - val_accuracy: 0.6001\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6689 - accuracy: 0.5710 - val_loss: 0.6909 - val_accuracy: 0.5333\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6712 - accuracy: 0.5672 - val_loss: 0.6638 - val_accuracy: 0.5825\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6535 - accuracy: 0.5977 - val_loss: 0.6502 - val_accuracy: 0.6015\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6513 - accuracy: 0.6013 - val_loss: 0.6525 - val_accuracy: 0.5991\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6506 - accuracy: 0.6022 - val_loss: 0.6489 - val_accuracy: 0.6023\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6514 - accuracy: 0.6014 - val_loss: 0.6545 - val_accuracy: 0.5934\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6494 - accuracy: 0.6037 - val_loss: 0.6444 - val_accuracy: 0.6102\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6510 - accuracy: 0.6022 - val_loss: 0.6470 - val_accuracy: 0.6115\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6490 - accuracy: 0.6047 - val_loss: 0.6454 - val_accuracy: 0.6081\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6606 - accuracy: 0.5907 - val_loss: 0.6574 - val_accuracy: 0.5951\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6508 - accuracy: 0.6025 - val_loss: 0.6445 - val_accuracy: 0.6100\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6480 - accuracy: 0.6076 - val_loss: 0.6436 - val_accuracy: 0.6135\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6476 - accuracy: 0.6067 - val_loss: 0.6432 - val_accuracy: 0.6139\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6756 - accuracy: 0.5632 - val_loss: 0.6985 - val_accuracy: 0.5106\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6940 - accuracy: 0.5140 - val_loss: 0.6933 - val_accuracy: 0.5144\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5186 - val_loss: 0.6929 - val_accuracy: 0.5172\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5207 - val_loss: 0.6918 - val_accuracy: 0.5205\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5262 - val_loss: 0.6907 - val_accuracy: 0.5277\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6883 - accuracy: 0.5360 - val_loss: 0.6955 - val_accuracy: 0.5144\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6651 - accuracy: 0.5857 - val_loss: 0.6611 - val_accuracy: 0.5814\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6502 - accuracy: 0.6055 - val_loss: 0.6453 - val_accuracy: 0.6114\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6483 - accuracy: 0.6073 - val_loss: 0.6591 - val_accuracy: 0.5899\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6478 - accuracy: 0.6083 - val_loss: 0.6529 - val_accuracy: 0.6085\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6470 - accuracy: 0.6090 - val_loss: 0.6428 - val_accuracy: 0.6131\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6464 - accuracy: 0.6095 - val_loss: 0.6451 - val_accuracy: 0.6081\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6464 - accuracy: 0.6095 - val_loss: 0.6401 - val_accuracy: 0.6196\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6454 - accuracy: 0.6116 - val_loss: 0.6445 - val_accuracy: 0.6130\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6446 - accuracy: 0.6119 - val_loss: 0.6398 - val_accuracy: 0.6178\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6483 - accuracy: 0.6086 - val_loss: 0.6641 - val_accuracy: 0.5705\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6496 - accuracy: 0.6080 - val_loss: 0.6646 - val_accuracy: 0.5835\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6470 - accuracy: 0.6101 - val_loss: 0.6431 - val_accuracy: 0.6159\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6460 - accuracy: 0.6118 - val_loss: 0.6398 - val_accuracy: 0.6203\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6429 - accuracy: 0.6149 - val_loss: 0.6421 - val_accuracy: 0.6140\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6433 - accuracy: 0.6150 - val_loss: 0.6407 - val_accuracy: 0.6158\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6449 - accuracy: 0.6129 - val_loss: 0.6437 - val_accuracy: 0.6140\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.49328315]\n",
      " [0.66843396]\n",
      " [0.62048703]\n",
      " [0.5466822 ]\n",
      " [0.5153217 ]\n",
      " [0.4304878 ]\n",
      " [0.64211905]\n",
      " [0.4411059 ]\n",
      " [0.6458488 ]\n",
      " [0.51557446]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_32 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_33 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_32 is normal keras bn layer\n",
      "q_activation_32      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_33 is normal keras bn layer\n",
      "q_activation_33      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 2.0706 - accuracy: 0.5011 - val_loss: 1.1122 - val_accuracy: 0.5024\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7446 - accuracy: 0.5012 - val_loss: 0.7215 - val_accuracy: 0.5040\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7160 - accuracy: 0.5033 - val_loss: 0.7187 - val_accuracy: 0.5028\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7054 - accuracy: 0.5039 - val_loss: 0.7002 - val_accuracy: 0.5027\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6987 - accuracy: 0.5039 - val_loss: 0.6979 - val_accuracy: 0.5022\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6972 - accuracy: 0.5048 - val_loss: 0.6970 - val_accuracy: 0.5025\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.5055 - val_loss: 0.6963 - val_accuracy: 0.5023\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6955 - accuracy: 0.5067 - val_loss: 0.6959 - val_accuracy: 0.5010\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5080 - val_loss: 0.6953 - val_accuracy: 0.5034\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5086 - val_loss: 0.6950 - val_accuracy: 0.5018\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6938 - accuracy: 0.5093 - val_loss: 0.6949 - val_accuracy: 0.5024\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5110 - val_loss: 0.6947 - val_accuracy: 0.5061\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5103 - val_loss: 0.6942 - val_accuracy: 0.5046\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6943 - val_accuracy: 0.5064\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6940 - val_accuracy: 0.5068\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5109 - val_loss: 0.6937 - val_accuracy: 0.5091\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5123 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.5098\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5136 - val_loss: 0.6938 - val_accuracy: 0.5075\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5128 - val_loss: 0.6936 - val_accuracy: 0.5116\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5138\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5147 - val_loss: 0.6929 - val_accuracy: 0.5143\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5144 - val_loss: 0.6931 - val_accuracy: 0.5114\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6933 - val_accuracy: 0.5115\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6926 - val_accuracy: 0.5144\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5166 - val_loss: 0.6930 - val_accuracy: 0.5127\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5152 - val_loss: 0.6930 - val_accuracy: 0.5118\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6928 - val_accuracy: 0.5168\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5176 - val_loss: 0.6930 - val_accuracy: 0.5149\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5183 - val_loss: 0.6926 - val_accuracy: 0.5176\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5175\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5194 - val_loss: 0.6928 - val_accuracy: 0.5127\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5193 - val_loss: 0.6926 - val_accuracy: 0.5134\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5223 - val_loss: 0.6928 - val_accuracy: 0.5177\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5215 - val_loss: 0.6924 - val_accuracy: 0.5086\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5232 - val_loss: 0.6922 - val_accuracy: 0.5236\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5237 - val_loss: 0.6919 - val_accuracy: 0.5216\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5261 - val_loss: 0.6906 - val_accuracy: 0.5303\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5274 - val_loss: 0.6902 - val_accuracy: 0.5205\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6884 - accuracy: 0.5314 - val_loss: 0.6899 - val_accuracy: 0.5268\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5360 - val_loss: 0.6892 - val_accuracy: 0.5283\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5406 - val_loss: 0.6915 - val_accuracy: 0.5205\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5462 - val_loss: 0.6900 - val_accuracy: 0.5329\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5531 - val_loss: 0.6821 - val_accuracy: 0.5512\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5577 - val_loss: 0.6816 - val_accuracy: 0.5509\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6776 - accuracy: 0.5618 - val_loss: 0.6766 - val_accuracy: 0.5669\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6759 - accuracy: 0.5641 - val_loss: 0.6793 - val_accuracy: 0.5594\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6740 - accuracy: 0.5674 - val_loss: 0.6708 - val_accuracy: 0.5761\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5731 - val_loss: 0.6750 - val_accuracy: 0.5717\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.5771 - val_loss: 0.6698 - val_accuracy: 0.5707\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5796 - val_loss: 0.6716 - val_accuracy: 0.5748\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6639 - accuracy: 0.5837 - val_loss: 0.6655 - val_accuracy: 0.5779\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6635 - accuracy: 0.5849 - val_loss: 0.6613 - val_accuracy: 0.5854\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6630 - accuracy: 0.5860 - val_loss: 0.6610 - val_accuracy: 0.5886\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5919 - val_loss: 0.6628 - val_accuracy: 0.5883\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6600 - accuracy: 0.5905 - val_loss: 0.6610 - val_accuracy: 0.5862\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5905 - val_loss: 0.6724 - val_accuracy: 0.5729\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6564 - accuracy: 0.5954 - val_loss: 0.6522 - val_accuracy: 0.6028\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6557 - accuracy: 0.5966 - val_loss: 0.6647 - val_accuracy: 0.5853\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6014 - val_loss: 0.6485 - val_accuracy: 0.6071\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6500 - accuracy: 0.6045 - val_loss: 0.6502 - val_accuracy: 0.6055\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6490 - accuracy: 0.6062 - val_loss: 0.6446 - val_accuracy: 0.6104\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6643 - accuracy: 0.5857 - val_loss: 0.6684 - val_accuracy: 0.5717\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6474 - accuracy: 0.6087 - val_loss: 0.6423 - val_accuracy: 0.6159\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6452 - accuracy: 0.6113 - val_loss: 0.6409 - val_accuracy: 0.6177\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6444 - accuracy: 0.6115 - val_loss: 0.6532 - val_accuracy: 0.6042\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6446 - accuracy: 0.6121 - val_loss: 0.6377 - val_accuracy: 0.6189\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6442 - accuracy: 0.6128 - val_loss: 0.6409 - val_accuracy: 0.6177\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6427 - accuracy: 0.6144 - val_loss: 0.6459 - val_accuracy: 0.6099\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6403 - accuracy: 0.6172 - val_loss: 0.6395 - val_accuracy: 0.6195\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6407 - accuracy: 0.6169 - val_loss: 0.6400 - val_accuracy: 0.6224\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6397 - accuracy: 0.6180 - val_loss: 0.6425 - val_accuracy: 0.6098\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6397 - accuracy: 0.6176 - val_loss: 0.6356 - val_accuracy: 0.6195\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6689 - accuracy: 0.5752 - val_loss: 0.6847 - val_accuracy: 0.5458\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6631 - accuracy: 0.5876 - val_loss: 0.6571 - val_accuracy: 0.5980\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6398 - accuracy: 0.6189 - val_loss: 0.6407 - val_accuracy: 0.6176\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6704 - accuracy: 0.5767 - val_loss: 0.7014 - val_accuracy: 0.5095\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6949 - accuracy: 0.5123 - val_loss: 0.6935 - val_accuracy: 0.5163\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5203 - val_loss: 0.6915 - val_accuracy: 0.5241\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5391 - val_loss: 0.6823 - val_accuracy: 0.5460\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6678 - accuracy: 0.5845 - val_loss: 0.6640 - val_accuracy: 0.5925\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6534 - accuracy: 0.6063 - val_loss: 0.6516 - val_accuracy: 0.6126\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6454 - accuracy: 0.6139 - val_loss: 0.6458 - val_accuracy: 0.6089\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6803 - accuracy: 0.5577 - val_loss: 0.6843 - val_accuracy: 0.5430\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6548 - accuracy: 0.5983 - val_loss: 0.6406 - val_accuracy: 0.6187\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6414 - accuracy: 0.6161 - val_loss: 0.6374 - val_accuracy: 0.6239\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6380 - accuracy: 0.6197 - val_loss: 0.6304 - val_accuracy: 0.6325\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6367 - accuracy: 0.6225 - val_loss: 0.6518 - val_accuracy: 0.5976\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6382 - accuracy: 0.6213 - val_loss: 0.6326 - val_accuracy: 0.6249\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6361 - accuracy: 0.6244 - val_loss: 0.6263 - val_accuracy: 0.6367\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6332 - accuracy: 0.6271 - val_loss: 0.6375 - val_accuracy: 0.6198\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6334 - accuracy: 0.6267 - val_loss: 0.6279 - val_accuracy: 0.6329\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6327 - accuracy: 0.6277 - val_loss: 0.6324 - val_accuracy: 0.6285\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6505 - accuracy: 0.6036 - val_loss: 0.7128 - val_accuracy: 0.4987\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5092 - val_loss: 0.6941 - val_accuracy: 0.5182\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5176 - val_loss: 0.6913 - val_accuracy: 0.5240\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5265 - val_loss: 0.6892 - val_accuracy: 0.5351\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6871 - accuracy: 0.5390 - val_loss: 0.6872 - val_accuracy: 0.5365\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5741 - val_loss: 0.6888 - val_accuracy: 0.5532\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6490 - accuracy: 0.6088 - val_loss: 0.6496 - val_accuracy: 0.6072\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6395 - accuracy: 0.6212 - val_loss: 0.6526 - val_accuracy: 0.6093\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6348 - accuracy: 0.6266 - val_loss: 0.6277 - val_accuracy: 0.6365\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6342 - accuracy: 0.6265 - val_loss: 0.6303 - val_accuracy: 0.6330\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6480 - accuracy: 0.6071 - val_loss: 0.7152 - val_accuracy: 0.4967\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6955 - accuracy: 0.5057 - val_loss: 0.6936 - val_accuracy: 0.5132\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5128 - val_loss: 0.6925 - val_accuracy: 0.5106\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5164 - val_loss: 0.6918 - val_accuracy: 0.5165\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5200 - val_loss: 0.6909 - val_accuracy: 0.5245\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5300 - val_loss: 0.6898 - val_accuracy: 0.5181\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5576 - val_loss: 0.6807 - val_accuracy: 0.5480\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_32\n",
      "cannot prune layer q_activation_32\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_33\n",
      "cannot prune layer q_activation_33\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6525 - accuracy: 0.6028 - val_loss: 0.6735 - val_accuracy: 0.5756\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6466 - accuracy: 0.6109 - val_loss: 0.6929 - val_accuracy: 0.5632\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6359 - accuracy: 0.6246 - val_loss: 0.6944 - val_accuracy: 0.5617\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6385 - accuracy: 0.6211 - val_loss: 0.6651 - val_accuracy: 0.5946\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6336 - accuracy: 0.6277 - val_loss: 0.6733 - val_accuracy: 0.5842\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6328 - accuracy: 0.6285 - val_loss: 0.6757 - val_accuracy: 0.5920\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6327 - accuracy: 0.6276 - val_loss: 0.6364 - val_accuracy: 0.6218\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6326 - accuracy: 0.6285 - val_loss: 0.6345 - val_accuracy: 0.6318\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6349 - accuracy: 0.6252 - val_loss: 0.6268 - val_accuracy: 0.6379\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6308 - accuracy: 0.6301 - val_loss: 0.6262 - val_accuracy: 0.6374\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6305 - accuracy: 0.6310 - val_loss: 0.6240 - val_accuracy: 0.6366\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6310 - val_loss: 0.6226 - val_accuracy: 0.6424\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6297 - accuracy: 0.6315 - val_loss: 0.6242 - val_accuracy: 0.6391\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6304 - accuracy: 0.6321 - val_loss: 0.6244 - val_accuracy: 0.6408\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6293 - accuracy: 0.6331 - val_loss: 0.6222 - val_accuracy: 0.6417\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6280 - accuracy: 0.6339 - val_loss: 0.6222 - val_accuracy: 0.6417\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6382 - accuracy: 0.6232 - val_loss: 0.6552 - val_accuracy: 0.5992\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6364 - accuracy: 0.6245 - val_loss: 0.6267 - val_accuracy: 0.6384\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6326 - accuracy: 0.6283 - val_loss: 0.6259 - val_accuracy: 0.6389\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6445 - accuracy: 0.6128 - val_loss: 0.6564 - val_accuracy: 0.5964\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6308 - accuracy: 0.6310 - val_loss: 0.6332 - val_accuracy: 0.6312\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6314 - val_loss: 0.6239 - val_accuracy: 0.6385\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.6353 - val_loss: 0.6281 - val_accuracy: 0.6325\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6379 - accuracy: 0.6245 - val_loss: 0.7952 - val_accuracy: 0.4940\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6640 - accuracy: 0.5868 - val_loss: 0.6554 - val_accuracy: 0.5991\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6323 - val_loss: 0.6276 - val_accuracy: 0.6362\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6270 - accuracy: 0.6356 - val_loss: 0.6218 - val_accuracy: 0.6406\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.6366 - val_loss: 0.6215 - val_accuracy: 0.6435\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6267 - accuracy: 0.6370 - val_loss: 0.6250 - val_accuracy: 0.6370\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6258 - accuracy: 0.6362 - val_loss: 0.6276 - val_accuracy: 0.6331\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6459 - accuracy: 0.6126 - val_loss: 0.7081 - val_accuracy: 0.5316\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6832 - accuracy: 0.5490 - val_loss: 0.6743 - val_accuracy: 0.5619\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6629 - accuracy: 0.5837 - val_loss: 0.6607 - val_accuracy: 0.5864\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6444 - accuracy: 0.6152 - val_loss: 0.6352 - val_accuracy: 0.6306\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6369 - accuracy: 0.6246 - val_loss: 0.6321 - val_accuracy: 0.6335\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6353 - accuracy: 0.6269 - val_loss: 0.6270 - val_accuracy: 0.6376\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6319 - accuracy: 0.6305 - val_loss: 0.6256 - val_accuracy: 0.6420\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6289 - accuracy: 0.6344 - val_loss: 0.6226 - val_accuracy: 0.6427\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6265 - accuracy: 0.6370 - val_loss: 0.6303 - val_accuracy: 0.6337\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6254 - accuracy: 0.6374 - val_loss: 0.6230 - val_accuracy: 0.6431\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6261 - accuracy: 0.6365 - val_loss: 0.6224 - val_accuracy: 0.6400\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6320 - accuracy: 0.6296 - val_loss: 0.6352 - val_accuracy: 0.6251\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.6366 - val_loss: 0.6240 - val_accuracy: 0.6384\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6245 - accuracy: 0.6388 - val_loss: 0.6168 - val_accuracy: 0.6475\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6255 - accuracy: 0.6377 - val_loss: 0.6160 - val_accuracy: 0.6510\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6243 - accuracy: 0.6380 - val_loss: 0.6263 - val_accuracy: 0.6358\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6250 - accuracy: 0.6371 - val_loss: 0.6194 - val_accuracy: 0.6441\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6245 - accuracy: 0.6377 - val_loss: 0.6190 - val_accuracy: 0.6440\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6251 - accuracy: 0.6377 - val_loss: 0.6176 - val_accuracy: 0.6462\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6305 - accuracy: 0.6325 - val_loss: 0.6217 - val_accuracy: 0.6449\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.55482876]\n",
      " [0.59929675]\n",
      " [0.61627674]\n",
      " [0.64574015]\n",
      " [0.71000576]\n",
      " [0.4455624 ]\n",
      " [0.6686183 ]\n",
      " [0.4733197 ]\n",
      " [0.6041224 ]\n",
      " [0.22041702]]\n",
      "########### FOUND NEW BEST METRIC 0.4101397583345465 ##############\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_34 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_35 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_34 is normal keras bn layer\n",
      "q_activation_34      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_35 is normal keras bn layer\n",
      "q_activation_35      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 1.2723 - accuracy: 0.5020 - val_loss: 0.8170 - val_accuracy: 0.5022\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7765 - accuracy: 0.5019 - val_loss: 0.7560 - val_accuracy: 0.5030\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7446 - accuracy: 0.5007 - val_loss: 0.7339 - val_accuracy: 0.5015\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7280 - accuracy: 0.5019 - val_loss: 0.7228 - val_accuracy: 0.5045\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7198 - accuracy: 0.5022 - val_loss: 0.7133 - val_accuracy: 0.5057\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7175 - accuracy: 0.5029 - val_loss: 0.7145 - val_accuracy: 0.5053\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7128 - accuracy: 0.5036 - val_loss: 0.7114 - val_accuracy: 0.5034\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7092 - accuracy: 0.5022 - val_loss: 0.7063 - val_accuracy: 0.5065\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7057 - accuracy: 0.5037 - val_loss: 0.7043 - val_accuracy: 0.5052\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7036 - accuracy: 0.5036 - val_loss: 0.7028 - val_accuracy: 0.5075\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7020 - accuracy: 0.5036 - val_loss: 0.7014 - val_accuracy: 0.5083\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7006 - accuracy: 0.5031 - val_loss: 0.7004 - val_accuracy: 0.5084\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6996 - accuracy: 0.5038 - val_loss: 0.6995 - val_accuracy: 0.5078\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6988 - accuracy: 0.5038 - val_loss: 0.6987 - val_accuracy: 0.5059\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6980 - accuracy: 0.5037 - val_loss: 0.6979 - val_accuracy: 0.5068\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6971 - accuracy: 0.5054 - val_loss: 0.6975 - val_accuracy: 0.5041\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5061 - val_loss: 0.6970 - val_accuracy: 0.5064\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5063 - val_loss: 0.6965 - val_accuracy: 0.5027\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5071 - val_loss: 0.6957 - val_accuracy: 0.5042\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6950 - accuracy: 0.5062 - val_loss: 0.6952 - val_accuracy: 0.5086\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5084 - val_loss: 0.6998 - val_accuracy: 0.5017\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5080 - val_loss: 0.6985 - val_accuracy: 0.5027\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6939 - accuracy: 0.5086 - val_loss: 0.6943 - val_accuracy: 0.5087\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5096 - val_loss: 0.6946 - val_accuracy: 0.5073\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5102 - val_loss: 0.6945 - val_accuracy: 0.5044\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5109 - val_loss: 0.6939 - val_accuracy: 0.5080\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5110 - val_loss: 0.6940 - val_accuracy: 0.5098\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5111 - val_loss: 0.6936 - val_accuracy: 0.5094\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5125 - val_loss: 0.6939 - val_accuracy: 0.5045\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6939 - val_accuracy: 0.5046\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5128 - val_loss: 0.6934 - val_accuracy: 0.5132\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6934 - val_accuracy: 0.5097\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6936 - val_accuracy: 0.5042\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5134 - val_loss: 0.6933 - val_accuracy: 0.5061\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5144 - val_loss: 0.6933 - val_accuracy: 0.5078\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5151 - val_loss: 0.6934 - val_accuracy: 0.5060\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5159 - val_loss: 0.6932 - val_accuracy: 0.5084\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5156 - val_loss: 0.6928 - val_accuracy: 0.5125\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5173 - val_loss: 0.6929 - val_accuracy: 0.5151\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5171 - val_loss: 0.6928 - val_accuracy: 0.5130\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5178 - val_loss: 0.6933 - val_accuracy: 0.5094\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6927 - val_accuracy: 0.5173\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5189 - val_loss: 0.6927 - val_accuracy: 0.5116\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5216 - val_loss: 0.6923 - val_accuracy: 0.5176\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5221 - val_loss: 0.6925 - val_accuracy: 0.5164\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6935 - val_accuracy: 0.5091\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5241 - val_loss: 0.6918 - val_accuracy: 0.5219\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5247 - val_loss: 0.6916 - val_accuracy: 0.5250\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6900 - accuracy: 0.5268 - val_loss: 0.6918 - val_accuracy: 0.5224\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5273 - val_loss: 0.6914 - val_accuracy: 0.5233\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5278 - val_loss: 0.6914 - val_accuracy: 0.5256\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5290 - val_loss: 0.6918 - val_accuracy: 0.5226\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5289 - val_loss: 0.6911 - val_accuracy: 0.5247\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5291 - val_loss: 0.6909 - val_accuracy: 0.5250\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5309 - val_loss: 0.6904 - val_accuracy: 0.5285\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6883 - accuracy: 0.5334 - val_loss: 0.6900 - val_accuracy: 0.5284\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5324 - val_loss: 0.6905 - val_accuracy: 0.5280\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5338 - val_loss: 0.6898 - val_accuracy: 0.5320\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6878 - accuracy: 0.5339 - val_loss: 0.7129 - val_accuracy: 0.5058\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5348 - val_loss: 0.6895 - val_accuracy: 0.5332\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6873 - accuracy: 0.5369 - val_loss: 0.6890 - val_accuracy: 0.5309\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5373 - val_loss: 0.6899 - val_accuracy: 0.5294\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5369 - val_loss: 0.6907 - val_accuracy: 0.5326\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5391 - val_loss: 0.6885 - val_accuracy: 0.5365\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5404 - val_loss: 0.6889 - val_accuracy: 0.5363\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5410 - val_loss: 0.6879 - val_accuracy: 0.5403\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5428 - val_loss: 0.6878 - val_accuracy: 0.5424\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5434 - val_loss: 0.6876 - val_accuracy: 0.5384\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5450 - val_loss: 0.6862 - val_accuracy: 0.5462\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5452 - val_loss: 0.6862 - val_accuracy: 0.5451\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5480 - val_loss: 0.6850 - val_accuracy: 0.5510\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.5503 - val_loss: 0.6844 - val_accuracy: 0.5509\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6819 - accuracy: 0.5513 - val_loss: 0.6838 - val_accuracy: 0.5548\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.5534 - val_loss: 0.6824 - val_accuracy: 0.5555\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6807 - accuracy: 0.5557 - val_loss: 0.6823 - val_accuracy: 0.5585\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6799 - accuracy: 0.5577 - val_loss: 0.6827 - val_accuracy: 0.5594\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6791 - accuracy: 0.5595 - val_loss: 0.6797 - val_accuracy: 0.5628\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6782 - accuracy: 0.5603 - val_loss: 0.6798 - val_accuracy: 0.5624\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6774 - accuracy: 0.5636 - val_loss: 0.6792 - val_accuracy: 0.5645\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6763 - accuracy: 0.5644 - val_loss: 0.6790 - val_accuracy: 0.5582\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6757 - accuracy: 0.5663 - val_loss: 0.6794 - val_accuracy: 0.5604\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5683 - val_loss: 0.6765 - val_accuracy: 0.5707\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.5697 - val_loss: 0.6765 - val_accuracy: 0.5666\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5712 - val_loss: 0.6746 - val_accuracy: 0.5651\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6783 - accuracy: 0.5631 - val_loss: 0.6954 - val_accuracy: 0.5384\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5474 - val_loss: 0.6825 - val_accuracy: 0.5559\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.5650 - val_loss: 0.6760 - val_accuracy: 0.5667\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6732 - accuracy: 0.5714 - val_loss: 0.6725 - val_accuracy: 0.5737\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6716 - accuracy: 0.5748 - val_loss: 0.6723 - val_accuracy: 0.5768\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6707 - accuracy: 0.5759 - val_loss: 0.6706 - val_accuracy: 0.5754\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6699 - accuracy: 0.5771 - val_loss: 0.6701 - val_accuracy: 0.5763\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6688 - accuracy: 0.5789 - val_loss: 0.6709 - val_accuracy: 0.5728\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6684 - accuracy: 0.5795 - val_loss: 0.6711 - val_accuracy: 0.5751\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5811 - val_loss: 0.6716 - val_accuracy: 0.5778\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6689 - accuracy: 0.5807 - val_loss: 0.7349 - val_accuracy: 0.5371\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5721 - val_loss: 0.6743 - val_accuracy: 0.5712\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.6679 - accuracy: 0.5813 - val_loss: 0.6680 - val_accuracy: 0.5817\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6657 - accuracy: 0.5844 - val_loss: 0.6677 - val_accuracy: 0.5849\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6645 - accuracy: 0.5858 - val_loss: 0.6654 - val_accuracy: 0.5889\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6648 - accuracy: 0.5857 - val_loss: 0.6637 - val_accuracy: 0.5899\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_34\n",
      "cannot prune layer q_activation_34\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_35\n",
      "cannot prune layer q_activation_35\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6640 - accuracy: 0.5873 - val_loss: 0.6750 - val_accuracy: 0.5726\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6669 - accuracy: 0.5825 - val_loss: 0.6997 - val_accuracy: 0.5376\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6656 - accuracy: 0.5836 - val_loss: 0.6820 - val_accuracy: 0.5627\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6648 - accuracy: 0.5854 - val_loss: 0.6970 - val_accuracy: 0.5412\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6654 - accuracy: 0.5864 - val_loss: 0.7010 - val_accuracy: 0.5315\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6647 - accuracy: 0.5868 - val_loss: 0.6798 - val_accuracy: 0.5589\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6645 - accuracy: 0.5874 - val_loss: 0.6798 - val_accuracy: 0.5751\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6667 - accuracy: 0.5840 - val_loss: 0.6841 - val_accuracy: 0.5691\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5854 - val_loss: 0.6641 - val_accuracy: 0.5862\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6631 - accuracy: 0.5896 - val_loss: 0.6616 - val_accuracy: 0.5901\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6623 - accuracy: 0.5909 - val_loss: 0.6622 - val_accuracy: 0.5911\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6619 - accuracy: 0.5914 - val_loss: 0.6639 - val_accuracy: 0.5891\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6604 - accuracy: 0.5940 - val_loss: 0.6590 - val_accuracy: 0.5958\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6602 - accuracy: 0.5943 - val_loss: 0.6589 - val_accuracy: 0.5949\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6598 - accuracy: 0.5944 - val_loss: 0.6582 - val_accuracy: 0.5969\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6594 - accuracy: 0.5935 - val_loss: 0.6578 - val_accuracy: 0.5970\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6587 - accuracy: 0.5958 - val_loss: 0.6569 - val_accuracy: 0.5991\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6577 - accuracy: 0.5962 - val_loss: 0.6585 - val_accuracy: 0.5974\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6570 - accuracy: 0.5982 - val_loss: 0.6594 - val_accuracy: 0.5935\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6569 - accuracy: 0.5975 - val_loss: 0.6549 - val_accuracy: 0.6013\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6556 - accuracy: 0.5991 - val_loss: 0.6595 - val_accuracy: 0.5962\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6551 - accuracy: 0.5997 - val_loss: 0.6562 - val_accuracy: 0.6020\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.5999 - val_loss: 0.6564 - val_accuracy: 0.6002\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6550 - accuracy: 0.6002 - val_loss: 0.6532 - val_accuracy: 0.6043\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6548 - accuracy: 0.6006 - val_loss: 0.6547 - val_accuracy: 0.6016\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6541 - accuracy: 0.6024 - val_loss: 0.6531 - val_accuracy: 0.6097\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6548 - accuracy: 0.6016 - val_loss: 0.6509 - val_accuracy: 0.6097\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6528 - accuracy: 0.6037 - val_loss: 0.6516 - val_accuracy: 0.6069\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6525 - accuracy: 0.6043 - val_loss: 0.6534 - val_accuracy: 0.6049\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6521 - accuracy: 0.6053 - val_loss: 0.6508 - val_accuracy: 0.6098\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.6054 - val_loss: 0.6508 - val_accuracy: 0.6121\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6026 - val_loss: 0.6522 - val_accuracy: 0.6089\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6512 - accuracy: 0.6060 - val_loss: 0.6497 - val_accuracy: 0.6115\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6511 - accuracy: 0.6070 - val_loss: 0.6507 - val_accuracy: 0.6140\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6504 - accuracy: 0.6073 - val_loss: 0.6519 - val_accuracy: 0.6087\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6508 - accuracy: 0.6076 - val_loss: 0.6499 - val_accuracy: 0.6138\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6498 - accuracy: 0.6074 - val_loss: 0.6490 - val_accuracy: 0.6130\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6503 - accuracy: 0.6072 - val_loss: 0.6492 - val_accuracy: 0.6094\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6497 - accuracy: 0.6081 - val_loss: 0.6467 - val_accuracy: 0.6144\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6493 - accuracy: 0.6084 - val_loss: 0.6487 - val_accuracy: 0.6132\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6485 - accuracy: 0.6092 - val_loss: 0.6468 - val_accuracy: 0.6161\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6483 - accuracy: 0.6099 - val_loss: 0.6485 - val_accuracy: 0.6128\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6482 - accuracy: 0.6102 - val_loss: 0.6532 - val_accuracy: 0.6077\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6484 - accuracy: 0.6095 - val_loss: 0.6460 - val_accuracy: 0.6180\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6481 - accuracy: 0.6107 - val_loss: 0.6459 - val_accuracy: 0.6183\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6480 - accuracy: 0.6104 - val_loss: 0.6497 - val_accuracy: 0.6125\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6471 - accuracy: 0.6118 - val_loss: 0.6486 - val_accuracy: 0.6165\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6479 - accuracy: 0.6107 - val_loss: 0.6456 - val_accuracy: 0.6185\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6468 - accuracy: 0.6120 - val_loss: 0.6446 - val_accuracy: 0.6219\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6462 - accuracy: 0.6127 - val_loss: 0.6450 - val_accuracy: 0.6195\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.55878615]\n",
      " [0.5112261 ]\n",
      " [0.5230509 ]\n",
      " [0.56041914]\n",
      " [0.7452102 ]\n",
      " [0.3879968 ]\n",
      " [0.51623297]\n",
      " [0.6167935 ]\n",
      " [0.53936476]\n",
      " [0.4704292 ]]\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24, 12], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 150, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_36 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_37 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_36 is normal keras bn layer\n",
      "q_activation_36      quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_37 is normal keras bn layer\n",
      "q_activation_37      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [1 1 1 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 4.8034 - accuracy: 0.5000 - val_loss: 3.8401 - val_accuracy: 0.4980\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 2.6192 - accuracy: 0.5009 - val_loss: 2.2006 - val_accuracy: 0.4958\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 2.0608 - accuracy: 0.5009 - val_loss: 1.7311 - val_accuracy: 0.4998\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.6456 - accuracy: 0.5011 - val_loss: 1.4628 - val_accuracy: 0.5035\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.3038 - accuracy: 0.5008 - val_loss: 1.1323 - val_accuracy: 0.5030\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0082 - accuracy: 0.5020 - val_loss: 0.8865 - val_accuracy: 0.5038\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7786 - accuracy: 0.5027 - val_loss: 0.7358 - val_accuracy: 0.5027\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7274 - accuracy: 0.5026 - val_loss: 0.7202 - val_accuracy: 0.5027\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7147 - accuracy: 0.5029 - val_loss: 0.7083 - val_accuracy: 0.5046\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7059 - accuracy: 0.5038 - val_loss: 0.7045 - val_accuracy: 0.5072\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7027 - accuracy: 0.5038 - val_loss: 0.7023 - val_accuracy: 0.5062\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7006 - accuracy: 0.5049 - val_loss: 0.7000 - val_accuracy: 0.5059\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6989 - accuracy: 0.5056 - val_loss: 0.6985 - val_accuracy: 0.5058\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6977 - accuracy: 0.5057 - val_loss: 0.6976 - val_accuracy: 0.5062\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5055 - val_loss: 0.6967 - val_accuracy: 0.5055\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5070 - val_loss: 0.6960 - val_accuracy: 0.5055\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5075 - val_loss: 0.6955 - val_accuracy: 0.5062\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5076 - val_loss: 0.6951 - val_accuracy: 0.5063\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5090 - val_loss: 0.6949 - val_accuracy: 0.5068\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5082 - val_loss: 0.6946 - val_accuracy: 0.5050\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6937 - accuracy: 0.5093 - val_loss: 0.6945 - val_accuracy: 0.5063\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5093 - val_loss: 0.6941 - val_accuracy: 0.5075\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5104 - val_loss: 0.6942 - val_accuracy: 0.5070\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5110 - val_loss: 0.6938 - val_accuracy: 0.5071\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5097 - val_loss: 0.6933 - val_accuracy: 0.5083\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5101 - val_loss: 0.6940 - val_accuracy: 0.5044\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5109\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5123\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5145 - val_loss: 0.6926 - val_accuracy: 0.5153\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5147 - val_loss: 0.6927 - val_accuracy: 0.5117\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.6928 - val_accuracy: 0.5133\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5177 - val_loss: 0.6924 - val_accuracy: 0.5148\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5210 - val_loss: 0.6922 - val_accuracy: 0.5203\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6911 - accuracy: 0.5234 - val_loss: 0.6922 - val_accuracy: 0.5221\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5277 - val_loss: 0.6912 - val_accuracy: 0.5266\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5308 - val_loss: 0.6909 - val_accuracy: 0.5291\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6887 - accuracy: 0.5357 - val_loss: 0.6894 - val_accuracy: 0.5322\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5381 - val_loss: 0.6887 - val_accuracy: 0.5359\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5425 - val_loss: 0.6875 - val_accuracy: 0.5409\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5460 - val_loss: 0.6861 - val_accuracy: 0.5443\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5481 - val_loss: 0.6846 - val_accuracy: 0.5496\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5512 - val_loss: 0.6842 - val_accuracy: 0.5474\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6817 - accuracy: 0.5530 - val_loss: 0.6824 - val_accuracy: 0.5518\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.5555 - val_loss: 0.6811 - val_accuracy: 0.5559\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5586 - val_loss: 0.6797 - val_accuracy: 0.5613\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6777 - accuracy: 0.5620 - val_loss: 0.6801 - val_accuracy: 0.5563\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6771 - accuracy: 0.5642 - val_loss: 0.6773 - val_accuracy: 0.5617\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5648 - val_loss: 0.6771 - val_accuracy: 0.5585\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6754 - accuracy: 0.5675 - val_loss: 0.6819 - val_accuracy: 0.5509\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6756 - accuracy: 0.5667 - val_loss: 0.6794 - val_accuracy: 0.5622\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.5683 - val_loss: 0.6757 - val_accuracy: 0.5699\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6727 - accuracy: 0.5729 - val_loss: 0.6777 - val_accuracy: 0.5616\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5748 - val_loss: 0.6753 - val_accuracy: 0.5647\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5723 - val_loss: 0.6743 - val_accuracy: 0.5728\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6699 - accuracy: 0.5773 - val_loss: 0.6748 - val_accuracy: 0.5728\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6673 - accuracy: 0.5814 - val_loss: 0.6684 - val_accuracy: 0.5835\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6667 - accuracy: 0.5821 - val_loss: 0.6728 - val_accuracy: 0.5660\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6656 - accuracy: 0.5844 - val_loss: 0.6708 - val_accuracy: 0.5765\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5810 - val_loss: 0.6747 - val_accuracy: 0.5628\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6647 - accuracy: 0.5856 - val_loss: 0.6662 - val_accuracy: 0.5823\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6634 - accuracy: 0.5884 - val_loss: 0.6776 - val_accuracy: 0.5910\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6644 - accuracy: 0.5895 - val_loss: 0.6693 - val_accuracy: 0.5758\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6675 - accuracy: 0.5832 - val_loss: 0.6698 - val_accuracy: 0.5672\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6651 - accuracy: 0.5875 - val_loss: 0.6676 - val_accuracy: 0.5822\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6621 - accuracy: 0.5899 - val_loss: 0.6615 - val_accuracy: 0.5887\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6681 - accuracy: 0.5812 - val_loss: 0.6635 - val_accuracy: 0.5947\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5875 - val_loss: 0.6638 - val_accuracy: 0.5927\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6642 - accuracy: 0.5877 - val_loss: 0.6707 - val_accuracy: 0.5754\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6619 - accuracy: 0.5921 - val_loss: 0.6662 - val_accuracy: 0.5863\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6711 - accuracy: 0.5762 - val_loss: 0.6666 - val_accuracy: 0.5872\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6619 - accuracy: 0.5914 - val_loss: 0.6597 - val_accuracy: 0.5927\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6605 - accuracy: 0.5938 - val_loss: 0.6602 - val_accuracy: 0.5913\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6620 - accuracy: 0.5899 - val_loss: 0.6693 - val_accuracy: 0.5823\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6601 - accuracy: 0.5941 - val_loss: 0.6607 - val_accuracy: 0.5920\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6628 - accuracy: 0.5891 - val_loss: 0.6665 - val_accuracy: 0.5832\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5712 - val_loss: 0.6658 - val_accuracy: 0.5875\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6651 - accuracy: 0.5854 - val_loss: 0.6639 - val_accuracy: 0.5901\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5959 - val_loss: 0.6639 - val_accuracy: 0.5965\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6604 - accuracy: 0.5929 - val_loss: 0.6612 - val_accuracy: 0.5939\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6625 - accuracy: 0.5910 - val_loss: 0.7660 - val_accuracy: 0.5094\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6799 - accuracy: 0.5566 - val_loss: 0.6715 - val_accuracy: 0.5731\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6627 - accuracy: 0.5889 - val_loss: 0.6583 - val_accuracy: 0.5996\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6591 - accuracy: 0.5968 - val_loss: 0.6558 - val_accuracy: 0.6006\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6614 - accuracy: 0.5916 - val_loss: 0.6619 - val_accuracy: 0.5971\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6567 - accuracy: 0.6000 - val_loss: 0.6532 - val_accuracy: 0.6043\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.5986 - val_loss: 0.6580 - val_accuracy: 0.6019\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6583 - accuracy: 0.5962 - val_loss: 0.6605 - val_accuracy: 0.6023\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6034 - val_loss: 0.6568 - val_accuracy: 0.5967\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6569 - accuracy: 0.5981 - val_loss: 0.6537 - val_accuracy: 0.6052\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6552 - accuracy: 0.6021 - val_loss: 0.6556 - val_accuracy: 0.5984\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6662 - accuracy: 0.5858 - val_loss: 0.6653 - val_accuracy: 0.5811\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6560 - accuracy: 0.6014 - val_loss: 0.6520 - val_accuracy: 0.6056\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.5981 - val_loss: 0.6674 - val_accuracy: 0.5851\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6543 - accuracy: 0.6039 - val_loss: 0.6501 - val_accuracy: 0.6091\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6532 - accuracy: 0.6056 - val_loss: 0.6535 - val_accuracy: 0.6030\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6527 - accuracy: 0.6057 - val_loss: 0.6494 - val_accuracy: 0.6092\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6531 - accuracy: 0.6052 - val_loss: 0.6497 - val_accuracy: 0.6117\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6514 - accuracy: 0.6077 - val_loss: 0.6761 - val_accuracy: 0.5707\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6843 - accuracy: 0.5565 - val_loss: 0.6981 - val_accuracy: 0.5150\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6842 - accuracy: 0.5470 - val_loss: 0.6792 - val_accuracy: 0.5649\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6589 - accuracy: 0.5965 - val_loss: 0.6547 - val_accuracy: 0.6062\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6510 - accuracy: 0.6071 - val_loss: 0.6554 - val_accuracy: 0.6043\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.6048 - val_loss: 0.6568 - val_accuracy: 0.5986\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6516 - accuracy: 0.6070 - val_loss: 0.6506 - val_accuracy: 0.6112\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6527 - accuracy: 0.6082 - val_loss: 0.6534 - val_accuracy: 0.6069\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6517 - accuracy: 0.6073 - val_loss: 0.6569 - val_accuracy: 0.6105\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6501 - accuracy: 0.6087 - val_loss: 0.6475 - val_accuracy: 0.6142\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6612 - accuracy: 0.5942 - val_loss: 0.6552 - val_accuracy: 0.6030\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6694 - accuracy: 0.5828 - val_loss: 0.6790 - val_accuracy: 0.5577\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6557 - accuracy: 0.6015 - val_loss: 0.6564 - val_accuracy: 0.6083\n",
      "Epoch 111/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6635 - accuracy: 0.5922 - val_loss: 0.7221 - val_accuracy: 0.5194\n",
      "Epoch 112/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5224 - val_loss: 0.6907 - val_accuracy: 0.5322\n",
      "Epoch 113/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6876 - accuracy: 0.5369 - val_loss: 0.6857 - val_accuracy: 0.5422\n",
      "Epoch 114/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6824 - accuracy: 0.5495 - val_loss: 0.6804 - val_accuracy: 0.5554\n",
      "Epoch 115/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6770 - accuracy: 0.5615 - val_loss: 0.6756 - val_accuracy: 0.5633\n",
      "Epoch 116/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6712 - accuracy: 0.5718 - val_loss: 0.6691 - val_accuracy: 0.5743\n",
      "Epoch 117/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5837 - val_loss: 0.6640 - val_accuracy: 0.5855\n",
      "Epoch 118/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6601 - accuracy: 0.5930 - val_loss: 0.6580 - val_accuracy: 0.5962\n",
      "Epoch 119/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6559 - accuracy: 0.5990 - val_loss: 0.6573 - val_accuracy: 0.5959\n",
      "Epoch 120/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6660 - accuracy: 0.5845 - val_loss: 0.6543 - val_accuracy: 0.6038\n",
      "Epoch 121/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6554 - accuracy: 0.6013 - val_loss: 0.6569 - val_accuracy: 0.6027\n",
      "Epoch 122/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6519 - accuracy: 0.6060 - val_loss: 0.6486 - val_accuracy: 0.6120\n",
      "Epoch 123/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6511 - accuracy: 0.6071 - val_loss: 0.6500 - val_accuracy: 0.6097\n",
      "Epoch 124/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6496 - accuracy: 0.6094 - val_loss: 0.6464 - val_accuracy: 0.6142\n",
      "Epoch 125/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6510 - accuracy: 0.6088 - val_loss: 0.6491 - val_accuracy: 0.6105\n",
      "Epoch 126/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6023 - val_loss: 0.6517 - val_accuracy: 0.6094\n",
      "Epoch 127/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6107 - val_loss: 0.6488 - val_accuracy: 0.6043\n",
      "Epoch 128/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5651 - val_loss: 0.6866 - val_accuracy: 0.5473\n",
      "Epoch 129/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6753 - accuracy: 0.5621 - val_loss: 0.6713 - val_accuracy: 0.5722\n",
      "Epoch 130/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6598 - accuracy: 0.5909 - val_loss: 0.6540 - val_accuracy: 0.6048\n",
      "Epoch 131/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6042 - val_loss: 0.6535 - val_accuracy: 0.6047\n",
      "Epoch 132/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6513 - accuracy: 0.6072 - val_loss: 0.6464 - val_accuracy: 0.6138\n",
      "Epoch 133/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6112 - val_loss: 0.6446 - val_accuracy: 0.6116\n",
      "Epoch 134/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6095 - val_loss: 0.6457 - val_accuracy: 0.6175\n",
      "Epoch 135/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6487 - accuracy: 0.6112 - val_loss: 0.6427 - val_accuracy: 0.6195\n",
      "Epoch 136/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6476 - accuracy: 0.6120 - val_loss: 0.6493 - val_accuracy: 0.6108\n",
      "Epoch 137/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6627 - accuracy: 0.5891 - val_loss: 0.7030 - val_accuracy: 0.5215\n",
      "Epoch 138/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5264 - val_loss: 0.6884 - val_accuracy: 0.5390\n",
      "Epoch 139/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.5534 - val_loss: 0.6760 - val_accuracy: 0.5653\n",
      "Epoch 140/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6665 - accuracy: 0.5796 - val_loss: 0.6626 - val_accuracy: 0.5844\n",
      "Epoch 141/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6613 - accuracy: 0.5919 - val_loss: 0.6691 - val_accuracy: 0.5712\n",
      "Epoch 142/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6598 - accuracy: 0.5927 - val_loss: 0.6555 - val_accuracy: 0.6036\n",
      "Epoch 143/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6517 - accuracy: 0.6061 - val_loss: 0.6499 - val_accuracy: 0.6096\n",
      "Epoch 144/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6470 - accuracy: 0.6118 - val_loss: 0.6458 - val_accuracy: 0.6142\n",
      "Epoch 145/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6457 - accuracy: 0.6126 - val_loss: 0.6472 - val_accuracy: 0.6148\n",
      "Epoch 146/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6441 - accuracy: 0.6142 - val_loss: 0.6449 - val_accuracy: 0.6160\n",
      "Epoch 147/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6426 - accuracy: 0.6159 - val_loss: 0.6391 - val_accuracy: 0.6231\n",
      "Epoch 148/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6422 - accuracy: 0.6168 - val_loss: 0.6397 - val_accuracy: 0.6233\n",
      "Epoch 149/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6407 - accuracy: 0.6184 - val_loss: 0.6416 - val_accuracy: 0.6213\n",
      "Epoch 150/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6406 - accuracy: 0.6192 - val_loss: 0.6451 - val_accuracy: 0.6121\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_36\n",
      "cannot prune layer q_activation_36\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_37\n",
      "cannot prune layer q_activation_37\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6664 - accuracy: 0.5914 - val_loss: 0.6997 - val_accuracy: 0.5453\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6849 - accuracy: 0.5582 - val_loss: 0.6910 - val_accuracy: 0.5454\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6630 - accuracy: 0.5842 - val_loss: 0.7006 - val_accuracy: 0.5480\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6573 - accuracy: 0.5940 - val_loss: 0.7091 - val_accuracy: 0.5437\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6627 - accuracy: 0.5850 - val_loss: 0.6785 - val_accuracy: 0.5743\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6546 - accuracy: 0.5971 - val_loss: 0.6680 - val_accuracy: 0.5871\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6030 - val_loss: 0.6867 - val_accuracy: 0.5640\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6578 - accuracy: 0.5991 - val_loss: 0.6627 - val_accuracy: 0.5905\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6043 - val_loss: 0.6500 - val_accuracy: 0.6100\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6499 - accuracy: 0.6098 - val_loss: 0.6468 - val_accuracy: 0.6148\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6493 - accuracy: 0.6106 - val_loss: 0.6496 - val_accuracy: 0.6101\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6490 - accuracy: 0.6116 - val_loss: 0.6504 - val_accuracy: 0.6108\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6479 - accuracy: 0.6128 - val_loss: 0.6441 - val_accuracy: 0.6191\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6484 - accuracy: 0.6118 - val_loss: 0.6495 - val_accuracy: 0.6115\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6509 - accuracy: 0.6077 - val_loss: 0.6477 - val_accuracy: 0.6114\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6476 - accuracy: 0.6124 - val_loss: 0.6478 - val_accuracy: 0.6110\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6482 - accuracy: 0.6121 - val_loss: 0.6499 - val_accuracy: 0.6087\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6797 - accuracy: 0.5616 - val_loss: 0.6865 - val_accuracy: 0.5402\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6558 - accuracy: 0.6006 - val_loss: 0.6477 - val_accuracy: 0.6131\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6472 - accuracy: 0.6134 - val_loss: 0.6652 - val_accuracy: 0.5874\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6480 - accuracy: 0.6098 - val_loss: 0.6529 - val_accuracy: 0.6046\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6489 - accuracy: 0.6104 - val_loss: 0.6640 - val_accuracy: 0.5837\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6467 - accuracy: 0.6136 - val_loss: 0.6422 - val_accuracy: 0.6235\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6440 - accuracy: 0.6173 - val_loss: 0.6401 - val_accuracy: 0.6231\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6590 - accuracy: 0.5984 - val_loss: 0.6527 - val_accuracy: 0.6022\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6457 - accuracy: 0.6148 - val_loss: 0.6489 - val_accuracy: 0.6108\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6560 - accuracy: 0.6005 - val_loss: 0.6439 - val_accuracy: 0.6224\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6441 - accuracy: 0.6164 - val_loss: 0.6416 - val_accuracy: 0.6197\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6436 - accuracy: 0.6177 - val_loss: 0.6400 - val_accuracy: 0.6259\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6430 - accuracy: 0.6189 - val_loss: 0.6418 - val_accuracy: 0.6222\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6436 - accuracy: 0.6169 - val_loss: 0.6429 - val_accuracy: 0.6190\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6578 - accuracy: 0.5993 - val_loss: 0.6492 - val_accuracy: 0.6132\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6437 - accuracy: 0.6158 - val_loss: 0.6462 - val_accuracy: 0.6158\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6429 - accuracy: 0.6185 - val_loss: 0.6410 - val_accuracy: 0.6229\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6416 - accuracy: 0.6203 - val_loss: 0.6428 - val_accuracy: 0.6181\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6423 - accuracy: 0.6188 - val_loss: 0.6375 - val_accuracy: 0.6260\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6446 - accuracy: 0.6178 - val_loss: 0.6409 - val_accuracy: 0.6232\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6413 - accuracy: 0.6206 - val_loss: 0.6377 - val_accuracy: 0.6271\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6737 - accuracy: 0.5710 - val_loss: 0.6994 - val_accuracy: 0.5185\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6893 - accuracy: 0.5407 - val_loss: 0.6878 - val_accuracy: 0.5431\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6520 - accuracy: 0.6044 - val_loss: 0.6463 - val_accuracy: 0.6097\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6436 - accuracy: 0.6154 - val_loss: 0.6400 - val_accuracy: 0.6213\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6412 - accuracy: 0.6176 - val_loss: 0.6387 - val_accuracy: 0.6222\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6402 - accuracy: 0.6194 - val_loss: 0.6383 - val_accuracy: 0.6226\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6409 - accuracy: 0.6203 - val_loss: 0.6382 - val_accuracy: 0.6215\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6458 - accuracy: 0.6168 - val_loss: 0.6439 - val_accuracy: 0.6150\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6436 - accuracy: 0.6186 - val_loss: 0.6395 - val_accuracy: 0.6222\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6459 - accuracy: 0.6138 - val_loss: 0.6417 - val_accuracy: 0.6224\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6426 - accuracy: 0.6201 - val_loss: 0.6393 - val_accuracy: 0.6264\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6436 - accuracy: 0.6184 - val_loss: 0.6497 - val_accuracy: 0.6181\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.59087723]\n",
      " [0.69222325]\n",
      " [0.5334515 ]\n",
      " [0.6547705 ]\n",
      " [0.504764  ]\n",
      " [0.3894515 ]\n",
      " [0.6053732 ]\n",
      " [0.3910995 ]\n",
      " [0.4196989 ]\n",
      " [0.69702196]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = hyperparameter_search(data, HYPERPARAMETERS, param_grid, result_file=SAVE_FILE)\n",
    "    send_email_notification(\"All done with hyperparameter search\", 'Done!')\n",
    "except Exception as e:\n",
    "    print(\"Error encountered:\", e)\n",
    "    send_email_notification(\"Hyperparameter search ran into an error\", 'Go fix it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE REFORMATTING\n",
    "# (RUN LATER WHEN THEY ARENT BEING WRITTEN TO)\n",
    "\n",
    "def reformat_hyperparameter_results(input_file, output_file):\n",
    "    # Read the original JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process and format the metrics\n",
    "    for key, value in data.items():\n",
    "        if \"metrics\" in value:\n",
    "            value[\"metrics\"] = format_metrics(value[\"metrics\"])\n",
    "\n",
    "    # Write the updated data to the new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Define input and output file names\n",
    "input_file = 'one_layerDNN_results.json'\n",
    "output_file = 'one_layerDNN_results.json'\n",
    "\n",
    "# Call the function to reformat the JSON data\n",
    "reformat_hyperparameter_results(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Read / Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 24)                96        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 24)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12)                48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_5 (QActivatio  (None, 12)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_4 is normal keras bn layer\n",
      "q_activation_4       quantized_relu(15,0)\n",
      "dense2               u=12 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_5 is normal keras bn layer\n",
      "q_activation_5       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 0]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 1.7775 - accuracy: 0.5017 - val_loss: 0.7032 - val_accuracy: 0.5022\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6981 - accuracy: 0.5032 - val_loss: 0.6957 - val_accuracy: 0.5011\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6952 - accuracy: 0.5055 - val_loss: 0.6945 - val_accuracy: 0.5044\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6939 - accuracy: 0.5066 - val_loss: 0.6941 - val_accuracy: 0.5027\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6935 - accuracy: 0.5080 - val_loss: 0.6938 - val_accuracy: 0.5040\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5081 - val_loss: 0.6934 - val_accuracy: 0.5039\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.5035\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6932 - val_accuracy: 0.5037\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6932 - val_accuracy: 0.5048\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5056\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5118 - val_loss: 0.6931 - val_accuracy: 0.5054\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6932 - val_accuracy: 0.5053\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5081\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5132 - val_loss: 0.6929 - val_accuracy: 0.5066\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.5087\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5172 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5197 - val_loss: 0.6922 - val_accuracy: 0.5179\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6929 - val_accuracy: 0.5125\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5283 - val_loss: 0.6916 - val_accuracy: 0.5213\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5337 - val_loss: 0.6891 - val_accuracy: 0.5335\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5414 - val_loss: 0.6910 - val_accuracy: 0.5259\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5487 - val_loss: 0.6839 - val_accuracy: 0.5493\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6801 - accuracy: 0.5557 - val_loss: 0.6798 - val_accuracy: 0.5554\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6776 - accuracy: 0.5604 - val_loss: 0.6774 - val_accuracy: 0.5568\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.5662 - val_loss: 0.6771 - val_accuracy: 0.5611\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5698 - val_loss: 0.6748 - val_accuracy: 0.5670\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6696 - accuracy: 0.5732 - val_loss: 0.6814 - val_accuracy: 0.5527\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6666 - accuracy: 0.5789 - val_loss: 0.6653 - val_accuracy: 0.5805\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5817 - val_loss: 0.6641 - val_accuracy: 0.5804\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6628 - accuracy: 0.5839 - val_loss: 0.6816 - val_accuracy: 0.5592\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6611 - accuracy: 0.5860 - val_loss: 0.6585 - val_accuracy: 0.5866\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6591 - accuracy: 0.5885 - val_loss: 0.6521 - val_accuracy: 0.5953\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6576 - accuracy: 0.5915 - val_loss: 0.6563 - val_accuracy: 0.5918\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6558 - accuracy: 0.5936 - val_loss: 0.6528 - val_accuracy: 0.5999\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6641 - accuracy: 0.5754 - val_loss: 0.6919 - val_accuracy: 0.5160\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6721 - accuracy: 0.5635 - val_loss: 0.6626 - val_accuracy: 0.5817\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.5991 - val_loss: 0.6538 - val_accuracy: 0.5970\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.5975 - val_loss: 0.6470 - val_accuracy: 0.6047\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6497 - accuracy: 0.6022 - val_loss: 0.6469 - val_accuracy: 0.6072\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6496 - accuracy: 0.6021 - val_loss: 0.6453 - val_accuracy: 0.6061\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6545 - accuracy: 0.5973 - val_loss: 0.6579 - val_accuracy: 0.5914\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6046 - val_loss: 0.6677 - val_accuracy: 0.5503\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6476 - accuracy: 0.6050 - val_loss: 0.6480 - val_accuracy: 0.6039\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6458 - accuracy: 0.6074 - val_loss: 0.6390 - val_accuracy: 0.6163\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6449 - accuracy: 0.6084 - val_loss: 0.6454 - val_accuracy: 0.6050\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6450 - accuracy: 0.6084 - val_loss: 0.6356 - val_accuracy: 0.6194\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6429 - accuracy: 0.6114 - val_loss: 0.6349 - val_accuracy: 0.6229\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6456 - accuracy: 0.6083 - val_loss: 0.6760 - val_accuracy: 0.5729\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6427 - accuracy: 0.6110 - val_loss: 0.6346 - val_accuracy: 0.6187\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6409 - accuracy: 0.6149 - val_loss: 0.6345 - val_accuracy: 0.6205\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6410 - accuracy: 0.6135 - val_loss: 0.6336 - val_accuracy: 0.6213\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6404 - accuracy: 0.6159 - val_loss: 0.6368 - val_accuracy: 0.6178\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6385 - accuracy: 0.6172 - val_loss: 0.6333 - val_accuracy: 0.6207\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6387 - accuracy: 0.6176 - val_loss: 0.6320 - val_accuracy: 0.6247\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6365 - accuracy: 0.6194 - val_loss: 0.6408 - val_accuracy: 0.6156\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6360 - accuracy: 0.6214 - val_loss: 0.6370 - val_accuracy: 0.6211\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6358 - accuracy: 0.6223 - val_loss: 0.6330 - val_accuracy: 0.6243\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6395 - accuracy: 0.6199 - val_loss: 0.6383 - val_accuracy: 0.6186\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6364 - accuracy: 0.6230 - val_loss: 0.6407 - val_accuracy: 0.6161\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6359 - accuracy: 0.6219 - val_loss: 0.6451 - val_accuracy: 0.6135\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6577 - accuracy: 0.5890 - val_loss: 0.6390 - val_accuracy: 0.6236\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6345 - accuracy: 0.6228 - val_loss: 0.6360 - val_accuracy: 0.6190\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6345 - accuracy: 0.6228 - val_loss: 0.6244 - val_accuracy: 0.6350\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6327 - accuracy: 0.6248 - val_loss: 0.6349 - val_accuracy: 0.6175\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6327 - accuracy: 0.6243 - val_loss: 0.6293 - val_accuracy: 0.6317\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6332 - accuracy: 0.6245 - val_loss: 0.6242 - val_accuracy: 0.6391\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6316 - accuracy: 0.6256 - val_loss: 0.6222 - val_accuracy: 0.6356\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6314 - accuracy: 0.6266 - val_loss: 0.6615 - val_accuracy: 0.5901\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6330 - accuracy: 0.6249 - val_loss: 0.6277 - val_accuracy: 0.6320\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6449 - accuracy: 0.6095 - val_loss: 0.7222 - val_accuracy: 0.5085\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5104 - val_loss: 0.6937 - val_accuracy: 0.5113\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5177 - val_loss: 0.6932 - val_accuracy: 0.5180\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5288 - val_loss: 0.6892 - val_accuracy: 0.5306\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6802 - accuracy: 0.5600 - val_loss: 0.6746 - val_accuracy: 0.5692\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6661 - accuracy: 0.5834 - val_loss: 0.6623 - val_accuracy: 0.5905\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6553 - accuracy: 0.5969 - val_loss: 0.6532 - val_accuracy: 0.6009\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6486 - accuracy: 0.6053 - val_loss: 0.6389 - val_accuracy: 0.6156\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6478 - accuracy: 0.6057 - val_loss: 0.6411 - val_accuracy: 0.6112\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6414 - accuracy: 0.6154 - val_loss: 0.6419 - val_accuracy: 0.6119\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6409 - accuracy: 0.6159 - val_loss: 0.6367 - val_accuracy: 0.6199\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6825 - accuracy: 0.5502 - val_loss: 0.6866 - val_accuracy: 0.5451\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6000 - val_loss: 0.6546 - val_accuracy: 0.6007\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6399 - accuracy: 0.6165 - val_loss: 0.6411 - val_accuracy: 0.6182\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6396 - accuracy: 0.6182 - val_loss: 0.6493 - val_accuracy: 0.6042\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6629 - accuracy: 0.5796 - val_loss: 0.6971 - val_accuracy: 0.5070\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6832 - accuracy: 0.5505 - val_loss: 0.6813 - val_accuracy: 0.5550\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5651 - val_loss: 0.6951 - val_accuracy: 0.5046\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_4\n",
      "cannot prune layer q_activation_4\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_5\n",
      "cannot prune layer q_activation_5\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 8ms/step - loss: 0.6339 - accuracy: 0.6228 - val_loss: 0.6230 - val_accuracy: 0.6356\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6447 - accuracy: 0.6128 - val_loss: 0.6378 - val_accuracy: 0.6206\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6330 - accuracy: 0.6262 - val_loss: 0.6361 - val_accuracy: 0.6195\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6287 - val_loss: 0.6252 - val_accuracy: 0.6354\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.6280 - val_loss: 0.6382 - val_accuracy: 0.6091\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5304 - val_loss: 0.6993 - val_accuracy: 0.5013\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6966 - accuracy: 0.5020 - val_loss: 0.6957 - val_accuracy: 0.4996\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6948 - accuracy: 0.5032 - val_loss: 0.6946 - val_accuracy: 0.5007\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6940 - accuracy: 0.5036 - val_loss: 0.6940 - val_accuracy: 0.5008\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6935 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.5046\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5060 - val_loss: 0.6934 - val_accuracy: 0.5041\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6933 - val_accuracy: 0.5055\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5072 - val_loss: 0.6934 - val_accuracy: 0.5065\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5070 - val_loss: 0.6931 - val_accuracy: 0.5056\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6932 - val_accuracy: 0.5049\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5086 - val_loss: 0.6931 - val_accuracy: 0.5091\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6928 - accuracy: 0.5091 - val_loss: 0.6930 - val_accuracy: 0.5069\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5099 - val_loss: 0.6931 - val_accuracy: 0.5095\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5098 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6932 - val_accuracy: 0.5036\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6930 - val_accuracy: 0.5075\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6927 - val_accuracy: 0.5135\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5141 - val_loss: 0.6922 - val_accuracy: 0.5166\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5167 - val_loss: 0.6922 - val_accuracy: 0.5195\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5249 - val_loss: 0.6897 - val_accuracy: 0.5329\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6872 - accuracy: 0.5405 - val_loss: 0.6876 - val_accuracy: 0.5362\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6804 - accuracy: 0.5550 - val_loss: 0.6841 - val_accuracy: 0.5433\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6722 - accuracy: 0.5722 - val_loss: 0.6808 - val_accuracy: 0.5568\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6667 - accuracy: 0.5829 - val_loss: 0.6560 - val_accuracy: 0.5983\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6764 - accuracy: 0.5587 - val_loss: 0.6952 - val_accuracy: 0.5060\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6924 - accuracy: 0.5171 - val_loss: 0.6914 - val_accuracy: 0.5190\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5296 - val_loss: 0.6880 - val_accuracy: 0.5366\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6827 - accuracy: 0.5525 - val_loss: 0.6820 - val_accuracy: 0.5519\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6759 - accuracy: 0.5629 - val_loss: 0.6839 - val_accuracy: 0.5444\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6841 - accuracy: 0.5409 - val_loss: 0.6924 - val_accuracy: 0.5114\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6855 - accuracy: 0.5411 - val_loss: 0.6804 - val_accuracy: 0.5562\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6719 - accuracy: 0.5725 - val_loss: 0.6651 - val_accuracy: 0.5829\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6632 - accuracy: 0.5852 - val_loss: 0.6566 - val_accuracy: 0.5945\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5153 - val_loss: 0.6939 - val_accuracy: 0.5072\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6933 - accuracy: 0.5077 - val_loss: 0.6933 - val_accuracy: 0.5053\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6930 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5059\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5112 - val_loss: 0.6927 - val_accuracy: 0.5084\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6922 - val_accuracy: 0.5131\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5169 - val_loss: 0.6916 - val_accuracy: 0.5150\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5239 - val_loss: 0.6901 - val_accuracy: 0.5257\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6842 - accuracy: 0.5445 - val_loss: 0.6811 - val_accuracy: 0.5559\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6733 - accuracy: 0.5704 - val_loss: 0.6738 - val_accuracy: 0.5727\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6651 - accuracy: 0.5833 - val_loss: 0.6526 - val_accuracy: 0.5995\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6695 - accuracy: 0.5730 - val_loss: 0.6608 - val_accuracy: 0.5896\n"
     ]
    }
   ],
   "source": [
    "model, train_metrics = train_model(data, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.657699  ]\n",
      " [0.51863325]\n",
      " [0.55201584]\n",
      " [0.3855904 ]\n",
      " [0.49306977]\n",
      " [0.61651945]\n",
      " [0.5650373 ]\n",
      " [0.41414428]\n",
      " [0.47818124]\n",
      " [0.53208697]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8725768838361755, 0.2938063432972006), (0.9003789535053199, 0.2574546516296215), (0.929857163678764, 0.21175955326836746), (0.960246319778458, 0.15204934486660096), (0.980141378807754, 0.1017190408409066), (0.990016032648302, 0.0677396985291434), (0.995008016324151, 0.0405489251432534), (0.9990161783996502, 0.007810504032993905)]\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_batch_norm(dense_layer, bn_layer):\n",
    "    W, b = dense_layer.get_weights()\n",
    "    gamma, beta, moving_mean, moving_var = bn_layer.get_weights()\n",
    "\n",
    "    epsilon = bn_layer.epsilon\n",
    "    std = np.sqrt(moving_var + epsilon)\n",
    "    new_W = gamma / std * W\n",
    "    new_b = gamma / std * (b - moving_mean) + beta\n",
    "\n",
    "    return new_W, new_b\n",
    "\n",
    "def create_folded_model(original_model): # Fold batch normalization layers into dense layers\n",
    "    inputs = original_model.input\n",
    "    x = inputs\n",
    "    new_layers = []\n",
    "\n",
    "    for layer in original_model.layers:\n",
    "        if isinstance(layer, QDense):\n",
    "            next_layer = new_layers[-1] if new_layers else inputs\n",
    "            if isinstance(next_layer, BatchNormalization):\n",
    "                # Fold the BatchNormalization into the previous Dense layer\n",
    "                new_W, new_b = fold_batch_norm(layer, next_layer)\n",
    "                x = QDense(layer.units, weights=[new_W, new_b], kernel_quantizer=layer.kernel_quantizer, bias_quantizer=layer.bias_quantizer)(x)\n",
    "                new_layers.pop()  # Remove the BatchNormalization layer\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        elif not isinstance(layer, BatchNormalization):\n",
    "            x = layer(x)\n",
    "        new_layers.append(x)\n",
    "\n",
    "    outputs = x\n",
    "\n",
    "    new_model = Model(inputs, outputs)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDone():\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6, 6)\n",
    "    square = plt.Rectangle((0, 0), 1, 1, color='green')\n",
    "    ax.add_patch(square)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.title(\"DONE\", fontsize=20)\n",
    "\n",
    "showDone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_23\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_23\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_timed_input (InputLayer)  multiple                     0         ['y_timed_input[0][0]']       \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 24)                   2544      ['y_timed_input[3][0]']       \n",
      "                                                                                                  \n",
      " q_activation_32 (QActivati  (None, 24)                   0         ['dense1[3][0]']              \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " dense2 (QDense)             (None, 12)                   300       ['q_activation_32[3][0]']     \n",
      "                                                                                                  \n",
      " q_activation_33 (QActivati  (None, 12)                   0         ['dense2[3][0]']              \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 1)                    13        ['q_activation_33[3][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2857 (11.16 KB)\n",
      "Trainable params: 2857 (11.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Create the folded model\n",
    "new_model = create_folded_model(loaded_model)\n",
    "\n",
    "# Verify the new model\n",
    "new_model.summary()\n",
    "\n",
    "new_model.save(f'./DNN_L2_S24_best_performance_single_quant_10_15_folded.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the model when done\n",
    "model.save(f'./DNN_L3_S32_best_performance_quant.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_32 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 12)                300       \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 12)                48        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_33 (QActivati  (None, 12)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3001 (11.72 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 72 (288.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load in a saved model from the h5 file\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "loaded_model = load_model('./hyperparam_search_recent_best.h5', custom_objects=co)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.55482876]\n",
      " [0.59929675]\n",
      " [0.61627674]\n",
      " [0.64574015]\n",
      " [0.71000576]\n",
      " [0.4455624 ]\n",
      " [0.6686183 ]\n",
      " [0.4733197 ]\n",
      " [0.6041224 ]\n",
      " [0.22041702]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8729857127196989, 0.4101397583345465), (0.900317901121789, 0.3757461056922405), (0.9298059706946322, 0.327886155189984), (0.9598786860086966, 0.2547677973504149), (0.9800489640808273, 0.17924734313582763), (0.9901341031168926, 0.1335347212112389), (0.9949939708407937, 0.1035813073227544), (0.9990134103116892, 0.05488426262920367)]\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, loaded_model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write input data to file\n",
    "with open('DNN_hp_input_features.dat', 'w') as file:\n",
    "    for row in input_train_data_combined:\n",
    "        line = ' '.join(map(str, row))  # Convert each number to string and join with space\n",
    "        file.write(line + '\\n')\n",
    "# Write target data to file\n",
    "with open('./DNN_hp_predictions_small.dat', 'w') as file:\n",
    "    for score in target_test_data_coded:\n",
    "        file.write(str(score[0]) + '\\n')  # Convert number to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set display by time slice\n",
    "def display_dataset(input_dataset, target_dataset, i, gif=False):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset[i]\n",
    "    target_datapoint = target_dataset[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if input_datapoint.shape != (20, 13, 21):\n",
    "        raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point\")\n",
    "    \n",
    "    for i in range(8):\n",
    "        for j in range(13):\n",
    "            for k in range(21):\n",
    "                print(input_datapoint[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"--------\", i)\n",
    "\n",
    "    # Extracting the transverse momentum (pt) from the target_data dataset\n",
    "    pt = target_datapoint[8]  # Assuming the 9th variable is at index 8\n",
    "\n",
    "    fig, ax_main = plt.subplots(figsize=(8, 6))\n",
    "    divider = make_axes_locatable(ax_main)\n",
    "\n",
    "    # Add row sum plot as an inset to the main plot\n",
    "    ax_row = divider.append_axes(\"right\", size=\"20%\", pad=0.4)\n",
    "\n",
    "    # Add column sum plot below the main plot\n",
    "    ax_column = divider.append_axes(\"bottom\", size=\"20%\", pad=0.5)\n",
    "\n",
    "    # Initial plot\n",
    "    im = ax_main.imshow(input_datapoint[0, :, :], cmap='plasma')\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[2]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "    # Function to update the animation\n",
    "    def update(t):\n",
    "        # Update main plot\n",
    "        data = input_datapoint[t, :, :]\n",
    "        im.set_data(data)\n",
    "\n",
    "        # Update row sum plot\n",
    "        ax_row.clear()\n",
    "        ax_row.barh(np.arange(data.shape[0]), np.sum(data, axis=1), color='red')\n",
    "        ax_row.set_ylim(0, data.shape[0]-1)\n",
    "        ax_row.set_yticks(np.arange(data.shape[0]))\n",
    "        ax_row.set_xlim(np.min(input_datapoint[:, :, :].sum(axis=2)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=2)) * 1.1)\n",
    "        ax_row.set_xlabel(\"Row Sum\")\n",
    "\n",
    "        # Update column sum plot\n",
    "        ax_column.clear()\n",
    "        ax_column.bar(np.arange(data.shape[1]), np.sum(data, axis=0), color='blue')\n",
    "        ax_column.set_xlim(0, data.shape[1]-1)\n",
    "        ax_column.set_xticks(np.arange(data.shape[1]))\n",
    "        ax_column.set_ylim(np.min(input_datapoint[:, :, :].sum(axis=1)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=1)) * 1.1)\n",
    "        ax_column.set_ylabel(\"Column Sum\")\n",
    "\n",
    "        # Update labels and grid\n",
    "        ax_main.set_xlabel(\"X Position\")\n",
    "        ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "        # Update title for the entire figure\n",
    "        fig.suptitle(f\"Timestep: {t+1} | Data Point: {i} | pt: {pt:.2f} GeV\")\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=20, repeat=True)\n",
    "\n",
    "    gif_path = f\"data_point.gif\"\n",
    "    if gif:\n",
    "        # Save the animation as a GIF\n",
    "        writer = PillowWriter(fps=1000 // FRAME_TIME)\n",
    "        ani.save(gif_path, writer=writer)\n",
    "\n",
    "    plt.close()\n",
    "    return display(HTML(ani.to_jshtml())), gif_path\n",
    "\n",
    "def display_model_IO(input_dataset_combined, target_dataset_coded, i):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset_combined[i]\n",
    "    target_datapoint = target_dataset_coded[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        if input_datapoint.shape != (NUM_TIME_SLICES * 13 + 1,):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 2\")\n",
    "        input_datapoint = input_datapoint[:-1].reshape(NUM_TIME_SLICES, 13)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        if input_datapoint[0].shape != (NUM_TIME_SLICES, 13):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 3\")\n",
    "        input_datapoint = input_datapoint[0]\n",
    "        \n",
    "    # Extracting the label from the target datapoint\n",
    "    if target_datapoint[0] == 1:\n",
    "        label = f\"High p_t (over {TEST_PT_THRESHOLD} GeV)\"\n",
    "    elif target_datapoint[1] == 1:\n",
    "        label = f\"low p_t and negative charge\"\n",
    "    elif target_datapoint[2] == 1:\n",
    "        label = f\"low p_t and positive charge\"\n",
    "    else: \n",
    "        raise ValueError(\"Invalid labelling for the target data point\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax_main = plt.subplots(figsize=(4,4))\n",
    "    print(input_datapoint.shape)\n",
    "    print(input_datapoint)\n",
    "    im = ax_main.imshow(input_datapoint.T, cmap='coolwarm_r', vmin=-1, vmax=1)\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[0]))\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Update labels and grid\n",
    "    ax_main.set_xlabel(\"Time Slice\")\n",
    "    ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "\n",
    "    # Update title for the entire figure\n",
    "    fig.suptitle(f\"Data Point: {i} | label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_model_IO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rand_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m FRAME_TIME \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m  \u001b[38;5;66;03m# milliseconds between frames\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdisplay_model_IO\u001b[49m(input_data_combined_example, target_data_coded_example, rand_idx)\n\u001b[1;32m      5\u001b[0m animation, gif \u001b[38;5;241m=\u001b[39m display_dataset(input_data_example, target_data_example, rand_idx, gif\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_model_IO' is not defined"
     ]
    }
   ],
   "source": [
    "# DATASET DISPLAY\n",
    "rand_idx = random.randint(0, 100)\n",
    "FRAME_TIME = 120  # milliseconds between frames\n",
    "display_model_IO(input_data_combined_example, target_data_coded_example, rand_idx)\n",
    "animation, gif = display_dataset(input_data_example, target_data_example, rand_idx, gif=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import os\n",
    "os.environ['XILINX_HLS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis_HLS/2023.1'\n",
    "os.environ['XILINX_VIVADO'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vivado/2023.1'\n",
    "os.environ['XILINX_VITIS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis/2023.1'\n",
    "os.environ['XILINX_AP_INCLUDE'] = '/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/HLS_arbitrary_Precision_Types/include'\n",
    "os.environ['PATH'] = os.environ['XILINX_HLS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VITIS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_AP_INCLUDE'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "strip_model = strip_pruning(qmodel_pruned)\n",
    "hls_config = hls4ml.utils.config_from_keras_model(strip_model , granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    print(Layer)\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "# If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "hls_config['LayerName']['output_sigmoid']['Strategy'] = 'Stable'\n",
    "hls_config['LayerName']['output_sigmoid']['Precision'] = 'ap_fixed<32,8,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vitis')\n",
    "\n",
    "cfg['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg['HLSConfig'] = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'cnn_debug/'\n",
    "cfg['XilinxPart'] = 'xcku040-ffva1156-2-e'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()\n",
    "#hls_model.profile()\n",
    "hls_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in qmodel_pruned.layers:\n",
    "    for i, w in enumerate(layer.weights):\n",
    "        try:\n",
    "            print(\"weight is\", w.numpy(), \"for layer number\", i)  # TF 2.x\n",
    "        except Exception:\n",
    "            print(\"weight is\", layer.get_weights()[i], \"for layer number\", i) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Smart_Pixel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
