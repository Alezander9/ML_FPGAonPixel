{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 13:57:16.329944: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-17 13:57:16.416643: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-17 13:57:16.417605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 13:57:18.764622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.13.1\n",
      "keras version: 2.13.1\n",
      "qkeras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "# Machine Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, ReLU, Dropout, BatchNormalization, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow_model_optimization\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "import keras\n",
    "print(\"keras version:\",keras.__version__)\n",
    "import qkeras\n",
    "from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm\n",
    "from qkeras import quantized_relu, quantized_bits\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "print(\"qkeras version:\",keras.__version__)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Display and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Data management\n",
    "import psutil\n",
    "import h5py\n",
    "# Memory management\n",
    "import gc\n",
    "# Notifications\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def send_email_notification(subject, content):\n",
    "    sender_email = os.getenv('EMAIL_USER')\n",
    "    receiver_email = \"alexander.j.yue@gmail.com\"\n",
    "    password = os.getenv('EMAIL_PASS')\n",
    "\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = receiver_email\n",
    "    message[\"Subject\"] = subject\n",
    "    body = content\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message.as_string())\n",
    "\n",
    "# Memory monitoring functions\n",
    "def print_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory usage percentage: {memory.percent}%\")\n",
    "\n",
    "def print_cpu_usage():\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pixel cluster to transverse momentum dataset into the input_data and target_data\n",
    "def load_combine_shuffle_data_optimized_hdf5():\n",
    "    # Load the dataset from Kenny's computer\n",
    "    with h5py.File('/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/fl32_data_v3.hdf5', 'r') as h5f:\n",
    "        combined_input = None\n",
    "        combined_target = None\n",
    "\n",
    "        for data_type in ['sig', 'bkg']:\n",
    "            # Construct dataset names\n",
    "            input_dataset_name = f'{data_type}_input'\n",
    "            target_dataset_name = f'{data_type}_target'\n",
    "\n",
    "            # Check if the dataset exists and load data sequentially\n",
    "            if input_dataset_name in h5f and target_dataset_name in h5f:\n",
    "                input_data = h5f[input_dataset_name][:].astype(np.float32)\n",
    "                target_data = h5f[target_dataset_name][:].astype(np.float32)\n",
    "\n",
    "                if combined_input is None:\n",
    "                    combined_input = input_data\n",
    "                    combined_target = target_data\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "                else:\n",
    "                    print_memory_usage()\n",
    "                    combined_input = np.vstack((combined_input, input_data))\n",
    "                    combined_target = np.vstack((combined_target, target_data))\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "            else:\n",
    "                print(f\"Dataset {input_dataset_name} or {target_dataset_name} not found.\")\n",
    "\n",
    "        # Shuffling\n",
    "        indices = np.arange(combined_input.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        combined_input = combined_input[indices]\n",
    "        combined_target = combined_target[indices]\n",
    "\n",
    "        return combined_input, combined_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load dataset into memory\n",
    "    input_data, target_data = load_combine_shuffle_data_optimized_hdf5()\n",
    "    # Format the dataset into a 20x13x21 tensor (time, y, x)\n",
    "    input_data = input_data.reshape(input_data.shape[0],20,13,21)\n",
    "    return input_data, target_data\n",
    "\n",
    "def process_dataset(input_data, target_data, hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    TRAIN_PT_THRESHOLD = hyperparams[\"TRAIN_PT_THRESHOLD\"]\n",
    "    TEST_PT_THRESHOLD = hyperparams[\"TEST_PT_THRESHOLD\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "\n",
    "    # Split 80% of data into training data, 10% for validation data and 10% for testing data\n",
    "    input_train_data, input_temp, target_train_data, target_temp = \\\n",
    "    train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "    del input_data\n",
    "    del target_data\n",
    "    gc.collect()\n",
    "    input_validate_data, input_test_data, target_validate_data, target_test_data = \\\n",
    "    train_test_split(input_temp, target_temp, test_size=0.5, random_state=42)\n",
    "    del input_temp\n",
    "    del target_temp\n",
    "    gc.collect()\n",
    "\n",
    "    # Save some data for displaying\n",
    "    input_data_example = input_test_data[0:100,:]\n",
    "    target_data_example = target_test_data[0:100,:]\n",
    "\n",
    "    # Fit the scalers on the training data to it all scales the exact same\n",
    "    input_scaler = StandardScaler()\n",
    "    input_scaler.fit(input_train_data[:, :NUM_TIME_SLICES, :, :].reshape(-1,8*13))\n",
    "    y0_scaler = StandardScaler()\n",
    "    y0_scaler.fit(target_train_data[:,7].reshape(-1, 1))\n",
    "\n",
    "    # Process the data into input shape and labels for training\n",
    "    def process_data(input_data, target_data, pt_threshold):\n",
    "        if input_data.shape[1:] == (20, 13, 21) and target_data.shape[1:] == (13, ):\n",
    "\n",
    "            # Truncate down to first time slices\n",
    "            input_data = input_data[:, :NUM_TIME_SLICES, :, :]\n",
    "\n",
    "            # sum over the x axis to turn the input data into a 2D NUM_TIME_SLICES x 13 tensor (time, y)\n",
    "            input_data = np.sum(input_data, axis=3)\n",
    "\n",
    "            if OUTPUT == \"SOFTMAX\" or OUTPUT == \"LINEAR\":\n",
    "                # Encode the target data into one_hot encoding\n",
    "                one_hot = np.zeros((target_data.shape[0], 3))\n",
    "                # Assign 1 for p_t > pt_threshold in GeV, for low p_t put 1 in slot 2 for negative and a 1 in slot 3 for positive\n",
    "                one_hot[np.abs(target_data[:, 8]) >= pt_threshold, 0] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] > 0), 1] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] < 0), 2] = 1\n",
    "                label_data = one_hot\n",
    "            # elif OUTPUT == \"ARGMAX\": # DOES NOT WORK \n",
    "            #     label_data = np.argmax(one_hot, axis=1).astype(np.int64)\n",
    "            #     print(\"one hot is \", one_hot)\n",
    "            elif OUTPUT == \"SINGLE\":\n",
    "            # Binary labels for 0th category\n",
    "                label_data = (np.abs(target_data[:, 8]) >= pt_threshold).astype(np.int64)\n",
    "\n",
    "            # Flatten the input data\n",
    "            input_data = input_data.reshape(-1,NUM_TIME_SLICES*13)\n",
    "\n",
    "            # Normalize the input data to have mean| 0 and std 1\n",
    "            \n",
    "            input_data = input_scaler.transform(input_data)\n",
    "            # # Replace all values < 1 with 1 so they log to 0\n",
    "            # input_data = np.where(np.abs(input_data) < 1.0, 1.0, input_data)\n",
    "            # # Apply logarithmic scaling\n",
    "            # input_data = np.log(np.abs(input_data)) * np.sign(input_data)\n",
    "            # # Min-max normalization (global)\n",
    "            # min_val = np.min(input_data)\n",
    "            # max_val = np.max(input_data)\n",
    "            # print(f\"max of log of data is {max_val} and min is {min_val}\")\n",
    "            # input_data = (input_data) / np.max([max_val,min_val])\n",
    "\n",
    "            # Get the y_0 data\n",
    "            y0_data = target_data[:,7].reshape(-1, 1)\n",
    "            y0_data = y0_scaler.transform(y0_data)\n",
    "            # Combine with input data\n",
    "            if (MODEL_TYPE == \"DNN\"):\n",
    "                # For DNN we concatenate in the y_0 data\n",
    "                input_data_combined = np.hstack((input_data, y0_data))\n",
    "\n",
    "                \n",
    "            elif (MODEL_TYPE == \"CNN\"):\n",
    "                # Reshape data into a matrix for the convolutions\n",
    "                input_data= input_data.reshape(-1, NUM_TIME_SLICES, 13)\n",
    "                # Package with the y_0 data to be added later\n",
    "                input_data_combined = [input_data, y0_data]\n",
    "\n",
    "            return input_data_combined, label_data\n",
    "        else:\n",
    "            raise ValueError(\"Wrong array shape!\")\n",
    "\n",
    "    # Apply data processing to our datasets\n",
    "    input_train_data_combined, target_train_data_coded = process_data(input_train_data, target_train_data, TRAIN_PT_THRESHOLD)\n",
    "    input_validate_data_combined, target_validate_data_coded = process_data(input_validate_data, target_validate_data, TRAIN_PT_THRESHOLD)\n",
    "    input_test_data_combined, target_test_data_coded = process_data(input_test_data, target_test_data, TEST_PT_THRESHOLD)\n",
    "\n",
    "    # Save some data for displaying\n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        input_data_combined_example = input_test_data_combined[0:100,:]\n",
    "        if OUTPUT == \"ARGMAX\" or OUTPUT == \"SINGLE\":\n",
    "            target_data_coded_example = target_test_data_coded[0:100]\n",
    "        else:\n",
    "            target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    elif MODEL_TYPE == \"CNN\":\n",
    "        input_data_combined_example = np.hstack((input_test_data_combined[0][0:100,:].reshape(100, -1), input_test_data_combined[1][0:100,:]))\n",
    "        target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    processed_dataset = {\n",
    "        \"input_train_data_combined\": input_train_data_combined,\n",
    "        \"target_train_data_coded\": target_train_data_coded,\n",
    "        \"input_validate_data_combined\": input_validate_data_combined,\n",
    "        \"target_validate_data_coded\": target_validate_data_coded,\n",
    "        \"input_test_data_combined\": input_test_data_combined,\n",
    "        \"target_test_data_coded\": target_test_data_coded,\n",
    "\n",
    "        \"input_data_example\": input_data_example,\n",
    "        \"target_data_example\": target_data_example,\n",
    "        \"input_data_combined_example\": input_data_combined_example,\n",
    "        \"target_data_coded_example\": target_data_coded_example,\n",
    "    }\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class ArgmaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ArgmaxLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(tf.argmax(inputs, axis=-1), dtype=tf.int64)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ArgmaxLayer, self).get_config()\n",
    "        return config\n",
    "    \n",
    "def qDNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    DNN_LAYERS = hyperparams[\"DNN_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    y_timed_input = Input(shape=(NUM_TIME_SLICES*13 + 1,), name='y_timed_input')\n",
    "    layer = y_timed_input\n",
    "    \n",
    "    for i, size in enumerate(DNN_LAYERS):\n",
    "        layer = QDense(size, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name=f'dense{i+1}')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS))(layer)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = ArgmaxLayer(name='output_argmax')(output)\n",
    "        print(f\"Argmax output dtype: {output.dtype}\")\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        output = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS), \n",
    "                bias_quantizer=quantized_bits(BIAS_BITS), \n",
    "                # activation='sigmoid', \n",
    "                name='dense_output')(layer)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported output type\")\n",
    "   \n",
    "    model = Model(inputs=y_timed_input, outputs=output)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "              loss='mse', \n",
    "              metrics=[Precision()])\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def qCNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    CONV_LAYER_DEPTHS = hyperparams[\"CONV_LAYER_DEPTHS\"]\n",
    "    CONV_LAYER_KERNELS = hyperparams[\"CONV_LAYER_KERNELS\"]\n",
    "    CONV_LAYER_STRIDES = hyperparams[\"CONV_LAYER_STRIDES\"]\n",
    "    MAX_POOLING_SIZE = hyperparams[\"MAX_POOLING_SIZE\"]\n",
    "    FLATTENED_LAYERS = hyperparams[\"FLATTENED_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    INTEGER_BITS = hyperparams[\"INTEGER_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "\n",
    "    y_profile_input = Input(shape=(NUM_TIME_SLICES, 13, 1), name='y_profile_input')  # Adjust the shape based on your input\n",
    "    layer = y_profile_input\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(len(CONV_LAYER_DEPTHS)):\n",
    "        layer = QConv2D(\n",
    "        CONV_LAYER_DEPTHS[i],\n",
    "        kernel_size=CONV_LAYER_KERNELS[i],\n",
    "        strides=CONV_LAYER_STRIDES[i],\n",
    "        kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        padding='same',\n",
    "        use_bias=True,\n",
    "        name=f'conv{i+1}'\n",
    "        )(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS), name=f'relu{i+1}')(layer)\n",
    "        layer = MaxPooling2D(pool_size=MAX_POOLING_SIZE, name=f'maxpool{i+1}')(layer)\n",
    "\n",
    "    # Flatten the output to feed into a dense layer\n",
    "    layer = Flatten(name='flattened')(layer)\n",
    "\n",
    "    # Flatten and concatenate with y0 input\n",
    "    y0_input = Input(shape=(1,), name='y0_input')\n",
    "    layer = Concatenate(name='concat')([layer, y0_input])\n",
    "\n",
    "    # Post-flattening dense layers\n",
    "    for i in range(len(FLATTENED_LAYERS)):\n",
    "        layer = QDense(FLATTENED_LAYERS[i], kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name=f'dense{i+1}')(layer)\n",
    "        layer = QActivation(quantized_relu(10), name=f'relu{len(CONV_LAYER_DEPTHS)+i+1}')(layer)\n",
    "\n",
    "    # Output layer (adjust based on your classification problem)\n",
    "    output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    # layer = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0), \n",
    "    #                bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name='output_dense')(layer)\n",
    "    # output = Activation(\"sigmoid\", name='output_sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=[y_profile_input, y0_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy']) # loss='binary_crossentropy'\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Pruning the model\n",
    "def pruneFunction(layer, train_data_size, hyperparams):\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    FINAL_SPARSITY = hyperparams[\"FINAL_SPARSITY\"]\n",
    "    PRUNE_START_EPOCH = hyperparams[\"PRUNE_START_EPOCH\"]\n",
    "    NUM_PRUNE_EPOCHS = hyperparams[\"NUM_PRUNE_EPOCHS\"]\n",
    "\n",
    "    steps_per_epoch = train_data_size // BATCH_SIZE #input_train_data_combined.shape[0]\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.0,\n",
    "            final_sparsity=FINAL_SPARSITY,\n",
    "            begin_step=steps_per_epoch * PRUNE_START_EPOCH,\n",
    "            end_step=steps_per_epoch * (PRUNE_START_EPOCH + NUM_PRUNE_EPOCHS),\n",
    "            frequency=steps_per_epoch # prune after every epoch\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "    if isinstance(layer, QDense):\n",
    "        if layer.name != 'output_softmax' and layer.name != 'dense2':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        elif layer.name != 'output_softmax' and layer.name != 'dense1':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        else:\n",
    "            print(f\"cannot prune layer {layer.name}\")\n",
    "            return layer\n",
    "\n",
    "    else:\n",
    "        print(f\"cannot prune layer {layer.name}\")\n",
    "        return layer\n",
    "    \n",
    "def pruneFunctionWrapper(train_data_size, hyperparams):\n",
    "    def wrapper(layer):\n",
    "        return pruneFunction(layer, train_data_size, hyperparams)\n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights = layer.get_weights()[0]\n",
    "            total_params += weights.size\n",
    "            zero_params += np.sum(weights == 0)\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, hyperparams):\n",
    "    input_train_data_combined = data[\"input_train_data_combined\"]\n",
    "    target_train_data_coded = data[\"target_train_data_coded\"]\n",
    "    input_validate_data_combined = data[\"input_validate_data_combined\"]\n",
    "    target_validate_data_coded = data[\"target_validate_data_coded\"]\n",
    "\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    PATIENCE = hyperparams[\"PATIENCE\"]\n",
    "    EPOCHS = hyperparams[\"EPOCHS\"]\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    POST_PRUNE_EPOCHS = hyperparams[\"POST_PRUNE_EPOCHS\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define the model\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        model = qDNNmodel(hyperparams)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        model = qCNNmodel(hyperparams)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported model type\")\n",
    "\n",
    "    model.summary()\n",
    "    print_qmodel_summary(model)\n",
    "    print(f\"Initial Sparsity: {calculate_sparsity(model) * 100:.2f}%\")\n",
    "\n",
    "    train_metrics = {}\n",
    "\n",
    "    # Train the model\n",
    "    earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=PATIENCE, restore_best_weights=True)\n",
    "    print(\"shape 12323 is \", target_train_data_coded.shape, \"data is like\", target_validate_data_coded[1:5])\n",
    "    history = model.fit(\n",
    "        input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "        validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[earlyStop_callback]\n",
    "    )\n",
    "    # Best at this step val_loss 0.7085\n",
    "    train_metrics[\"val_loss\"] = history.history['val_loss'][-1]\n",
    "\n",
    "    # Prune the DNN model \n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        model_pruned = keras.models.clone_model(model, clone_function=pruneFunctionWrapper(input_train_data_combined.shape[0], hyperparams))\n",
    "        model_pruned.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Re-train the pruned model\n",
    "        history = model_pruned.fit(\n",
    "            input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "            validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "            epochs=POST_PRUNE_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks = [pruning_callbacks.UpdatePruningStep()]\n",
    "        ) \n",
    "        # best at this step val_loss 0.4314\n",
    "\n",
    "        model = strip_pruning(model_pruned)\n",
    "        # train_metrics[\"pruned_sparsity\"] = calculate_sparsity(model)\n",
    "\n",
    "    try:\n",
    "        train_metrics[\"pruned_val_loss\"] = history.history['val_loss'][-1]\n",
    "    except:\n",
    "        print(\"Error: no post-pruning val_loss found\")\n",
    "        train_metrics[\"pruned_val_loss\"] = train_metrics[\"val_loss\"]\n",
    "\n",
    "    return model, train_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data, model, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    input_test_data_combined = data[\"input_test_data_combined\"]\n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "\n",
    "    \n",
    "    # Test the model at threshold 0.5\n",
    "    predictions = model.predict(input_test_data_combined)\n",
    "    print(predictions[:10, :])\n",
    "    predictions_prob = predictions[:,0]\n",
    "    predictions_labels = (predictions_prob >= 0.5).astype(int).flatten()\n",
    "\n",
    "    # Test the model at different thresholds\n",
    "    thresholds = np.linspace(0.0, 1.0, 1000)\n",
    "    signal_efficiencies = []\n",
    "    background_rejections = []\n",
    "    max_sum_se = 0\n",
    "    max_sum_br = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # predicted_class = ((predictions_prob[:, 0] + threshold > predictions_prob[:, 1]) & (predictions_prob[:, 0] + threshold > predictions_prob[:, 2])).astype(int)\n",
    "        predicted_class = (predictions_prob > threshold).astype(int)\n",
    "        # Compute confusion matrix\n",
    "        if OUTPUT == \"SINGLE\":\n",
    "            cm = confusion_matrix(target_test_data_coded[:], predicted_class)\n",
    "        else:\n",
    "            cm = confusion_matrix(target_test_data_coded[:, 0], predicted_class)\n",
    "\n",
    "        # Calculate signal efficiency and background rejection\n",
    "        signal_efficiency = cm[1, 1] / np.sum(cm[1, :])\n",
    "        background_rejection = cm[0, 0] / np.sum(cm[0, :])\n",
    "\n",
    "        # Store metrics\n",
    "        signal_efficiencies.append(signal_efficiency)\n",
    "        background_rejections.append(background_rejection)\n",
    "\n",
    "        # get maximum added score\n",
    "        if signal_efficiency + background_rejection > max_sum_se + max_sum_br:\n",
    "            max_sum_se = signal_efficiency\n",
    "            max_sum_br = background_rejection\n",
    "    \n",
    "    test_results = {\n",
    "        \"predictions_prob\": predictions_prob,\n",
    "        \"predictions_labels\": predictions_labels,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"signal_efficiencies\": signal_efficiencies,\n",
    "        \"background_rejections\": background_rejections,\n",
    "        \"max_sum_se\": max_sum_se,\n",
    "        \"max_sum_br\": max_sum_br,\n",
    "    }\n",
    "\n",
    "    return test_results\n",
    "\n",
    "def ShowConfusionMatrix(data, test_results, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "    predictions_labels = test_results[\"predictions_labels\"]\n",
    "\n",
    "    if OUTPUT == \"SINGLE\":\n",
    "        cm = confusion_matrix(target_test_data_coded[:], predictions_labels)\n",
    "    else:\n",
    "        cm = confusion_matrix(target_test_data_coded[:, 0], predictions_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='YlGnBu')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def showMetricsByThreshold(test_results):\n",
    "    thresholds = test_results[\"thresholds\"]\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, signal_efficiencies, label='Signal Efficiency')\n",
    "    plt.plot(thresholds, background_rejections, label='Background Rejection')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Effect of Threshold on Signal Efficiency and Background Rejection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def showEfficiencyVSRejection(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(signal_efficiencies, background_rejections, marker='o')\n",
    "    plt.xlabel('Signal Efficiency')\n",
    "    plt.ylabel('Background Rejection')\n",
    "    plt.title('Background Rejection vs. Signal Efficiency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def find_closest(sorted_array, value):\n",
    "    # Ensure the array is a NumPy array\n",
    "    sorted_array = np.array(sorted_array)\n",
    "    # Compute the absolute difference\n",
    "    abs_diff = np.abs(sorted_array - value)\n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = np.argmin(abs_diff)\n",
    "    return closest_index\n",
    "\n",
    "def getTargetMetrics(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    target_efficiencies = [0.873, 0.90, 0.93, 0.96, 0.98, 0.99, 0.995, 0.999]\n",
    "    metrics = []\n",
    "    for target in target_efficiencies:\n",
    "        index = find_closest(signal_efficiencies, target)\n",
    "        metrics.append((signal_efficiencies[index], background_rejections[index]))\n",
    "        # print(f\"Signal Efficiency: {signal_efficiencies[index]*100:.1f}%,\",f\"Background Rejections: {background_rejections[index]*100:.1f}%\")\n",
    "    return metrics\n",
    "\n",
    "def displayPerformance(data, test_results, metrics, hyperparams):\n",
    "    ShowConfusionMatrix(data, test_results, hyperparams)\n",
    "    showMetricsByThreshold(test_results)\n",
    "    showEfficiencyVSRejection(test_results)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(metrics):\n",
    "    # Convert metrics list of tuples to a formatted string\n",
    "    return \", \".join([f\"({m1:.2f}, {m2:.2f})\" for m1, m2 in metrics])\n",
    "\n",
    "def hyperparameter_search(data, base_hyperparams, param_grid, result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file if it exists\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        hyperparams = dict(zip(keys, v))\n",
    "        # Update base hyperparameters with the current set\n",
    "        current_hyperparams = base_hyperparams.copy()\n",
    "        current_hyperparams.update(hyperparams)\n",
    "\n",
    "        # Convert hyperparameters to a string for use as a dictionary key\n",
    "        hyperparams_str = json.dumps(current_hyperparams, sort_keys=True)\n",
    "\n",
    "        # Check if these hyperparameters have been tried before\n",
    "        if hyperparams_str in all_results:\n",
    "            print(f\"Skipping already tested hyperparameters: {current_hyperparams}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Testing hyperparameters: {current_hyperparams}\")\n",
    "\n",
    "        # Train the model\n",
    "        model, train_metrics = train_model(data, current_hyperparams)\n",
    "\n",
    "        # Test the model\n",
    "        test_results = test_model(data, model, current_hyperparams)\n",
    "        metrics = format_metrics(getTargetMetrics(test_results))\n",
    "        \n",
    "        test_scores = {\n",
    "            \"max_sum_se\": test_results[\"max_sum_se\"],\n",
    "            \"max_sum_br\": test_results[\"max_sum_br\"],\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        # Add all keys and values from train_metrics into test_scores\n",
    "        test_scores.update(train_metrics)\n",
    "\n",
    "        # Save the results to the file\n",
    "        all_results[hyperparams_str] = test_scores\n",
    "        with open(result_file, 'w') as file:\n",
    "            json.dump(all_results, file, indent=4)\n",
    "\n",
    "\n",
    "        # If new best found, email alex\n",
    "        if test_scores[\"pruned_val_loss\"] < find_min_pruned_val_loss(result_file):\n",
    "\n",
    "            # Format metrics for the email\n",
    "            metrics_str = \", \".join([f\"({m1:.3f}, {m2:.3f})\" for m1, m2 in test_scores[\"metrics\"]])\n",
    "\n",
    "            # email results\n",
    "            model_name = \"undefined\"\n",
    "            if (current_hyperparams[\"MODEL_TYPE\"] == \"DNN\"):\n",
    "                model_name = \"Dean\"\n",
    "            elif (current_hyperparams[\"MODEL_TYPE\"] == \"CNN\"):\n",
    "                model_name = \"Connor\"\n",
    "            send_email_notification(\"ML Training Report\", \n",
    "                f' \\\n",
    "                Your model {model_name} has finished training. \\\n",
    "                He got a grade of {test_scores[\"max_sum_se\"]*100:.1f}% in SE and {test_scores[\"max_sum_br\"]*100:.1f}% in BR. \\\n",
    "                New lowest validated loss: {test_scores[\"pruned_val_loss\"]} \\n \\\n",
    "                All metrics: {metrics_str} \\n \\\n",
    "                Hyperparams: {hyperparams_str} \\\n",
    "                ')\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def find_min_pruned_val_loss(result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        print(f\"No results found in {result_file}\")\n",
    "        return None\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    min_hyperparams = None\n",
    "\n",
    "    # Iterate through the results to find the minimum pruned_val_loss\n",
    "    for hyperparams_str, results in all_results.items():\n",
    "        if \"pruned_val_loss\" in results:\n",
    "            pruned_val_loss = results[\"pruned_val_loss\"]\n",
    "            if pruned_val_loss < min_loss:\n",
    "                min_loss = pruned_val_loss\n",
    "                min_hyperparams = hyperparams_str\n",
    "\n",
    "    # Print the hyperparameters with the minimum pruned_val_loss\n",
    "    if min_hyperparams is not None:\n",
    "        print(f\"Hyperparameters with minimum pruned_val_loss: {min_hyperparams}\")\n",
    "        print(f\"Minimum pruned_val_loss: {min_loss}\")\n",
    "    else:\n",
    "        print(\"No entry with pruned_val_loss found\")\n",
    "\n",
    "    return min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 6, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 4, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [64], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.4, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 4}\n",
      "Minimum pruned_val_loss: 0.6911848783493042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6911848783493042"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    # Model Type\n",
    "    \"MODEL_TYPE\": \"DNN\",  # DNN or CNN\n",
    "    # Input format\n",
    "    \"NUM_TIME_SLICES\": 8,\n",
    "    \"TRAIN_PT_THRESHOLD\": 2,  # in GeV\n",
    "    \"TEST_PT_THRESHOLD\": 2,  # in GeV\n",
    "    # DNN model mormat\n",
    "    \"DNN_LAYERS\": [8],\n",
    "    # CNN model format\n",
    "    \"CONV_LAYER_DEPTHS\": [4, 7],\n",
    "    \"CONV_LAYER_KERNELS\": [(3, 3), (3, 3)],\n",
    "    \"CONV_LAYER_STRIDES\": [(1, 1), (1, 1)],\n",
    "    \"FLATTENED_LAYERS\": [7],\n",
    "    \"MAX_POOLING_SIZE\": (2, 2),\n",
    "    # Output function\n",
    "    \"OUTPUT\": \"SINGLE\", # SOFTMAX or ARGMAX (not working) or LINEAR or SINGLE\n",
    "    # Model quantization\n",
    "    \"WEIGHTS_BITS\": 10,\n",
    "    \"BIAS_BITS\": 10,\n",
    "    \"ACTIVATION_BITS\": 15,\n",
    "    \"INTEGER_BITS\": 2,\n",
    "    # Training\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"BATCH_SIZE\": 1024,  # Number of samples per gradient update\n",
    "    \"EPOCHS\": 100,  # Number of epochs to train\n",
    "    \"PATIENCE\": 20,  # Stop after this number of epochs without improvement\n",
    "    # Pruning\n",
    "    \"PRUNE_START_EPOCH\": 0,  # Number of epochs before pruning\n",
    "    \"NUM_PRUNE_EPOCHS\": 10,\n",
    "    \"FINAL_SPARSITY\": 0.0,\n",
    "    \"POST_PRUNE_EPOCHS\": 50,\n",
    "}\n",
    "\n",
    "SAVE_FILE = \"tiny_single_output\"+HYPERPARAMETERS[\"MODEL_TYPE\"]+\"_results.json\"\n",
    "\n",
    "param_grid = {\n",
    "    \"DNN_LAYERS\": [[32], [8], [16, 8]],\n",
    "    \"FINAL_SPARSITY\": [0.0, 0.3, 0.5], \n",
    "    \"LEARNING_RATE\": [0.005, 0.001, 0.0005],\n",
    "}\n",
    "\n",
    "find_min_pruned_val_loss(result_file=(SAVE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 376.23 GB\n",
      "Available memory: 294.35 GB\n",
      "Used memory: 71.16 GB\n",
      "Memory usage percentage: 21.8%\n",
      "Total memory: 376.23 GB\n",
      "Available memory: 282.78 GB\n",
      "Used memory: 82.70 GB\n",
      "Memory usage percentage: 24.8%\n"
     ]
    }
   ],
   "source": [
    "input_data, target_data = load_dataset()\n",
    "data = process_dataset(input_data, target_data, HYPERPARAMETERS) # Depends only on: NUM_TIME_SLICES MODEL_TYPE TRAIN_PT_THRESHOLD TEST_PT_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32)                128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization  is normal keras bn layer\n",
      "q_activation         quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 1.0083 - accuracy: 0.5021 - val_loss: 0.7014 - val_accuracy: 0.5033\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.5056 - val_loss: 0.6964 - val_accuracy: 0.5037\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5069 - val_loss: 0.6943 - val_accuracy: 0.5083\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6937 - accuracy: 0.5093 - val_loss: 0.6949 - val_accuracy: 0.5075\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6951 - val_accuracy: 0.5069\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5099 - val_loss: 0.6937 - val_accuracy: 0.5087\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6933 - val_accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5097 - val_loss: 0.6932 - val_accuracy: 0.5146\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6945 - val_accuracy: 0.5119\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5131 - val_loss: 0.6941 - val_accuracy: 0.5118\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5121 - val_loss: 0.6959 - val_accuracy: 0.5010\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5126 - val_loss: 0.6931 - val_accuracy: 0.5139\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6925 - val_accuracy: 0.5127\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5131\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.5128\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6925 - val_accuracy: 0.5129\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6925 - val_accuracy: 0.5143\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5154 - val_loss: 0.6932 - val_accuracy: 0.5084\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6925 - val_accuracy: 0.5110\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6924 - val_accuracy: 0.5176\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5148 - val_loss: 0.6932 - val_accuracy: 0.5107\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5149 - val_loss: 0.6924 - val_accuracy: 0.5170\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5153 - val_loss: 0.6922 - val_accuracy: 0.5136\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5159 - val_loss: 0.6927 - val_accuracy: 0.5154\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5156 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5156 - val_loss: 0.6926 - val_accuracy: 0.5083\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5167 - val_loss: 0.6920 - val_accuracy: 0.5154\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5172 - val_loss: 0.6927 - val_accuracy: 0.5147\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6936 - val_accuracy: 0.5132\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5172 - val_loss: 0.6935 - val_accuracy: 0.5107\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5162 - val_loss: 0.6918 - val_accuracy: 0.5176\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5181 - val_loss: 0.6919 - val_accuracy: 0.5202\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5163 - val_loss: 0.6918 - val_accuracy: 0.5180\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5188 - val_loss: 0.6935 - val_accuracy: 0.5122\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5171 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5187 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5165 - val_loss: 0.6939 - val_accuracy: 0.5147\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5169 - val_loss: 0.6923 - val_accuracy: 0.5147\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5192 - val_loss: 0.6926 - val_accuracy: 0.5083\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6926 - val_accuracy: 0.5091\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5194 - val_loss: 0.6920 - val_accuracy: 0.5117\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5181 - val_loss: 0.6915 - val_accuracy: 0.5153\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5193 - val_loss: 0.6915 - val_accuracy: 0.5198\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5188 - val_loss: 0.6917 - val_accuracy: 0.5181\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5183 - val_loss: 0.6927 - val_accuracy: 0.5098\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5188 - val_loss: 0.6912 - val_accuracy: 0.5176\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5196 - val_loss: 0.6918 - val_accuracy: 0.5174\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5189 - val_loss: 0.6919 - val_accuracy: 0.5187\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5203 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5209 - val_loss: 0.6919 - val_accuracy: 0.5177\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5164\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5182 - val_loss: 0.7033 - val_accuracy: 0.5050\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5153 - val_loss: 0.6914 - val_accuracy: 0.5183\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5190 - val_loss: 0.6918 - val_accuracy: 0.5200\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5205 - val_loss: 0.6915 - val_accuracy: 0.5160\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5208 - val_loss: 0.6920 - val_accuracy: 0.5124\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5191 - val_loss: 0.6920 - val_accuracy: 0.5181\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5201 - val_loss: 0.6914 - val_accuracy: 0.5168\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5197 - val_loss: 0.6922 - val_accuracy: 0.5131\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5195 - val_loss: 0.6918 - val_accuracy: 0.5185\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5198 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5197 - val_loss: 0.6918 - val_accuracy: 0.5141\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5193 - val_loss: 0.6911 - val_accuracy: 0.5174\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5196 - val_loss: 0.6915 - val_accuracy: 0.5168\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5200\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5201 - val_loss: 0.6918 - val_accuracy: 0.5133\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5207 - val_loss: 0.6919 - val_accuracy: 0.5187\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5189 - val_loss: 0.6914 - val_accuracy: 0.5198\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5198 - val_loss: 0.6932 - val_accuracy: 0.5071\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5213 - val_loss: 0.6929 - val_accuracy: 0.5063\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5196 - val_loss: 0.6913 - val_accuracy: 0.5211\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5213 - val_loss: 0.6914 - val_accuracy: 0.5214\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5211 - val_loss: 0.6917 - val_accuracy: 0.5142\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5208 - val_loss: 0.6916 - val_accuracy: 0.5200\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5209 - val_loss: 0.6912 - val_accuracy: 0.5215\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5198 - val_loss: 0.6914 - val_accuracy: 0.5192\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5173\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5219 - val_loss: 0.6918 - val_accuracy: 0.5182\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5201 - val_loss: 0.6914 - val_accuracy: 0.5166\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5190 - val_loss: 0.6917 - val_accuracy: 0.5204\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5210 - val_loss: 0.6909 - val_accuracy: 0.5178\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5198 - val_loss: 0.6917 - val_accuracy: 0.5164\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5210 - val_loss: 0.6916 - val_accuracy: 0.5210\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5206 - val_loss: 0.6916 - val_accuracy: 0.5150\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5208 - val_loss: 0.6917 - val_accuracy: 0.5146\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5211 - val_loss: 0.6910 - val_accuracy: 0.5199\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5195 - val_loss: 0.6920 - val_accuracy: 0.5179\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5200 - val_loss: 0.6917 - val_accuracy: 0.5131\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5211 - val_loss: 0.6909 - val_accuracy: 0.5179\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5208 - val_loss: 0.6910 - val_accuracy: 0.5211\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5202 - val_loss: 0.6910 - val_accuracy: 0.5231\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5212 - val_loss: 0.6919 - val_accuracy: 0.5143\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5204 - val_loss: 0.6916 - val_accuracy: 0.5203\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5215 - val_loss: 0.6915 - val_accuracy: 0.5229\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5221 - val_loss: 0.6919 - val_accuracy: 0.5125\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5206 - val_loss: 0.6931 - val_accuracy: 0.5104\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization\n",
      "cannot prune layer q_activation\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.6901 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5206\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5211 - val_loss: 0.6913 - val_accuracy: 0.5194\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5213 - val_loss: 0.6916 - val_accuracy: 0.5160\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5222 - val_loss: 0.6917 - val_accuracy: 0.5197\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5219 - val_loss: 0.6919 - val_accuracy: 0.5204\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5225 - val_loss: 0.6910 - val_accuracy: 0.5164\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5209 - val_loss: 0.6909 - val_accuracy: 0.5208\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5224 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5194\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5207 - val_loss: 0.6912 - val_accuracy: 0.5186\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5215 - val_loss: 0.6912 - val_accuracy: 0.5160\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5218 - val_loss: 0.6922 - val_accuracy: 0.5200\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5214 - val_loss: 0.6908 - val_accuracy: 0.5205\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5209 - val_loss: 0.6927 - val_accuracy: 0.5136\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5216 - val_loss: 0.6906 - val_accuracy: 0.5212\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5215 - val_loss: 0.6912 - val_accuracy: 0.5210\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5210 - val_loss: 0.6913 - val_accuracy: 0.5225\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5220 - val_loss: 0.6910 - val_accuracy: 0.5202\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5217 - val_loss: 0.6929 - val_accuracy: 0.5143\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5214 - val_loss: 0.6928 - val_accuracy: 0.5145\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5222 - val_loss: 0.6910 - val_accuracy: 0.5212\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5214 - val_loss: 0.6914 - val_accuracy: 0.5158\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5216 - val_loss: 0.6905 - val_accuracy: 0.5205\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5215 - val_loss: 0.6919 - val_accuracy: 0.5218\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5214 - val_loss: 0.6918 - val_accuracy: 0.5168\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5226 - val_loss: 0.6927 - val_accuracy: 0.5200\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5231 - val_loss: 0.6913 - val_accuracy: 0.5225\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5217 - val_loss: 0.6923 - val_accuracy: 0.5171\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5217 - val_loss: 0.6909 - val_accuracy: 0.5224\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5221 - val_loss: 0.6911 - val_accuracy: 0.5229\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5222 - val_loss: 0.6920 - val_accuracy: 0.5155\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5222 - val_loss: 0.6919 - val_accuracy: 0.5175\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5228 - val_loss: 0.6921 - val_accuracy: 0.5205\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5219 - val_loss: 0.6914 - val_accuracy: 0.5235\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5221 - val_loss: 0.6917 - val_accuracy: 0.5146\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5215 - val_loss: 0.6921 - val_accuracy: 0.5214\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5214 - val_loss: 0.6910 - val_accuracy: 0.5187\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5225 - val_loss: 0.6909 - val_accuracy: 0.5241\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5228 - val_loss: 0.6910 - val_accuracy: 0.5189\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5228 - val_loss: 0.6914 - val_accuracy: 0.5186\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5242\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5228 - val_loss: 0.6922 - val_accuracy: 0.5192\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6909 - val_accuracy: 0.5192\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6914 - val_accuracy: 0.5221\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5236 - val_loss: 0.6918 - val_accuracy: 0.5134\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5224 - val_loss: 0.6916 - val_accuracy: 0.5223\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5223 - val_loss: 0.6908 - val_accuracy: 0.5208\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5230 - val_loss: 0.6911 - val_accuracy: 0.5192\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5221 - val_loss: 0.6924 - val_accuracy: 0.5202\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5219 - val_loss: 0.6909 - val_accuracy: 0.5172\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.49271134]\n",
      " [0.44613445]\n",
      " [0.36864293]\n",
      " [0.4831082 ]\n",
      " [0.4414056 ]\n",
      " [0.4911852 ]\n",
      " [0.47642758]\n",
      " [0.5030608 ]\n",
      " [0.5348677 ]\n",
      " [0.3936602 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.690896213054657\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_1 is normal keras bn layer\n",
      "q_activation_1       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.8280 - accuracy: 0.5023 - val_loss: 0.7278 - val_accuracy: 0.5013\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7196 - accuracy: 0.5034 - val_loss: 0.7140 - val_accuracy: 0.5055\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7095 - accuracy: 0.5047 - val_loss: 0.7078 - val_accuracy: 0.5032\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7043 - accuracy: 0.5054 - val_loss: 0.7031 - val_accuracy: 0.5047\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7013 - accuracy: 0.5064 - val_loss: 0.7018 - val_accuracy: 0.5054\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6993 - accuracy: 0.5066 - val_loss: 0.7001 - val_accuracy: 0.5075\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6979 - accuracy: 0.5076 - val_loss: 0.6991 - val_accuracy: 0.5066\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.5084 - val_loss: 0.6981 - val_accuracy: 0.5101\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5095 - val_loss: 0.6973 - val_accuracy: 0.5095\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5096 - val_loss: 0.6962 - val_accuracy: 0.5109\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5109 - val_loss: 0.6951 - val_accuracy: 0.5112\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5110 - val_loss: 0.6950 - val_accuracy: 0.5125\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5122 - val_loss: 0.6943 - val_accuracy: 0.5139\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5134 - val_loss: 0.6941 - val_accuracy: 0.5129\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5129 - val_loss: 0.6937 - val_accuracy: 0.5160\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5141 - val_loss: 0.6932 - val_accuracy: 0.5118\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5155 - val_loss: 0.6936 - val_accuracy: 0.5173\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5151 - val_loss: 0.6936 - val_accuracy: 0.5186\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5162 - val_loss: 0.6926 - val_accuracy: 0.5218\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5158 - val_loss: 0.6934 - val_accuracy: 0.5168\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5180 - val_loss: 0.6930 - val_accuracy: 0.5173\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5172 - val_loss: 0.6933 - val_accuracy: 0.5197\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5170 - val_loss: 0.6924 - val_accuracy: 0.5223\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5185 - val_loss: 0.6924 - val_accuracy: 0.5197\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5230\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5206 - val_loss: 0.6937 - val_accuracy: 0.5193\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5213 - val_loss: 0.6920 - val_accuracy: 0.5184\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5207 - val_loss: 0.6917 - val_accuracy: 0.5225\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5226 - val_loss: 0.6916 - val_accuracy: 0.5248\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5236 - val_loss: 0.6924 - val_accuracy: 0.5229\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5254 - val_loss: 0.6914 - val_accuracy: 0.5224\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5251 - val_loss: 0.6917 - val_accuracy: 0.5210\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5265 - val_loss: 0.6910 - val_accuracy: 0.5276\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5261 - val_loss: 0.6911 - val_accuracy: 0.5259\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5283 - val_loss: 0.6918 - val_accuracy: 0.5301\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5287 - val_loss: 0.6907 - val_accuracy: 0.5330\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5297 - val_loss: 0.6909 - val_accuracy: 0.5265\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5302 - val_loss: 0.6907 - val_accuracy: 0.5355\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5321 - val_loss: 0.6911 - val_accuracy: 0.5337\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5314 - val_loss: 0.6919 - val_accuracy: 0.5263\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5316 - val_loss: 0.6907 - val_accuracy: 0.5308\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5330 - val_loss: 0.6902 - val_accuracy: 0.5281\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5341 - val_loss: 0.6910 - val_accuracy: 0.5295\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5337 - val_loss: 0.6899 - val_accuracy: 0.5334\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5358 - val_loss: 0.6902 - val_accuracy: 0.5368\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5350 - val_loss: 0.6900 - val_accuracy: 0.5343\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6885 - accuracy: 0.5364 - val_loss: 0.6906 - val_accuracy: 0.5311\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5356 - val_loss: 0.6900 - val_accuracy: 0.5384\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5357 - val_loss: 0.6893 - val_accuracy: 0.5380\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5356 - val_loss: 0.6901 - val_accuracy: 0.5369\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5365 - val_loss: 0.6906 - val_accuracy: 0.5271\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5368 - val_loss: 0.6890 - val_accuracy: 0.5369\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5383 - val_loss: 0.6890 - val_accuracy: 0.5391\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5379 - val_loss: 0.6893 - val_accuracy: 0.5407\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5382 - val_loss: 0.6912 - val_accuracy: 0.5360\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5375 - val_loss: 0.6889 - val_accuracy: 0.5370\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5377 - val_loss: 0.6903 - val_accuracy: 0.5327\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5389 - val_loss: 0.6890 - val_accuracy: 0.5407\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5398 - val_loss: 0.6889 - val_accuracy: 0.5426\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5399 - val_loss: 0.6892 - val_accuracy: 0.5415\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5405 - val_loss: 0.6890 - val_accuracy: 0.5367\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5396 - val_loss: 0.6895 - val_accuracy: 0.5396\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5397 - val_loss: 0.6923 - val_accuracy: 0.5172\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5406 - val_loss: 0.6888 - val_accuracy: 0.5349\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5407 - val_loss: 0.6892 - val_accuracy: 0.5387\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5408 - val_loss: 0.6888 - val_accuracy: 0.5435\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5419 - val_loss: 0.6892 - val_accuracy: 0.5369\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5421 - val_loss: 0.6898 - val_accuracy: 0.5359\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5380\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5419 - val_loss: 0.6892 - val_accuracy: 0.5425\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5413 - val_loss: 0.6888 - val_accuracy: 0.5434\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5435 - val_loss: 0.6880 - val_accuracy: 0.5455\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5426 - val_loss: 0.6887 - val_accuracy: 0.5454\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5414 - val_loss: 0.6890 - val_accuracy: 0.5407\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5427 - val_loss: 0.6884 - val_accuracy: 0.5398\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5433 - val_loss: 0.6903 - val_accuracy: 0.5373\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5432 - val_loss: 0.6886 - val_accuracy: 0.5418\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5430 - val_loss: 0.6884 - val_accuracy: 0.5444\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5438 - val_loss: 0.6878 - val_accuracy: 0.5449\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5437 - val_loss: 0.6879 - val_accuracy: 0.5485\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5432 - val_loss: 0.6896 - val_accuracy: 0.5386\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5432 - val_loss: 0.6895 - val_accuracy: 0.5372\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5438 - val_loss: 0.6875 - val_accuracy: 0.5460\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5436 - val_loss: 0.6879 - val_accuracy: 0.5424\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5433 - val_loss: 0.6886 - val_accuracy: 0.5459\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5445 - val_loss: 0.6877 - val_accuracy: 0.5426\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5448 - val_loss: 0.6891 - val_accuracy: 0.5329\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5451 - val_loss: 0.6877 - val_accuracy: 0.5424\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5447 - val_loss: 0.6882 - val_accuracy: 0.5447\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5461 - val_loss: 0.6872 - val_accuracy: 0.5445\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5457 - val_loss: 0.6875 - val_accuracy: 0.5415\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5458 - val_loss: 0.6882 - val_accuracy: 0.5452\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5462 - val_loss: 0.6870 - val_accuracy: 0.5426\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5465 - val_loss: 0.6884 - val_accuracy: 0.5452\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5468 - val_loss: 0.6878 - val_accuracy: 0.5407\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5453 - val_loss: 0.6885 - val_accuracy: 0.5453\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5469 - val_loss: 0.6872 - val_accuracy: 0.5519\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.5465 - val_loss: 0.6875 - val_accuracy: 0.5400\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5465 - val_loss: 0.6889 - val_accuracy: 0.5401\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5467 - val_loss: 0.6871 - val_accuracy: 0.5469\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_1\n",
      "cannot prune layer q_activation_1\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.6849 - accuracy: 0.5472 - val_loss: 0.6879 - val_accuracy: 0.5457\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5475 - val_loss: 0.6866 - val_accuracy: 0.5507\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5473 - val_loss: 0.6858 - val_accuracy: 0.5503\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5467 - val_loss: 0.6859 - val_accuracy: 0.5530\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5472 - val_loss: 0.6869 - val_accuracy: 0.5476\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5473 - val_loss: 0.6877 - val_accuracy: 0.5444\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5482 - val_loss: 0.6877 - val_accuracy: 0.5504\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5485 - val_loss: 0.6868 - val_accuracy: 0.5429\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5484 - val_loss: 0.6868 - val_accuracy: 0.5509\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5483 - val_loss: 0.6878 - val_accuracy: 0.5470\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5480 - val_loss: 0.6864 - val_accuracy: 0.5451\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5494 - val_loss: 0.6880 - val_accuracy: 0.5433\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5479 - val_loss: 0.6864 - val_accuracy: 0.5465\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5495 - val_loss: 0.6882 - val_accuracy: 0.5389\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5497 - val_loss: 0.6873 - val_accuracy: 0.5490\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5498 - val_loss: 0.6875 - val_accuracy: 0.5421\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5509 - val_loss: 0.6872 - val_accuracy: 0.5510\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5491 - val_loss: 0.6868 - val_accuracy: 0.5531\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5485 - val_loss: 0.6870 - val_accuracy: 0.5483\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5490 - val_loss: 0.6878 - val_accuracy: 0.5375\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5489 - val_loss: 0.6868 - val_accuracy: 0.5459\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5505 - val_loss: 0.6873 - val_accuracy: 0.5431\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5498 - val_loss: 0.6895 - val_accuracy: 0.5467\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5495 - val_loss: 0.6870 - val_accuracy: 0.5522\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5501 - val_loss: 0.6869 - val_accuracy: 0.5487\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5512 - val_loss: 0.6882 - val_accuracy: 0.5339\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5492 - val_loss: 0.6864 - val_accuracy: 0.5554\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5502 - val_loss: 0.6870 - val_accuracy: 0.5479\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5501 - val_loss: 0.6857 - val_accuracy: 0.5484\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5499 - val_loss: 0.6854 - val_accuracy: 0.5491\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5516 - val_loss: 0.6864 - val_accuracy: 0.5512\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5505 - val_loss: 0.6879 - val_accuracy: 0.5506\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5500 - val_loss: 0.6866 - val_accuracy: 0.5519\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5513 - val_loss: 0.6864 - val_accuracy: 0.5477\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5501 - val_loss: 0.6869 - val_accuracy: 0.5494\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5507 - val_loss: 0.6873 - val_accuracy: 0.5472\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5521 - val_loss: 0.6868 - val_accuracy: 0.5526\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5503 - val_loss: 0.6863 - val_accuracy: 0.5438\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6832 - accuracy: 0.5513 - val_loss: 0.6853 - val_accuracy: 0.5527\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.6867 - val_accuracy: 0.5513\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5515 - val_loss: 0.6880 - val_accuracy: 0.5354\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5518 - val_loss: 0.6890 - val_accuracy: 0.5408\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5522 - val_loss: 0.6853 - val_accuracy: 0.5521\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6832 - accuracy: 0.5515 - val_loss: 0.6863 - val_accuracy: 0.5514\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5528 - val_loss: 0.6858 - val_accuracy: 0.5555\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5534 - val_loss: 0.6858 - val_accuracy: 0.5503\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5436 - val_loss: 0.6887 - val_accuracy: 0.5477\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5459 - val_loss: 0.6885 - val_accuracy: 0.5348\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5500 - val_loss: 0.6866 - val_accuracy: 0.5503\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.6872 - val_accuracy: 0.5484\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.594092  ]\n",
      " [0.5417926 ]\n",
      " [0.5669484 ]\n",
      " [0.5669919 ]\n",
      " [0.47461084]\n",
      " [0.5343758 ]\n",
      " [0.42844597]\n",
      " [0.48886654]\n",
      " [0.49869126]\n",
      " [0.51261735]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6871724724769592\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_2 is normal keras bn layer\n",
      "q_activation_2       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 1.0781 - accuracy: 0.5035 - val_loss: 0.7507 - val_accuracy: 0.5057\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7342 - accuracy: 0.5050 - val_loss: 0.7269 - val_accuracy: 0.5052\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7197 - accuracy: 0.5044 - val_loss: 0.7179 - val_accuracy: 0.5044\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7118 - accuracy: 0.5051 - val_loss: 0.7110 - val_accuracy: 0.5083\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7074 - accuracy: 0.5059 - val_loss: 0.7083 - val_accuracy: 0.5064\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7043 - accuracy: 0.5062 - val_loss: 0.7038 - val_accuracy: 0.5086\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7021 - accuracy: 0.5076 - val_loss: 0.7015 - val_accuracy: 0.5104\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7001 - accuracy: 0.5083 - val_loss: 0.6998 - val_accuracy: 0.5091\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6988 - accuracy: 0.5089 - val_loss: 0.6984 - val_accuracy: 0.5124\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5104 - val_loss: 0.6975 - val_accuracy: 0.5130\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5100 - val_loss: 0.6966 - val_accuracy: 0.5126\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5105 - val_loss: 0.6980 - val_accuracy: 0.5063\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5112 - val_loss: 0.6958 - val_accuracy: 0.5133\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5117 - val_loss: 0.6955 - val_accuracy: 0.5088\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5125 - val_loss: 0.6953 - val_accuracy: 0.5078\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5124 - val_loss: 0.6945 - val_accuracy: 0.5147\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5132 - val_loss: 0.6948 - val_accuracy: 0.5061\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5148 - val_loss: 0.6941 - val_accuracy: 0.5084\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5130 - val_loss: 0.6949 - val_accuracy: 0.5055\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6936 - val_accuracy: 0.5175\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5146 - val_loss: 0.6944 - val_accuracy: 0.5105\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5155 - val_loss: 0.6938 - val_accuracy: 0.5123\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5154 - val_loss: 0.6940 - val_accuracy: 0.5128\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5147 - val_loss: 0.6937 - val_accuracy: 0.5168\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5149 - val_loss: 0.6930 - val_accuracy: 0.5205\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5166 - val_loss: 0.6935 - val_accuracy: 0.5127\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5163 - val_loss: 0.6932 - val_accuracy: 0.5165\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.6929 - val_accuracy: 0.5190\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5167 - val_loss: 0.6931 - val_accuracy: 0.5223\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5203\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5173 - val_loss: 0.6938 - val_accuracy: 0.5158\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5162 - val_loss: 0.6927 - val_accuracy: 0.5206\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5178 - val_loss: 0.6930 - val_accuracy: 0.5169\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5175 - val_loss: 0.6928 - val_accuracy: 0.5172\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5180 - val_loss: 0.6928 - val_accuracy: 0.5196\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.5223\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5191 - val_loss: 0.6926 - val_accuracy: 0.5185\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5068\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5209 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5207 - val_loss: 0.6927 - val_accuracy: 0.5215\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5215 - val_loss: 0.6932 - val_accuracy: 0.5176\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5200 - val_loss: 0.6927 - val_accuracy: 0.5227\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5213 - val_loss: 0.6925 - val_accuracy: 0.5147\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5222 - val_loss: 0.6921 - val_accuracy: 0.5205\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5219 - val_loss: 0.6923 - val_accuracy: 0.5229\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5229 - val_loss: 0.6921 - val_accuracy: 0.5234\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5226 - val_loss: 0.6921 - val_accuracy: 0.5197\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5234 - val_loss: 0.6919 - val_accuracy: 0.5242\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5241 - val_loss: 0.6921 - val_accuracy: 0.5246\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5247 - val_loss: 0.6924 - val_accuracy: 0.5207\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5241 - val_loss: 0.6920 - val_accuracy: 0.5252\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5255 - val_loss: 0.6922 - val_accuracy: 0.5249\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5256 - val_loss: 0.6917 - val_accuracy: 0.5240\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5263 - val_loss: 0.6916 - val_accuracy: 0.5241\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5270\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5267 - val_loss: 0.6915 - val_accuracy: 0.5287\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5274 - val_loss: 0.6918 - val_accuracy: 0.5301\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5257 - val_loss: 0.6913 - val_accuracy: 0.5240\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5282 - val_loss: 0.6914 - val_accuracy: 0.5307\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5286 - val_loss: 0.6917 - val_accuracy: 0.5238\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5287 - val_loss: 0.6931 - val_accuracy: 0.5191\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5295 - val_loss: 0.6912 - val_accuracy: 0.5254\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5296 - val_loss: 0.6910 - val_accuracy: 0.5286\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5316 - val_loss: 0.6912 - val_accuracy: 0.5283\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5310 - val_loss: 0.6904 - val_accuracy: 0.5339\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5310 - val_loss: 0.6917 - val_accuracy: 0.5281\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5297 - val_loss: 0.6910 - val_accuracy: 0.5312\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5322 - val_loss: 0.6908 - val_accuracy: 0.5264\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5330 - val_loss: 0.6901 - val_accuracy: 0.5287\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5343 - val_loss: 0.6899 - val_accuracy: 0.5325\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5344 - val_loss: 0.6902 - val_accuracy: 0.5358\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5351 - val_loss: 0.6902 - val_accuracy: 0.5260\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5356 - val_loss: 0.6905 - val_accuracy: 0.5330\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5345 - val_loss: 0.6895 - val_accuracy: 0.5390\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5363 - val_loss: 0.6899 - val_accuracy: 0.5339\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5364 - val_loss: 0.6892 - val_accuracy: 0.5418\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5375 - val_loss: 0.6894 - val_accuracy: 0.5307\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5372 - val_loss: 0.6900 - val_accuracy: 0.5383\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5378 - val_loss: 0.6901 - val_accuracy: 0.5374\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5373 - val_loss: 0.6889 - val_accuracy: 0.5372\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5381 - val_loss: 0.6893 - val_accuracy: 0.5389\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5380 - val_loss: 0.6898 - val_accuracy: 0.5374\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5385 - val_loss: 0.6892 - val_accuracy: 0.5421\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5395 - val_loss: 0.6891 - val_accuracy: 0.5395\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5402 - val_loss: 0.6893 - val_accuracy: 0.5360\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5394 - val_loss: 0.6889 - val_accuracy: 0.5420\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5394 - val_loss: 0.6885 - val_accuracy: 0.5357\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5403 - val_loss: 0.6887 - val_accuracy: 0.5417\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5406 - val_loss: 0.6892 - val_accuracy: 0.5367\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5413 - val_loss: 0.6895 - val_accuracy: 0.5377\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5408 - val_loss: 0.6884 - val_accuracy: 0.5437\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5412 - val_loss: 0.6884 - val_accuracy: 0.5387\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5413 - val_loss: 0.6886 - val_accuracy: 0.5424\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5418 - val_loss: 0.6885 - val_accuracy: 0.5454\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5408 - val_loss: 0.6884 - val_accuracy: 0.5414\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5420 - val_loss: 0.6886 - val_accuracy: 0.5423\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5431 - val_loss: 0.6884 - val_accuracy: 0.5460\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5437 - val_loss: 0.6881 - val_accuracy: 0.5435\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5418 - val_loss: 0.6886 - val_accuracy: 0.5407\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5422 - val_loss: 0.6894 - val_accuracy: 0.5256\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_2\n",
      "cannot prune layer q_activation_2\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6863 - accuracy: 0.5426 - val_loss: 0.6883 - val_accuracy: 0.5430\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5416 - val_loss: 0.6884 - val_accuracy: 0.5412\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5442 - val_loss: 0.6884 - val_accuracy: 0.5438\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5434 - val_loss: 0.6882 - val_accuracy: 0.5408\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5430 - val_loss: 0.6878 - val_accuracy: 0.5438\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5444 - val_loss: 0.6873 - val_accuracy: 0.5453\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5445 - val_loss: 0.6875 - val_accuracy: 0.5479\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5449 - val_loss: 0.6883 - val_accuracy: 0.5467\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5443 - val_loss: 0.6899 - val_accuracy: 0.5302\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5450 - val_loss: 0.6883 - val_accuracy: 0.5461\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5450 - val_loss: 0.6880 - val_accuracy: 0.5433\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5457 - val_loss: 0.6881 - val_accuracy: 0.5440\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5457 - val_loss: 0.6885 - val_accuracy: 0.5393\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5462 - val_loss: 0.6889 - val_accuracy: 0.5401\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5458 - val_loss: 0.6877 - val_accuracy: 0.5408\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5475 - val_loss: 0.6877 - val_accuracy: 0.5455\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5468 - val_loss: 0.6878 - val_accuracy: 0.5440\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5467 - val_loss: 0.6879 - val_accuracy: 0.5483\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5468 - val_loss: 0.6880 - val_accuracy: 0.5436\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5480 - val_loss: 0.6879 - val_accuracy: 0.5388\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5476 - val_loss: 0.6871 - val_accuracy: 0.5487\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5482 - val_loss: 0.6872 - val_accuracy: 0.5410\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5484 - val_loss: 0.6870 - val_accuracy: 0.5498\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5479 - val_loss: 0.6879 - val_accuracy: 0.5434\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5477 - val_loss: 0.6870 - val_accuracy: 0.5464\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5493 - val_loss: 0.6866 - val_accuracy: 0.5524\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5488 - val_loss: 0.6874 - val_accuracy: 0.5482\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5482 - val_loss: 0.6876 - val_accuracy: 0.5392\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5486 - val_loss: 0.6868 - val_accuracy: 0.5449\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5491 - val_loss: 0.6866 - val_accuracy: 0.5473\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5490 - val_loss: 0.6866 - val_accuracy: 0.5469\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5495 - val_loss: 0.6866 - val_accuracy: 0.5502\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5499 - val_loss: 0.6886 - val_accuracy: 0.5499\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5497 - val_loss: 0.6871 - val_accuracy: 0.5494\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5487 - val_loss: 0.6865 - val_accuracy: 0.5476\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5493 - val_loss: 0.6862 - val_accuracy: 0.5500\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5497 - val_loss: 0.6866 - val_accuracy: 0.5487\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5490 - val_loss: 0.6874 - val_accuracy: 0.5446\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5493 - val_loss: 0.6875 - val_accuracy: 0.5431\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5494 - val_loss: 0.6877 - val_accuracy: 0.5374\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5499 - val_loss: 0.6864 - val_accuracy: 0.5470\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5498 - val_loss: 0.6865 - val_accuracy: 0.5475\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5502 - val_loss: 0.6869 - val_accuracy: 0.5529\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5504 - val_loss: 0.6858 - val_accuracy: 0.5526\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5499 - val_loss: 0.6871 - val_accuracy: 0.5485\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5500 - val_loss: 0.6871 - val_accuracy: 0.5495\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5519 - val_loss: 0.6865 - val_accuracy: 0.5476\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5499 - val_loss: 0.6858 - val_accuracy: 0.5502\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5512 - val_loss: 0.6876 - val_accuracy: 0.5421\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5502 - val_loss: 0.6859 - val_accuracy: 0.5524\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.54515415]\n",
      " [0.48840824]\n",
      " [0.48026648]\n",
      " [0.5145495 ]\n",
      " [0.49188948]\n",
      " [0.48059466]\n",
      " [0.55572367]\n",
      " [0.49326596]\n",
      " [0.53241146]\n",
      " [0.5027146 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_3 is normal keras bn layer\n",
      "q_activation_3       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.7218 - accuracy: 0.5041 - val_loss: 0.6975 - val_accuracy: 0.5053\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5077 - val_loss: 0.6957 - val_accuracy: 0.5083\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5100 - val_loss: 0.6940 - val_accuracy: 0.5073\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.5129\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5118 - val_loss: 0.6944 - val_accuracy: 0.5065\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5111 - val_loss: 0.6932 - val_accuracy: 0.5137\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5109 - val_loss: 0.6940 - val_accuracy: 0.5134\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5120 - val_loss: 0.6927 - val_accuracy: 0.5154\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5125 - val_loss: 0.6953 - val_accuracy: 0.5024\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5140 - val_loss: 0.6934 - val_accuracy: 0.5160\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5124 - val_loss: 0.6931 - val_accuracy: 0.5044\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6927 - val_accuracy: 0.5162\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5050\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5155 - val_loss: 0.6934 - val_accuracy: 0.5161\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5130 - val_loss: 0.6924 - val_accuracy: 0.5093\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5154 - val_loss: 0.6921 - val_accuracy: 0.5157\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6939 - val_accuracy: 0.5050\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5151 - val_loss: 0.6919 - val_accuracy: 0.5178\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6928 - val_accuracy: 0.5185\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5150 - val_loss: 0.6920 - val_accuracy: 0.5199\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5165 - val_loss: 0.6935 - val_accuracy: 0.5137\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5167 - val_loss: 0.6929 - val_accuracy: 0.5163\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5158 - val_loss: 0.6919 - val_accuracy: 0.5181\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6937 - val_accuracy: 0.5158\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6921 - val_accuracy: 0.5184\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5175 - val_loss: 0.6922 - val_accuracy: 0.5181\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5175 - val_loss: 0.6925 - val_accuracy: 0.5167\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5173 - val_loss: 0.6917 - val_accuracy: 0.5142\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5187 - val_loss: 0.6917 - val_accuracy: 0.5165\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5173 - val_loss: 0.6918 - val_accuracy: 0.5208\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5191 - val_loss: 0.6919 - val_accuracy: 0.5150\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5180 - val_loss: 0.6919 - val_accuracy: 0.5135\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6915 - val_accuracy: 0.5142\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5185 - val_loss: 0.6916 - val_accuracy: 0.5163\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5195 - val_loss: 0.6917 - val_accuracy: 0.5145\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6922 - val_accuracy: 0.5193\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5179 - val_loss: 0.6916 - val_accuracy: 0.5151\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5185 - val_loss: 0.6914 - val_accuracy: 0.5205\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5185 - val_loss: 0.6914 - val_accuracy: 0.5191\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5194 - val_loss: 0.6913 - val_accuracy: 0.5165\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5191 - val_loss: 0.6918 - val_accuracy: 0.5125\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5178\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5198 - val_loss: 0.6917 - val_accuracy: 0.5163\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5207 - val_loss: 0.6920 - val_accuracy: 0.5156\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5202 - val_loss: 0.6928 - val_accuracy: 0.5187\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5208 - val_loss: 0.6934 - val_accuracy: 0.5076\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5203 - val_loss: 0.6914 - val_accuracy: 0.5170\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5197 - val_loss: 0.6915 - val_accuracy: 0.5205\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5196 - val_loss: 0.6920 - val_accuracy: 0.5183\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5213 - val_loss: 0.6931 - val_accuracy: 0.5092\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5190 - val_loss: 0.6921 - val_accuracy: 0.5118\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5206 - val_loss: 0.6915 - val_accuracy: 0.5174\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5198 - val_loss: 0.6916 - val_accuracy: 0.5198\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5213 - val_loss: 0.6914 - val_accuracy: 0.5190\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5201 - val_loss: 0.6912 - val_accuracy: 0.5207\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5214 - val_loss: 0.6922 - val_accuracy: 0.5168\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5189 - val_loss: 0.6909 - val_accuracy: 0.5170\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5208 - val_loss: 0.6912 - val_accuracy: 0.5171\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5211 - val_loss: 0.6936 - val_accuracy: 0.5096\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5197 - val_loss: 0.6911 - val_accuracy: 0.5209\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5221 - val_loss: 0.6919 - val_accuracy: 0.5207\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5222 - val_loss: 0.6927 - val_accuracy: 0.5132\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5207 - val_loss: 0.6928 - val_accuracy: 0.5085\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5154\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5220 - val_loss: 0.6911 - val_accuracy: 0.5227\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5213 - val_loss: 0.6918 - val_accuracy: 0.5189\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5215 - val_loss: 0.6914 - val_accuracy: 0.5197\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5209 - val_loss: 0.6916 - val_accuracy: 0.5208\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5224 - val_loss: 0.6930 - val_accuracy: 0.5134\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5210 - val_loss: 0.6914 - val_accuracy: 0.5112\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5222 - val_loss: 0.6911 - val_accuracy: 0.5157\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5218 - val_loss: 0.6915 - val_accuracy: 0.5180\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5210 - val_loss: 0.6918 - val_accuracy: 0.5142\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5213 - val_loss: 0.6910 - val_accuracy: 0.5200\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5212 - val_loss: 0.6910 - val_accuracy: 0.5200\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5214 - val_loss: 0.6915 - val_accuracy: 0.5182\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5205 - val_loss: 0.6920 - val_accuracy: 0.5185\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_3\n",
      "cannot prune layer q_activation_3\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6905 - accuracy: 0.5201 - val_loss: 0.6917 - val_accuracy: 0.5196\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5220 - val_loss: 0.6912 - val_accuracy: 0.5180\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5216 - val_loss: 0.6920 - val_accuracy: 0.5189\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5207 - val_loss: 0.6917 - val_accuracy: 0.5186\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5225 - val_loss: 0.6919 - val_accuracy: 0.5174\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6928 - val_accuracy: 0.5135\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5201 - val_loss: 0.6924 - val_accuracy: 0.5148\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5215 - val_loss: 0.6915 - val_accuracy: 0.5179\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5230 - val_loss: 0.6911 - val_accuracy: 0.5166\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5220 - val_loss: 0.6909 - val_accuracy: 0.5202\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5238 - val_loss: 0.6917 - val_accuracy: 0.5189\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5227 - val_loss: 0.6909 - val_accuracy: 0.5217\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5226 - val_loss: 0.6917 - val_accuracy: 0.5201\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5228\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5227 - val_loss: 0.6916 - val_accuracy: 0.5182\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5231 - val_loss: 0.6916 - val_accuracy: 0.5198\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5234 - val_loss: 0.6914 - val_accuracy: 0.5208\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5222 - val_loss: 0.6916 - val_accuracy: 0.5195\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5245 - val_loss: 0.6912 - val_accuracy: 0.5166\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5229 - val_loss: 0.6916 - val_accuracy: 0.5237\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5234 - val_loss: 0.6930 - val_accuracy: 0.5233\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5235 - val_loss: 0.6910 - val_accuracy: 0.5237\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5225 - val_loss: 0.6912 - val_accuracy: 0.5227\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5233 - val_loss: 0.6910 - val_accuracy: 0.5182\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5240 - val_loss: 0.6926 - val_accuracy: 0.5185\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5230 - val_loss: 0.6905 - val_accuracy: 0.5209\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5235 - val_loss: 0.6920 - val_accuracy: 0.5228\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5235 - val_loss: 0.6905 - val_accuracy: 0.5206\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5241 - val_loss: 0.6910 - val_accuracy: 0.5176\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5196\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5233 - val_loss: 0.6910 - val_accuracy: 0.5203\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5229 - val_loss: 0.6913 - val_accuracy: 0.5207\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5229 - val_loss: 0.6909 - val_accuracy: 0.5220\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5234 - val_loss: 0.6908 - val_accuracy: 0.5221\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5236 - val_loss: 0.6922 - val_accuracy: 0.5188\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5236 - val_loss: 0.6909 - val_accuracy: 0.5236\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5229 - val_loss: 0.6904 - val_accuracy: 0.5229\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5242 - val_loss: 0.6923 - val_accuracy: 0.5141\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5230 - val_loss: 0.6907 - val_accuracy: 0.5234\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5242 - val_loss: 0.6909 - val_accuracy: 0.5203\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5238 - val_loss: 0.6910 - val_accuracy: 0.5231\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5245 - val_loss: 0.6910 - val_accuracy: 0.5233\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5252 - val_loss: 0.6902 - val_accuracy: 0.5245\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5248 - val_loss: 0.6906 - val_accuracy: 0.5195\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5242 - val_loss: 0.6905 - val_accuracy: 0.5218\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5254 - val_loss: 0.6911 - val_accuracy: 0.5186\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5254 - val_loss: 0.6901 - val_accuracy: 0.5182\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5243 - val_loss: 0.6905 - val_accuracy: 0.5212\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5251 - val_loss: 0.6914 - val_accuracy: 0.5166\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5236 - val_loss: 0.6903 - val_accuracy: 0.5284\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.53180575]\n",
      " [0.49617183]\n",
      " [0.38453466]\n",
      " [0.49902287]\n",
      " [0.49902287]\n",
      " [0.5029075 ]\n",
      " [0.546709  ]\n",
      " [0.5037157 ]\n",
      " [0.53655463]\n",
      " [0.5368955 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_4 is normal keras bn layer\n",
      "q_activation_4       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.9856 - accuracy: 0.5013 - val_loss: 0.7180 - val_accuracy: 0.4993\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7126 - accuracy: 0.5017 - val_loss: 0.7095 - val_accuracy: 0.4987\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7038 - accuracy: 0.5032 - val_loss: 0.7045 - val_accuracy: 0.4991\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6997 - accuracy: 0.5051 - val_loss: 0.7005 - val_accuracy: 0.5025\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5053 - val_loss: 0.6986 - val_accuracy: 0.5041\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5068 - val_loss: 0.6978 - val_accuracy: 0.5097\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5090 - val_loss: 0.6967 - val_accuracy: 0.5075\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5099 - val_loss: 0.6956 - val_accuracy: 0.5040\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5114 - val_loss: 0.6950 - val_accuracy: 0.5135\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5117 - val_loss: 0.6945 - val_accuracy: 0.5139\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5133 - val_loss: 0.6942 - val_accuracy: 0.5130\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5138 - val_loss: 0.6948 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6938 - val_accuracy: 0.5182\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5140 - val_loss: 0.6940 - val_accuracy: 0.5035\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5131 - val_loss: 0.6934 - val_accuracy: 0.5172\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5149 - val_loss: 0.6949 - val_accuracy: 0.5100\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5149 - val_loss: 0.6934 - val_accuracy: 0.5108\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5167 - val_loss: 0.6928 - val_accuracy: 0.5150\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5179 - val_loss: 0.6932 - val_accuracy: 0.5218\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5177 - val_loss: 0.6930 - val_accuracy: 0.5142\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5174 - val_loss: 0.6927 - val_accuracy: 0.5186\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6928 - val_accuracy: 0.5179\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5184 - val_loss: 0.6925 - val_accuracy: 0.5195\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5191 - val_loss: 0.6929 - val_accuracy: 0.5201\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6934 - val_accuracy: 0.5184\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5195 - val_loss: 0.6926 - val_accuracy: 0.5211\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5210 - val_loss: 0.6927 - val_accuracy: 0.5231\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5214 - val_loss: 0.6922 - val_accuracy: 0.5201\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6925 - val_accuracy: 0.5186\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5218 - val_loss: 0.6934 - val_accuracy: 0.5181\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5218\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5242 - val_loss: 0.6924 - val_accuracy: 0.5199\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5229 - val_loss: 0.6942 - val_accuracy: 0.5222\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5248 - val_loss: 0.6919 - val_accuracy: 0.5203\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5246 - val_loss: 0.6917 - val_accuracy: 0.5233\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5248 - val_loss: 0.6921 - val_accuracy: 0.5237\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5253 - val_loss: 0.6924 - val_accuracy: 0.5218\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5253 - val_loss: 0.6931 - val_accuracy: 0.5209\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5261 - val_loss: 0.6921 - val_accuracy: 0.5162\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5265 - val_loss: 0.6916 - val_accuracy: 0.5224\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5259 - val_loss: 0.6918 - val_accuracy: 0.5190\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5265 - val_loss: 0.6917 - val_accuracy: 0.5245\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5274 - val_loss: 0.6919 - val_accuracy: 0.5183\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5279 - val_loss: 0.6920 - val_accuracy: 0.5177\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5282 - val_loss: 0.6922 - val_accuracy: 0.5138\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5288 - val_loss: 0.6910 - val_accuracy: 0.5257\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5289 - val_loss: 0.6910 - val_accuracy: 0.5213\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5287 - val_loss: 0.6910 - val_accuracy: 0.5261\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5297 - val_loss: 0.6913 - val_accuracy: 0.5253\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5315 - val_loss: 0.6907 - val_accuracy: 0.5278\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5304 - val_loss: 0.6906 - val_accuracy: 0.5280\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5308 - val_loss: 0.6908 - val_accuracy: 0.5293\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5313 - val_loss: 0.6908 - val_accuracy: 0.5292\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5327 - val_loss: 0.6919 - val_accuracy: 0.5203\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5328 - val_loss: 0.6911 - val_accuracy: 0.5233\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5316 - val_loss: 0.6905 - val_accuracy: 0.5314\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5329 - val_loss: 0.6902 - val_accuracy: 0.5314\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5332 - val_loss: 0.6908 - val_accuracy: 0.5293\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5342 - val_loss: 0.6913 - val_accuracy: 0.5224\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5337 - val_loss: 0.6901 - val_accuracy: 0.5354\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5347 - val_loss: 0.6892 - val_accuracy: 0.5377\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5342 - val_loss: 0.6896 - val_accuracy: 0.5346\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5341 - val_loss: 0.6899 - val_accuracy: 0.5297\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5359 - val_loss: 0.6897 - val_accuracy: 0.5314\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5352 - val_loss: 0.6906 - val_accuracy: 0.5254\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5359 - val_loss: 0.6904 - val_accuracy: 0.5371\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5367 - val_loss: 0.6895 - val_accuracy: 0.5371\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5383 - val_loss: 0.6895 - val_accuracy: 0.5373\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5362 - val_loss: 0.6889 - val_accuracy: 0.5426\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5364 - val_loss: 0.6890 - val_accuracy: 0.5394\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5375 - val_loss: 0.6898 - val_accuracy: 0.5410\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5364 - val_loss: 0.6899 - val_accuracy: 0.5375\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5366 - val_loss: 0.6901 - val_accuracy: 0.5384\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5376 - val_loss: 0.6892 - val_accuracy: 0.5380\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5377 - val_loss: 0.6899 - val_accuracy: 0.5350\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5374 - val_loss: 0.6898 - val_accuracy: 0.5393\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5378 - val_loss: 0.6896 - val_accuracy: 0.5269\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5387 - val_loss: 0.6904 - val_accuracy: 0.5397\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5377 - val_loss: 0.6886 - val_accuracy: 0.5380\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5379 - val_loss: 0.6900 - val_accuracy: 0.5267\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5387 - val_loss: 0.6892 - val_accuracy: 0.5427\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5385 - val_loss: 0.6887 - val_accuracy: 0.5395\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5386 - val_loss: 0.6898 - val_accuracy: 0.5389\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5393 - val_loss: 0.6891 - val_accuracy: 0.5407\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5390 - val_loss: 0.6880 - val_accuracy: 0.5415\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5397 - val_loss: 0.6890 - val_accuracy: 0.5415\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5396 - val_loss: 0.6887 - val_accuracy: 0.5418\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5404 - val_loss: 0.6887 - val_accuracy: 0.5338\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5397 - val_loss: 0.6891 - val_accuracy: 0.5328\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5397 - val_loss: 0.6881 - val_accuracy: 0.5421\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5390 - val_loss: 0.6890 - val_accuracy: 0.5370\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5402 - val_loss: 0.6894 - val_accuracy: 0.5414\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5418 - val_loss: 0.6887 - val_accuracy: 0.5389\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5404 - val_loss: 0.6891 - val_accuracy: 0.5423\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5406 - val_loss: 0.6890 - val_accuracy: 0.5345\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5409 - val_loss: 0.6890 - val_accuracy: 0.5375\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5406 - val_loss: 0.6891 - val_accuracy: 0.5312\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5406 - val_loss: 0.6889 - val_accuracy: 0.5434\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5416 - val_loss: 0.6900 - val_accuracy: 0.5396\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5406 - val_loss: 0.6908 - val_accuracy: 0.5348\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_4\n",
      "cannot prune layer q_activation_4\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6863 - accuracy: 0.5417 - val_loss: 0.6946 - val_accuracy: 0.5193\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5407 - val_loss: 0.6965 - val_accuracy: 0.5264\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5397 - val_loss: 0.7025 - val_accuracy: 0.5106\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5401 - val_loss: 0.6997 - val_accuracy: 0.5237\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5388 - val_loss: 0.7134 - val_accuracy: 0.5162\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5351 - val_loss: 0.6927 - val_accuracy: 0.5329\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5363 - val_loss: 0.6920 - val_accuracy: 0.5230\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5379 - val_loss: 0.6905 - val_accuracy: 0.5344\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5366 - val_loss: 0.6891 - val_accuracy: 0.5403\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5376 - val_loss: 0.6887 - val_accuracy: 0.5416\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5380 - val_loss: 0.6892 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5379 - val_loss: 0.6886 - val_accuracy: 0.5412\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5385 - val_loss: 0.6893 - val_accuracy: 0.5408\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5384 - val_loss: 0.6897 - val_accuracy: 0.5418\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5389 - val_loss: 0.6890 - val_accuracy: 0.5408\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5393 - val_loss: 0.6897 - val_accuracy: 0.5424\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5406 - val_loss: 0.6886 - val_accuracy: 0.5427\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5407 - val_loss: 0.6889 - val_accuracy: 0.5421\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5401 - val_loss: 0.6886 - val_accuracy: 0.5370\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5397 - val_loss: 0.6889 - val_accuracy: 0.5329\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5398 - val_loss: 0.6888 - val_accuracy: 0.5416\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5409 - val_loss: 0.6891 - val_accuracy: 0.5371\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5403 - val_loss: 0.6885 - val_accuracy: 0.5411\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5396 - val_loss: 0.6890 - val_accuracy: 0.5390\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5390 - val_loss: 0.6896 - val_accuracy: 0.5377\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5409 - val_loss: 0.6889 - val_accuracy: 0.5360\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5394 - val_loss: 0.6889 - val_accuracy: 0.5358\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5404 - val_loss: 0.6892 - val_accuracy: 0.5426\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5414 - val_loss: 0.6884 - val_accuracy: 0.5394\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5408 - val_loss: 0.6890 - val_accuracy: 0.5420\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5410 - val_loss: 0.6895 - val_accuracy: 0.5444\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5422 - val_loss: 0.6887 - val_accuracy: 0.5379\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5411 - val_loss: 0.6894 - val_accuracy: 0.5413\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5417 - val_loss: 0.6889 - val_accuracy: 0.5369\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5413 - val_loss: 0.6896 - val_accuracy: 0.5413\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5420 - val_loss: 0.6888 - val_accuracy: 0.5435\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5410 - val_loss: 0.6884 - val_accuracy: 0.5412\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5416 - val_loss: 0.6886 - val_accuracy: 0.5413\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5417 - val_loss: 0.6887 - val_accuracy: 0.5436\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5424 - val_loss: 0.6883 - val_accuracy: 0.5441\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5423 - val_loss: 0.6887 - val_accuracy: 0.5420\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5432 - val_loss: 0.6887 - val_accuracy: 0.5343\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5435 - val_loss: 0.6881 - val_accuracy: 0.5445\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5428 - val_loss: 0.6879 - val_accuracy: 0.5454\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5431 - val_loss: 0.6898 - val_accuracy: 0.5340\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5433 - val_loss: 0.6880 - val_accuracy: 0.5456\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5428 - val_loss: 0.6885 - val_accuracy: 0.5367\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5424 - val_loss: 0.6885 - val_accuracy: 0.5438\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5419 - val_loss: 0.6886 - val_accuracy: 0.5382\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5436 - val_loss: 0.6893 - val_accuracy: 0.5402\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5527928 ]\n",
      " [0.43001884]\n",
      " [0.3409859 ]\n",
      " [0.5174154 ]\n",
      " [0.52414083]\n",
      " [0.5141346 ]\n",
      " [0.5237605 ]\n",
      " [0.4863425 ]\n",
      " [0.5131716 ]\n",
      " [0.5148257 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_5 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_5 is normal keras bn layer\n",
      "q_activation_5       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.8246 - accuracy: 0.5013 - val_loss: 0.7381 - val_accuracy: 0.5028\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7264 - accuracy: 0.5021 - val_loss: 0.7238 - val_accuracy: 0.5059\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7138 - accuracy: 0.5032 - val_loss: 0.7125 - val_accuracy: 0.5075\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7072 - accuracy: 0.5049 - val_loss: 0.7088 - val_accuracy: 0.5080\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7040 - accuracy: 0.5045 - val_loss: 0.7045 - val_accuracy: 0.5063\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7010 - accuracy: 0.5055 - val_loss: 0.7018 - val_accuracy: 0.5083\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6990 - accuracy: 0.5060 - val_loss: 0.7000 - val_accuracy: 0.5121\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6982 - accuracy: 0.5071 - val_loss: 0.6981 - val_accuracy: 0.5075\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.5086 - val_loss: 0.6970 - val_accuracy: 0.5096\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5086 - val_loss: 0.6963 - val_accuracy: 0.5125\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5105 - val_loss: 0.6966 - val_accuracy: 0.5085\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5107 - val_loss: 0.6956 - val_accuracy: 0.5118\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5114 - val_loss: 0.6950 - val_accuracy: 0.5082\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5123 - val_loss: 0.6947 - val_accuracy: 0.5096\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5136 - val_loss: 0.6955 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5127 - val_loss: 0.6943 - val_accuracy: 0.5128\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5154 - val_loss: 0.6945 - val_accuracy: 0.5181\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5150 - val_loss: 0.6938 - val_accuracy: 0.5161\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5163 - val_loss: 0.6940 - val_accuracy: 0.5131\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5167 - val_loss: 0.6936 - val_accuracy: 0.5130\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5160 - val_loss: 0.6965 - val_accuracy: 0.5159\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5167 - val_loss: 0.6932 - val_accuracy: 0.5139\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5167 - val_loss: 0.6934 - val_accuracy: 0.5097\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5174 - val_loss: 0.6927 - val_accuracy: 0.5165\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5175 - val_loss: 0.6928 - val_accuracy: 0.5185\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5180 - val_loss: 0.6926 - val_accuracy: 0.5170\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5175 - val_loss: 0.6927 - val_accuracy: 0.5150\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6928 - val_accuracy: 0.5168\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.5244\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5196 - val_loss: 0.6922 - val_accuracy: 0.5222\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5185 - val_loss: 0.6934 - val_accuracy: 0.5092\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5203 - val_loss: 0.6924 - val_accuracy: 0.5217\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5201 - val_loss: 0.6923 - val_accuracy: 0.5208\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5219 - val_loss: 0.6933 - val_accuracy: 0.5105\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5219 - val_loss: 0.6922 - val_accuracy: 0.5239\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5220 - val_loss: 0.6924 - val_accuracy: 0.5172\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5214 - val_loss: 0.6923 - val_accuracy: 0.5253\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5232 - val_loss: 0.6924 - val_accuracy: 0.5222\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5225 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5233 - val_loss: 0.6921 - val_accuracy: 0.5227\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5224 - val_loss: 0.6926 - val_accuracy: 0.5171\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5233 - val_loss: 0.6920 - val_accuracy: 0.5251\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5243 - val_loss: 0.6927 - val_accuracy: 0.5206\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5235 - val_loss: 0.6934 - val_accuracy: 0.5217\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5239 - val_loss: 0.6920 - val_accuracy: 0.5277\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5243 - val_loss: 0.6923 - val_accuracy: 0.5245\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5246 - val_loss: 0.6923 - val_accuracy: 0.5269\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5246 - val_loss: 0.6921 - val_accuracy: 0.5242\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5258 - val_loss: 0.6920 - val_accuracy: 0.5218\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5262 - val_loss: 0.6920 - val_accuracy: 0.5277\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5252 - val_loss: 0.6918 - val_accuracy: 0.5282\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5271 - val_loss: 0.6918 - val_accuracy: 0.5250\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5290 - val_loss: 0.6921 - val_accuracy: 0.5259\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5272 - val_loss: 0.6917 - val_accuracy: 0.5294\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5285 - val_loss: 0.6916 - val_accuracy: 0.5283\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5292 - val_loss: 0.6914 - val_accuracy: 0.5286\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5299 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5300 - val_loss: 0.6916 - val_accuracy: 0.5228\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5309 - val_loss: 0.6920 - val_accuracy: 0.5290\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5304 - val_loss: 0.6910 - val_accuracy: 0.5319\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5305 - val_loss: 0.6913 - val_accuracy: 0.5319\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5303 - val_loss: 0.6912 - val_accuracy: 0.5246\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5318 - val_loss: 0.6913 - val_accuracy: 0.5324\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5316 - val_loss: 0.6909 - val_accuracy: 0.5300\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5319 - val_loss: 0.6905 - val_accuracy: 0.5294\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5321 - val_loss: 0.6914 - val_accuracy: 0.5288\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5324 - val_loss: 0.6910 - val_accuracy: 0.5290\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5325 - val_loss: 0.6914 - val_accuracy: 0.5290\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5327 - val_loss: 0.6912 - val_accuracy: 0.5258\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5337 - val_loss: 0.6906 - val_accuracy: 0.5265\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5333 - val_loss: 0.6906 - val_accuracy: 0.5297\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5348 - val_loss: 0.6912 - val_accuracy: 0.5267\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5344 - val_loss: 0.6914 - val_accuracy: 0.5291\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5349 - val_loss: 0.6904 - val_accuracy: 0.5334\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5341 - val_loss: 0.6908 - val_accuracy: 0.5267\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5357 - val_loss: 0.6901 - val_accuracy: 0.5371\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5350 - val_loss: 0.6900 - val_accuracy: 0.5374\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5366 - val_loss: 0.6909 - val_accuracy: 0.5379\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5351 - val_loss: 0.6903 - val_accuracy: 0.5351\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5360 - val_loss: 0.6903 - val_accuracy: 0.5368\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5361 - val_loss: 0.6897 - val_accuracy: 0.5385\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5367 - val_loss: 0.6903 - val_accuracy: 0.5281\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5368 - val_loss: 0.6905 - val_accuracy: 0.5331\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5382 - val_loss: 0.6898 - val_accuracy: 0.5355\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5368 - val_loss: 0.6894 - val_accuracy: 0.5374\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5373 - val_loss: 0.6902 - val_accuracy: 0.5377\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5381 - val_loss: 0.6895 - val_accuracy: 0.5348\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5379 - val_loss: 0.6893 - val_accuracy: 0.5351\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5387 - val_loss: 0.6899 - val_accuracy: 0.5356\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5380 - val_loss: 0.6900 - val_accuracy: 0.5394\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5385 - val_loss: 0.6893 - val_accuracy: 0.5331\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5386 - val_loss: 0.6888 - val_accuracy: 0.5383\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5377 - val_loss: 0.6898 - val_accuracy: 0.5376\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5383 - val_loss: 0.6905 - val_accuracy: 0.5333\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5397 - val_loss: 0.6905 - val_accuracy: 0.5424\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5399 - val_loss: 0.6897 - val_accuracy: 0.5395\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5390 - val_loss: 0.6900 - val_accuracy: 0.5422\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5391 - val_loss: 0.6884 - val_accuracy: 0.5419\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5408 - val_loss: 0.6899 - val_accuracy: 0.5334\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5410 - val_loss: 0.6887 - val_accuracy: 0.5385\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_5\n",
      "cannot prune layer q_activation_5\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6869 - accuracy: 0.5402 - val_loss: 0.6958 - val_accuracy: 0.5266\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5394 - val_loss: 0.6990 - val_accuracy: 0.5160\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5369 - val_loss: 0.6933 - val_accuracy: 0.5269\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5376 - val_loss: 0.7023 - val_accuracy: 0.5152\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5360 - val_loss: 0.6919 - val_accuracy: 0.5328\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5367 - val_loss: 0.6942 - val_accuracy: 0.5248\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5355 - val_loss: 0.6941 - val_accuracy: 0.5195\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5365 - val_loss: 0.6955 - val_accuracy: 0.5086\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5383 - val_loss: 0.6895 - val_accuracy: 0.5345\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5394 - val_loss: 0.6899 - val_accuracy: 0.5366\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5397 - val_loss: 0.6890 - val_accuracy: 0.5379\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5402 - val_loss: 0.6892 - val_accuracy: 0.5373\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5402 - val_loss: 0.6895 - val_accuracy: 0.5333\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5404 - val_loss: 0.6896 - val_accuracy: 0.5333\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5406 - val_loss: 0.6892 - val_accuracy: 0.5392\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5405 - val_loss: 0.6890 - val_accuracy: 0.5406\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5404 - val_loss: 0.6894 - val_accuracy: 0.5415\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5409 - val_loss: 0.6888 - val_accuracy: 0.5393\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5409 - val_loss: 0.6891 - val_accuracy: 0.5362\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5421 - val_loss: 0.6890 - val_accuracy: 0.5341\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5378\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5422 - val_loss: 0.6885 - val_accuracy: 0.5377\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5422 - val_loss: 0.6887 - val_accuracy: 0.5420\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5420 - val_loss: 0.6885 - val_accuracy: 0.5395\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5429 - val_loss: 0.6883 - val_accuracy: 0.5455\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5430 - val_loss: 0.6882 - val_accuracy: 0.5409\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5437 - val_loss: 0.6874 - val_accuracy: 0.5442\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5441 - val_loss: 0.6878 - val_accuracy: 0.5392\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5447 - val_loss: 0.6887 - val_accuracy: 0.5463\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5451 - val_loss: 0.6874 - val_accuracy: 0.5475\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5451 - val_loss: 0.6877 - val_accuracy: 0.5478\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5455 - val_loss: 0.6880 - val_accuracy: 0.5443\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5442 - val_loss: 0.6873 - val_accuracy: 0.5480\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5448 - val_loss: 0.6879 - val_accuracy: 0.5445\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5453 - val_loss: 0.6874 - val_accuracy: 0.5432\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5459 - val_loss: 0.6870 - val_accuracy: 0.5464\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5452 - val_loss: 0.6871 - val_accuracy: 0.5485\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5467 - val_loss: 0.6869 - val_accuracy: 0.5468\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5461 - val_loss: 0.6872 - val_accuracy: 0.5483\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5455 - val_loss: 0.6872 - val_accuracy: 0.5493\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5460 - val_loss: 0.6873 - val_accuracy: 0.5496\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5459 - val_loss: 0.6868 - val_accuracy: 0.5503\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5467 - val_loss: 0.6869 - val_accuracy: 0.5494\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5459 - val_loss: 0.6873 - val_accuracy: 0.5466\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5470 - val_loss: 0.6870 - val_accuracy: 0.5487\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5468 - val_loss: 0.6871 - val_accuracy: 0.5478\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5468 - val_loss: 0.6866 - val_accuracy: 0.5470\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5473 - val_loss: 0.6865 - val_accuracy: 0.5517\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5466 - val_loss: 0.6869 - val_accuracy: 0.5491\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5481 - val_loss: 0.6865 - val_accuracy: 0.5501\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.54238933]\n",
      " [0.45857644]\n",
      " [0.4156205 ]\n",
      " [0.52702904]\n",
      " [0.4643085 ]\n",
      " [0.51389855]\n",
      " [0.44813442]\n",
      " [0.4800812 ]\n",
      " [0.5253803 ]\n",
      " [0.45461053]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_6 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_6 is normal keras bn layer\n",
      "q_activation_6       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 6ms/step - loss: 0.8848 - accuracy: 0.5005 - val_loss: 0.6976 - val_accuracy: 0.5040\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5059 - val_loss: 0.6950 - val_accuracy: 0.5078\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5060 - val_loss: 0.6946 - val_accuracy: 0.5078\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5089 - val_loss: 0.6938 - val_accuracy: 0.5106\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5079 - val_loss: 0.6943 - val_accuracy: 0.5042\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5093 - val_loss: 0.6936 - val_accuracy: 0.5028\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6935 - val_accuracy: 0.5050\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6938 - val_accuracy: 0.5079\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5124 - val_loss: 0.6928 - val_accuracy: 0.5112\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5127 - val_loss: 0.6932 - val_accuracy: 0.5100\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5129\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5130 - val_loss: 0.6928 - val_accuracy: 0.5140\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5065\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6934 - val_accuracy: 0.5126\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6932 - val_accuracy: 0.5135\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5137 - val_loss: 0.6943 - val_accuracy: 0.5092\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6936 - val_accuracy: 0.5115\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5129 - val_loss: 0.6927 - val_accuracy: 0.5140\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5151 - val_loss: 0.6931 - val_accuracy: 0.5120\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6945 - val_accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5128\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6925 - val_accuracy: 0.5148\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5147 - val_loss: 0.6924 - val_accuracy: 0.5141\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.5154\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6925 - val_accuracy: 0.5167\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5169 - val_loss: 0.6927 - val_accuracy: 0.5127\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6925 - val_accuracy: 0.5160\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.6929 - val_accuracy: 0.5161\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.6925 - val_accuracy: 0.5107\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5178 - val_loss: 0.6919 - val_accuracy: 0.5167\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5138\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6927 - val_accuracy: 0.5145\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5172 - val_loss: 0.6930 - val_accuracy: 0.5181\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5163 - val_loss: 0.6923 - val_accuracy: 0.5157\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5163 - val_loss: 0.6924 - val_accuracy: 0.5201\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5181 - val_loss: 0.6925 - val_accuracy: 0.5138\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5168 - val_loss: 0.6924 - val_accuracy: 0.5093\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5185 - val_loss: 0.6936 - val_accuracy: 0.5150\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5174 - val_loss: 0.6918 - val_accuracy: 0.5165\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5196 - val_loss: 0.6918 - val_accuracy: 0.5200\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5173 - val_loss: 0.6924 - val_accuracy: 0.5191\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5183 - val_loss: 0.6924 - val_accuracy: 0.5129\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6934 - val_accuracy: 0.5150\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6920 - val_accuracy: 0.5183\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5180 - val_loss: 0.6933 - val_accuracy: 0.5187\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5192 - val_loss: 0.6920 - val_accuracy: 0.5193\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5193 - val_loss: 0.6920 - val_accuracy: 0.5155\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5183 - val_loss: 0.6923 - val_accuracy: 0.5095\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5201 - val_loss: 0.6918 - val_accuracy: 0.5190\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5194 - val_loss: 0.6918 - val_accuracy: 0.5217\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5190 - val_loss: 0.6928 - val_accuracy: 0.5156\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5182 - val_loss: 0.6923 - val_accuracy: 0.5114\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.6917 - val_accuracy: 0.5199\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5193 - val_loss: 0.6922 - val_accuracy: 0.5107\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5190 - val_loss: 0.6917 - val_accuracy: 0.5152\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5190 - val_loss: 0.6923 - val_accuracy: 0.5192\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5198 - val_loss: 0.6921 - val_accuracy: 0.5160\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5196 - val_loss: 0.6923 - val_accuracy: 0.5179\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5189 - val_loss: 0.6919 - val_accuracy: 0.5176\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5205 - val_loss: 0.6920 - val_accuracy: 0.5184\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5186\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5209 - val_loss: 0.6920 - val_accuracy: 0.5206\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5205 - val_loss: 0.6932 - val_accuracy: 0.5148\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5199 - val_loss: 0.6921 - val_accuracy: 0.5181\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5211 - val_loss: 0.6922 - val_accuracy: 0.5182\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5195 - val_loss: 0.6925 - val_accuracy: 0.5178\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5210 - val_loss: 0.6929 - val_accuracy: 0.5170\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5203 - val_loss: 0.6918 - val_accuracy: 0.5174\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5202 - val_loss: 0.6913 - val_accuracy: 0.5195\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5191 - val_loss: 0.6918 - val_accuracy: 0.5170\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5207 - val_loss: 0.6917 - val_accuracy: 0.5157\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5204 - val_loss: 0.6927 - val_accuracy: 0.5090\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5203 - val_loss: 0.6920 - val_accuracy: 0.5148\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5198 - val_loss: 0.6925 - val_accuracy: 0.5130\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5198 - val_loss: 0.6918 - val_accuracy: 0.5166\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5203 - val_loss: 0.6923 - val_accuracy: 0.5129\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5211 - val_loss: 0.6923 - val_accuracy: 0.5172\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5212 - val_loss: 0.6934 - val_accuracy: 0.5084\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5216 - val_loss: 0.6912 - val_accuracy: 0.5196\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5213 - val_loss: 0.6917 - val_accuracy: 0.5164\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5214 - val_loss: 0.6913 - val_accuracy: 0.5183\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6919 - val_accuracy: 0.5183\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5203\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5190\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5212 - val_loss: 0.6917 - val_accuracy: 0.5203\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5211 - val_loss: 0.6914 - val_accuracy: 0.5166\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6929 - val_accuracy: 0.5158\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5213 - val_loss: 0.6907 - val_accuracy: 0.5195\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5210 - val_loss: 0.6910 - val_accuracy: 0.5182\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5198\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5223 - val_loss: 0.6908 - val_accuracy: 0.5154\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5220 - val_loss: 0.6910 - val_accuracy: 0.5215\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5221 - val_loss: 0.6912 - val_accuracy: 0.5218\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5219 - val_loss: 0.6919 - val_accuracy: 0.5193\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5205 - val_loss: 0.6909 - val_accuracy: 0.5205\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5204 - val_loss: 0.6906 - val_accuracy: 0.5210\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5201 - val_loss: 0.6918 - val_accuracy: 0.5133\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5233 - val_loss: 0.6912 - val_accuracy: 0.5191\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_6\n",
      "cannot prune layer q_activation_6\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6902 - accuracy: 0.5207 - val_loss: 0.6913 - val_accuracy: 0.5203\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5191\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5207 - val_loss: 0.6944 - val_accuracy: 0.5147\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5188 - val_loss: 0.6957 - val_accuracy: 0.5145\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5184 - val_loss: 0.6966 - val_accuracy: 0.5092\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5166 - val_loss: 0.6919 - val_accuracy: 0.5115\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5177 - val_loss: 0.6927 - val_accuracy: 0.5166\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5165 - val_loss: 0.6915 - val_accuracy: 0.5182\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5172 - val_loss: 0.6911 - val_accuracy: 0.5185\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5175 - val_loss: 0.6912 - val_accuracy: 0.5201\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5176 - val_loss: 0.6914 - val_accuracy: 0.5144\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5177 - val_loss: 0.6912 - val_accuracy: 0.5149\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5186 - val_loss: 0.6907 - val_accuracy: 0.5185\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5177 - val_loss: 0.6917 - val_accuracy: 0.5142\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5193 - val_loss: 0.6911 - val_accuracy: 0.5178\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5199 - val_loss: 0.6908 - val_accuracy: 0.5185\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5195 - val_loss: 0.6909 - val_accuracy: 0.5210\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5192 - val_loss: 0.6906 - val_accuracy: 0.5205\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5187 - val_loss: 0.6909 - val_accuracy: 0.5196\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5188 - val_loss: 0.6908 - val_accuracy: 0.5187\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5194 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5199 - val_loss: 0.6913 - val_accuracy: 0.5171\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5191 - val_loss: 0.6911 - val_accuracy: 0.5198\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5196 - val_loss: 0.6913 - val_accuracy: 0.5203\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5207 - val_loss: 0.6909 - val_accuracy: 0.5187\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5190 - val_loss: 0.6911 - val_accuracy: 0.5209\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5203 - val_loss: 0.6911 - val_accuracy: 0.5216\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5223 - val_loss: 0.6909 - val_accuracy: 0.5183\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5207 - val_loss: 0.6905 - val_accuracy: 0.5216\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5203 - val_loss: 0.6907 - val_accuracy: 0.5236\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5205 - val_loss: 0.6915 - val_accuracy: 0.5191\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5218 - val_loss: 0.6909 - val_accuracy: 0.5177\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5213 - val_loss: 0.6906 - val_accuracy: 0.5218\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5216 - val_loss: 0.6906 - val_accuracy: 0.5175\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5196\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5215 - val_loss: 0.6907 - val_accuracy: 0.5239\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5226 - val_loss: 0.6907 - val_accuracy: 0.5213\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5228 - val_loss: 0.6904 - val_accuracy: 0.5218\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5215 - val_loss: 0.6911 - val_accuracy: 0.5187\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5221 - val_loss: 0.6903 - val_accuracy: 0.5204\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5226 - val_loss: 0.6906 - val_accuracy: 0.5231\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5226 - val_loss: 0.6907 - val_accuracy: 0.5206\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6900 - val_accuracy: 0.5226\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5221 - val_loss: 0.6904 - val_accuracy: 0.5197\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5220 - val_loss: 0.6903 - val_accuracy: 0.5184\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5230 - val_loss: 0.6922 - val_accuracy: 0.5208\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5224 - val_loss: 0.6899 - val_accuracy: 0.5253\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5238 - val_loss: 0.6903 - val_accuracy: 0.5215\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5234 - val_loss: 0.6905 - val_accuracy: 0.5204\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5235 - val_loss: 0.6916 - val_accuracy: 0.5195\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5043121 ]\n",
      " [0.47999275]\n",
      " [0.35167792]\n",
      " [0.49804348]\n",
      " [0.49804348]\n",
      " [0.46505955]\n",
      " [0.45588347]\n",
      " [0.4970996 ]\n",
      " [0.50893104]\n",
      " [0.5061359 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_7 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_7 is normal keras bn layer\n",
      "q_activation_7       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.9097 - accuracy: 0.5009 - val_loss: 0.7285 - val_accuracy: 0.5016\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7184 - accuracy: 0.5026 - val_loss: 0.7155 - val_accuracy: 0.5034\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7135 - accuracy: 0.5026 - val_loss: 0.7156 - val_accuracy: 0.5020\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7077 - accuracy: 0.5025 - val_loss: 0.7112 - val_accuracy: 0.5067\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7037 - accuracy: 0.5044 - val_loss: 0.7018 - val_accuracy: 0.5076\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7005 - accuracy: 0.5047 - val_loss: 0.6993 - val_accuracy: 0.5056\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5060 - val_loss: 0.6980 - val_accuracy: 0.5073\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6973 - accuracy: 0.5070 - val_loss: 0.6972 - val_accuracy: 0.5103\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5081 - val_loss: 0.6966 - val_accuracy: 0.5052\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5087 - val_loss: 0.6955 - val_accuracy: 0.5078\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6972 - accuracy: 0.5071 - val_loss: 0.6952 - val_accuracy: 0.5104\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5091 - val_loss: 0.6946 - val_accuracy: 0.5090\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5096 - val_loss: 0.6947 - val_accuracy: 0.5082\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5110 - val_loss: 0.6950 - val_accuracy: 0.5121\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5112 - val_loss: 0.6938 - val_accuracy: 0.5150\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5118 - val_loss: 0.6935 - val_accuracy: 0.5127\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5129 - val_loss: 0.6933 - val_accuracy: 0.5156\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5136 - val_loss: 0.6933 - val_accuracy: 0.5145\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6985 - val_accuracy: 0.5066\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5133 - val_loss: 0.6928 - val_accuracy: 0.5190\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5195\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5150 - val_loss: 0.6930 - val_accuracy: 0.5155\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5158 - val_loss: 0.6937 - val_accuracy: 0.5105\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5166 - val_loss: 0.6929 - val_accuracy: 0.5202\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5169 - val_loss: 0.6928 - val_accuracy: 0.5192\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5163\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5186 - val_loss: 0.6937 - val_accuracy: 0.5066\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5176 - val_loss: 0.6928 - val_accuracy: 0.5188\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5182 - val_loss: 0.6963 - val_accuracy: 0.5053\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5186 - val_loss: 0.6938 - val_accuracy: 0.5059\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5198 - val_loss: 0.6925 - val_accuracy: 0.5216\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5207 - val_loss: 0.6925 - val_accuracy: 0.5134\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5195 - val_loss: 0.6932 - val_accuracy: 0.5166\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5215 - val_loss: 0.6918 - val_accuracy: 0.5181\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5216 - val_loss: 0.6921 - val_accuracy: 0.5192\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5214 - val_loss: 0.6927 - val_accuracy: 0.5201\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5218 - val_loss: 0.6936 - val_accuracy: 0.5145\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6932 - val_accuracy: 0.5201\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5257\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5229 - val_loss: 0.6915 - val_accuracy: 0.5257\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5233 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6918 - val_accuracy: 0.5213\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5257 - val_loss: 0.6918 - val_accuracy: 0.5206\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5244 - val_loss: 0.6910 - val_accuracy: 0.5243\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5247 - val_loss: 0.6913 - val_accuracy: 0.5253\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5182\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6914 - val_accuracy: 0.5289\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5270 - val_loss: 0.6910 - val_accuracy: 0.5292\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5282 - val_loss: 0.6912 - val_accuracy: 0.5273\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5294 - val_loss: 0.6911 - val_accuracy: 0.5261\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5303 - val_loss: 0.6910 - val_accuracy: 0.5307\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5299 - val_loss: 0.6912 - val_accuracy: 0.5287\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5305 - val_loss: 0.6906 - val_accuracy: 0.5310\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5316 - val_loss: 0.6910 - val_accuracy: 0.5308\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5317 - val_loss: 0.6900 - val_accuracy: 0.5310\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5327 - val_loss: 0.6900 - val_accuracy: 0.5311\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5327 - val_loss: 0.6896 - val_accuracy: 0.5350\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5338 - val_loss: 0.6894 - val_accuracy: 0.5344\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5347 - val_loss: 0.6899 - val_accuracy: 0.5341\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5346 - val_loss: 0.6892 - val_accuracy: 0.5361\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5355 - val_loss: 0.6898 - val_accuracy: 0.5287\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5351 - val_loss: 0.6896 - val_accuracy: 0.5310\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5363 - val_loss: 0.6889 - val_accuracy: 0.5372\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5368 - val_loss: 0.6887 - val_accuracy: 0.5390\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5378 - val_loss: 0.6896 - val_accuracy: 0.5347\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5374 - val_loss: 0.6905 - val_accuracy: 0.5334\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5385 - val_loss: 0.6882 - val_accuracy: 0.5406\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5381 - val_loss: 0.6888 - val_accuracy: 0.5377\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5387 - val_loss: 0.6885 - val_accuracy: 0.5361\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5396 - val_loss: 0.6887 - val_accuracy: 0.5420\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5384 - val_loss: 0.6895 - val_accuracy: 0.5348\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5384 - val_loss: 0.6895 - val_accuracy: 0.5352\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5390 - val_loss: 0.6890 - val_accuracy: 0.5369\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5389 - val_loss: 0.6897 - val_accuracy: 0.5358\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5399 - val_loss: 0.6909 - val_accuracy: 0.5353\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5394 - val_loss: 0.6895 - val_accuracy: 0.5288\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5414 - val_loss: 0.6886 - val_accuracy: 0.5381\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5378\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5412 - val_loss: 0.6890 - val_accuracy: 0.5389\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5406 - val_loss: 0.6882 - val_accuracy: 0.5429\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5291 - val_loss: 0.6922 - val_accuracy: 0.5248\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5401 - val_loss: 0.6920 - val_accuracy: 0.5255\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5378\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5422 - val_loss: 0.6881 - val_accuracy: 0.5366\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5415 - val_loss: 0.6904 - val_accuracy: 0.5330\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5419 - val_loss: 0.6891 - val_accuracy: 0.5372\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5417 - val_loss: 0.6879 - val_accuracy: 0.5411\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5415 - val_loss: 0.6885 - val_accuracy: 0.5370\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5419 - val_loss: 0.6882 - val_accuracy: 0.5369\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5432 - val_loss: 0.6872 - val_accuracy: 0.5405\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5430 - val_loss: 0.6889 - val_accuracy: 0.5398\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5437 - val_loss: 0.6888 - val_accuracy: 0.5348\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5435 - val_loss: 0.6875 - val_accuracy: 0.5434\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5435 - val_loss: 0.6875 - val_accuracy: 0.5426\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5447 - val_loss: 0.6870 - val_accuracy: 0.5456\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5439 - val_loss: 0.6879 - val_accuracy: 0.5394\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5452 - val_loss: 0.6880 - val_accuracy: 0.5454\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5459 - val_loss: 0.6893 - val_accuracy: 0.5375\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5456 - val_loss: 0.6873 - val_accuracy: 0.5490\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5458 - val_loss: 0.6873 - val_accuracy: 0.5455\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_7\n",
      "cannot prune layer q_activation_7\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5449 - val_loss: 0.6934 - val_accuracy: 0.5342\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6863 - accuracy: 0.5439 - val_loss: 0.7580 - val_accuracy: 0.5105\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5314 - val_loss: 0.7083 - val_accuracy: 0.5159\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5299 - val_loss: 0.7686 - val_accuracy: 0.5050\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5178 - val_loss: 0.6981 - val_accuracy: 0.5043\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6990 - val_accuracy: 0.5031\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6957 - val_accuracy: 0.5122\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5189 - val_loss: 0.6939 - val_accuracy: 0.5133\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5181\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6917 - val_accuracy: 0.5204\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5238 - val_loss: 0.6920 - val_accuracy: 0.5183\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5225 - val_loss: 0.6918 - val_accuracy: 0.5230\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5241 - val_loss: 0.6915 - val_accuracy: 0.5228\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5249 - val_loss: 0.6916 - val_accuracy: 0.5248\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5247 - val_loss: 0.6916 - val_accuracy: 0.5238\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5254 - val_loss: 0.6918 - val_accuracy: 0.5231\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6902 - accuracy: 0.5262 - val_loss: 0.6913 - val_accuracy: 0.5243\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5271 - val_loss: 0.6912 - val_accuracy: 0.5278\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5273 - val_loss: 0.6914 - val_accuracy: 0.5248\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5280 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6899 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5234\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5281 - val_loss: 0.6917 - val_accuracy: 0.5247\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5281 - val_loss: 0.6912 - val_accuracy: 0.5255\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5290 - val_loss: 0.6911 - val_accuracy: 0.5280\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5300 - val_loss: 0.6912 - val_accuracy: 0.5275\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5298 - val_loss: 0.6907 - val_accuracy: 0.5290\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5301 - val_loss: 0.6911 - val_accuracy: 0.5256\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6893 - accuracy: 0.5292 - val_loss: 0.6909 - val_accuracy: 0.5277\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5300 - val_loss: 0.6910 - val_accuracy: 0.5273\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5305 - val_loss: 0.6916 - val_accuracy: 0.5225\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5296 - val_loss: 0.6914 - val_accuracy: 0.5280\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5315 - val_loss: 0.6917 - val_accuracy: 0.5275\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5300 - val_loss: 0.6909 - val_accuracy: 0.5278\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6889 - accuracy: 0.5313 - val_loss: 0.6910 - val_accuracy: 0.5315\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5310 - val_loss: 0.6913 - val_accuracy: 0.5287\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6888 - accuracy: 0.5312 - val_loss: 0.6913 - val_accuracy: 0.5296\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5319 - val_loss: 0.6909 - val_accuracy: 0.5307\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5313 - val_loss: 0.6909 - val_accuracy: 0.5273\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5325 - val_loss: 0.6908 - val_accuracy: 0.5306\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5323 - val_loss: 0.6907 - val_accuracy: 0.5300\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5320 - val_loss: 0.6913 - val_accuracy: 0.5299\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5318 - val_loss: 0.6907 - val_accuracy: 0.5308\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6885 - accuracy: 0.5324 - val_loss: 0.6907 - val_accuracy: 0.5337\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5274\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5311 - val_loss: 0.6908 - val_accuracy: 0.5293\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5336 - val_loss: 0.6909 - val_accuracy: 0.5299\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6885 - accuracy: 0.5315 - val_loss: 0.6905 - val_accuracy: 0.5330\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5312 - val_loss: 0.6909 - val_accuracy: 0.5296\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6885 - accuracy: 0.5320 - val_loss: 0.6902 - val_accuracy: 0.5304\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5321 - val_loss: 0.6916 - val_accuracy: 0.5295\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5516276 ]\n",
      " [0.47115833]\n",
      " [0.4584468 ]\n",
      " [0.5217108 ]\n",
      " [0.58586395]\n",
      " [0.47295308]\n",
      " [0.42901093]\n",
      " [0.52336806]\n",
      " [0.5070867 ]\n",
      " [0.602911  ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_8 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_8 is normal keras bn layer\n",
      "q_activation_8       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 6ms/step - loss: 1.3717 - accuracy: 0.5023 - val_loss: 0.8908 - val_accuracy: 0.4983\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.8420 - accuracy: 0.5000 - val_loss: 0.7914 - val_accuracy: 0.4959\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7614 - accuracy: 0.5023 - val_loss: 0.7378 - val_accuracy: 0.5006\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7352 - accuracy: 0.5037 - val_loss: 0.7320 - val_accuracy: 0.5015\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7287 - accuracy: 0.5039 - val_loss: 0.7213 - val_accuracy: 0.5007\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7183 - accuracy: 0.5044 - val_loss: 0.7146 - val_accuracy: 0.5007\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7153 - accuracy: 0.5038 - val_loss: 0.7107 - val_accuracy: 0.5035\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7102 - accuracy: 0.5039 - val_loss: 0.7079 - val_accuracy: 0.5048\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7075 - accuracy: 0.5052 - val_loss: 0.7061 - val_accuracy: 0.5042\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7053 - accuracy: 0.5050 - val_loss: 0.7043 - val_accuracy: 0.5032\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7039 - accuracy: 0.5055 - val_loss: 0.7030 - val_accuracy: 0.5065\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7018 - accuracy: 0.5062 - val_loss: 0.7016 - val_accuracy: 0.5060\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7004 - accuracy: 0.5070 - val_loss: 0.7007 - val_accuracy: 0.5052\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.5074 - val_loss: 0.6999 - val_accuracy: 0.5063\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6985 - accuracy: 0.5083 - val_loss: 0.6990 - val_accuracy: 0.5046\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5096 - val_loss: 0.6998 - val_accuracy: 0.5090\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5098 - val_loss: 0.6969 - val_accuracy: 0.5075\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5112 - val_loss: 0.6964 - val_accuracy: 0.5108\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5114 - val_loss: 0.6955 - val_accuracy: 0.5127\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5124 - val_loss: 0.6952 - val_accuracy: 0.5113\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5133 - val_loss: 0.6949 - val_accuracy: 0.5131\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5139 - val_loss: 0.6949 - val_accuracy: 0.5086\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5138 - val_loss: 0.6944 - val_accuracy: 0.5110\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5139 - val_loss: 0.6939 - val_accuracy: 0.5135\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5139 - val_loss: 0.6940 - val_accuracy: 0.5125\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5155 - val_loss: 0.6936 - val_accuracy: 0.5103\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5155 - val_loss: 0.6933 - val_accuracy: 0.5171\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5164 - val_loss: 0.6936 - val_accuracy: 0.5108\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5172 - val_loss: 0.6933 - val_accuracy: 0.5182\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5181 - val_loss: 0.6943 - val_accuracy: 0.5139\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.6934 - val_accuracy: 0.5163\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5179 - val_loss: 0.6929 - val_accuracy: 0.5146\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6934 - val_accuracy: 0.5170\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5201 - val_loss: 0.6924 - val_accuracy: 0.5214\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5194 - val_loss: 0.6926 - val_accuracy: 0.5211\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6936 - val_accuracy: 0.5181\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5210 - val_loss: 0.6926 - val_accuracy: 0.5183\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6928 - val_accuracy: 0.5146\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5231 - val_loss: 0.6925 - val_accuracy: 0.5210\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5242\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5238 - val_loss: 0.6924 - val_accuracy: 0.5214\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5236 - val_loss: 0.6921 - val_accuracy: 0.5173\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5238 - val_loss: 0.6929 - val_accuracy: 0.5226\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5249 - val_loss: 0.6920 - val_accuracy: 0.5253\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5248 - val_loss: 0.6920 - val_accuracy: 0.5246\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5257 - val_loss: 0.6932 - val_accuracy: 0.5235\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5260 - val_loss: 0.6921 - val_accuracy: 0.5255\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5268 - val_loss: 0.6920 - val_accuracy: 0.5201\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5265 - val_loss: 0.6932 - val_accuracy: 0.5236\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5292\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5279 - val_loss: 0.6921 - val_accuracy: 0.5256\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5288 - val_loss: 0.6915 - val_accuracy: 0.5253\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5299 - val_loss: 0.6916 - val_accuracy: 0.5302\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5297 - val_loss: 0.6911 - val_accuracy: 0.5309\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5289 - val_loss: 0.6914 - val_accuracy: 0.5274\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5303 - val_loss: 0.6913 - val_accuracy: 0.5294\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5290 - val_loss: 0.6914 - val_accuracy: 0.5272\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5308 - val_loss: 0.6917 - val_accuracy: 0.5209\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5304 - val_loss: 0.6914 - val_accuracy: 0.5224\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5319 - val_loss: 0.6917 - val_accuracy: 0.5295\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5324 - val_loss: 0.6920 - val_accuracy: 0.5288\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5320 - val_loss: 0.6918 - val_accuracy: 0.5247\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5325 - val_loss: 0.6914 - val_accuracy: 0.5222\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5335 - val_loss: 0.6916 - val_accuracy: 0.5312\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5344 - val_loss: 0.6910 - val_accuracy: 0.5318\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5347 - val_loss: 0.6909 - val_accuracy: 0.5255\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5351 - val_loss: 0.6915 - val_accuracy: 0.5322\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5352 - val_loss: 0.6911 - val_accuracy: 0.5343\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5359 - val_loss: 0.6914 - val_accuracy: 0.5267\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5363 - val_loss: 0.6902 - val_accuracy: 0.5403\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5358 - val_loss: 0.6902 - val_accuracy: 0.5352\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5371 - val_loss: 0.6903 - val_accuracy: 0.5324\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5375 - val_loss: 0.6903 - val_accuracy: 0.5368\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5384 - val_loss: 0.6907 - val_accuracy: 0.5330\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5387 - val_loss: 0.6908 - val_accuracy: 0.5395\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5381 - val_loss: 0.6906 - val_accuracy: 0.5397\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5383 - val_loss: 0.6904 - val_accuracy: 0.5309\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5396 - val_loss: 0.6914 - val_accuracy: 0.5330\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5391 - val_loss: 0.6897 - val_accuracy: 0.5394\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5393 - val_loss: 0.6895 - val_accuracy: 0.5384\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5392 - val_loss: 0.6893 - val_accuracy: 0.5409\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5401 - val_loss: 0.6899 - val_accuracy: 0.5423\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5407 - val_loss: 0.6900 - val_accuracy: 0.5391\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5412 - val_loss: 0.6895 - val_accuracy: 0.5359\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5414 - val_loss: 0.6890 - val_accuracy: 0.5422\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5414 - val_loss: 0.6897 - val_accuracy: 0.5418\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5419 - val_loss: 0.6894 - val_accuracy: 0.5410\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5428\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5427 - val_loss: 0.6893 - val_accuracy: 0.5404\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5425 - val_loss: 0.6895 - val_accuracy: 0.5410\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5431 - val_loss: 0.6893 - val_accuracy: 0.5429\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5431 - val_loss: 0.6894 - val_accuracy: 0.5388\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5425 - val_loss: 0.6893 - val_accuracy: 0.5418\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5425 - val_loss: 0.6898 - val_accuracy: 0.5326\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5436 - val_loss: 0.6890 - val_accuracy: 0.5337\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5389\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5400 - val_loss: 0.6884 - val_accuracy: 0.5427\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5418 - val_loss: 0.6892 - val_accuracy: 0.5423\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5411 - val_loss: 0.6901 - val_accuracy: 0.5318\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5418 - val_loss: 0.6900 - val_accuracy: 0.5358\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_8\n",
      "cannot prune layer q_activation_8\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6878 - accuracy: 0.5420 - val_loss: 0.6962 - val_accuracy: 0.5185\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5412 - val_loss: 0.7065 - val_accuracy: 0.5128\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5280 - val_loss: 0.7143 - val_accuracy: 0.5073\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.7061 - val_accuracy: 0.5072\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5191 - val_loss: 0.7133 - val_accuracy: 0.5089\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5190 - val_loss: 0.7007 - val_accuracy: 0.5105\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5171 - val_loss: 0.6990 - val_accuracy: 0.5108\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.6939 - val_accuracy: 0.5153\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6923 - val_accuracy: 0.5189\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5223 - val_loss: 0.6917 - val_accuracy: 0.5205\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6915 - val_accuracy: 0.5238\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5246 - val_loss: 0.6916 - val_accuracy: 0.5216\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5247 - val_loss: 0.6916 - val_accuracy: 0.5246\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5248 - val_loss: 0.6915 - val_accuracy: 0.5268\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5259 - val_loss: 0.6914 - val_accuracy: 0.5234\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5270 - val_loss: 0.6914 - val_accuracy: 0.5255\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5268 - val_loss: 0.6914 - val_accuracy: 0.5230\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5262\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5271 - val_loss: 0.6914 - val_accuracy: 0.5262\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5275 - val_loss: 0.6916 - val_accuracy: 0.5261\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5267 - val_loss: 0.6917 - val_accuracy: 0.5207\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5276 - val_loss: 0.6913 - val_accuracy: 0.5251\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5281 - val_loss: 0.6912 - val_accuracy: 0.5273\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5278 - val_loss: 0.6912 - val_accuracy: 0.5297\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5282 - val_loss: 0.6910 - val_accuracy: 0.5249\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5283 - val_loss: 0.6910 - val_accuracy: 0.5284\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5276 - val_loss: 0.6913 - val_accuracy: 0.5269\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5278 - val_loss: 0.6909 - val_accuracy: 0.5268\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5276 - val_loss: 0.6908 - val_accuracy: 0.5283\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6899 - accuracy: 0.5286 - val_loss: 0.6910 - val_accuracy: 0.5280\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5281 - val_loss: 0.6915 - val_accuracy: 0.5262\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5291 - val_loss: 0.6908 - val_accuracy: 0.5256\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5291 - val_loss: 0.6908 - val_accuracy: 0.5298\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5292 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5285 - val_loss: 0.6909 - val_accuracy: 0.5280\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5280 - val_loss: 0.6910 - val_accuracy: 0.5278\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5278 - val_loss: 0.6923 - val_accuracy: 0.5128\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5296 - val_loss: 0.6907 - val_accuracy: 0.5289\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5292 - val_loss: 0.6909 - val_accuracy: 0.5236\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5288 - val_loss: 0.6905 - val_accuracy: 0.5288\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5297 - val_loss: 0.6909 - val_accuracy: 0.5277\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5297 - val_loss: 0.6907 - val_accuracy: 0.5310\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5297 - val_loss: 0.6910 - val_accuracy: 0.5311\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5296 - val_loss: 0.6907 - val_accuracy: 0.5290\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5304 - val_loss: 0.6909 - val_accuracy: 0.5281\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5294 - val_loss: 0.6905 - val_accuracy: 0.5307\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5302 - val_loss: 0.6909 - val_accuracy: 0.5313\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5309 - val_loss: 0.6905 - val_accuracy: 0.5306\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5307 - val_loss: 0.6908 - val_accuracy: 0.5313\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5309 - val_loss: 0.6915 - val_accuracy: 0.5307\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.52505076]\n",
      " [0.4882121 ]\n",
      " [0.50194377]\n",
      " [0.5527388 ]\n",
      " [0.4975322 ]\n",
      " [0.5000148 ]\n",
      " [0.52294016]\n",
      " [0.52423984]\n",
      " [0.50194377]\n",
      " [0.54687417]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 8)                 32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_9 (QActivatio  (None, 8)                 0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_9 is normal keras bn layer\n",
      "q_activation_9       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.1418 - accuracy: 0.4986 - val_loss: 0.7005 - val_accuracy: 0.5003\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5020 - val_loss: 0.6945 - val_accuracy: 0.5024\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5032 - val_loss: 0.6944 - val_accuracy: 0.5063\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5040 - val_loss: 0.6937 - val_accuracy: 0.5082\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5049 - val_loss: 0.6940 - val_accuracy: 0.5053\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5055 - val_loss: 0.6938 - val_accuracy: 0.5069\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5050 - val_loss: 0.7030 - val_accuracy: 0.5002\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5053 - val_loss: 0.6940 - val_accuracy: 0.5042\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5047 - val_loss: 0.6947 - val_accuracy: 0.5061\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5047 - val_loss: 0.6949 - val_accuracy: 0.5007\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5045 - val_loss: 0.6943 - val_accuracy: 0.5051\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5066 - val_loss: 0.6927 - val_accuracy: 0.5087\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5017\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5086 - val_loss: 0.6932 - val_accuracy: 0.5072\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5093 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5085 - val_loss: 0.6928 - val_accuracy: 0.5073\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5089 - val_loss: 0.6927 - val_accuracy: 0.5112\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5101 - val_loss: 0.6927 - val_accuracy: 0.5117\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5101 - val_loss: 0.6928 - val_accuracy: 0.5116\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6927 - val_accuracy: 0.5109\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5112 - val_loss: 0.6926 - val_accuracy: 0.5144\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5120 - val_loss: 0.6923 - val_accuracy: 0.5132\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6927 - val_accuracy: 0.5138\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6926 - val_accuracy: 0.5117\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5118 - val_loss: 0.6924 - val_accuracy: 0.5141\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5117 - val_loss: 0.6924 - val_accuracy: 0.5160\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5138 - val_loss: 0.6924 - val_accuracy: 0.5135\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6924 - val_accuracy: 0.5157\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5127 - val_loss: 0.6925 - val_accuracy: 0.5114\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6928 - val_accuracy: 0.5104\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5137 - val_loss: 0.6930 - val_accuracy: 0.5020\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6921 - val_accuracy: 0.5166\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5136 - val_loss: 0.6922 - val_accuracy: 0.5119\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5147 - val_loss: 0.6919 - val_accuracy: 0.5149\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.6922 - val_accuracy: 0.5145\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5152 - val_loss: 0.6920 - val_accuracy: 0.5174\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5145 - val_loss: 0.6935 - val_accuracy: 0.5121\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6921 - val_accuracy: 0.5155\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5155 - val_loss: 0.6921 - val_accuracy: 0.5165\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5157 - val_loss: 0.6923 - val_accuracy: 0.5153\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5149 - val_loss: 0.6921 - val_accuracy: 0.5165\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5140 - val_loss: 0.6921 - val_accuracy: 0.5159\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5141 - val_loss: 0.6925 - val_accuracy: 0.5128\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5153 - val_loss: 0.6916 - val_accuracy: 0.5190\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5163 - val_loss: 0.6921 - val_accuracy: 0.5144\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5151 - val_loss: 0.6920 - val_accuracy: 0.5117\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5147 - val_loss: 0.6919 - val_accuracy: 0.5144\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5145 - val_loss: 0.6917 - val_accuracy: 0.5151\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6921 - val_accuracy: 0.5168\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5155 - val_loss: 0.6926 - val_accuracy: 0.5107\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5153 - val_loss: 0.6917 - val_accuracy: 0.5164\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5147 - val_loss: 0.6919 - val_accuracy: 0.5164\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5153 - val_loss: 0.6917 - val_accuracy: 0.5167\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5156 - val_loss: 0.6927 - val_accuracy: 0.5140\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5149 - val_loss: 0.6919 - val_accuracy: 0.5183\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5162\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5147 - val_loss: 0.6918 - val_accuracy: 0.5177\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5154 - val_loss: 0.6926 - val_accuracy: 0.5125\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5153 - val_loss: 0.6917 - val_accuracy: 0.5147\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5157 - val_loss: 0.6937 - val_accuracy: 0.5120\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5149 - val_loss: 0.6919 - val_accuracy: 0.5146\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5164 - val_loss: 0.6919 - val_accuracy: 0.5172\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5137 - val_loss: 0.6914 - val_accuracy: 0.5190\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5166 - val_loss: 0.6914 - val_accuracy: 0.5175\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5154 - val_loss: 0.6918 - val_accuracy: 0.5081\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5148 - val_loss: 0.6917 - val_accuracy: 0.5159\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5157 - val_loss: 0.6916 - val_accuracy: 0.5169\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5170 - val_loss: 0.6917 - val_accuracy: 0.5169\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5162 - val_loss: 0.6917 - val_accuracy: 0.5175\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5155 - val_loss: 0.6913 - val_accuracy: 0.5192\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6918 - val_accuracy: 0.5163\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5164 - val_loss: 0.6916 - val_accuracy: 0.5178\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5154 - val_loss: 0.6918 - val_accuracy: 0.5141\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5160 - val_loss: 0.6913 - val_accuracy: 0.5157\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5152 - val_loss: 0.6918 - val_accuracy: 0.5170\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5153 - val_loss: 0.6916 - val_accuracy: 0.5168\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5164 - val_loss: 0.6911 - val_accuracy: 0.5169\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5157 - val_loss: 0.6926 - val_accuracy: 0.5116\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6916 - val_accuracy: 0.5173\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5163 - val_loss: 0.6922 - val_accuracy: 0.5122\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5169 - val_loss: 0.6930 - val_accuracy: 0.5122\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5166 - val_loss: 0.6911 - val_accuracy: 0.5218\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5153\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5172 - val_loss: 0.6921 - val_accuracy: 0.5149\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6920 - val_accuracy: 0.5161\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5183 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5169 - val_loss: 0.6917 - val_accuracy: 0.5173\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5177 - val_loss: 0.6920 - val_accuracy: 0.5151\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5171 - val_loss: 0.6916 - val_accuracy: 0.5162\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5172 - val_loss: 0.6915 - val_accuracy: 0.5180\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5184 - val_loss: 0.6923 - val_accuracy: 0.5078\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5183 - val_loss: 0.6918 - val_accuracy: 0.5166\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5162 - val_loss: 0.6944 - val_accuracy: 0.5073\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5185 - val_loss: 0.6917 - val_accuracy: 0.5144\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5180 - val_loss: 0.6933 - val_accuracy: 0.5118\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5183 - val_loss: 0.6917 - val_accuracy: 0.5119\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6916 - val_accuracy: 0.5157\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5165 - val_loss: 0.6920 - val_accuracy: 0.5142\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6915 - val_accuracy: 0.5168\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_9\n",
      "cannot prune layer q_activation_9\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.6912 - accuracy: 0.5175 - val_loss: 0.6915 - val_accuracy: 0.5176\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5172 - val_loss: 0.6916 - val_accuracy: 0.5139\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5173 - val_loss: 0.6915 - val_accuracy: 0.5152\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6917 - val_accuracy: 0.5178\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5165 - val_loss: 0.6917 - val_accuracy: 0.5173\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5118\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5164 - val_loss: 0.6922 - val_accuracy: 0.5142\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5154\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5190\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5170 - val_loss: 0.6922 - val_accuracy: 0.5106\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5185 - val_loss: 0.6915 - val_accuracy: 0.5174\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5167 - val_loss: 0.6921 - val_accuracy: 0.5149\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5166 - val_loss: 0.6923 - val_accuracy: 0.5078\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5185 - val_loss: 0.6919 - val_accuracy: 0.5113\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5165 - val_loss: 0.6917 - val_accuracy: 0.5186\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.6928 - val_accuracy: 0.5156\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5175 - val_loss: 0.6918 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5172 - val_loss: 0.6917 - val_accuracy: 0.5141\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5176 - val_loss: 0.6917 - val_accuracy: 0.5117\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5172 - val_loss: 0.6918 - val_accuracy: 0.5141\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5169 - val_loss: 0.6913 - val_accuracy: 0.5152\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5172 - val_loss: 0.6918 - val_accuracy: 0.5145\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5177 - val_loss: 0.6943 - val_accuracy: 0.5088\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5171 - val_loss: 0.6929 - val_accuracy: 0.5082\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5155 - val_loss: 0.6914 - val_accuracy: 0.5148\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5174 - val_loss: 0.6913 - val_accuracy: 0.5148\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5170 - val_loss: 0.6922 - val_accuracy: 0.5126\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5171 - val_loss: 0.6927 - val_accuracy: 0.5136\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5172 - val_loss: 0.6917 - val_accuracy: 0.5155\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5177 - val_loss: 0.6915 - val_accuracy: 0.5128\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5172 - val_loss: 0.6915 - val_accuracy: 0.5156\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5177 - val_loss: 0.6917 - val_accuracy: 0.5147\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5160 - val_loss: 0.6918 - val_accuracy: 0.5132\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5170 - val_loss: 0.6916 - val_accuracy: 0.5161\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5173 - val_loss: 0.6919 - val_accuracy: 0.5135\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5166 - val_loss: 0.6920 - val_accuracy: 0.5123\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5179 - val_loss: 0.6928 - val_accuracy: 0.5147\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5173 - val_loss: 0.6920 - val_accuracy: 0.5130\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5161 - val_loss: 0.6929 - val_accuracy: 0.5119\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5165 - val_loss: 0.6915 - val_accuracy: 0.5180\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5175 - val_loss: 0.6924 - val_accuracy: 0.5115\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5178 - val_loss: 0.6921 - val_accuracy: 0.5096\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.6922 - val_accuracy: 0.5148\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5127\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5167 - val_loss: 0.6920 - val_accuracy: 0.5142\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5174 - val_loss: 0.6914 - val_accuracy: 0.5141\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5169 - val_loss: 0.6917 - val_accuracy: 0.5152\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5168 - val_loss: 0.6918 - val_accuracy: 0.5099\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5177 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5155 - val_loss: 0.6914 - val_accuracy: 0.5168\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5137489 ]\n",
      " [0.50390416]\n",
      " [0.4400655 ]\n",
      " [0.5043532 ]\n",
      " [0.50390416]\n",
      " [0.50390416]\n",
      " [0.5013205 ]\n",
      " [0.507397  ]\n",
      " [0.5414566 ]\n",
      " [0.4025777 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_10 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_10 is normal keras bn layer\n",
      "q_activation_10      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.4153 - accuracy: 0.4999 - val_loss: 0.8835 - val_accuracy: 0.4955\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.8545 - accuracy: 0.5002 - val_loss: 0.9163 - val_accuracy: 0.4980\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7986 - accuracy: 0.5010 - val_loss: 0.7838 - val_accuracy: 0.4986\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7524 - accuracy: 0.5010 - val_loss: 0.7487 - val_accuracy: 0.4992\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7288 - accuracy: 0.5008 - val_loss: 0.7224 - val_accuracy: 0.5029\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7171 - accuracy: 0.5011 - val_loss: 0.7128 - val_accuracy: 0.5028\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7192 - accuracy: 0.5012 - val_loss: 0.7138 - val_accuracy: 0.5019\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7077 - accuracy: 0.5013 - val_loss: 0.7038 - val_accuracy: 0.5021\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5019 - val_loss: 0.7012 - val_accuracy: 0.5017\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7002 - accuracy: 0.5018 - val_loss: 0.7037 - val_accuracy: 0.5039\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6986 - accuracy: 0.5018 - val_loss: 0.6990 - val_accuracy: 0.5028\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.5020 - val_loss: 0.6981 - val_accuracy: 0.5041\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5035 - val_loss: 0.6969 - val_accuracy: 0.5048\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5034 - val_loss: 0.6958 - val_accuracy: 0.5053\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5032 - val_loss: 0.6955 - val_accuracy: 0.5061\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5046 - val_loss: 0.6952 - val_accuracy: 0.5038\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5038 - val_loss: 0.6947 - val_accuracy: 0.5064\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5047 - val_loss: 0.6943 - val_accuracy: 0.5066\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5051 - val_loss: 0.6944 - val_accuracy: 0.5035\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5047 - val_loss: 0.6942 - val_accuracy: 0.5031\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5061 - val_loss: 0.6938 - val_accuracy: 0.5061\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5063 - val_loss: 0.6942 - val_accuracy: 0.5054\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5061 - val_loss: 0.6937 - val_accuracy: 0.5066\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5073 - val_loss: 0.6940 - val_accuracy: 0.5063\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5064 - val_loss: 0.6935 - val_accuracy: 0.5078\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5070 - val_loss: 0.6937 - val_accuracy: 0.5074\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6931 - accuracy: 0.5074 - val_loss: 0.6939 - val_accuracy: 0.5038\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5079 - val_loss: 0.6937 - val_accuracy: 0.5091\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5083 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6928 - val_accuracy: 0.5123\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5092 - val_loss: 0.6928 - val_accuracy: 0.5114\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5091 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5095\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5110 - val_loss: 0.6941 - val_accuracy: 0.5048\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5119 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5116 - val_loss: 0.6929 - val_accuracy: 0.5139\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6929 - val_accuracy: 0.5138\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5131 - val_loss: 0.6927 - val_accuracy: 0.5146\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5118 - val_loss: 0.6926 - val_accuracy: 0.5194\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5134\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5122\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5172\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5150\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5168\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5133 - val_loss: 0.6931 - val_accuracy: 0.5158\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6925 - val_accuracy: 0.5157\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5128 - val_loss: 0.6926 - val_accuracy: 0.5155\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5139 - val_loss: 0.6928 - val_accuracy: 0.5175\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5130 - val_loss: 0.6926 - val_accuracy: 0.5139\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5147 - val_loss: 0.6935 - val_accuracy: 0.5153\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5137 - val_loss: 0.6929 - val_accuracy: 0.5150\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5142 - val_loss: 0.6926 - val_accuracy: 0.5148\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5155 - val_loss: 0.6928 - val_accuracy: 0.5117\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.5170\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6926 - val_accuracy: 0.5163\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5157 - val_loss: 0.6927 - val_accuracy: 0.5156\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5148 - val_loss: 0.6926 - val_accuracy: 0.5160\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5159 - val_loss: 0.6925 - val_accuracy: 0.5173\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5148 - val_loss: 0.6932 - val_accuracy: 0.5100\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5150 - val_loss: 0.6927 - val_accuracy: 0.5155\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5152 - val_loss: 0.6933 - val_accuracy: 0.5118\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6924 - val_accuracy: 0.5173\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5164 - val_loss: 0.6926 - val_accuracy: 0.5190\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6923 - val_accuracy: 0.5202\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6923 - val_accuracy: 0.5194\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5162 - val_loss: 0.6924 - val_accuracy: 0.5172\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5075\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5159 - val_loss: 0.6927 - val_accuracy: 0.5155\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5167 - val_loss: 0.6923 - val_accuracy: 0.5181\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6926 - val_accuracy: 0.5172\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5105\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5138 - val_loss: 0.6925 - val_accuracy: 0.5170\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6926 - val_accuracy: 0.5147\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5161\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5165 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5176 - val_loss: 0.6930 - val_accuracy: 0.5095\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5168 - val_loss: 0.6926 - val_accuracy: 0.5183\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6941 - val_accuracy: 0.5040\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5161 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5166 - val_loss: 0.6925 - val_accuracy: 0.5171\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5156 - val_loss: 0.6926 - val_accuracy: 0.5129\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.6927 - val_accuracy: 0.5151\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5157 - val_loss: 0.6934 - val_accuracy: 0.5138\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5170 - val_loss: 0.6927 - val_accuracy: 0.5161\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.6925 - val_accuracy: 0.5153\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_10\n",
      "cannot prune layer q_activation_10\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6923 - val_accuracy: 0.5184\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5169 - val_loss: 0.6933 - val_accuracy: 0.5029\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6924 - val_accuracy: 0.5180\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5164 - val_loss: 0.6923 - val_accuracy: 0.5182\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5159 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5153\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6929 - val_accuracy: 0.5130\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5170 - val_loss: 0.6927 - val_accuracy: 0.5164\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5165 - val_loss: 0.6929 - val_accuracy: 0.5106\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5158 - val_loss: 0.6932 - val_accuracy: 0.5072\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5184\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5171 - val_loss: 0.6924 - val_accuracy: 0.5125\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 4s 8ms/step - loss: 0.6920 - accuracy: 0.5162 - val_loss: 0.6926 - val_accuracy: 0.5174\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 3s 8ms/step - loss: 0.6920 - accuracy: 0.5158 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6925 - val_accuracy: 0.5158\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 3s 8ms/step - loss: 0.6920 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 4s 9ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5175\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6927 - val_accuracy: 0.5172\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 4s 8ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6929 - val_accuracy: 0.5151\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.6921 - accuracy: 0.5163 - val_loss: 0.6929 - val_accuracy: 0.5129\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 3s 8ms/step - loss: 0.6921 - accuracy: 0.5168 - val_loss: 0.6930 - val_accuracy: 0.5131\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 4s 10ms/step - loss: 0.6921 - accuracy: 0.5174 - val_loss: 0.6938 - val_accuracy: 0.5150\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 4s 9ms/step - loss: 0.6921 - accuracy: 0.5177 - val_loss: 0.6932 - val_accuracy: 0.5144\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 5s 12ms/step - loss: 0.6922 - accuracy: 0.5164 - val_loss: 0.6928 - val_accuracy: 0.5157\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 4s 10ms/step - loss: 0.6922 - accuracy: 0.5167 - val_loss: 0.6934 - val_accuracy: 0.5177\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5175 - val_loss: 0.6931 - val_accuracy: 0.5173\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5166 - val_loss: 0.6930 - val_accuracy: 0.5154\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5181 - val_loss: 0.6930 - val_accuracy: 0.5139\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5188 - val_loss: 0.6932 - val_accuracy: 0.5148\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5175 - val_loss: 0.6928 - val_accuracy: 0.5104\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5179 - val_loss: 0.6925 - val_accuracy: 0.5181\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5182 - val_loss: 0.6943 - val_accuracy: 0.5168\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5172 - val_loss: 0.6926 - val_accuracy: 0.5133\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5180 - val_loss: 0.6922 - val_accuracy: 0.5176\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5176 - val_loss: 0.6943 - val_accuracy: 0.5149\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5177 - val_loss: 0.6941 - val_accuracy: 0.5171\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5181\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5178 - val_loss: 0.6922 - val_accuracy: 0.5152\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5187 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5180 - val_loss: 0.6944 - val_accuracy: 0.5136\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5183 - val_loss: 0.6950 - val_accuracy: 0.5137\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5120\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5183 - val_loss: 0.6947 - val_accuracy: 0.5167\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5197 - val_loss: 0.6920 - val_accuracy: 0.5182\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5189 - val_loss: 0.6927 - val_accuracy: 0.5120\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5187 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5191 - val_loss: 0.6943 - val_accuracy: 0.5114\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5199 - val_loss: 0.6922 - val_accuracy: 0.5162\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5195 - val_loss: 0.6921 - val_accuracy: 0.5158\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5193758 ]\n",
      " [0.47413164]\n",
      " [0.4713085 ]\n",
      " [0.4863208 ]\n",
      " [0.46533448]\n",
      " [0.48709333]\n",
      " [0.5074612 ]\n",
      " [0.48669544]\n",
      " [0.54234684]\n",
      " [0.47419506]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_11 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_11 is normal keras bn layer\n",
      "q_activation_11      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.9379 - accuracy: 0.4999 - val_loss: 0.8197 - val_accuracy: 0.5012\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7859 - accuracy: 0.5011 - val_loss: 0.7656 - val_accuracy: 0.5019\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7559 - accuracy: 0.5008 - val_loss: 0.7475 - val_accuracy: 0.5022\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7448 - accuracy: 0.5018 - val_loss: 0.7433 - val_accuracy: 0.5012\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7360 - accuracy: 0.5010 - val_loss: 0.7299 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7298 - accuracy: 0.5019 - val_loss: 0.7298 - val_accuracy: 0.4999\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7272 - accuracy: 0.5025 - val_loss: 0.7257 - val_accuracy: 0.4990\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7248 - accuracy: 0.5022 - val_loss: 0.7216 - val_accuracy: 0.4978\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7186 - accuracy: 0.5023 - val_loss: 0.7192 - val_accuracy: 0.5001\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7155 - accuracy: 0.5026 - val_loss: 0.7182 - val_accuracy: 0.5002\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7707 - accuracy: 0.5027 - val_loss: 0.9333 - val_accuracy: 0.5018\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7275 - accuracy: 0.4993 - val_loss: 0.7139 - val_accuracy: 0.5003\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7127 - accuracy: 0.5014 - val_loss: 0.7098 - val_accuracy: 0.5001\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7098 - accuracy: 0.5031 - val_loss: 0.7088 - val_accuracy: 0.4991\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7082 - accuracy: 0.5028 - val_loss: 0.7083 - val_accuracy: 0.5022\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7067 - accuracy: 0.5024 - val_loss: 0.7063 - val_accuracy: 0.4993\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7055 - accuracy: 0.5025 - val_loss: 0.7056 - val_accuracy: 0.5004\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7046 - accuracy: 0.5025 - val_loss: 0.7043 - val_accuracy: 0.5016\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7038 - accuracy: 0.5031 - val_loss: 0.7035 - val_accuracy: 0.5005\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7033 - accuracy: 0.5025 - val_loss: 0.7034 - val_accuracy: 0.5011\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5026 - val_loss: 0.7025 - val_accuracy: 0.4996\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7012 - accuracy: 0.5026 - val_loss: 0.7016 - val_accuracy: 0.5003\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.5033 - val_loss: 0.7185 - val_accuracy: 0.4997\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7016 - accuracy: 0.5023 - val_loss: 0.7006 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6993 - accuracy: 0.5037 - val_loss: 0.7000 - val_accuracy: 0.5015\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.5037 - val_loss: 0.6995 - val_accuracy: 0.4992\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6978 - accuracy: 0.5027 - val_loss: 0.6987 - val_accuracy: 0.5024\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5030 - val_loss: 0.6989 - val_accuracy: 0.5032\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5035 - val_loss: 0.6980 - val_accuracy: 0.5025\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5043 - val_loss: 0.6969 - val_accuracy: 0.5028\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5042 - val_loss: 0.6963 - val_accuracy: 0.5014\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5049 - val_loss: 0.6960 - val_accuracy: 0.5032\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5042 - val_loss: 0.6957 - val_accuracy: 0.5031\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5058 - val_loss: 0.6952 - val_accuracy: 0.5047\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5056 - val_loss: 0.6948 - val_accuracy: 0.5059\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5077 - val_loss: 0.6946 - val_accuracy: 0.5070\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5070 - val_loss: 0.6943 - val_accuracy: 0.5055\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5073 - val_loss: 0.6944 - val_accuracy: 0.5080\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5069 - val_loss: 0.6941 - val_accuracy: 0.5119\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5074 - val_loss: 0.6940 - val_accuracy: 0.5083\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6937 - val_accuracy: 0.5042\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5072 - val_loss: 0.6942 - val_accuracy: 0.5039\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5077 - val_loss: 0.6940 - val_accuracy: 0.5068\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5076 - val_loss: 0.6937 - val_accuracy: 0.5073\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5077 - val_loss: 0.6935 - val_accuracy: 0.5053\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5077 - val_loss: 0.6937 - val_accuracy: 0.5084\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5078 - val_loss: 0.6935 - val_accuracy: 0.5074\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5085 - val_loss: 0.6937 - val_accuracy: 0.5048\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5090 - val_loss: 0.6935 - val_accuracy: 0.5089\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5078 - val_loss: 0.6933 - val_accuracy: 0.5074\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5082 - val_loss: 0.6933 - val_accuracy: 0.5093\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6938 - val_accuracy: 0.5068\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5076 - val_loss: 0.6934 - val_accuracy: 0.5096\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5088 - val_loss: 0.6931 - val_accuracy: 0.5097\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5086 - val_loss: 0.6934 - val_accuracy: 0.5060\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5077\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5093 - val_loss: 0.6937 - val_accuracy: 0.5049\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5093 - val_loss: 0.6935 - val_accuracy: 0.5072\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5086\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5084 - val_loss: 0.6932 - val_accuracy: 0.5092\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5108 - val_loss: 0.6933 - val_accuracy: 0.5079\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5093 - val_loss: 0.6933 - val_accuracy: 0.5109\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5095 - val_loss: 0.6933 - val_accuracy: 0.5092\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5099 - val_loss: 0.6934 - val_accuracy: 0.5084\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5098 - val_loss: 0.6935 - val_accuracy: 0.5058\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6936 - val_accuracy: 0.5084\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6934 - val_accuracy: 0.5105\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5109 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5114 - val_loss: 0.6934 - val_accuracy: 0.5124\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5101 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5117 - val_loss: 0.6934 - val_accuracy: 0.5117\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5110\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5106 - val_loss: 0.6934 - val_accuracy: 0.5069\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5113 - val_loss: 0.6933 - val_accuracy: 0.5094\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5113 - val_loss: 0.6935 - val_accuracy: 0.5097\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5111 - val_loss: 0.6938 - val_accuracy: 0.5107\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5111 - val_loss: 0.6934 - val_accuracy: 0.5099\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5113 - val_loss: 0.6932 - val_accuracy: 0.5101\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5113 - val_loss: 0.6934 - val_accuracy: 0.5122\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5114 - val_loss: 0.6933 - val_accuracy: 0.5107\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_11\n",
      "cannot prune layer q_activation_11\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 5ms/step - loss: 0.6926 - accuracy: 0.5099 - val_loss: 0.6936 - val_accuracy: 0.5101\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5098 - val_loss: 0.6932 - val_accuracy: 0.5108\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6933 - val_accuracy: 0.5104\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6934 - val_accuracy: 0.5071\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5099 - val_loss: 0.6935 - val_accuracy: 0.5075\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5112 - val_loss: 0.6934 - val_accuracy: 0.5074\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5106 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6936 - val_accuracy: 0.5088\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6935 - val_accuracy: 0.5109\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5108 - val_loss: 0.6934 - val_accuracy: 0.5114\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5106 - val_loss: 0.6936 - val_accuracy: 0.5081\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5111 - val_loss: 0.6933 - val_accuracy: 0.5112\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5110 - val_loss: 0.6937 - val_accuracy: 0.5095\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5114 - val_loss: 0.6934 - val_accuracy: 0.5110\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6935 - val_accuracy: 0.5101\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5116 - val_loss: 0.6933 - val_accuracy: 0.5082\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5120 - val_loss: 0.6935 - val_accuracy: 0.5072\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5115 - val_loss: 0.6940 - val_accuracy: 0.5114\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5121 - val_loss: 0.6933 - val_accuracy: 0.5116\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.5059\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5120 - val_loss: 0.6938 - val_accuracy: 0.5089\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5121 - val_loss: 0.6934 - val_accuracy: 0.5114\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6934 - val_accuracy: 0.5096\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5118 - val_loss: 0.6933 - val_accuracy: 0.5110\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5123 - val_loss: 0.6933 - val_accuracy: 0.5081\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5128 - val_loss: 0.6941 - val_accuracy: 0.5095\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.5125\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5135 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6922 - accuracy: 0.5129 - val_loss: 0.6935 - val_accuracy: 0.5130\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5121 - val_loss: 0.6934 - val_accuracy: 0.5066\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.5107\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5127 - val_loss: 0.6933 - val_accuracy: 0.5104\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5127 - val_loss: 0.6938 - val_accuracy: 0.5096\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6930 - val_accuracy: 0.5147\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5124 - val_loss: 0.6932 - val_accuracy: 0.5088\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5121 - val_loss: 0.6930 - val_accuracy: 0.5153\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5138 - val_loss: 0.6933 - val_accuracy: 0.5133\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6932 - val_accuracy: 0.5146\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5123\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5127\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5124 - val_loss: 0.6930 - val_accuracy: 0.5160\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5152\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5130 - val_loss: 0.6932 - val_accuracy: 0.5139\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5126 - val_loss: 0.6929 - val_accuracy: 0.5160\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5129 - val_loss: 0.6929 - val_accuracy: 0.5130\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6932 - val_accuracy: 0.5123\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5130 - val_loss: 0.6930 - val_accuracy: 0.5112\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5131 - val_loss: 0.6930 - val_accuracy: 0.5120\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5167727 ]\n",
      " [0.47017086]\n",
      " [0.49378377]\n",
      " [0.49377072]\n",
      " [0.46918398]\n",
      " [0.47859538]\n",
      " [0.50255364]\n",
      " [0.50840104]\n",
      " [0.49999124]\n",
      " [0.5086561 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_12 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_12 is normal keras bn layer\n",
      "q_activation_12      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 4ms/step - loss: 0.7137 - accuracy: 0.5036 - val_loss: 0.6974 - val_accuracy: 0.5069\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5068 - val_loss: 0.6946 - val_accuracy: 0.5047\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5070 - val_loss: 0.6939 - val_accuracy: 0.5070\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5075 - val_loss: 0.6935 - val_accuracy: 0.5069\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5093 - val_loss: 0.6944 - val_accuracy: 0.5049\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.5042\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5077 - val_loss: 0.6940 - val_accuracy: 0.5074\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6931 - val_accuracy: 0.5055\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6931 - val_accuracy: 0.5083\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5089 - val_loss: 0.6930 - val_accuracy: 0.5027\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5087 - val_loss: 0.6931 - val_accuracy: 0.5069\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6927 - val_accuracy: 0.5076\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5092 - val_loss: 0.6929 - val_accuracy: 0.5098\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5112 - val_loss: 0.6927 - val_accuracy: 0.5126\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5111 - val_loss: 0.6926 - val_accuracy: 0.5115\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5104 - val_loss: 0.6931 - val_accuracy: 0.5090\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5110 - val_loss: 0.6928 - val_accuracy: 0.5098\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5112 - val_loss: 0.6928 - val_accuracy: 0.5065\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5102 - val_loss: 0.6952 - val_accuracy: 0.5119\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5110 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5056\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5108 - val_loss: 0.6927 - val_accuracy: 0.5068\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5116\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5119\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5132 - val_loss: 0.6923 - val_accuracy: 0.5107\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5120 - val_loss: 0.6928 - val_accuracy: 0.5105\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5119 - val_loss: 0.6927 - val_accuracy: 0.5067\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5132\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5137 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6922 - val_accuracy: 0.5137\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5132 - val_loss: 0.6925 - val_accuracy: 0.5142\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5145 - val_loss: 0.6923 - val_accuracy: 0.5127\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6931 - val_accuracy: 0.5087\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5139 - val_loss: 0.6926 - val_accuracy: 0.5149\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5135 - val_loss: 0.6936 - val_accuracy: 0.5041\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5112\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5126 - val_loss: 0.6927 - val_accuracy: 0.5075\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5031\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5139 - val_loss: 0.6932 - val_accuracy: 0.5032\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5145 - val_loss: 0.6924 - val_accuracy: 0.5140\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5131 - val_loss: 0.6930 - val_accuracy: 0.5123\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6924 - val_accuracy: 0.5106\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5131 - val_loss: 0.6923 - val_accuracy: 0.5139\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5135 - val_loss: 0.6922 - val_accuracy: 0.5163\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5151 - val_loss: 0.6929 - val_accuracy: 0.5046\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5143 - val_loss: 0.6923 - val_accuracy: 0.5159\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5138 - val_loss: 0.6924 - val_accuracy: 0.5125\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5146 - val_loss: 0.6925 - val_accuracy: 0.5113\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5130 - val_loss: 0.6922 - val_accuracy: 0.5165\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5146 - val_loss: 0.6922 - val_accuracy: 0.5086\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_12\n",
      "cannot prune layer q_activation_12\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6930 - val_accuracy: 0.5136\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5124 - val_loss: 0.6927 - val_accuracy: 0.5067\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5116 - val_loss: 0.6959 - val_accuracy: 0.5055\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5125\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5094 - val_loss: 0.6928 - val_accuracy: 0.5100\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5112 - val_loss: 0.6926 - val_accuracy: 0.5115\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5117 - val_loss: 0.6932 - val_accuracy: 0.5094\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5133 - val_loss: 0.6925 - val_accuracy: 0.5122\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5128 - val_loss: 0.6925 - val_accuracy: 0.5109\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5121 - val_loss: 0.6925 - val_accuracy: 0.5128\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5116 - val_loss: 0.6924 - val_accuracy: 0.5137\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5090\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5119 - val_loss: 0.6926 - val_accuracy: 0.5065\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5142 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5127 - val_loss: 0.6929 - val_accuracy: 0.5122\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5126 - val_loss: 0.6924 - val_accuracy: 0.5123\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5132\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5067\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5129 - val_loss: 0.6926 - val_accuracy: 0.5127\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5131 - val_loss: 0.6927 - val_accuracy: 0.5084\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5128 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5130 - val_loss: 0.6921 - val_accuracy: 0.5139\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6926 - val_accuracy: 0.5086\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6922 - val_accuracy: 0.5145\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5127 - val_loss: 0.6926 - val_accuracy: 0.5096\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5153 - val_loss: 0.6930 - val_accuracy: 0.5042\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6922 - val_accuracy: 0.5112\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5151\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6923 - val_accuracy: 0.5125\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6929 - val_accuracy: 0.5146\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6919 - val_accuracy: 0.5137\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5149 - val_loss: 0.6923 - val_accuracy: 0.5159\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6923 - val_accuracy: 0.5125\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5159 - val_loss: 0.6920 - val_accuracy: 0.5181\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5159 - val_loss: 0.6924 - val_accuracy: 0.5175\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5147 - val_loss: 0.6917 - val_accuracy: 0.5166\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5163 - val_loss: 0.6918 - val_accuracy: 0.5118\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6920 - val_accuracy: 0.5138\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6920 - val_accuracy: 0.5128\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5174 - val_loss: 0.6918 - val_accuracy: 0.5150\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5169 - val_loss: 0.6918 - val_accuracy: 0.5113\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5172 - val_loss: 0.6922 - val_accuracy: 0.5056\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5185 - val_loss: 0.6917 - val_accuracy: 0.5166\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5179 - val_loss: 0.6918 - val_accuracy: 0.5183\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5180 - val_loss: 0.6921 - val_accuracy: 0.5228\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5181 - val_loss: 0.6927 - val_accuracy: 0.5141\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5189 - val_loss: 0.6915 - val_accuracy: 0.5139\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5178 - val_loss: 0.6917 - val_accuracy: 0.5212\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5187 - val_loss: 0.6912 - val_accuracy: 0.5246\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5287694 ]\n",
      " [0.49826488]\n",
      " [0.4560505 ]\n",
      " [0.5176141 ]\n",
      " [0.5195251 ]\n",
      " [0.515619  ]\n",
      " [0.5085752 ]\n",
      " [0.49277112]\n",
      " [0.515619  ]\n",
      " [0.47289672]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_13 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_13 is normal keras bn layer\n",
      "q_activation_13      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 2.2125 - accuracy: 0.5030 - val_loss: 1.3910 - val_accuracy: 0.4978\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 1.0157 - accuracy: 0.4990 - val_loss: 0.8601 - val_accuracy: 0.4950\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.8357 - accuracy: 0.4977 - val_loss: 0.8026 - val_accuracy: 0.4950\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7983 - accuracy: 0.4972 - val_loss: 0.7764 - val_accuracy: 0.4972\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7718 - accuracy: 0.4958 - val_loss: 0.7528 - val_accuracy: 0.4965\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7395 - accuracy: 0.4963 - val_loss: 0.7255 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.4976 - val_loss: 0.7167 - val_accuracy: 0.4944\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7132 - accuracy: 0.4988 - val_loss: 0.7111 - val_accuracy: 0.4961\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7068 - accuracy: 0.4993 - val_loss: 0.7050 - val_accuracy: 0.4991\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.5000 - val_loss: 0.7016 - val_accuracy: 0.4982\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6999 - accuracy: 0.5016 - val_loss: 0.6997 - val_accuracy: 0.4986\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6982 - accuracy: 0.5014 - val_loss: 0.6981 - val_accuracy: 0.4989\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6970 - accuracy: 0.5037 - val_loss: 0.6972 - val_accuracy: 0.5014\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.5050 - val_loss: 0.6961 - val_accuracy: 0.5033\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5058 - val_loss: 0.6955 - val_accuracy: 0.5046\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5057 - val_loss: 0.6951 - val_accuracy: 0.5038\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5069 - val_loss: 0.6947 - val_accuracy: 0.5029\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5063 - val_loss: 0.6946 - val_accuracy: 0.5030\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5072 - val_loss: 0.6945 - val_accuracy: 0.5059\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5071 - val_loss: 0.6942 - val_accuracy: 0.5034\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5079 - val_loss: 0.6938 - val_accuracy: 0.5064\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5104 - val_loss: 0.6945 - val_accuracy: 0.5029\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5084 - val_loss: 0.6939 - val_accuracy: 0.5055\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5098 - val_loss: 0.6934 - val_accuracy: 0.5078\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5124\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6929 - val_accuracy: 0.5138\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5125\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5132 - val_loss: 0.6928 - val_accuracy: 0.5139\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5095\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5146\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5139 - val_loss: 0.6926 - val_accuracy: 0.5167\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.5138\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5134\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5182\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5161 - val_loss: 0.6955 - val_accuracy: 0.5078\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6924 - val_accuracy: 0.5209\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5159 - val_loss: 0.6924 - val_accuracy: 0.5169\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5157 - val_loss: 0.6928 - val_accuracy: 0.5124\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6924 - val_accuracy: 0.5208\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5163 - val_loss: 0.6921 - val_accuracy: 0.5202\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5219\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5184 - val_loss: 0.6921 - val_accuracy: 0.5197\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5177 - val_loss: 0.6922 - val_accuracy: 0.5205\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5185 - val_loss: 0.6930 - val_accuracy: 0.5178\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5245\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5199 - val_loss: 0.6924 - val_accuracy: 0.5177\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5197 - val_loss: 0.6923 - val_accuracy: 0.5214\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5199 - val_loss: 0.6920 - val_accuracy: 0.5192\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6919 - val_accuracy: 0.5207\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6918 - val_accuracy: 0.5249\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6920 - val_accuracy: 0.5196\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5210 - val_loss: 0.6920 - val_accuracy: 0.5228\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5221 - val_loss: 0.6918 - val_accuracy: 0.5235\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5216 - val_loss: 0.6921 - val_accuracy: 0.5178\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5207\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5228 - val_loss: 0.6921 - val_accuracy: 0.5216\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6913 - val_accuracy: 0.5269\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5221 - val_loss: 0.6918 - val_accuracy: 0.5223\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5234 - val_loss: 0.6918 - val_accuracy: 0.5232\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5231 - val_loss: 0.6923 - val_accuracy: 0.5174\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5232 - val_loss: 0.6913 - val_accuracy: 0.5280\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6926 - val_accuracy: 0.5199\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5245 - val_loss: 0.6913 - val_accuracy: 0.5241\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6912 - val_accuracy: 0.5244\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5234 - val_loss: 0.6919 - val_accuracy: 0.5199\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5229 - val_loss: 0.6915 - val_accuracy: 0.5216\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5225 - val_loss: 0.6927 - val_accuracy: 0.5209\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5239\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5241 - val_loss: 0.6912 - val_accuracy: 0.5251\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5247 - val_loss: 0.6914 - val_accuracy: 0.5292\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5238 - val_loss: 0.6915 - val_accuracy: 0.5266\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5244 - val_loss: 0.6911 - val_accuracy: 0.5262\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5256 - val_loss: 0.6909 - val_accuracy: 0.5297\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5240 - val_loss: 0.6922 - val_accuracy: 0.5162\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5249 - val_loss: 0.6907 - val_accuracy: 0.5278\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5254 - val_loss: 0.6905 - val_accuracy: 0.5308\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5253 - val_loss: 0.6912 - val_accuracy: 0.5247\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5258 - val_loss: 0.6915 - val_accuracy: 0.5197\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5255 - val_loss: 0.6907 - val_accuracy: 0.5290\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5262 - val_loss: 0.6904 - val_accuracy: 0.5276\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5243 - val_loss: 0.6914 - val_accuracy: 0.5236\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5235 - val_loss: 0.6911 - val_accuracy: 0.5236\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6916 - val_accuracy: 0.5279\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5254 - val_loss: 0.6964 - val_accuracy: 0.5124\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5254\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5233 - val_loss: 0.6915 - val_accuracy: 0.5259\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6912 - val_accuracy: 0.5264\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5233 - val_loss: 0.6913 - val_accuracy: 0.5240\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5237 - val_loss: 0.6910 - val_accuracy: 0.5284\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6910 - val_accuracy: 0.5272\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5234 - val_loss: 0.6911 - val_accuracy: 0.5262\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5242 - val_loss: 0.6915 - val_accuracy: 0.5219\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6910 - val_accuracy: 0.5265\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5241 - val_loss: 0.6911 - val_accuracy: 0.5230\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5242 - val_loss: 0.6910 - val_accuracy: 0.5230\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5241 - val_loss: 0.6908 - val_accuracy: 0.5284\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5250 - val_loss: 0.6917 - val_accuracy: 0.5160\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5225 - val_loss: 0.6916 - val_accuracy: 0.5249\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_13\n",
      "cannot prune layer q_activation_13\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 5ms/step - loss: 0.6908 - accuracy: 0.5245 - val_loss: 0.6949 - val_accuracy: 0.5133\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5165 - val_loss: 0.6921 - val_accuracy: 0.5166\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6940 - val_accuracy: 0.5112\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5148 - val_loss: 0.6924 - val_accuracy: 0.5111\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6918 - val_accuracy: 0.5177\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5172 - val_loss: 0.6930 - val_accuracy: 0.5144\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6949 - val_accuracy: 0.5132\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5147 - val_loss: 0.6921 - val_accuracy: 0.5181\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5152 - val_loss: 0.6919 - val_accuracy: 0.5179\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5159 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5108\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6920 - val_accuracy: 0.5144\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5181 - val_loss: 0.6920 - val_accuracy: 0.5179\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5184 - val_loss: 0.6920 - val_accuracy: 0.5234\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5191 - val_loss: 0.6918 - val_accuracy: 0.5204\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5186 - val_loss: 0.6917 - val_accuracy: 0.5206\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6920 - val_accuracy: 0.5227\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5179 - val_loss: 0.6923 - val_accuracy: 0.5147\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5198 - val_loss: 0.6916 - val_accuracy: 0.5222\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6918 - val_accuracy: 0.5230\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5199 - val_loss: 0.6915 - val_accuracy: 0.5240\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5226\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5204 - val_loss: 0.6915 - val_accuracy: 0.5242\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5205 - val_loss: 0.6914 - val_accuracy: 0.5215\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5203 - val_loss: 0.6919 - val_accuracy: 0.5235\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5197 - val_loss: 0.6917 - val_accuracy: 0.5186\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5207 - val_loss: 0.6915 - val_accuracy: 0.5222\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5202 - val_loss: 0.6915 - val_accuracy: 0.5238\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5210 - val_loss: 0.6916 - val_accuracy: 0.5220\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5213 - val_loss: 0.6915 - val_accuracy: 0.5238\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5215 - val_loss: 0.6914 - val_accuracy: 0.5210\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5206 - val_loss: 0.6917 - val_accuracy: 0.5202\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5205 - val_loss: 0.6916 - val_accuracy: 0.5219\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5215 - val_loss: 0.6917 - val_accuracy: 0.5251\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5223 - val_loss: 0.6918 - val_accuracy: 0.5214\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5218 - val_loss: 0.6914 - val_accuracy: 0.5247\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5209 - val_loss: 0.6917 - val_accuracy: 0.5224\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5220 - val_loss: 0.6913 - val_accuracy: 0.5227\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5218 - val_loss: 0.6913 - val_accuracy: 0.5283\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5207 - val_loss: 0.6922 - val_accuracy: 0.5203\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5216 - val_loss: 0.6912 - val_accuracy: 0.5227\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5220 - val_loss: 0.6912 - val_accuracy: 0.5259\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5227 - val_loss: 0.6915 - val_accuracy: 0.5245\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6913 - val_accuracy: 0.5236\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6920 - val_accuracy: 0.5199\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6923 - val_accuracy: 0.5198\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5208 - val_loss: 0.6919 - val_accuracy: 0.5180\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5206 - val_loss: 0.6919 - val_accuracy: 0.5201\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5212 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5184\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.51970047]\n",
      " [0.4964528 ]\n",
      " [0.4568876 ]\n",
      " [0.5000882 ]\n",
      " [0.4960817 ]\n",
      " [0.5071995 ]\n",
      " [0.39113945]\n",
      " [0.49285823]\n",
      " [0.48911703]\n",
      " [0.5014577 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_14 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_14 is normal keras bn layer\n",
      "q_activation_14      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.0163 - accuracy: 0.5016 - val_loss: 0.7790 - val_accuracy: 0.5019\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7573 - accuracy: 0.4998 - val_loss: 0.7355 - val_accuracy: 0.5040\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7295 - accuracy: 0.4991 - val_loss: 0.7198 - val_accuracy: 0.5027\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.4982 - val_loss: 0.7101 - val_accuracy: 0.5042\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7110 - accuracy: 0.4996 - val_loss: 0.7058 - val_accuracy: 0.5040\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7079 - accuracy: 0.5007 - val_loss: 0.7021 - val_accuracy: 0.5041\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7057 - accuracy: 0.5004 - val_loss: 0.7148 - val_accuracy: 0.5019\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7039 - accuracy: 0.5006 - val_loss: 0.6995 - val_accuracy: 0.5036\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7023 - accuracy: 0.5001 - val_loss: 0.7110 - val_accuracy: 0.5004\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7008 - accuracy: 0.5009 - val_loss: 0.6969 - val_accuracy: 0.5050\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.5017 - val_loss: 0.6957 - val_accuracy: 0.5032\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5019 - val_loss: 0.6952 - val_accuracy: 0.5052\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5046 - val_loss: 0.6949 - val_accuracy: 0.5062\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6973 - accuracy: 0.5034 - val_loss: 0.6944 - val_accuracy: 0.5082\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6969 - accuracy: 0.5050 - val_loss: 0.6940 - val_accuracy: 0.5104\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6966 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.5145\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5082 - val_loss: 0.6937 - val_accuracy: 0.5090\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6965 - accuracy: 0.5075 - val_loss: 0.7058 - val_accuracy: 0.5018\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5086 - val_loss: 0.6940 - val_accuracy: 0.5101\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5083 - val_loss: 0.6940 - val_accuracy: 0.5098\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.5086 - val_loss: 0.6942 - val_accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5089 - val_loss: 0.6938 - val_accuracy: 0.5090\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5078 - val_loss: 0.6938 - val_accuracy: 0.5088\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5085 - val_loss: 0.6938 - val_accuracy: 0.5109\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5095 - val_loss: 0.6936 - val_accuracy: 0.5119\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5090 - val_loss: 0.6935 - val_accuracy: 0.5124\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5100 - val_loss: 0.6937 - val_accuracy: 0.5128\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.5082 - val_loss: 0.6931 - val_accuracy: 0.5140\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5106 - val_loss: 0.6930 - val_accuracy: 0.5154\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5100 - val_loss: 0.6939 - val_accuracy: 0.5106\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.5150\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5106 - val_loss: 0.6929 - val_accuracy: 0.5156\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5109 - val_loss: 0.7013 - val_accuracy: 0.5055\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5106 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5109 - val_loss: 0.6932 - val_accuracy: 0.5144\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5116 - val_loss: 0.6986 - val_accuracy: 0.5032\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5102 - val_loss: 0.6932 - val_accuracy: 0.5156\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5116 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5104 - val_loss: 0.6931 - val_accuracy: 0.5168\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5119 - val_loss: 0.6938 - val_accuracy: 0.5116\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5110 - val_loss: 0.6975 - val_accuracy: 0.5032\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5108 - val_loss: 0.6937 - val_accuracy: 0.5122\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5112 - val_loss: 0.6967 - val_accuracy: 0.5088\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5111 - val_loss: 0.6956 - val_accuracy: 0.5049\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5098 - val_loss: 0.6952 - val_accuracy: 0.5099\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5111 - val_loss: 0.6949 - val_accuracy: 0.5038\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5111 - val_loss: 0.6939 - val_accuracy: 0.5120\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5103 - val_loss: 0.6948 - val_accuracy: 0.5106\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5113 - val_loss: 0.6939 - val_accuracy: 0.5074\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5103 - val_loss: 0.6935 - val_accuracy: 0.5170\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5122 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5132 - val_loss: 0.6935 - val_accuracy: 0.5119\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_14\n",
      "cannot prune layer q_activation_14\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6944 - accuracy: 0.5100 - val_loss: 0.6944 - val_accuracy: 0.5128\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5098 - val_loss: 0.6946 - val_accuracy: 0.5106\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5080 - val_loss: 0.6941 - val_accuracy: 0.5102\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5076 - val_loss: 0.6939 - val_accuracy: 0.5092\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5075 - val_loss: 0.6960 - val_accuracy: 0.5084\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5075 - val_loss: 0.6959 - val_accuracy: 0.5045\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5073 - val_loss: 0.6960 - val_accuracy: 0.5032\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5073 - val_loss: 0.6950 - val_accuracy: 0.5026\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5079 - val_loss: 0.6970 - val_accuracy: 0.5017\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5072 - val_loss: 0.6940 - val_accuracy: 0.5144\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5091 - val_loss: 0.6941 - val_accuracy: 0.5132\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5098 - val_loss: 0.6950 - val_accuracy: 0.5101\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5105 - val_loss: 0.6933 - val_accuracy: 0.5059\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5102 - val_loss: 0.6932 - val_accuracy: 0.5164\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5108 - val_loss: 0.6931 - val_accuracy: 0.5061\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5109 - val_loss: 0.6930 - val_accuracy: 0.5133\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5106 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5122 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5056\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6929 - val_accuracy: 0.5040\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5133 - val_loss: 0.6935 - val_accuracy: 0.5099\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6934 - val_accuracy: 0.5084\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6929 - val_accuracy: 0.5054\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5133 - val_loss: 0.6929 - val_accuracy: 0.5163\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5080\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5139\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5128 - val_loss: 0.6930 - val_accuracy: 0.5088\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5137 - val_loss: 0.6929 - val_accuracy: 0.5133\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6928 - val_accuracy: 0.5141\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5148\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6929 - val_accuracy: 0.5136\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5166\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6929 - val_accuracy: 0.5115\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6929 - val_accuracy: 0.5149\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5160\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6929 - val_accuracy: 0.5145\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5143 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5143 - val_loss: 0.6930 - val_accuracy: 0.5122\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5125\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6928 - val_accuracy: 0.5142\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5151 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6930 - val_accuracy: 0.5015\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5140 - val_loss: 0.6931 - val_accuracy: 0.5160\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5145 - val_loss: 0.6927 - val_accuracy: 0.5169\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5138 - val_loss: 0.6927 - val_accuracy: 0.5160\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.50682414]\n",
      " [0.50682414]\n",
      " [0.47186506]\n",
      " [0.50682414]\n",
      " [0.46808618]\n",
      " [0.49051717]\n",
      " [0.51527476]\n",
      " [0.5034162 ]\n",
      " [0.49135882]\n",
      " [0.49400228]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_15 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_15 is normal keras bn layer\n",
      "q_activation_15      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.9444 - accuracy: 0.4988 - val_loss: 0.7013 - val_accuracy: 0.4999\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6965 - accuracy: 0.5031 - val_loss: 0.6949 - val_accuracy: 0.5062\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5054 - val_loss: 0.6940 - val_accuracy: 0.5085\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5062 - val_loss: 0.6936 - val_accuracy: 0.5081\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5070 - val_loss: 0.6935 - val_accuracy: 0.5090\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5073 - val_loss: 0.6932 - val_accuracy: 0.5066\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5081 - val_loss: 0.6943 - val_accuracy: 0.5072\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5074 - val_loss: 0.6939 - val_accuracy: 0.5077\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6963 - val_accuracy: 0.5039\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5074 - val_loss: 0.6939 - val_accuracy: 0.5087\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5083 - val_loss: 0.6933 - val_accuracy: 0.5049\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5074 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6951 - val_accuracy: 0.5066\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5088 - val_loss: 0.6938 - val_accuracy: 0.5044\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5083 - val_loss: 0.6935 - val_accuracy: 0.5076\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6934 - val_accuracy: 0.5071\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5084 - val_loss: 0.6928 - val_accuracy: 0.5098\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5074\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5103 - val_loss: 0.6938 - val_accuracy: 0.5079\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5097 - val_loss: 0.6935 - val_accuracy: 0.5022\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5092 - val_loss: 0.6929 - val_accuracy: 0.5086\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6934 - val_accuracy: 0.5087\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5072\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5125\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5103 - val_loss: 0.6941 - val_accuracy: 0.5078\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5116 - val_loss: 0.6933 - val_accuracy: 0.5092\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6937 - val_accuracy: 0.5119\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5120 - val_loss: 0.6937 - val_accuracy: 0.5026\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5122 - val_loss: 0.6929 - val_accuracy: 0.5051\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5116 - val_loss: 0.6929 - val_accuracy: 0.5144\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5119 - val_loss: 0.6928 - val_accuracy: 0.5116\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5132 - val_loss: 0.6925 - val_accuracy: 0.5138\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5130 - val_loss: 0.6932 - val_accuracy: 0.5055\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5118 - val_loss: 0.6931 - val_accuracy: 0.5093\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6925 - val_accuracy: 0.5149\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5125 - val_loss: 0.6926 - val_accuracy: 0.5106\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5122 - val_loss: 0.6925 - val_accuracy: 0.5118\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5144 - val_loss: 0.6929 - val_accuracy: 0.5119\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5131 - val_loss: 0.6925 - val_accuracy: 0.5092\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5131 - val_loss: 0.6925 - val_accuracy: 0.5099\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5143 - val_loss: 0.6926 - val_accuracy: 0.5110\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5138 - val_loss: 0.6926 - val_accuracy: 0.5131\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5139 - val_loss: 0.6923 - val_accuracy: 0.5135\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5131 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5138 - val_loss: 0.6923 - val_accuracy: 0.5127\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5157\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5107\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5125 - val_loss: 0.6926 - val_accuracy: 0.5106\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5134 - val_loss: 0.6923 - val_accuracy: 0.5137\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5142 - val_loss: 0.6922 - val_accuracy: 0.5128\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5144 - val_loss: 0.6924 - val_accuracy: 0.5163\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5133 - val_loss: 0.6924 - val_accuracy: 0.5110\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5153 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5143 - val_loss: 0.6922 - val_accuracy: 0.5127\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5134 - val_loss: 0.6934 - val_accuracy: 0.5137\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5147 - val_loss: 0.6922 - val_accuracy: 0.5136\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5141 - val_loss: 0.6920 - val_accuracy: 0.5156\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5146 - val_loss: 0.6922 - val_accuracy: 0.5123\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5145 - val_loss: 0.6922 - val_accuracy: 0.5160\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5155 - val_loss: 0.6920 - val_accuracy: 0.5096\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5088\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5141 - val_loss: 0.6922 - val_accuracy: 0.5102\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5148 - val_loss: 0.6923 - val_accuracy: 0.5073\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.5131\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5148 - val_loss: 0.6924 - val_accuracy: 0.5154\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5158 - val_loss: 0.6922 - val_accuracy: 0.5096\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5155 - val_loss: 0.6923 - val_accuracy: 0.5142\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5155 - val_loss: 0.6941 - val_accuracy: 0.5087\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5148 - val_loss: 0.6923 - val_accuracy: 0.5108\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5122\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5149 - val_loss: 0.6919 - val_accuracy: 0.5143\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5154 - val_loss: 0.6917 - val_accuracy: 0.5133\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5154 - val_loss: 0.6925 - val_accuracy: 0.5161\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6923 - val_accuracy: 0.5117\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5076\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5149 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5150 - val_loss: 0.6920 - val_accuracy: 0.5111\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6917 - val_accuracy: 0.5154\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5149 - val_loss: 0.6924 - val_accuracy: 0.5122\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5155 - val_loss: 0.6924 - val_accuracy: 0.5154\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5167 - val_loss: 0.6919 - val_accuracy: 0.5140\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5160 - val_loss: 0.6922 - val_accuracy: 0.5146\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5157 - val_loss: 0.6921 - val_accuracy: 0.5165\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5154 - val_loss: 0.6925 - val_accuracy: 0.5075\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6927 - val_accuracy: 0.5078\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5157 - val_loss: 0.6922 - val_accuracy: 0.5116\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6916 - val_accuracy: 0.5138\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5148 - val_loss: 0.6920 - val_accuracy: 0.5131\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6919 - val_accuracy: 0.5143\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5168 - val_loss: 0.6919 - val_accuracy: 0.5144\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5151 - val_loss: 0.6921 - val_accuracy: 0.5118\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5147 - val_loss: 0.6921 - val_accuracy: 0.5151\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5161 - val_loss: 0.6918 - val_accuracy: 0.5166\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5138 - val_loss: 0.6918 - val_accuracy: 0.5139\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5155 - val_loss: 0.6922 - val_accuracy: 0.5107\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5156 - val_loss: 0.6920 - val_accuracy: 0.5117\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5159 - val_loss: 0.6919 - val_accuracy: 0.5130\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5152 - val_loss: 0.6919 - val_accuracy: 0.5178\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5166\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5156 - val_loss: 0.6916 - val_accuracy: 0.5129\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_15\n",
      "cannot prune layer q_activation_15\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6920 - val_accuracy: 0.5151\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6968 - val_accuracy: 0.5088\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5095\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5120 - val_loss: 0.6939 - val_accuracy: 0.5057\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5113 - val_loss: 0.6931 - val_accuracy: 0.5064\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5093 - val_loss: 0.6936 - val_accuracy: 0.5059\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5035\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5099 - val_loss: 0.6928 - val_accuracy: 0.5046\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5097 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5115 - val_loss: 0.6926 - val_accuracy: 0.5065\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5107 - val_loss: 0.6924 - val_accuracy: 0.5092\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5117 - val_loss: 0.6924 - val_accuracy: 0.5091\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5107 - val_loss: 0.6925 - val_accuracy: 0.5082\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5127 - val_loss: 0.6924 - val_accuracy: 0.5092\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5105 - val_loss: 0.6925 - val_accuracy: 0.5102\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5120 - val_loss: 0.6923 - val_accuracy: 0.5076\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5108 - val_loss: 0.6925 - val_accuracy: 0.5100\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5117 - val_loss: 0.6924 - val_accuracy: 0.5111\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5115 - val_loss: 0.6924 - val_accuracy: 0.5106\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5117 - val_loss: 0.6922 - val_accuracy: 0.5098\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5109 - val_loss: 0.6922 - val_accuracy: 0.5127\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5112 - val_loss: 0.6925 - val_accuracy: 0.5096\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5122 - val_loss: 0.6926 - val_accuracy: 0.5070\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5123 - val_loss: 0.6923 - val_accuracy: 0.5086\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5114 - val_loss: 0.6927 - val_accuracy: 0.5107\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5120 - val_loss: 0.6925 - val_accuracy: 0.5104\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5115 - val_loss: 0.6927 - val_accuracy: 0.5119\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5128 - val_loss: 0.6922 - val_accuracy: 0.5110\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5117 - val_loss: 0.6923 - val_accuracy: 0.5077\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5118 - val_loss: 0.6923 - val_accuracy: 0.5125\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5110 - val_loss: 0.6926 - val_accuracy: 0.5066\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5120 - val_loss: 0.6922 - val_accuracy: 0.5089\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5110 - val_loss: 0.6926 - val_accuracy: 0.5113\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5118 - val_loss: 0.6924 - val_accuracy: 0.5098\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5117 - val_loss: 0.6923 - val_accuracy: 0.5109\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5118\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5114 - val_loss: 0.6922 - val_accuracy: 0.5113\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5124 - val_loss: 0.6921 - val_accuracy: 0.5110\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5119 - val_loss: 0.6924 - val_accuracy: 0.5098\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5115 - val_loss: 0.6924 - val_accuracy: 0.5141\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5123 - val_loss: 0.6923 - val_accuracy: 0.5141\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5119 - val_loss: 0.6923 - val_accuracy: 0.5124\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5121 - val_loss: 0.6926 - val_accuracy: 0.5144\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5127 - val_loss: 0.6923 - val_accuracy: 0.5101\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5118 - val_loss: 0.6919 - val_accuracy: 0.5147\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5147 - val_loss: 0.6920 - val_accuracy: 0.5076\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5126 - val_loss: 0.6919 - val_accuracy: 0.5127\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5128 - val_loss: 0.6921 - val_accuracy: 0.5104\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5138 - val_loss: 0.6920 - val_accuracy: 0.5142\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5125\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.50390625]\n",
      " [0.50390625]\n",
      " [0.53320223]\n",
      " [0.50390625]\n",
      " [0.50390625]\n",
      " [0.53320223]\n",
      " [0.47216904]\n",
      " [0.53320223]\n",
      " [0.56602585]\n",
      " [0.50390625]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_16 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_16 is normal keras bn layer\n",
      "q_activation_16      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.1746 - accuracy: 0.4992 - val_loss: 0.7708 - val_accuracy: 0.4999\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7373 - accuracy: 0.5000 - val_loss: 0.7285 - val_accuracy: 0.4961\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7196 - accuracy: 0.4999 - val_loss: 0.7161 - val_accuracy: 0.4947\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7108 - accuracy: 0.5012 - val_loss: 0.7073 - val_accuracy: 0.4963\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7060 - accuracy: 0.5018 - val_loss: 0.7037 - val_accuracy: 0.4972\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7033 - accuracy: 0.5020 - val_loss: 0.7022 - val_accuracy: 0.4975\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7006 - accuracy: 0.5032 - val_loss: 0.6996 - val_accuracy: 0.4986\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6985 - accuracy: 0.5031 - val_loss: 0.6980 - val_accuracy: 0.4987\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5037 - val_loss: 0.6968 - val_accuracy: 0.5010\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5028 - val_loss: 0.6962 - val_accuracy: 0.5021\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.5040 - val_loss: 0.6956 - val_accuracy: 0.5034\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.5048 - val_loss: 0.6953 - val_accuracy: 0.5041\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5050 - val_loss: 0.6945 - val_accuracy: 0.5034\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5060 - val_loss: 0.6943 - val_accuracy: 0.5070\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6957 - accuracy: 0.5061 - val_loss: 0.6966 - val_accuracy: 0.5050\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5070 - val_loss: 0.6937 - val_accuracy: 0.5087\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5076 - val_loss: 0.6935 - val_accuracy: 0.5052\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5079 - val_loss: 0.6937 - val_accuracy: 0.5071\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5087 - val_loss: 0.6935 - val_accuracy: 0.5088\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5086 - val_loss: 0.6939 - val_accuracy: 0.5081\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5095 - val_loss: 0.6937 - val_accuracy: 0.5082\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5098 - val_loss: 0.6932 - val_accuracy: 0.5093\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5101 - val_loss: 0.6930 - val_accuracy: 0.5116\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5112\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5102 - val_loss: 0.6930 - val_accuracy: 0.5037\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6932 - val_accuracy: 0.5131\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5120 - val_loss: 0.6930 - val_accuracy: 0.5137\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5121 - val_loss: 0.6937 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5126 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5124 - val_loss: 0.6930 - val_accuracy: 0.5127\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5123 - val_loss: 0.6929 - val_accuracy: 0.5117\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5129\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6930 - val_accuracy: 0.5103\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5119 - val_loss: 0.6931 - val_accuracy: 0.5101\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5127\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5119\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6933 - val_accuracy: 0.5073\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5131 - val_loss: 0.6928 - val_accuracy: 0.5162\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5143\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5130\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6937 - val_accuracy: 0.5078\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5150 - val_loss: 0.6927 - val_accuracy: 0.5085\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5146 - val_loss: 0.6942 - val_accuracy: 0.4990\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5140\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5141 - val_loss: 0.6931 - val_accuracy: 0.5063\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6932 - val_accuracy: 0.5108\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6935 - val_accuracy: 0.5113\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6927 - val_accuracy: 0.5148\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5154 - val_loss: 0.6932 - val_accuracy: 0.5127\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5154 - val_loss: 0.6927 - val_accuracy: 0.5155\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5157 - val_loss: 0.6930 - val_accuracy: 0.5151\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5160 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6926 - val_accuracy: 0.5147\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5167 - val_loss: 0.6932 - val_accuracy: 0.5096\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5165 - val_loss: 0.6925 - val_accuracy: 0.5158\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6936 - val_accuracy: 0.5129\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5163\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5183 - val_loss: 0.6924 - val_accuracy: 0.5181\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5184 - val_loss: 0.6929 - val_accuracy: 0.5183\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.5173\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5183 - val_loss: 0.6928 - val_accuracy: 0.5160\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5192 - val_loss: 0.6928 - val_accuracy: 0.5145\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5189 - val_loss: 0.6926 - val_accuracy: 0.5150\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5179 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5180 - val_loss: 0.6926 - val_accuracy: 0.5196\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5189 - val_loss: 0.6925 - val_accuracy: 0.5179\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5192 - val_loss: 0.6922 - val_accuracy: 0.5202\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5194 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5194 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5201 - val_loss: 0.6926 - val_accuracy: 0.5193\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5186\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5195 - val_loss: 0.6924 - val_accuracy: 0.5185\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5199 - val_loss: 0.6925 - val_accuracy: 0.5183\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5196 - val_loss: 0.6925 - val_accuracy: 0.5167\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5194 - val_loss: 0.6922 - val_accuracy: 0.5166\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5202 - val_loss: 0.6925 - val_accuracy: 0.5178\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5207 - val_loss: 0.6923 - val_accuracy: 0.5187\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5204 - val_loss: 0.6927 - val_accuracy: 0.5123\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5204 - val_loss: 0.6923 - val_accuracy: 0.5208\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5217 - val_loss: 0.6926 - val_accuracy: 0.5181\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5219 - val_loss: 0.6922 - val_accuracy: 0.5208\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5215 - val_loss: 0.6924 - val_accuracy: 0.5175\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5219 - val_loss: 0.6931 - val_accuracy: 0.5185\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5223 - val_loss: 0.6921 - val_accuracy: 0.5233\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6927 - val_accuracy: 0.5111\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5230 - val_loss: 0.6920 - val_accuracy: 0.5220\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5225 - val_loss: 0.6923 - val_accuracy: 0.5188\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5232 - val_loss: 0.6918 - val_accuracy: 0.5227\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5238 - val_loss: 0.6924 - val_accuracy: 0.5210\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5250 - val_loss: 0.6923 - val_accuracy: 0.5183\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5254 - val_loss: 0.6921 - val_accuracy: 0.5217\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5246 - val_loss: 0.6918 - val_accuracy: 0.5257\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5251 - val_loss: 0.6918 - val_accuracy: 0.5215\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_16\n",
      "cannot prune layer q_activation_16\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6910 - accuracy: 0.5249 - val_loss: 0.6957 - val_accuracy: 0.5032\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5225 - val_loss: 0.7022 - val_accuracy: 0.5092\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5158 - val_loss: 0.6982 - val_accuracy: 0.5029\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5147 - val_loss: 0.6943 - val_accuracy: 0.5184\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5149 - val_loss: 0.6985 - val_accuracy: 0.5113\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6934 - val_accuracy: 0.5092\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6930 - val_accuracy: 0.5146\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5059\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5165\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6922 - val_accuracy: 0.5170\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5161 - val_loss: 0.6922 - val_accuracy: 0.5171\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5160 - val_loss: 0.6922 - val_accuracy: 0.5160\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5162 - val_loss: 0.6920 - val_accuracy: 0.5176\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6921 - val_accuracy: 0.5137\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5173 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5171 - val_loss: 0.6920 - val_accuracy: 0.5181\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5158\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5171 - val_loss: 0.6922 - val_accuracy: 0.5145\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5166 - val_loss: 0.6919 - val_accuracy: 0.5195\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5172 - val_loss: 0.6920 - val_accuracy: 0.5198\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6919 - val_accuracy: 0.5188\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5181 - val_loss: 0.6920 - val_accuracy: 0.5152\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6923 - val_accuracy: 0.5146\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5134 - val_loss: 0.6923 - val_accuracy: 0.5150\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5168\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5170 - val_loss: 0.6919 - val_accuracy: 0.5206\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5173 - val_loss: 0.6919 - val_accuracy: 0.5179\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5190 - val_loss: 0.6920 - val_accuracy: 0.5202\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5174 - val_loss: 0.6919 - val_accuracy: 0.5158\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5180 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.5166\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5177 - val_loss: 0.6918 - val_accuracy: 0.5199\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5184 - val_loss: 0.6918 - val_accuracy: 0.5185\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5179 - val_loss: 0.6919 - val_accuracy: 0.5160\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5186 - val_loss: 0.6919 - val_accuracy: 0.5172\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5181 - val_loss: 0.6919 - val_accuracy: 0.5184\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5194 - val_loss: 0.6919 - val_accuracy: 0.5194\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5173 - val_loss: 0.6920 - val_accuracy: 0.5132\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5181 - val_loss: 0.6918 - val_accuracy: 0.5154\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5186 - val_loss: 0.6918 - val_accuracy: 0.5178\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5175 - val_loss: 0.6921 - val_accuracy: 0.5200\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5183 - val_loss: 0.6920 - val_accuracy: 0.5177\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5179 - val_loss: 0.6918 - val_accuracy: 0.5206\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5183 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5187 - val_loss: 0.6918 - val_accuracy: 0.5196\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5187 - val_loss: 0.6919 - val_accuracy: 0.5174\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5186 - val_loss: 0.6917 - val_accuracy: 0.5202\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5191 - val_loss: 0.6919 - val_accuracy: 0.5212\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.4917773 ]\n",
      " [0.52100885]\n",
      " [0.45864356]\n",
      " [0.5146806 ]\n",
      " [0.4999932 ]\n",
      " [0.53594697]\n",
      " [0.459651  ]\n",
      " [0.52412677]\n",
      " [0.51268566]\n",
      " [0.5242003 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_17 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_17 is normal keras bn layer\n",
      "q_activation_17      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 2.4865 - accuracy: 0.4992 - val_loss: 1.9730 - val_accuracy: 0.5026\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 1.7705 - accuracy: 0.4990 - val_loss: 1.4339 - val_accuracy: 0.5010\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 1.2635 - accuracy: 0.4995 - val_loss: 1.0687 - val_accuracy: 0.4991\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.9246 - accuracy: 0.4984 - val_loss: 0.7760 - val_accuracy: 0.4984\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7572 - accuracy: 0.4976 - val_loss: 0.7412 - val_accuracy: 0.4998\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7351 - accuracy: 0.4986 - val_loss: 0.7292 - val_accuracy: 0.5007\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7251 - accuracy: 0.4989 - val_loss: 0.7190 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7194 - accuracy: 0.4984 - val_loss: 0.7149 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7147 - accuracy: 0.5001 - val_loss: 0.7122 - val_accuracy: 0.5040\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7110 - accuracy: 0.5006 - val_loss: 0.7076 - val_accuracy: 0.5049\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7075 - accuracy: 0.5005 - val_loss: 0.7052 - val_accuracy: 0.5057\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7050 - accuracy: 0.5018 - val_loss: 0.7028 - val_accuracy: 0.5069\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7031 - accuracy: 0.5015 - val_loss: 0.7010 - val_accuracy: 0.5068\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7014 - accuracy: 0.5022 - val_loss: 0.6996 - val_accuracy: 0.5069\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6999 - accuracy: 0.5022 - val_loss: 0.6984 - val_accuracy: 0.5075\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.5035 - val_loss: 0.6974 - val_accuracy: 0.5065\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5043 - val_loss: 0.6965 - val_accuracy: 0.5046\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5046 - val_loss: 0.6957 - val_accuracy: 0.5050\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5045 - val_loss: 0.6951 - val_accuracy: 0.5057\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5050 - val_loss: 0.6947 - val_accuracy: 0.5061\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5056 - val_loss: 0.6945 - val_accuracy: 0.5081\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5061 - val_loss: 0.6941 - val_accuracy: 0.5101\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5065 - val_loss: 0.6940 - val_accuracy: 0.5096\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5072\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5073 - val_loss: 0.6939 - val_accuracy: 0.5075\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5078 - val_loss: 0.6938 - val_accuracy: 0.5089\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5074 - val_loss: 0.6936 - val_accuracy: 0.5091\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5103\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5077 - val_loss: 0.6963 - val_accuracy: 0.5049\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5069 - val_loss: 0.6932 - val_accuracy: 0.5106\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5078 - val_loss: 0.6933 - val_accuracy: 0.5077\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5078\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5090 - val_loss: 0.6932 - val_accuracy: 0.5089\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5093 - val_loss: 0.6932 - val_accuracy: 0.5087\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6932 - val_accuracy: 0.5085\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5081 - val_loss: 0.6933 - val_accuracy: 0.5051\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5087\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5100 - val_loss: 0.6930 - val_accuracy: 0.5097\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5097 - val_loss: 0.6930 - val_accuracy: 0.5089\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5076\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5104 - val_loss: 0.6930 - val_accuracy: 0.5097\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6929 - val_accuracy: 0.5113\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5094\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.5054\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5099 - val_loss: 0.6929 - val_accuracy: 0.5108\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5104\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5112 - val_loss: 0.6930 - val_accuracy: 0.5077\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5138\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5107\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5116\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6929 - val_accuracy: 0.5117\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5115 - val_loss: 0.6928 - val_accuracy: 0.5107\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6932 - val_accuracy: 0.5105\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5096\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5114 - val_loss: 0.6929 - val_accuracy: 0.5103\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6928 - val_accuracy: 0.5130\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5115 - val_loss: 0.6930 - val_accuracy: 0.5112\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5115\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5116 - val_loss: 0.6930 - val_accuracy: 0.5048\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5129 - val_loss: 0.6935 - val_accuracy: 0.5100\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.5032\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6952 - val_accuracy: 0.5118\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5122 - val_loss: 0.6926 - val_accuracy: 0.5134\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5115 - val_loss: 0.6956 - val_accuracy: 0.5058\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5134 - val_loss: 0.6928 - val_accuracy: 0.5136\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5134\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5123\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5131\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6929 - val_accuracy: 0.5154\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6928 - val_accuracy: 0.5168\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5103\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5092\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5150\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6927 - val_accuracy: 0.5151\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6927 - val_accuracy: 0.5164\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6929 - val_accuracy: 0.5137\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6928 - val_accuracy: 0.5148\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6927 - val_accuracy: 0.5165\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5147 - val_loss: 0.6927 - val_accuracy: 0.5132\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_17\n",
      "cannot prune layer q_activation_17\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6952 - val_accuracy: 0.5089\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5108 - val_loss: 0.6955 - val_accuracy: 0.4998\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6994 - val_accuracy: 0.5049\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5041 - val_loss: 0.6958 - val_accuracy: 0.5033\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5065 - val_loss: 0.7036 - val_accuracy: 0.5002\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5062 - val_loss: 0.6946 - val_accuracy: 0.5038\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5072 - val_loss: 0.6935 - val_accuracy: 0.5083\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5081 - val_loss: 0.7023 - val_accuracy: 0.5069\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5070 - val_loss: 0.6946 - val_accuracy: 0.5097\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.5089\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5093 - val_loss: 0.6939 - val_accuracy: 0.5092\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5085 - val_loss: 0.6985 - val_accuracy: 0.5038\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5056\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6931 - val_accuracy: 0.5122\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5085\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5092 - val_loss: 0.6931 - val_accuracy: 0.5103\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6932 - val_accuracy: 0.5038\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5087 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5093\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6932 - val_accuracy: 0.5065\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5096\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5093 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5090 - val_loss: 0.6931 - val_accuracy: 0.5086\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6930 - val_accuracy: 0.5088\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6931 - val_accuracy: 0.5088\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6930 - val_accuracy: 0.5071\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5098 - val_loss: 0.6930 - val_accuracy: 0.5151\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5105 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5109 - val_loss: 0.6929 - val_accuracy: 0.5112\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5109 - val_loss: 0.6930 - val_accuracy: 0.5092\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5112\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5087\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5104 - val_loss: 0.6931 - val_accuracy: 0.5066\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5122\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5105 - val_loss: 0.6930 - val_accuracy: 0.5092\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.5061\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5105\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.5093\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5118\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5087\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6931 - val_accuracy: 0.5118\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5122\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5101 - val_loss: 0.6931 - val_accuracy: 0.5117\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5105\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5095 - val_loss: 0.6933 - val_accuracy: 0.5069\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5098 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5103\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5006437 ]\n",
      " [0.49495178]\n",
      " [0.52547264]\n",
      " [0.5082039 ]\n",
      " [0.50066847]\n",
      " [0.50189793]\n",
      " [0.51101667]\n",
      " [0.50817853]\n",
      " [0.50585014]\n",
      " [0.50777453]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_18 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_19 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_18 is normal keras bn layer\n",
      "q_activation_18      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_19 is normal keras bn layer\n",
      "q_activation_19      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.8867 - accuracy: 0.5018 - val_loss: 0.6983 - val_accuracy: 0.5028\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5043 - val_loss: 0.6950 - val_accuracy: 0.5038\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5049 - val_loss: 0.6942 - val_accuracy: 0.5041\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5063 - val_loss: 0.6938 - val_accuracy: 0.5056\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5074 - val_loss: 0.6936 - val_accuracy: 0.5060\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5081 - val_loss: 0.6964 - val_accuracy: 0.4993\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5061 - val_loss: 0.6933 - val_accuracy: 0.5061\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5082 - val_loss: 0.6932 - val_accuracy: 0.5086\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5065\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5074 - val_loss: 0.6936 - val_accuracy: 0.5090\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6928 - val_accuracy: 0.5061\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5132\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5052\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5123 - val_loss: 0.6937 - val_accuracy: 0.5082\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5109 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5134 - val_loss: 0.6921 - val_accuracy: 0.5150\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5139 - val_loss: 0.6922 - val_accuracy: 0.5066\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6926 - val_accuracy: 0.5074\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.5117\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5154 - val_loss: 0.6919 - val_accuracy: 0.5108\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5149 - val_loss: 0.6933 - val_accuracy: 0.5113\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5103\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5162 - val_loss: 0.6919 - val_accuracy: 0.5152\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5160 - val_loss: 0.6928 - val_accuracy: 0.5154\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5144 - val_loss: 0.7145 - val_accuracy: 0.5076\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5123 - val_loss: 0.6937 - val_accuracy: 0.5092\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6949 - accuracy: 0.5111 - val_loss: 0.6941 - val_accuracy: 0.5100\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5122 - val_loss: 0.6918 - val_accuracy: 0.5163\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5152 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5150 - val_loss: 0.6926 - val_accuracy: 0.5134\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5195 - val_loss: 0.6907 - val_accuracy: 0.5188\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5212 - val_loss: 0.6937 - val_accuracy: 0.5103\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5268 - val_loss: 0.6924 - val_accuracy: 0.5152\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5291 - val_loss: 0.6897 - val_accuracy: 0.5276\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5338 - val_loss: 0.6916 - val_accuracy: 0.5275\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5362 - val_loss: 0.6886 - val_accuracy: 0.5244\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5414 - val_loss: 0.6896 - val_accuracy: 0.5278\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5438 - val_loss: 0.6859 - val_accuracy: 0.5319\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5462 - val_loss: 0.6822 - val_accuracy: 0.5367\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5499 - val_loss: 0.6794 - val_accuracy: 0.5458\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5522 - val_loss: 0.6773 - val_accuracy: 0.5552\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5518 - val_loss: 0.6793 - val_accuracy: 0.5492\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6772 - accuracy: 0.5551 - val_loss: 0.6824 - val_accuracy: 0.5428\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5567 - val_loss: 0.6745 - val_accuracy: 0.5620\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6756 - accuracy: 0.5559 - val_loss: 0.6717 - val_accuracy: 0.5579\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5596 - val_loss: 0.7153 - val_accuracy: 0.5163\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6746 - accuracy: 0.5585 - val_loss: 0.6913 - val_accuracy: 0.5354\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5599 - val_loss: 0.6786 - val_accuracy: 0.5449\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5628 - val_loss: 0.6664 - val_accuracy: 0.5719\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5634 - val_loss: 0.6812 - val_accuracy: 0.5441\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5153 - val_loss: 0.6950 - val_accuracy: 0.5044\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5079 - val_loss: 0.6931 - val_accuracy: 0.5044\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5087 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6923 - val_accuracy: 0.5124\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5140 - val_loss: 0.6921 - val_accuracy: 0.5106\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5187 - val_loss: 0.6911 - val_accuracy: 0.5223\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5261 - val_loss: 0.7003 - val_accuracy: 0.5019\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5397 - val_loss: 0.6828 - val_accuracy: 0.5429\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.5468 - val_loss: 0.6772 - val_accuracy: 0.5531\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5521 - val_loss: 0.6827 - val_accuracy: 0.5443\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5572 - val_loss: 0.6956 - val_accuracy: 0.5234\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5601 - val_loss: 0.6728 - val_accuracy: 0.5605\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5632 - val_loss: 0.6708 - val_accuracy: 0.5600\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5647 - val_loss: 0.6849 - val_accuracy: 0.5571\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.5657 - val_loss: 0.6751 - val_accuracy: 0.5526\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6701 - accuracy: 0.5679 - val_loss: 0.6703 - val_accuracy: 0.5617\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5692 - val_loss: 0.6846 - val_accuracy: 0.5471\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5702 - val_loss: 0.6729 - val_accuracy: 0.5579\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5690 - val_loss: 0.6632 - val_accuracy: 0.5739\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5669 - val_loss: 0.6697 - val_accuracy: 0.5614\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6754 - accuracy: 0.5652 - val_loss: 0.6714 - val_accuracy: 0.5617\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5678 - val_loss: 0.6614 - val_accuracy: 0.5786\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5632 - val_loss: 0.6954 - val_accuracy: 0.5217\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6784 - accuracy: 0.5610 - val_loss: 0.6668 - val_accuracy: 0.5635\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5645 - val_loss: 0.6656 - val_accuracy: 0.5664\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5653 - val_loss: 0.6735 - val_accuracy: 0.5559\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5626 - val_loss: 0.7422 - val_accuracy: 0.5288\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5633 - val_loss: 0.6835 - val_accuracy: 0.5482\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5631 - val_loss: 0.7150 - val_accuracy: 0.5587\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6820 - accuracy: 0.5601 - val_loss: 0.6773 - val_accuracy: 0.5482\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6817 - accuracy: 0.5593 - val_loss: 0.6831 - val_accuracy: 0.5508\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5581 - val_loss: 0.7100 - val_accuracy: 0.5596\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5588 - val_loss: 0.6777 - val_accuracy: 0.5431\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5572 - val_loss: 0.6897 - val_accuracy: 0.5724\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5571 - val_loss: 0.6785 - val_accuracy: 0.5499\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5567 - val_loss: 0.6832 - val_accuracy: 0.5249\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5536 - val_loss: 0.6828 - val_accuracy: 0.5745\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5524 - val_loss: 0.6728 - val_accuracy: 0.5829\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.6999 - val_accuracy: 0.5230\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.5545 - val_loss: 0.6849 - val_accuracy: 0.5283\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5538 - val_loss: 0.7461 - val_accuracy: 0.5179\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5519 - val_loss: 0.6938 - val_accuracy: 0.5089\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_18\n",
      "cannot prune layer q_activation_18\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_19\n",
      "cannot prune layer q_activation_19\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6738 - accuracy: 0.5697 - val_loss: 0.6872 - val_accuracy: 0.5482\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6759 - accuracy: 0.5691 - val_loss: 0.7100 - val_accuracy: 0.5277\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5636 - val_loss: 0.6686 - val_accuracy: 0.5690\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6783 - accuracy: 0.5654 - val_loss: 0.7089 - val_accuracy: 0.5249\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5651 - val_loss: 0.7059 - val_accuracy: 0.5147\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6817 - accuracy: 0.5603 - val_loss: 0.7063 - val_accuracy: 0.5634\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5359 - val_loss: 0.6966 - val_accuracy: 0.4979\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7006 - accuracy: 0.5105 - val_loss: 0.6920 - val_accuracy: 0.5093\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5333 - val_loss: 0.6824 - val_accuracy: 0.5450\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5459 - val_loss: 0.6929 - val_accuracy: 0.5197\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5494 - val_loss: 0.6790 - val_accuracy: 0.5391\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5540 - val_loss: 0.6732 - val_accuracy: 0.5662\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6835 - accuracy: 0.5581 - val_loss: 0.6865 - val_accuracy: 0.5353\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6849 - accuracy: 0.5555 - val_loss: 0.6847 - val_accuracy: 0.5366\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5553 - val_loss: 0.6875 - val_accuracy: 0.5309\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5543 - val_loss: 0.7998 - val_accuracy: 0.4987\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5531 - val_loss: 0.6949 - val_accuracy: 0.5305\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5512 - val_loss: 0.6827 - val_accuracy: 0.5709\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6864 - accuracy: 0.5487 - val_loss: 0.6907 - val_accuracy: 0.5643\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6857 - accuracy: 0.5490 - val_loss: 0.6729 - val_accuracy: 0.5526\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5470 - val_loss: 0.6798 - val_accuracy: 0.5753\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5475 - val_loss: 0.6846 - val_accuracy: 0.5663\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5458 - val_loss: 0.6814 - val_accuracy: 0.5194\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5503 - val_loss: 0.6875 - val_accuracy: 0.5205\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6832 - accuracy: 0.5498 - val_loss: 0.6810 - val_accuracy: 0.5690\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6806 - accuracy: 0.5537 - val_loss: 0.6901 - val_accuracy: 0.5039\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5502 - val_loss: 0.7257 - val_accuracy: 0.5011\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5490 - val_loss: 0.6785 - val_accuracy: 0.5632\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.5507 - val_loss: 0.6777 - val_accuracy: 0.5682\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5463 - val_loss: 0.6700 - val_accuracy: 0.5804\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6797 - accuracy: 0.5546 - val_loss: 0.7329 - val_accuracy: 0.5200\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5554 - val_loss: 0.6896 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6808 - accuracy: 0.5540 - val_loss: 0.6761 - val_accuracy: 0.5691\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6785 - accuracy: 0.5577 - val_loss: 0.6753 - val_accuracy: 0.5673\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5598 - val_loss: 0.6699 - val_accuracy: 0.5765\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6770 - accuracy: 0.5595 - val_loss: 0.6659 - val_accuracy: 0.5786\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5586 - val_loss: 0.6676 - val_accuracy: 0.5777\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6765 - accuracy: 0.5614 - val_loss: 0.6726 - val_accuracy: 0.5734\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6802 - accuracy: 0.5576 - val_loss: 0.7201 - val_accuracy: 0.4987\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7114 - accuracy: 0.5000 - val_loss: 0.7145 - val_accuracy: 0.5013\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7087 - accuracy: 0.5002 - val_loss: 0.7092 - val_accuracy: 0.4997\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7081 - accuracy: 0.4996 - val_loss: 0.7097 - val_accuracy: 0.5001\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7082 - accuracy: 0.4995 - val_loss: 0.7057 - val_accuracy: 0.5001\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7077 - accuracy: 0.5016 - val_loss: 0.7044 - val_accuracy: 0.5001\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7073 - accuracy: 0.5017 - val_loss: 0.7179 - val_accuracy: 0.5039\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7059 - accuracy: 0.5037 - val_loss: 0.6989 - val_accuracy: 0.5003\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7031 - accuracy: 0.5076 - val_loss: 0.7567 - val_accuracy: 0.5022\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5299 - val_loss: 0.7437 - val_accuracy: 0.5118\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5440 - val_loss: 0.6995 - val_accuracy: 0.5036\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5542 - val_loss: 0.7425 - val_accuracy: 0.5431\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.63197905]\n",
      " [0.79255724]\n",
      " [0.5752537 ]\n",
      " [0.77751184]\n",
      " [0.67966664]\n",
      " [0.1668436 ]\n",
      " [0.54612315]\n",
      " [0.3950644 ]\n",
      " [0.7070758 ]\n",
      " [0.3002965 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_20 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_21 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_20 is normal keras bn layer\n",
      "q_activation_20      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_21 is normal keras bn layer\n",
      "q_activation_21      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 3.5777 - accuracy: 0.5000 - val_loss: 2.3057 - val_accuracy: 0.5028\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.9821 - accuracy: 0.5017 - val_loss: 1.5704 - val_accuracy: 0.5009\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0179 - accuracy: 0.5017 - val_loss: 0.7444 - val_accuracy: 0.5022\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7350 - accuracy: 0.5019 - val_loss: 0.7309 - val_accuracy: 0.5050\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7296 - accuracy: 0.5021 - val_loss: 0.7539 - val_accuracy: 0.5038\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7284 - accuracy: 0.5012 - val_loss: 0.7139 - val_accuracy: 0.5042\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7118 - accuracy: 0.5025 - val_loss: 0.7083 - val_accuracy: 0.5060\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7076 - accuracy: 0.5025 - val_loss: 0.7056 - val_accuracy: 0.5058\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7049 - accuracy: 0.5035 - val_loss: 0.7032 - val_accuracy: 0.5056\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7030 - accuracy: 0.5033 - val_loss: 0.7014 - val_accuracy: 0.5069\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7015 - accuracy: 0.5034 - val_loss: 0.7001 - val_accuracy: 0.5065\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7001 - accuracy: 0.5043 - val_loss: 0.6989 - val_accuracy: 0.5069\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5046 - val_loss: 0.6982 - val_accuracy: 0.5063\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6981 - accuracy: 0.5047 - val_loss: 0.6976 - val_accuracy: 0.5049\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5038 - val_loss: 0.6970 - val_accuracy: 0.5052\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5039 - val_loss: 0.6965 - val_accuracy: 0.5044\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5042 - val_loss: 0.6961 - val_accuracy: 0.5048\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5038 - val_loss: 0.6957 - val_accuracy: 0.5039\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5038 - val_loss: 0.6953 - val_accuracy: 0.5042\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5047 - val_loss: 0.6951 - val_accuracy: 0.5046\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5056 - val_loss: 0.6948 - val_accuracy: 0.5056\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5048 - val_loss: 0.6943 - val_accuracy: 0.5066\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5068 - val_loss: 0.6946 - val_accuracy: 0.5066\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5062 - val_loss: 0.6941 - val_accuracy: 0.5051\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6942 - val_accuracy: 0.5034\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5068 - val_loss: 0.6938 - val_accuracy: 0.5063\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5075 - val_loss: 0.6939 - val_accuracy: 0.5038\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5079 - val_loss: 0.6938 - val_accuracy: 0.5053\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6935 - val_accuracy: 0.5101\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5092 - val_loss: 0.6930 - val_accuracy: 0.5117\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5107\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5103 - val_loss: 0.6933 - val_accuracy: 0.5077\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6930 - val_accuracy: 0.5111\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5110 - val_loss: 0.6929 - val_accuracy: 0.5136\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6926 - val_accuracy: 0.5114\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5135 - val_loss: 0.6923 - val_accuracy: 0.5148\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5159 - val_loss: 0.6922 - val_accuracy: 0.5141\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5173 - val_loss: 0.6921 - val_accuracy: 0.5141\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5197 - val_loss: 0.6917 - val_accuracy: 0.5218\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5216 - val_loss: 0.6919 - val_accuracy: 0.5121\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5254 - val_loss: 0.6915 - val_accuracy: 0.5177\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5281 - val_loss: 0.6900 - val_accuracy: 0.5263\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5311 - val_loss: 0.6900 - val_accuracy: 0.5315\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5334 - val_loss: 0.6885 - val_accuracy: 0.5314\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5366 - val_loss: 0.6879 - val_accuracy: 0.5350\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5392 - val_loss: 0.6864 - val_accuracy: 0.5337\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5430 - val_loss: 0.6849 - val_accuracy: 0.5421\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5468 - val_loss: 0.6867 - val_accuracy: 0.5348\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5488 - val_loss: 0.6815 - val_accuracy: 0.5536\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5517 - val_loss: 0.6801 - val_accuracy: 0.5546\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5560 - val_loss: 0.6813 - val_accuracy: 0.5440\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5477 - val_loss: 0.6810 - val_accuracy: 0.5520\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5583 - val_loss: 0.6941 - val_accuracy: 0.5418\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6766 - accuracy: 0.5594 - val_loss: 0.6783 - val_accuracy: 0.5617\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5568 - val_loss: 0.6793 - val_accuracy: 0.5504\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5647 - val_loss: 0.6721 - val_accuracy: 0.5683\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5689 - val_loss: 0.6734 - val_accuracy: 0.5677\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5673 - val_loss: 0.6808 - val_accuracy: 0.5550\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5680 - val_loss: 0.6719 - val_accuracy: 0.5664\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5639 - val_loss: 0.6705 - val_accuracy: 0.5747\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5363 - val_loss: 0.6873 - val_accuracy: 0.5334\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5555 - val_loss: 0.6893 - val_accuracy: 0.5307\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6762 - accuracy: 0.5576 - val_loss: 0.6782 - val_accuracy: 0.5559\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.5671 - val_loss: 0.6699 - val_accuracy: 0.5683\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5688 - val_loss: 0.6673 - val_accuracy: 0.5703\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6715 - accuracy: 0.5689 - val_loss: 0.6672 - val_accuracy: 0.5767\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5747 - val_loss: 0.6641 - val_accuracy: 0.5831\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6670 - accuracy: 0.5773 - val_loss: 0.6727 - val_accuracy: 0.5671\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5692 - val_loss: 0.6786 - val_accuracy: 0.5607\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6691 - accuracy: 0.5775 - val_loss: 0.6641 - val_accuracy: 0.5860\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5690 - val_loss: 0.6733 - val_accuracy: 0.5680\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.5723 - val_loss: 0.6677 - val_accuracy: 0.5744\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5818 - val_loss: 0.6688 - val_accuracy: 0.5663\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5738 - val_loss: 0.6678 - val_accuracy: 0.5750\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6658 - accuracy: 0.5810 - val_loss: 0.6641 - val_accuracy: 0.5817\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6805 - accuracy: 0.5591 - val_loss: 0.7090 - val_accuracy: 0.5084\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5091 - val_loss: 0.6969 - val_accuracy: 0.5125\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5138 - val_loss: 0.6947 - val_accuracy: 0.5129\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5166 - val_loss: 0.6931 - val_accuracy: 0.5206\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5195 - val_loss: 0.6920 - val_accuracy: 0.5216\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5242 - val_loss: 0.6905 - val_accuracy: 0.5276\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5317 - val_loss: 0.6883 - val_accuracy: 0.5348\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5399 - val_loss: 0.6864 - val_accuracy: 0.5382\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5516 - val_loss: 0.6863 - val_accuracy: 0.5433\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5617 - val_loss: 0.6746 - val_accuracy: 0.5687\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.5673 - val_loss: 0.6732 - val_accuracy: 0.5742\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6809 - accuracy: 0.5533 - val_loss: 0.6897 - val_accuracy: 0.5311\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_20\n",
      "cannot prune layer q_activation_20\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_21\n",
      "cannot prune layer q_activation_21\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6680 - accuracy: 0.5767 - val_loss: 0.6649 - val_accuracy: 0.5763\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6675 - accuracy: 0.5755 - val_loss: 0.6656 - val_accuracy: 0.5777\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6712 - accuracy: 0.5689 - val_loss: 0.6651 - val_accuracy: 0.5853\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6785 - accuracy: 0.5554 - val_loss: 0.6776 - val_accuracy: 0.5607\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5678 - val_loss: 0.6649 - val_accuracy: 0.5800\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6676 - accuracy: 0.5736 - val_loss: 0.6633 - val_accuracy: 0.5796\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5653 - val_loss: 0.6691 - val_accuracy: 0.5731\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6660 - accuracy: 0.5782 - val_loss: 0.6638 - val_accuracy: 0.5781\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6648 - accuracy: 0.5805 - val_loss: 0.6616 - val_accuracy: 0.5888\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6723 - accuracy: 0.5677 - val_loss: 0.6693 - val_accuracy: 0.5716\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5767 - val_loss: 0.6636 - val_accuracy: 0.5807\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6639 - accuracy: 0.5808 - val_loss: 0.6614 - val_accuracy: 0.5863\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7000 - accuracy: 0.5097 - val_loss: 0.6945 - val_accuracy: 0.5067\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.6923 - val_accuracy: 0.5156\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5163 - val_loss: 0.6903 - val_accuracy: 0.5205\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6857 - accuracy: 0.5383 - val_loss: 0.6817 - val_accuracy: 0.5495\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5615 - val_loss: 0.6725 - val_accuracy: 0.5699\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6681 - accuracy: 0.5750 - val_loss: 0.6655 - val_accuracy: 0.5781\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5729 - val_loss: 0.6664 - val_accuracy: 0.5732\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5563 - val_loss: 0.7067 - val_accuracy: 0.5104\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6961 - accuracy: 0.5133 - val_loss: 0.6935 - val_accuracy: 0.5168\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5236 - val_loss: 0.6901 - val_accuracy: 0.5280\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6887 - accuracy: 0.5323 - val_loss: 0.6867 - val_accuracy: 0.5382\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5398 - val_loss: 0.6838 - val_accuracy: 0.5446\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5444 - val_loss: 0.6816 - val_accuracy: 0.5493\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6811 - accuracy: 0.5500 - val_loss: 0.6783 - val_accuracy: 0.5574\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5556 - val_loss: 0.6771 - val_accuracy: 0.5553\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6750 - accuracy: 0.5617 - val_loss: 0.6752 - val_accuracy: 0.5623\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.5632 - val_loss: 0.6712 - val_accuracy: 0.5673\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6735 - accuracy: 0.5636 - val_loss: 0.6676 - val_accuracy: 0.5717\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5728 - val_loss: 0.6660 - val_accuracy: 0.5781\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5764 - val_loss: 0.6691 - val_accuracy: 0.5697\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.5658 - val_loss: 0.6764 - val_accuracy: 0.5622\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6655 - accuracy: 0.5780 - val_loss: 0.6621 - val_accuracy: 0.5806\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6658 - accuracy: 0.5775 - val_loss: 0.7363 - val_accuracy: 0.5333\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6651 - accuracy: 0.5766 - val_loss: 0.6971 - val_accuracy: 0.5531\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6636 - accuracy: 0.5798 - val_loss: 0.6736 - val_accuracy: 0.5597\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6623 - accuracy: 0.5802 - val_loss: 0.6615 - val_accuracy: 0.5839\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6659 - accuracy: 0.5744 - val_loss: 0.6608 - val_accuracy: 0.5811\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6640 - accuracy: 0.5783 - val_loss: 0.6560 - val_accuracy: 0.5896\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7014 - accuracy: 0.5065 - val_loss: 0.6955 - val_accuracy: 0.5027\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5120 - val_loss: 0.6921 - val_accuracy: 0.5163\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5190 - val_loss: 0.6906 - val_accuracy: 0.5241\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5246 - val_loss: 0.6886 - val_accuracy: 0.5318\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5347 - val_loss: 0.6856 - val_accuracy: 0.5399\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5495 - val_loss: 0.6752 - val_accuracy: 0.5569\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6707 - accuracy: 0.5679 - val_loss: 0.6661 - val_accuracy: 0.5778\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6827 - accuracy: 0.5475 - val_loss: 0.6821 - val_accuracy: 0.5440\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5579 - val_loss: 0.6703 - val_accuracy: 0.5679\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.5749 - val_loss: 0.6639 - val_accuracy: 0.5773\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.48813772]\n",
      " [0.34591693]\n",
      " [0.5266944 ]\n",
      " [0.5017159 ]\n",
      " [0.60312015]\n",
      " [0.4395824 ]\n",
      " [0.24618518]\n",
      " [0.17970991]\n",
      " [0.53319013]\n",
      " [0.62421435]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6638756990432739\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_22 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_23 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_22 is normal keras bn layer\n",
      "q_activation_22      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_23 is normal keras bn layer\n",
      "q_activation_23      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 4.9261 - accuracy: 0.4993 - val_loss: 3.8541 - val_accuracy: 0.5021\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 2.5156 - accuracy: 0.5035 - val_loss: 1.9565 - val_accuracy: 0.5074\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.3146 - accuracy: 0.5025 - val_loss: 1.0048 - val_accuracy: 0.5055\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9340 - accuracy: 0.5012 - val_loss: 0.8473 - val_accuracy: 0.5040\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7695 - accuracy: 0.5008 - val_loss: 0.7258 - val_accuracy: 0.5025\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7190 - accuracy: 0.5011 - val_loss: 0.7114 - val_accuracy: 0.5030\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7095 - accuracy: 0.5018 - val_loss: 0.7058 - val_accuracy: 0.5043\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7048 - accuracy: 0.5015 - val_loss: 0.7026 - val_accuracy: 0.5044\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7018 - accuracy: 0.5030 - val_loss: 0.7005 - val_accuracy: 0.5036\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7000 - accuracy: 0.5026 - val_loss: 0.6988 - val_accuracy: 0.5020\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6984 - accuracy: 0.5032 - val_loss: 0.6976 - val_accuracy: 0.5009\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6971 - accuracy: 0.5039 - val_loss: 0.6967 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5038 - val_loss: 0.6959 - val_accuracy: 0.5015\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5047 - val_loss: 0.6953 - val_accuracy: 0.5022\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5052 - val_loss: 0.6947 - val_accuracy: 0.5040\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5053 - val_loss: 0.6941 - val_accuracy: 0.5049\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5051 - val_loss: 0.6942 - val_accuracy: 0.5059\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5066 - val_loss: 0.6935 - val_accuracy: 0.5062\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5078 - val_loss: 0.6947 - val_accuracy: 0.5038\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5076 - val_loss: 0.6932 - val_accuracy: 0.5071\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6931 - val_accuracy: 0.5075\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5089 - val_loss: 0.6930 - val_accuracy: 0.5080\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5102 - val_loss: 0.6929 - val_accuracy: 0.5083\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5114 - val_loss: 0.6928 - val_accuracy: 0.5103\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5122 - val_loss: 0.6927 - val_accuracy: 0.5094\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5126 - val_loss: 0.6927 - val_accuracy: 0.5132\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6924 - val_accuracy: 0.5112\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5138 - val_loss: 0.6922 - val_accuracy: 0.5154\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5145 - val_loss: 0.6922 - val_accuracy: 0.5146\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5168 - val_loss: 0.6919 - val_accuracy: 0.5198\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5192 - val_loss: 0.6920 - val_accuracy: 0.5187\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5211 - val_loss: 0.6918 - val_accuracy: 0.5187\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6912 - val_accuracy: 0.5240\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5274 - val_loss: 0.6907 - val_accuracy: 0.5277\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5296 - val_loss: 0.6904 - val_accuracy: 0.5277\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5321 - val_loss: 0.6893 - val_accuracy: 0.5343\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5339 - val_loss: 0.6888 - val_accuracy: 0.5333\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5357 - val_loss: 0.6886 - val_accuracy: 0.5387\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5372 - val_loss: 0.6877 - val_accuracy: 0.5361\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5390 - val_loss: 0.6870 - val_accuracy: 0.5423\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5400 - val_loss: 0.6883 - val_accuracy: 0.5383\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5421 - val_loss: 0.6862 - val_accuracy: 0.5453\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5429 - val_loss: 0.6853 - val_accuracy: 0.5467\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5448 - val_loss: 0.6859 - val_accuracy: 0.5438\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5478 - val_loss: 0.6848 - val_accuracy: 0.5467\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5493 - val_loss: 0.6834 - val_accuracy: 0.5477\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5452 - val_loss: 0.6917 - val_accuracy: 0.5259\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5447 - val_loss: 0.6839 - val_accuracy: 0.5500\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6821 - accuracy: 0.5521 - val_loss: 0.6823 - val_accuracy: 0.5534\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6814 - accuracy: 0.5539 - val_loss: 0.6819 - val_accuracy: 0.5508\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.5545 - val_loss: 0.6827 - val_accuracy: 0.5515\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.5565 - val_loss: 0.6811 - val_accuracy: 0.5533\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6801 - accuracy: 0.5566 - val_loss: 0.6798 - val_accuracy: 0.5592\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5588 - val_loss: 0.6818 - val_accuracy: 0.5548\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5581 - val_loss: 0.6788 - val_accuracy: 0.5572\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6790 - accuracy: 0.5581 - val_loss: 0.6795 - val_accuracy: 0.5568\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6807 - accuracy: 0.5542 - val_loss: 0.6804 - val_accuracy: 0.5545\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6801 - accuracy: 0.5555 - val_loss: 0.6801 - val_accuracy: 0.5516\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6791 - accuracy: 0.5589 - val_loss: 0.6818 - val_accuracy: 0.5537\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5622 - val_loss: 0.6797 - val_accuracy: 0.5633\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5637 - val_loss: 0.6767 - val_accuracy: 0.5597\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6782 - accuracy: 0.5613 - val_loss: 0.6775 - val_accuracy: 0.5594\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6801 - accuracy: 0.5590 - val_loss: 0.6784 - val_accuracy: 0.5600\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5636 - val_loss: 0.6764 - val_accuracy: 0.5604\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6803 - accuracy: 0.5579 - val_loss: 0.7072 - val_accuracy: 0.5094\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5285 - val_loss: 0.6859 - val_accuracy: 0.5435\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5507 - val_loss: 0.6809 - val_accuracy: 0.5517\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6800 - accuracy: 0.5564 - val_loss: 0.6813 - val_accuracy: 0.5558\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6771 - accuracy: 0.5621 - val_loss: 0.6778 - val_accuracy: 0.5594\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6762 - accuracy: 0.5657 - val_loss: 0.6751 - val_accuracy: 0.5637\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5408 - val_loss: 0.6911 - val_accuracy: 0.5295\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5446 - val_loss: 0.6824 - val_accuracy: 0.5519\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.5548 - val_loss: 0.6794 - val_accuracy: 0.5602\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5614 - val_loss: 0.6757 - val_accuracy: 0.5696\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5648 - val_loss: 0.6743 - val_accuracy: 0.5698\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6744 - accuracy: 0.5690 - val_loss: 0.6760 - val_accuracy: 0.5652\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5700 - val_loss: 0.6761 - val_accuracy: 0.5702\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.5709 - val_loss: 0.6752 - val_accuracy: 0.5679\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5689 - val_loss: 0.6819 - val_accuracy: 0.5619\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6728 - accuracy: 0.5713 - val_loss: 0.6709 - val_accuracy: 0.5707\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5711 - val_loss: 0.6709 - val_accuracy: 0.5718\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5611 - val_loss: 0.6966 - val_accuracy: 0.5143\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5406 - val_loss: 0.6807 - val_accuracy: 0.5539\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5580 - val_loss: 0.6846 - val_accuracy: 0.5515\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6756 - accuracy: 0.5644 - val_loss: 0.6742 - val_accuracy: 0.5708\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.5687 - val_loss: 0.6855 - val_accuracy: 0.5558\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5713 - val_loss: 0.6852 - val_accuracy: 0.5575\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5748 - val_loss: 0.6743 - val_accuracy: 0.5682\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6744 - accuracy: 0.5693 - val_loss: 0.6861 - val_accuracy: 0.5459\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.5704 - val_loss: 0.6747 - val_accuracy: 0.5672\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5784 - val_loss: 0.6681 - val_accuracy: 0.5745\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6681 - accuracy: 0.5791 - val_loss: 0.6695 - val_accuracy: 0.5676\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5806 - val_loss: 0.6690 - val_accuracy: 0.5794\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6672 - accuracy: 0.5813 - val_loss: 0.6677 - val_accuracy: 0.5783\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6667 - accuracy: 0.5821 - val_loss: 0.6658 - val_accuracy: 0.5826\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5785 - val_loss: 0.6636 - val_accuracy: 0.5836\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5798 - val_loss: 0.6668 - val_accuracy: 0.5827\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6665 - accuracy: 0.5822 - val_loss: 0.7046 - val_accuracy: 0.5499\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6695 - accuracy: 0.5784 - val_loss: 0.6655 - val_accuracy: 0.5779\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6655 - accuracy: 0.5840 - val_loss: 0.6654 - val_accuracy: 0.5751\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_22\n",
      "cannot prune layer q_activation_22\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_23\n",
      "cannot prune layer q_activation_23\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6647 - accuracy: 0.5852 - val_loss: 0.6709 - val_accuracy: 0.5783\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6657 - accuracy: 0.5829 - val_loss: 0.6674 - val_accuracy: 0.5813\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6665 - accuracy: 0.5812 - val_loss: 0.6644 - val_accuracy: 0.5840\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6646 - accuracy: 0.5846 - val_loss: 0.6652 - val_accuracy: 0.5739\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5846 - val_loss: 0.6636 - val_accuracy: 0.5856\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6661 - accuracy: 0.5825 - val_loss: 0.6671 - val_accuracy: 0.5834\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5783 - val_loss: 0.6606 - val_accuracy: 0.5924\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.5766 - val_loss: 0.6906 - val_accuracy: 0.5283\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6795 - accuracy: 0.5560 - val_loss: 0.6733 - val_accuracy: 0.5700\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5734 - val_loss: 0.6649 - val_accuracy: 0.5843\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6665 - accuracy: 0.5809 - val_loss: 0.6630 - val_accuracy: 0.5832\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.5832 - val_loss: 0.6621 - val_accuracy: 0.5817\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6640 - accuracy: 0.5851 - val_loss: 0.6598 - val_accuracy: 0.5907\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6614 - accuracy: 0.5893 - val_loss: 0.6575 - val_accuracy: 0.5938\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6619 - accuracy: 0.5893 - val_loss: 0.6582 - val_accuracy: 0.5945\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6623 - accuracy: 0.5880 - val_loss: 0.6603 - val_accuracy: 0.5853\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5824 - val_loss: 0.6833 - val_accuracy: 0.5545\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5839 - val_loss: 0.6756 - val_accuracy: 0.5776\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6612 - accuracy: 0.5901 - val_loss: 0.6584 - val_accuracy: 0.5932\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6796 - accuracy: 0.5723 - val_loss: 0.7221 - val_accuracy: 0.5205\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7010 - accuracy: 0.5302 - val_loss: 0.6931 - val_accuracy: 0.5389\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5410 - val_loss: 0.6860 - val_accuracy: 0.5430\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5457 - val_loss: 0.6828 - val_accuracy: 0.5500\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6820 - accuracy: 0.5519 - val_loss: 0.6803 - val_accuracy: 0.5545\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6785 - accuracy: 0.5580 - val_loss: 0.6825 - val_accuracy: 0.5467\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5641 - val_loss: 0.6714 - val_accuracy: 0.5690\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6719 - accuracy: 0.5680 - val_loss: 0.6686 - val_accuracy: 0.5754\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6871 - accuracy: 0.5433 - val_loss: 0.6825 - val_accuracy: 0.5525\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6751 - accuracy: 0.5635 - val_loss: 0.6709 - val_accuracy: 0.5724\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6693 - accuracy: 0.5736 - val_loss: 0.6652 - val_accuracy: 0.5807\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5806 - val_loss: 0.6615 - val_accuracy: 0.5860\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6771 - accuracy: 0.5616 - val_loss: 0.6706 - val_accuracy: 0.5704\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6634 - accuracy: 0.5823 - val_loss: 0.6617 - val_accuracy: 0.5848\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6629 - accuracy: 0.5831 - val_loss: 0.6681 - val_accuracy: 0.5663\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6637 - accuracy: 0.5829 - val_loss: 0.6707 - val_accuracy: 0.5526\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5827 - val_loss: 0.6687 - val_accuracy: 0.5747\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6617 - accuracy: 0.5856 - val_loss: 0.6686 - val_accuracy: 0.5797\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6610 - accuracy: 0.5879 - val_loss: 0.8326 - val_accuracy: 0.5579\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6688 - accuracy: 0.5761 - val_loss: 0.6758 - val_accuracy: 0.5756\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6597 - accuracy: 0.5890 - val_loss: 0.6609 - val_accuracy: 0.5819\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5894 - val_loss: 0.6568 - val_accuracy: 0.5887\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6599 - accuracy: 0.5900 - val_loss: 0.6594 - val_accuracy: 0.5865\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6583 - accuracy: 0.5910 - val_loss: 0.6616 - val_accuracy: 0.5838\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5849 - val_loss: 0.7171 - val_accuracy: 0.5312\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5455 - val_loss: 0.6749 - val_accuracy: 0.5596\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6679 - accuracy: 0.5731 - val_loss: 0.6629 - val_accuracy: 0.5825\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.5913 - val_loss: 0.6922 - val_accuracy: 0.5434\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6600 - accuracy: 0.5887 - val_loss: 0.6578 - val_accuracy: 0.5940\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6567 - accuracy: 0.5935 - val_loss: 0.6552 - val_accuracy: 0.5977\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6553 - accuracy: 0.5955 - val_loss: 0.6586 - val_accuracy: 0.5873\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.49830568]\n",
      " [0.42971623]\n",
      " [0.24524224]\n",
      " [0.42855072]\n",
      " [0.17766309]\n",
      " [0.62018496]\n",
      " [0.4458571 ]\n",
      " [0.3874336 ]\n",
      " [0.5136663 ]\n",
      " [0.63322836]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6586488485336304\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_24 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_25 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_24 is normal keras bn layer\n",
      "q_activation_24      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_25 is normal keras bn layer\n",
      "q_activation_25      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.8254 - accuracy: 0.5009 - val_loss: 0.6999 - val_accuracy: 0.5020\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5040 - val_loss: 0.6964 - val_accuracy: 0.5084\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5053 - val_loss: 0.6949 - val_accuracy: 0.5070\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5069 - val_loss: 0.6939 - val_accuracy: 0.5040\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5077 - val_loss: 0.6937 - val_accuracy: 0.5070\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5079 - val_loss: 0.6935 - val_accuracy: 0.5081\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6933 - val_accuracy: 0.5047\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5100 - val_loss: 0.6936 - val_accuracy: 0.5066\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5098 - val_loss: 0.6930 - val_accuracy: 0.5090\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5095 - val_loss: 0.6934 - val_accuracy: 0.5091\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5126 - val_loss: 0.6933 - val_accuracy: 0.5114\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5123 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.5131\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5162 - val_loss: 0.6984 - val_accuracy: 0.4993\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5205 - val_loss: 0.6938 - val_accuracy: 0.5030\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5215 - val_loss: 0.6934 - val_accuracy: 0.5049\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5249 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5271 - val_loss: 0.6887 - val_accuracy: 0.5363\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5350 - val_loss: 0.6981 - val_accuracy: 0.5047\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5398 - val_loss: 0.6875 - val_accuracy: 0.5321\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5423 - val_loss: 0.6982 - val_accuracy: 0.5142\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6827 - accuracy: 0.5484 - val_loss: 0.6818 - val_accuracy: 0.5494\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5503 - val_loss: 0.6817 - val_accuracy: 0.5466\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.5512 - val_loss: 0.6787 - val_accuracy: 0.5500\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5564 - val_loss: 0.6771 - val_accuracy: 0.5545\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5602 - val_loss: 0.6850 - val_accuracy: 0.5478\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.5600 - val_loss: 0.6889 - val_accuracy: 0.5345\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5650 - val_loss: 0.6704 - val_accuracy: 0.5685\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5671 - val_loss: 0.6720 - val_accuracy: 0.5662\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6718 - accuracy: 0.5668 - val_loss: 0.6666 - val_accuracy: 0.5734\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6705 - accuracy: 0.5684 - val_loss: 0.6697 - val_accuracy: 0.5686\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6699 - accuracy: 0.5686 - val_loss: 0.6963 - val_accuracy: 0.5438\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5724 - val_loss: 0.6612 - val_accuracy: 0.5804\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6733 - accuracy: 0.5634 - val_loss: 0.6730 - val_accuracy: 0.5586\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6676 - accuracy: 0.5727 - val_loss: 0.6626 - val_accuracy: 0.5825\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6671 - accuracy: 0.5721 - val_loss: 0.6665 - val_accuracy: 0.5715\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5727 - val_loss: 0.6614 - val_accuracy: 0.5772\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5719 - val_loss: 0.6554 - val_accuracy: 0.5858\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5758 - val_loss: 0.6609 - val_accuracy: 0.5762\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5757 - val_loss: 0.6564 - val_accuracy: 0.5879\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5763 - val_loss: 0.6624 - val_accuracy: 0.5756\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6613 - accuracy: 0.5792 - val_loss: 0.6537 - val_accuracy: 0.5893\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5797 - val_loss: 0.6717 - val_accuracy: 0.5698\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5854 - val_loss: 0.6592 - val_accuracy: 0.5836\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6618 - accuracy: 0.5825 - val_loss: 0.6593 - val_accuracy: 0.5833\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.5849 - val_loss: 0.6558 - val_accuracy: 0.5873\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6598 - accuracy: 0.5841 - val_loss: 0.6672 - val_accuracy: 0.5709\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6577 - accuracy: 0.5870 - val_loss: 0.6618 - val_accuracy: 0.5808\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6581 - accuracy: 0.5844 - val_loss: 0.6608 - val_accuracy: 0.5774\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6576 - accuracy: 0.5858 - val_loss: 0.6603 - val_accuracy: 0.5801\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6586 - accuracy: 0.5831 - val_loss: 0.6506 - val_accuracy: 0.5935\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6574 - accuracy: 0.5856 - val_loss: 0.6527 - val_accuracy: 0.5927\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6694 - accuracy: 0.5632 - val_loss: 0.6958 - val_accuracy: 0.5087\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5130 - val_loss: 0.6919 - val_accuracy: 0.5064\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.5614 - val_loss: 0.6764 - val_accuracy: 0.5564\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5795 - val_loss: 0.6691 - val_accuracy: 0.5659\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6630 - accuracy: 0.5774 - val_loss: 0.6622 - val_accuracy: 0.5796\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6585 - accuracy: 0.5851 - val_loss: 0.6679 - val_accuracy: 0.5809\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6656 - accuracy: 0.5756 - val_loss: 0.6630 - val_accuracy: 0.5715\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.5841 - val_loss: 0.6613 - val_accuracy: 0.5832\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6620 - accuracy: 0.5794 - val_loss: 0.6834 - val_accuracy: 0.5485\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6602 - accuracy: 0.5827 - val_loss: 0.6629 - val_accuracy: 0.5759\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.5877 - val_loss: 0.6905 - val_accuracy: 0.5442\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5160 - val_loss: 0.6939 - val_accuracy: 0.5014\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5038 - val_loss: 0.6934 - val_accuracy: 0.5023\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5053 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5061 - val_loss: 0.6930 - val_accuracy: 0.5049\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5075 - val_loss: 0.6928 - val_accuracy: 0.5049\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5095 - val_loss: 0.6925 - val_accuracy: 0.5116\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5133 - val_loss: 0.6920 - val_accuracy: 0.5131\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5180 - val_loss: 0.6906 - val_accuracy: 0.5223\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_24\n",
      "cannot prune layer q_activation_24\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_25\n",
      "cannot prune layer q_activation_25\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 8s 7ms/step - loss: 0.6614 - accuracy: 0.5798 - val_loss: 0.6780 - val_accuracy: 0.5599\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6579 - accuracy: 0.5847 - val_loss: 0.6621 - val_accuracy: 0.5859\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6592 - accuracy: 0.5839 - val_loss: 0.7932 - val_accuracy: 0.5372\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.5917 - val_loss: 0.6586 - val_accuracy: 0.5851\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6544 - accuracy: 0.5908 - val_loss: 0.6594 - val_accuracy: 0.5834\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6652 - accuracy: 0.5727 - val_loss: 0.6951 - val_accuracy: 0.5016\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5065 - val_loss: 0.6925 - val_accuracy: 0.5126\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5127 - val_loss: 0.6919 - val_accuracy: 0.5149\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5210 - val_loss: 0.6911 - val_accuracy: 0.5137\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5377 - val_loss: 0.6832 - val_accuracy: 0.5404\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6805 - accuracy: 0.5497 - val_loss: 0.7125 - val_accuracy: 0.5134\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5554 - val_loss: 0.6736 - val_accuracy: 0.5588\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6697 - accuracy: 0.5669 - val_loss: 0.6665 - val_accuracy: 0.5640\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6627 - accuracy: 0.5772 - val_loss: 0.6666 - val_accuracy: 0.5608\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6630 - accuracy: 0.5763 - val_loss: 0.6630 - val_accuracy: 0.5708\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6594 - accuracy: 0.5821 - val_loss: 0.6563 - val_accuracy: 0.5828\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6576 - accuracy: 0.5872 - val_loss: 0.6640 - val_accuracy: 0.5755\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6565 - accuracy: 0.5875 - val_loss: 0.6557 - val_accuracy: 0.5899\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6571 - accuracy: 0.5859 - val_loss: 0.6510 - val_accuracy: 0.5928\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6649 - accuracy: 0.5860 - val_loss: 0.6752 - val_accuracy: 0.5804\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5849 - val_loss: 0.6621 - val_accuracy: 0.5839\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6551 - accuracy: 0.5925 - val_loss: 0.6535 - val_accuracy: 0.5942\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.5917 - val_loss: 0.6641 - val_accuracy: 0.5722\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.5933 - val_loss: 0.6557 - val_accuracy: 0.5885\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.5922 - val_loss: 0.6652 - val_accuracy: 0.5793\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6529 - accuracy: 0.5946 - val_loss: 0.6481 - val_accuracy: 0.6037\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6567 - accuracy: 0.5898 - val_loss: 0.6861 - val_accuracy: 0.5559\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6527 - accuracy: 0.5944 - val_loss: 0.6485 - val_accuracy: 0.6016\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.5964 - val_loss: 0.6542 - val_accuracy: 0.5891\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5715 - val_loss: 0.6628 - val_accuracy: 0.5818\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5841 - val_loss: 0.6525 - val_accuracy: 0.5967\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6553 - accuracy: 0.5903 - val_loss: 0.6564 - val_accuracy: 0.5832\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6535 - accuracy: 0.5934 - val_loss: 0.6585 - val_accuracy: 0.5876\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.5965 - val_loss: 0.6533 - val_accuracy: 0.5958\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6520 - accuracy: 0.5949 - val_loss: 0.6491 - val_accuracy: 0.5941\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.5942 - val_loss: 0.6487 - val_accuracy: 0.5974\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.5966 - val_loss: 0.6518 - val_accuracy: 0.5885\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6506 - accuracy: 0.5963 - val_loss: 0.6425 - val_accuracy: 0.6062\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6573 - accuracy: 0.5868 - val_loss: 0.7050 - val_accuracy: 0.5035\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5203 - val_loss: 0.6887 - val_accuracy: 0.5218\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5530 - val_loss: 0.7050 - val_accuracy: 0.5197\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5517 - val_loss: 0.6954 - val_accuracy: 0.5118\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6882 - accuracy: 0.5286 - val_loss: 0.6830 - val_accuracy: 0.5438\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6716 - accuracy: 0.5704 - val_loss: 0.6656 - val_accuracy: 0.5798\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6625 - accuracy: 0.5814 - val_loss: 0.6647 - val_accuracy: 0.5727\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6589 - accuracy: 0.5856 - val_loss: 0.6890 - val_accuracy: 0.5522\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6648 - accuracy: 0.5803 - val_loss: 0.6645 - val_accuracy: 0.5745\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6660 - accuracy: 0.5870 - val_loss: 0.6504 - val_accuracy: 0.6015\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.5945 - val_loss: 0.6518 - val_accuracy: 0.5921\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.5925 - val_loss: 0.6513 - val_accuracy: 0.6010\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.43460578]\n",
      " [0.5799792 ]\n",
      " [0.51660883]\n",
      " [0.55524623]\n",
      " [0.1607517 ]\n",
      " [0.568764  ]\n",
      " [0.31825298]\n",
      " [0.4139826 ]\n",
      " [0.51660883]\n",
      " [0.6069512 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_26 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_27 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_26 is normal keras bn layer\n",
      "q_activation_26      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_27 is normal keras bn layer\n",
      "q_activation_27      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 1.4694 - accuracy: 0.5016 - val_loss: 0.7300 - val_accuracy: 0.5049\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7169 - accuracy: 0.5030 - val_loss: 0.7076 - val_accuracy: 0.5065\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7049 - accuracy: 0.5039 - val_loss: 0.7011 - val_accuracy: 0.5090\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7002 - accuracy: 0.5051 - val_loss: 0.6984 - val_accuracy: 0.5102\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6978 - accuracy: 0.5057 - val_loss: 0.6968 - val_accuracy: 0.5095\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5070 - val_loss: 0.6960 - val_accuracy: 0.5095\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5082 - val_loss: 0.6953 - val_accuracy: 0.5087\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5097 - val_loss: 0.6946 - val_accuracy: 0.5123\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5100 - val_loss: 0.6946 - val_accuracy: 0.5084\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5111 - val_loss: 0.6940 - val_accuracy: 0.5097\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5123 - val_loss: 0.6935 - val_accuracy: 0.5131\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5133 - val_loss: 0.6933 - val_accuracy: 0.5148\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5137 - val_loss: 0.6933 - val_accuracy: 0.5141\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5121 - val_loss: 0.6933 - val_accuracy: 0.5135\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5136 - val_loss: 0.6930 - val_accuracy: 0.5174\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5139 - val_loss: 0.6928 - val_accuracy: 0.5108\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5141 - val_loss: 0.6926 - val_accuracy: 0.5157\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5142 - val_loss: 0.6931 - val_accuracy: 0.5169\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5144 - val_loss: 0.6927 - val_accuracy: 0.5116\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6927 - val_accuracy: 0.5169\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5150 - val_loss: 0.6922 - val_accuracy: 0.5178\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6925 - val_accuracy: 0.5162\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5169 - val_loss: 0.6921 - val_accuracy: 0.5137\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5165 - val_loss: 0.6916 - val_accuracy: 0.5187\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5196 - val_loss: 0.6918 - val_accuracy: 0.5158\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5213 - val_loss: 0.6913 - val_accuracy: 0.5186\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6900 - val_accuracy: 0.5297\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5291 - val_loss: 0.6893 - val_accuracy: 0.5313\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5338 - val_loss: 0.6885 - val_accuracy: 0.5350\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6863 - val_accuracy: 0.5413\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5421 - val_loss: 0.6857 - val_accuracy: 0.5421\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5464 - val_loss: 0.6828 - val_accuracy: 0.5502\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6826 - accuracy: 0.5506 - val_loss: 0.6872 - val_accuracy: 0.5383\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5569 - val_loss: 0.6783 - val_accuracy: 0.5597\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5610 - val_loss: 0.6777 - val_accuracy: 0.5611\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5645 - val_loss: 0.6742 - val_accuracy: 0.5611\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6732 - accuracy: 0.5695 - val_loss: 0.6697 - val_accuracy: 0.5760\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5719 - val_loss: 0.6701 - val_accuracy: 0.5772\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6704 - accuracy: 0.5746 - val_loss: 0.6696 - val_accuracy: 0.5782\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5769 - val_loss: 0.6712 - val_accuracy: 0.5673\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.5811 - val_loss: 0.6654 - val_accuracy: 0.5862\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5829 - val_loss: 0.6716 - val_accuracy: 0.5751\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6660 - accuracy: 0.5832 - val_loss: 0.6744 - val_accuracy: 0.5663\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5851 - val_loss: 0.6584 - val_accuracy: 0.5948\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6699 - accuracy: 0.5775 - val_loss: 0.7021 - val_accuracy: 0.5186\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6728 - accuracy: 0.5706 - val_loss: 0.6655 - val_accuracy: 0.5890\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5820 - val_loss: 0.6643 - val_accuracy: 0.5824\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6643 - accuracy: 0.5888 - val_loss: 0.6582 - val_accuracy: 0.5958\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5732 - val_loss: 0.6710 - val_accuracy: 0.5771\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6696 - accuracy: 0.5807 - val_loss: 0.6642 - val_accuracy: 0.5860\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5902 - val_loss: 0.6575 - val_accuracy: 0.6029\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6607 - accuracy: 0.5939 - val_loss: 0.6594 - val_accuracy: 0.5969\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5946 - val_loss: 0.6576 - val_accuracy: 0.5991\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5967 - val_loss: 0.6666 - val_accuracy: 0.5865\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6602 - accuracy: 0.5962 - val_loss: 0.6594 - val_accuracy: 0.5993\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.5817 - val_loss: 0.6969 - val_accuracy: 0.5386\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6790 - accuracy: 0.5600 - val_loss: 0.6676 - val_accuracy: 0.5853\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6606 - accuracy: 0.5937 - val_loss: 0.7432 - val_accuracy: 0.5738\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6611 - accuracy: 0.5969 - val_loss: 0.6544 - val_accuracy: 0.6043\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.5959 - val_loss: 0.6659 - val_accuracy: 0.6065\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5990 - val_loss: 0.6565 - val_accuracy: 0.5998\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6566 - accuracy: 0.6013 - val_loss: 0.6513 - val_accuracy: 0.6110\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6563 - accuracy: 0.6030 - val_loss: 0.6514 - val_accuracy: 0.6107\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6578 - accuracy: 0.6037 - val_loss: 0.6526 - val_accuracy: 0.6078\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6723 - accuracy: 0.5792 - val_loss: 0.6629 - val_accuracy: 0.5828\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6072 - val_loss: 0.6475 - val_accuracy: 0.6185\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.6007 - val_loss: 0.6566 - val_accuracy: 0.6021\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6099 - val_loss: 0.6483 - val_accuracy: 0.6166\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6511 - accuracy: 0.6110 - val_loss: 0.6502 - val_accuracy: 0.6129\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6510 - accuracy: 0.6108 - val_loss: 0.6463 - val_accuracy: 0.6225\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6517 - accuracy: 0.6128 - val_loss: 0.6445 - val_accuracy: 0.6223\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6094 - val_loss: 0.6463 - val_accuracy: 0.6177\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6084 - val_loss: 0.6473 - val_accuracy: 0.6210\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6120 - val_loss: 0.6446 - val_accuracy: 0.6243\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6121 - val_loss: 0.6460 - val_accuracy: 0.6176\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6557 - accuracy: 0.6052 - val_loss: 0.6489 - val_accuracy: 0.6148\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5717 - val_loss: 0.6688 - val_accuracy: 0.5833\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5989 - val_loss: 0.6511 - val_accuracy: 0.6141\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6098 - val_loss: 0.6443 - val_accuracy: 0.6222\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6480 - accuracy: 0.6158 - val_loss: 0.6541 - val_accuracy: 0.6052\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6463 - accuracy: 0.6167 - val_loss: 0.6447 - val_accuracy: 0.6193\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6478 - accuracy: 0.6159 - val_loss: 0.6550 - val_accuracy: 0.6059\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5828 - val_loss: 0.6738 - val_accuracy: 0.5714\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5746 - val_loss: 0.6685 - val_accuracy: 0.5816\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5817 - val_loss: 0.6640 - val_accuracy: 0.5872\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6644 - accuracy: 0.5886 - val_loss: 0.6567 - val_accuracy: 0.5997\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6607 - accuracy: 0.5958 - val_loss: 0.6531 - val_accuracy: 0.6065\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6007 - val_loss: 0.6502 - val_accuracy: 0.6133\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6549 - accuracy: 0.6060 - val_loss: 0.6484 - val_accuracy: 0.6121\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6089 - val_loss: 0.6436 - val_accuracy: 0.6236\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6499 - accuracy: 0.6136 - val_loss: 0.6489 - val_accuracy: 0.6173\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6477 - accuracy: 0.6166 - val_loss: 0.6399 - val_accuracy: 0.6265\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6466 - accuracy: 0.6181 - val_loss: 0.6634 - val_accuracy: 0.6000\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6461 - accuracy: 0.6189 - val_loss: 0.6425 - val_accuracy: 0.6236\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6453 - accuracy: 0.6196 - val_loss: 0.6435 - val_accuracy: 0.6224\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6451 - accuracy: 0.6194 - val_loss: 0.6424 - val_accuracy: 0.6239\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6452 - accuracy: 0.6198 - val_loss: 0.6399 - val_accuracy: 0.6273\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6446 - accuracy: 0.6205 - val_loss: 0.6519 - val_accuracy: 0.6115\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6442 - accuracy: 0.6202 - val_loss: 0.6389 - val_accuracy: 0.6280\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_26\n",
      "cannot prune layer q_activation_26\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_27\n",
      "cannot prune layer q_activation_27\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 6ms/step - loss: 0.6467 - accuracy: 0.6184 - val_loss: 0.6481 - val_accuracy: 0.6135\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6456 - accuracy: 0.6180 - val_loss: 0.6644 - val_accuracy: 0.5973\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6459 - accuracy: 0.6197 - val_loss: 0.6593 - val_accuracy: 0.5989\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5271 - val_loss: 0.6972 - val_accuracy: 0.5088\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6971 - accuracy: 0.5043 - val_loss: 0.6957 - val_accuracy: 0.5077\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6954 - accuracy: 0.5043 - val_loss: 0.7022 - val_accuracy: 0.5002\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5034 - val_loss: 0.6948 - val_accuracy: 0.5045\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5039 - val_loss: 0.6946 - val_accuracy: 0.5033\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5046 - val_loss: 0.6939 - val_accuracy: 0.5065\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5049 - val_loss: 0.6935 - val_accuracy: 0.5067\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5065 - val_loss: 0.6934 - val_accuracy: 0.5049\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6935 - accuracy: 0.5075 - val_loss: 0.6935 - val_accuracy: 0.5018\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5092 - val_loss: 0.6931 - val_accuracy: 0.5037\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5111 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5147 - val_loss: 0.6924 - val_accuracy: 0.5197\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5178 - val_loss: 0.6922 - val_accuracy: 0.5194\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6903 - val_accuracy: 0.5278\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6876 - accuracy: 0.5376 - val_loss: 0.6845 - val_accuracy: 0.5498\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6811 - accuracy: 0.5590 - val_loss: 0.6793 - val_accuracy: 0.5623\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5685 - val_loss: 0.6738 - val_accuracy: 0.5785\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5779 - val_loss: 0.6839 - val_accuracy: 0.5603\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6704 - accuracy: 0.5824 - val_loss: 0.6677 - val_accuracy: 0.5880\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.5934 - val_loss: 0.6604 - val_accuracy: 0.6032\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6629 - accuracy: 0.5972 - val_loss: 0.6567 - val_accuracy: 0.6080\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6616 - accuracy: 0.5999 - val_loss: 0.6574 - val_accuracy: 0.6081\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6588 - accuracy: 0.6025 - val_loss: 0.6600 - val_accuracy: 0.6049\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6570 - accuracy: 0.6057 - val_loss: 0.6525 - val_accuracy: 0.6132\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.6081 - val_loss: 0.6506 - val_accuracy: 0.6171\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6595 - accuracy: 0.6035 - val_loss: 0.6563 - val_accuracy: 0.6050\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.6015 - val_loss: 0.7518 - val_accuracy: 0.5189\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7002 - accuracy: 0.5196 - val_loss: 0.6933 - val_accuracy: 0.5254\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5387 - val_loss: 0.6852 - val_accuracy: 0.5530\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6785 - accuracy: 0.5669 - val_loss: 0.6733 - val_accuracy: 0.5824\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6671 - accuracy: 0.5902 - val_loss: 0.6596 - val_accuracy: 0.6055\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6027 - val_loss: 0.6520 - val_accuracy: 0.6120\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6562 - accuracy: 0.6081 - val_loss: 0.6517 - val_accuracy: 0.6134\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6617 - accuracy: 0.5997 - val_loss: 0.6682 - val_accuracy: 0.5859\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.6063 - val_loss: 0.6511 - val_accuracy: 0.6167\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6054 - val_loss: 0.6641 - val_accuracy: 0.5858\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.6107 - val_loss: 0.6545 - val_accuracy: 0.6111\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6558 - accuracy: 0.6080 - val_loss: 0.6522 - val_accuracy: 0.6160\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6107 - val_loss: 0.6502 - val_accuracy: 0.6098\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6112 - val_loss: 0.6454 - val_accuracy: 0.6238\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6103 - val_loss: 0.6566 - val_accuracy: 0.6122\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6104 - val_loss: 0.6470 - val_accuracy: 0.6203\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6557 - accuracy: 0.6075 - val_loss: 0.6604 - val_accuracy: 0.6052\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6702 - accuracy: 0.5844 - val_loss: 0.6658 - val_accuracy: 0.5969\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6069 - val_loss: 0.6455 - val_accuracy: 0.6175\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6129 - val_loss: 0.6429 - val_accuracy: 0.6239\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5769 - val_loss: 0.6749 - val_accuracy: 0.5786\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.56838036]\n",
      " [0.5467338 ]\n",
      " [0.39709756]\n",
      " [0.52838206]\n",
      " [0.52136457]\n",
      " [0.52010477]\n",
      " [0.511523  ]\n",
      " [0.49453235]\n",
      " [0.518509  ]\n",
      " [0.5416345 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_28 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_29 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_28 is normal keras bn layer\n",
      "q_activation_28      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_29 is normal keras bn layer\n",
      "q_activation_29      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.9524 - accuracy: 0.4985 - val_loss: 0.7330 - val_accuracy: 0.5033\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7177 - accuracy: 0.5018 - val_loss: 0.7100 - val_accuracy: 0.5058\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7078 - accuracy: 0.5029 - val_loss: 0.7057 - val_accuracy: 0.5061\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7038 - accuracy: 0.5031 - val_loss: 0.7023 - val_accuracy: 0.5056\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7014 - accuracy: 0.5027 - val_loss: 0.7006 - val_accuracy: 0.5033\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6991 - accuracy: 0.5029 - val_loss: 0.6986 - val_accuracy: 0.5033\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5036 - val_loss: 0.6972 - val_accuracy: 0.5038\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5040 - val_loss: 0.6962 - val_accuracy: 0.5039\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5046 - val_loss: 0.6952 - val_accuracy: 0.5057\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5054 - val_loss: 0.6947 - val_accuracy: 0.5052\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5063\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5069 - val_loss: 0.6938 - val_accuracy: 0.5090\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5088 - val_loss: 0.6937 - val_accuracy: 0.5081\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5093 - val_loss: 0.6938 - val_accuracy: 0.5102\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5088 - val_loss: 0.6935 - val_accuracy: 0.5100\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.5077\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5099 - val_loss: 0.6934 - val_accuracy: 0.5091\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5104 - val_loss: 0.6932 - val_accuracy: 0.5112\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6931 - val_accuracy: 0.5117\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5108 - val_loss: 0.6931 - val_accuracy: 0.5117\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.5116\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5151\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5128 - val_loss: 0.6928 - val_accuracy: 0.5132\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6927 - val_accuracy: 0.5141\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5145 - val_loss: 0.6927 - val_accuracy: 0.5138\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5145 - val_loss: 0.6927 - val_accuracy: 0.5135\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5123\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5146 - val_loss: 0.6926 - val_accuracy: 0.5148\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5139\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5161 - val_loss: 0.6924 - val_accuracy: 0.5147\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6923 - val_accuracy: 0.5163\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6926 - val_accuracy: 0.5137\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5166 - val_loss: 0.6926 - val_accuracy: 0.5138\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5164 - val_loss: 0.6922 - val_accuracy: 0.5166\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5167 - val_loss: 0.6925 - val_accuracy: 0.5150\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6922 - val_accuracy: 0.5143\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5182 - val_loss: 0.6922 - val_accuracy: 0.5157\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6923 - val_accuracy: 0.5178\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5175 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5192 - val_loss: 0.6922 - val_accuracy: 0.5156\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5184 - val_loss: 0.6923 - val_accuracy: 0.5145\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5197 - val_loss: 0.6922 - val_accuracy: 0.5159\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5188 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5201 - val_loss: 0.6920 - val_accuracy: 0.5158\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5204 - val_loss: 0.6920 - val_accuracy: 0.5141\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5209 - val_loss: 0.6919 - val_accuracy: 0.5171\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5208 - val_loss: 0.6920 - val_accuracy: 0.5185\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5207 - val_loss: 0.6919 - val_accuracy: 0.5150\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6919 - val_accuracy: 0.5180\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5163\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5218 - val_loss: 0.6917 - val_accuracy: 0.5177\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5230 - val_loss: 0.6917 - val_accuracy: 0.5172\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5232 - val_loss: 0.6914 - val_accuracy: 0.5200\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5237 - val_loss: 0.6914 - val_accuracy: 0.5189\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5248 - val_loss: 0.6913 - val_accuracy: 0.5207\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5246 - val_loss: 0.6914 - val_accuracy: 0.5215\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5243 - val_loss: 0.6910 - val_accuracy: 0.5238\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5268 - val_loss: 0.6910 - val_accuracy: 0.5201\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5267 - val_loss: 0.6905 - val_accuracy: 0.5235\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5273 - val_loss: 0.6903 - val_accuracy: 0.5229\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5292 - val_loss: 0.6904 - val_accuracy: 0.5209\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5301 - val_loss: 0.6897 - val_accuracy: 0.5272\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5314 - val_loss: 0.6897 - val_accuracy: 0.5265\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5323 - val_loss: 0.6897 - val_accuracy: 0.5276\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5335 - val_loss: 0.6893 - val_accuracy: 0.5233\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5350 - val_loss: 0.6887 - val_accuracy: 0.5341\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5362 - val_loss: 0.6886 - val_accuracy: 0.5330\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5385 - val_loss: 0.6882 - val_accuracy: 0.5352\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5386 - val_loss: 0.6874 - val_accuracy: 0.5400\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5403 - val_loss: 0.6873 - val_accuracy: 0.5351\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5407 - val_loss: 0.6872 - val_accuracy: 0.5407\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5419 - val_loss: 0.6865 - val_accuracy: 0.5433\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5423 - val_loss: 0.6863 - val_accuracy: 0.5429\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5446 - val_loss: 0.6862 - val_accuracy: 0.5449\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5447 - val_loss: 0.6871 - val_accuracy: 0.5362\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5441 - val_loss: 0.6868 - val_accuracy: 0.5457\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5439 - val_loss: 0.6854 - val_accuracy: 0.5486\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5444 - val_loss: 0.6964 - val_accuracy: 0.5067\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5444 - val_loss: 0.6851 - val_accuracy: 0.5475\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5437 - val_loss: 0.6854 - val_accuracy: 0.5470\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5441 - val_loss: 0.6848 - val_accuracy: 0.5461\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5465 - val_loss: 0.6846 - val_accuracy: 0.5483\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5478 - val_loss: 0.6844 - val_accuracy: 0.5481\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5481 - val_loss: 0.6843 - val_accuracy: 0.5498\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.5503 - val_loss: 0.6824 - val_accuracy: 0.5479\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6816 - accuracy: 0.5517 - val_loss: 0.6831 - val_accuracy: 0.5503\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5540 - val_loss: 0.6821 - val_accuracy: 0.5499\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.5538 - val_loss: 0.6818 - val_accuracy: 0.5515\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6809 - accuracy: 0.5543 - val_loss: 0.6818 - val_accuracy: 0.5551\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.5550 - val_loss: 0.6816 - val_accuracy: 0.5577\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6802 - accuracy: 0.5566 - val_loss: 0.6807 - val_accuracy: 0.5548\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5578 - val_loss: 0.6809 - val_accuracy: 0.5545\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5569 - val_loss: 0.6807 - val_accuracy: 0.5519\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5584 - val_loss: 0.6801 - val_accuracy: 0.5549\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5588 - val_loss: 0.6792 - val_accuracy: 0.5608\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6788 - accuracy: 0.5598 - val_loss: 0.6790 - val_accuracy: 0.5619\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_28\n",
      "cannot prune layer q_activation_28\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_29\n",
      "cannot prune layer q_activation_29\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6785 - accuracy: 0.5609 - val_loss: 0.6871 - val_accuracy: 0.5496\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6784 - accuracy: 0.5606 - val_loss: 0.6830 - val_accuracy: 0.5513\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6782 - accuracy: 0.5612 - val_loss: 0.6930 - val_accuracy: 0.5428\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6790 - accuracy: 0.5592 - val_loss: 0.6973 - val_accuracy: 0.5219\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6785 - accuracy: 0.5598 - val_loss: 0.6855 - val_accuracy: 0.5538\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6780 - accuracy: 0.5612 - val_loss: 0.6893 - val_accuracy: 0.5460\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6782 - accuracy: 0.5619 - val_loss: 0.6851 - val_accuracy: 0.5525\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6777 - accuracy: 0.5638 - val_loss: 0.6782 - val_accuracy: 0.5646\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6772 - accuracy: 0.5659 - val_loss: 0.6769 - val_accuracy: 0.5701\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6766 - accuracy: 0.5669 - val_loss: 0.6778 - val_accuracy: 0.5674\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6764 - accuracy: 0.5674 - val_loss: 0.6760 - val_accuracy: 0.5697\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5690 - val_loss: 0.6800 - val_accuracy: 0.5654\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6754 - accuracy: 0.5694 - val_loss: 0.6781 - val_accuracy: 0.5679\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6753 - accuracy: 0.5698 - val_loss: 0.6755 - val_accuracy: 0.5738\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6749 - accuracy: 0.5711 - val_loss: 0.6755 - val_accuracy: 0.5747\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5713 - val_loss: 0.6749 - val_accuracy: 0.5747\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5717 - val_loss: 0.6774 - val_accuracy: 0.5681\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6740 - accuracy: 0.5732 - val_loss: 0.6740 - val_accuracy: 0.5788\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6737 - accuracy: 0.5732 - val_loss: 0.6748 - val_accuracy: 0.5746\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6734 - accuracy: 0.5737 - val_loss: 0.6731 - val_accuracy: 0.5772\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5748 - val_loss: 0.6737 - val_accuracy: 0.5758\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5751 - val_loss: 0.6734 - val_accuracy: 0.5756\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6729 - accuracy: 0.5745 - val_loss: 0.6745 - val_accuracy: 0.5736\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6726 - accuracy: 0.5754 - val_loss: 0.6742 - val_accuracy: 0.5765\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6722 - accuracy: 0.5765 - val_loss: 0.6750 - val_accuracy: 0.5739\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6721 - accuracy: 0.5768 - val_loss: 0.6721 - val_accuracy: 0.5815\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5764 - val_loss: 0.6743 - val_accuracy: 0.5732\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5781 - val_loss: 0.6723 - val_accuracy: 0.5790\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5790 - val_loss: 0.6710 - val_accuracy: 0.5806\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5782 - val_loss: 0.6709 - val_accuracy: 0.5824\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5790 - val_loss: 0.6716 - val_accuracy: 0.5795\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6704 - accuracy: 0.5796 - val_loss: 0.6700 - val_accuracy: 0.5861\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6703 - accuracy: 0.5803 - val_loss: 0.6712 - val_accuracy: 0.5799\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6700 - accuracy: 0.5810 - val_loss: 0.6698 - val_accuracy: 0.5829\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5647 - val_loss: 0.6761 - val_accuracy: 0.5688\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5740 - val_loss: 0.6719 - val_accuracy: 0.5780\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5795 - val_loss: 0.6698 - val_accuracy: 0.5840\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5812 - val_loss: 0.6697 - val_accuracy: 0.5851\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6692 - accuracy: 0.5825 - val_loss: 0.6690 - val_accuracy: 0.5863\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.5832 - val_loss: 0.6688 - val_accuracy: 0.5846\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5831 - val_loss: 0.6684 - val_accuracy: 0.5872\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.5845 - val_loss: 0.6688 - val_accuracy: 0.5871\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6685 - accuracy: 0.5845 - val_loss: 0.6678 - val_accuracy: 0.5897\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6681 - accuracy: 0.5846 - val_loss: 0.6672 - val_accuracy: 0.5873\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6681 - accuracy: 0.5854 - val_loss: 0.6687 - val_accuracy: 0.5858\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6678 - accuracy: 0.5858 - val_loss: 0.6671 - val_accuracy: 0.5908\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6676 - accuracy: 0.5872 - val_loss: 0.6669 - val_accuracy: 0.5887\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6672 - accuracy: 0.5870 - val_loss: 0.6655 - val_accuracy: 0.5933\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6671 - accuracy: 0.5866 - val_loss: 0.6661 - val_accuracy: 0.5893\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6670 - accuracy: 0.5869 - val_loss: 0.6672 - val_accuracy: 0.5862\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.59152454]\n",
      " [0.5024879 ]\n",
      " [0.30662644]\n",
      " [0.4050146 ]\n",
      " [0.5122552 ]\n",
      " [0.5921538 ]\n",
      " [0.34931403]\n",
      " [0.394228  ]\n",
      " [0.6214111 ]\n",
      " [0.57594335]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_30 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_31 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_30 is normal keras bn layer\n",
      "q_activation_30      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_31 is normal keras bn layer\n",
      "q_activation_31      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.7283 - accuracy: 0.5029 - val_loss: 0.6960 - val_accuracy: 0.5071\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5050 - val_loss: 0.6945 - val_accuracy: 0.5064\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5064 - val_loss: 0.6938 - val_accuracy: 0.5086\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5069 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.5109\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6936 - val_accuracy: 0.5074\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6928 - val_accuracy: 0.5150\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6928 - val_accuracy: 0.5117\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5109 - val_loss: 0.6924 - val_accuracy: 0.5096\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6923 - val_accuracy: 0.5120\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5100 - val_loss: 0.6944 - val_accuracy: 0.5099\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5123 - val_loss: 0.6929 - val_accuracy: 0.5134\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5133 - val_loss: 0.6921 - val_accuracy: 0.5138\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5137 - val_loss: 0.6921 - val_accuracy: 0.5142\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6921 - val_accuracy: 0.5120\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5149 - val_loss: 0.6918 - val_accuracy: 0.5102\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5147 - val_loss: 0.6916 - val_accuracy: 0.5145\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5180\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5162 - val_loss: 0.6911 - val_accuracy: 0.5186\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5171 - val_loss: 0.6909 - val_accuracy: 0.5198\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5158 - val_loss: 0.6903 - val_accuracy: 0.5202\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5198 - val_loss: 0.6904 - val_accuracy: 0.5221\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5214 - val_loss: 0.6940 - val_accuracy: 0.5133\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5238 - val_loss: 0.6902 - val_accuracy: 0.5221\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5265 - val_loss: 0.6887 - val_accuracy: 0.5242\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5271 - val_loss: 0.6867 - val_accuracy: 0.5337\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5296 - val_loss: 0.6873 - val_accuracy: 0.5359\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5331 - val_loss: 0.6848 - val_accuracy: 0.5383\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5394 - val_loss: 0.6835 - val_accuracy: 0.5494\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5406 - val_loss: 0.7106 - val_accuracy: 0.5061\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5187 - val_loss: 0.6925 - val_accuracy: 0.5095\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5109 - val_loss: 0.6933 - val_accuracy: 0.5048\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5081\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5186 - val_loss: 0.6914 - val_accuracy: 0.5174\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5313 - val_loss: 0.6896 - val_accuracy: 0.5236\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5278 - val_loss: 0.6980 - val_accuracy: 0.5066\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5182 - val_loss: 0.6918 - val_accuracy: 0.5105\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6924 - val_accuracy: 0.5167\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5119 - val_loss: 0.6924 - val_accuracy: 0.5110\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5122 - val_loss: 0.6921 - val_accuracy: 0.5099\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5173 - val_loss: 0.6912 - val_accuracy: 0.5140\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5187 - val_loss: 0.6927 - val_accuracy: 0.5077\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5161 - val_loss: 0.6923 - val_accuracy: 0.5160\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5179 - val_loss: 0.6948 - val_accuracy: 0.5075\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5073 - val_loss: 0.6925 - val_accuracy: 0.5050\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5073 - val_loss: 0.6925 - val_accuracy: 0.5036\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5093 - val_loss: 0.6923 - val_accuracy: 0.5057\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5098 - val_loss: 0.6924 - val_accuracy: 0.5098\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5121 - val_loss: 0.6921 - val_accuracy: 0.5099\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_30\n",
      "cannot prune layer q_activation_30\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_31\n",
      "cannot prune layer q_activation_31\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6901 - accuracy: 0.5222 - val_loss: 0.6926 - val_accuracy: 0.5104\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5213 - val_loss: 0.6932 - val_accuracy: 0.5042\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5143 - val_loss: 0.7053 - val_accuracy: 0.5038\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5246 - val_loss: 0.7040 - val_accuracy: 0.5017\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5332 - val_loss: 0.6880 - val_accuracy: 0.5275\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5412 - val_loss: 0.7062 - val_accuracy: 0.5265\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.5484 - val_loss: 0.6869 - val_accuracy: 0.5239\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5513 - val_loss: 0.6903 - val_accuracy: 0.5359\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5491 - val_loss: 0.6810 - val_accuracy: 0.5485\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5573 - val_loss: 0.6748 - val_accuracy: 0.5663\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6778 - accuracy: 0.5594 - val_loss: 0.6760 - val_accuracy: 0.5595\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6773 - accuracy: 0.5604 - val_loss: 0.6744 - val_accuracy: 0.5625\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6763 - accuracy: 0.5617 - val_loss: 0.6872 - val_accuracy: 0.5257\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6769 - accuracy: 0.5641 - val_loss: 0.6775 - val_accuracy: 0.5640\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6737 - accuracy: 0.5676 - val_loss: 0.6723 - val_accuracy: 0.5721\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6789 - accuracy: 0.5621 - val_loss: 0.6780 - val_accuracy: 0.5660\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5484 - val_loss: 0.6812 - val_accuracy: 0.5562\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5587 - val_loss: 0.6736 - val_accuracy: 0.5723\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.5675 - val_loss: 0.6723 - val_accuracy: 0.5747\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6751 - accuracy: 0.5686 - val_loss: 0.6798 - val_accuracy: 0.5547\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.5696 - val_loss: 0.6773 - val_accuracy: 0.5579\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5412 - val_loss: 0.6873 - val_accuracy: 0.5372\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5545 - val_loss: 0.6794 - val_accuracy: 0.5592\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6757 - accuracy: 0.5687 - val_loss: 0.6751 - val_accuracy: 0.5734\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6750 - accuracy: 0.5704 - val_loss: 0.6743 - val_accuracy: 0.5740\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6742 - accuracy: 0.5717 - val_loss: 0.6757 - val_accuracy: 0.5666\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5716 - val_loss: 0.6754 - val_accuracy: 0.5703\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5667 - val_loss: 0.7004 - val_accuracy: 0.5067\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5028 - val_loss: 0.6931 - val_accuracy: 0.5057\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5202 - val_loss: 0.6894 - val_accuracy: 0.5304\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5421 - val_loss: 0.6824 - val_accuracy: 0.5493\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6806 - accuracy: 0.5519 - val_loss: 0.6789 - val_accuracy: 0.5574\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6783 - accuracy: 0.5579 - val_loss: 0.6763 - val_accuracy: 0.5653\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6768 - accuracy: 0.5607 - val_loss: 0.6875 - val_accuracy: 0.5404\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5670 - val_loss: 0.6725 - val_accuracy: 0.5629\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5720 - val_loss: 0.6749 - val_accuracy: 0.5725\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6714 - accuracy: 0.5734 - val_loss: 0.6718 - val_accuracy: 0.5777\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6720 - accuracy: 0.5727 - val_loss: 0.6989 - val_accuracy: 0.5220\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5573 - val_loss: 0.6773 - val_accuracy: 0.5603\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6730 - accuracy: 0.5719 - val_loss: 0.6713 - val_accuracy: 0.5823\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5672 - val_loss: 0.6819 - val_accuracy: 0.5460\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.5606 - val_loss: 0.6723 - val_accuracy: 0.5728\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5693 - val_loss: 0.6673 - val_accuracy: 0.5794\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6731 - accuracy: 0.5694 - val_loss: 0.7096 - val_accuracy: 0.5020\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6743 - accuracy: 0.5662 - val_loss: 0.6664 - val_accuracy: 0.5792\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6703 - accuracy: 0.5766 - val_loss: 0.6715 - val_accuracy: 0.5762\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5787 - val_loss: 0.6640 - val_accuracy: 0.5827\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6772 - accuracy: 0.5594 - val_loss: 0.6788 - val_accuracy: 0.5493\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6733 - accuracy: 0.5675 - val_loss: 0.6799 - val_accuracy: 0.5645\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6706 - accuracy: 0.5752 - val_loss: 0.6692 - val_accuracy: 0.5807\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.5447618 ]\n",
      " [0.43869933]\n",
      " [0.57286525]\n",
      " [0.48713407]\n",
      " [0.4330225 ]\n",
      " [0.38866293]\n",
      " [0.40630168]\n",
      " [0.3607917 ]\n",
      " [0.5293383 ]\n",
      " [0.6608461 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_32 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_33 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_32 is normal keras bn layer\n",
      "q_activation_32      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_33 is normal keras bn layer\n",
      "q_activation_33      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 2.5501 - accuracy: 0.4980 - val_loss: 0.9253 - val_accuracy: 0.4997\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7447 - accuracy: 0.5011 - val_loss: 0.7200 - val_accuracy: 0.5001\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7127 - accuracy: 0.5012 - val_loss: 0.7094 - val_accuracy: 0.4996\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7096 - accuracy: 0.5018 - val_loss: 0.7066 - val_accuracy: 0.5020\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7067 - accuracy: 0.5019 - val_loss: 0.7087 - val_accuracy: 0.4996\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7038 - accuracy: 0.5023 - val_loss: 0.7013 - val_accuracy: 0.5034\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7019 - accuracy: 0.5017 - val_loss: 0.7023 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7006 - accuracy: 0.5020 - val_loss: 0.7012 - val_accuracy: 0.5002\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6997 - accuracy: 0.5029 - val_loss: 0.6990 - val_accuracy: 0.5046\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6990 - accuracy: 0.5027 - val_loss: 0.6986 - val_accuracy: 0.5027\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6981 - accuracy: 0.5038 - val_loss: 0.6997 - val_accuracy: 0.5064\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5049 - val_loss: 0.6994 - val_accuracy: 0.5051\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6971 - accuracy: 0.5033 - val_loss: 0.6958 - val_accuracy: 0.5025\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5040 - val_loss: 0.6951 - val_accuracy: 0.5030\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5054 - val_loss: 0.7015 - val_accuracy: 0.5036\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5048 - val_loss: 0.6942 - val_accuracy: 0.5056\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5061 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5074 - val_loss: 0.6932 - val_accuracy: 0.5085\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5085 - val_loss: 0.6933 - val_accuracy: 0.5059\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5066\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.5070\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6930 - val_accuracy: 0.5072\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5075\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6928 - val_accuracy: 0.5082\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5131 - val_loss: 0.6928 - val_accuracy: 0.5103\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5103\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5139 - val_loss: 0.6934 - val_accuracy: 0.5105\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5146 - val_loss: 0.6924 - val_accuracy: 0.5137\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5146 - val_loss: 0.6925 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5162 - val_loss: 0.6921 - val_accuracy: 0.5128\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5153 - val_loss: 0.6922 - val_accuracy: 0.5140\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5158 - val_loss: 0.6920 - val_accuracy: 0.5152\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5161 - val_loss: 0.6920 - val_accuracy: 0.5079\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5174 - val_loss: 0.6916 - val_accuracy: 0.5154\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5175 - val_loss: 0.6912 - val_accuracy: 0.5166\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5184 - val_loss: 0.6987 - val_accuracy: 0.5162\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5210 - val_loss: 0.6914 - val_accuracy: 0.5173\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5223 - val_loss: 0.6902 - val_accuracy: 0.5223\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5255 - val_loss: 0.6892 - val_accuracy: 0.5279\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5301 - val_loss: 0.6879 - val_accuracy: 0.5336\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6862 - val_accuracy: 0.5414\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5441 - val_loss: 0.6842 - val_accuracy: 0.5485\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5488 - val_loss: 0.6848 - val_accuracy: 0.5456\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5538 - val_loss: 0.6782 - val_accuracy: 0.5609\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5580 - val_loss: 0.6769 - val_accuracy: 0.5640\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5621 - val_loss: 0.6734 - val_accuracy: 0.5664\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5659 - val_loss: 0.6759 - val_accuracy: 0.5661\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5688 - val_loss: 0.6744 - val_accuracy: 0.5641\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.5735 - val_loss: 0.6795 - val_accuracy: 0.5590\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5731 - val_loss: 0.6728 - val_accuracy: 0.5707\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6691 - accuracy: 0.5767 - val_loss: 0.6651 - val_accuracy: 0.5846\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6676 - accuracy: 0.5788 - val_loss: 0.6647 - val_accuracy: 0.5825\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5617 - val_loss: 0.6767 - val_accuracy: 0.5589\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6715 - accuracy: 0.5706 - val_loss: 0.6671 - val_accuracy: 0.5725\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6674 - accuracy: 0.5779 - val_loss: 0.6604 - val_accuracy: 0.5845\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5814 - val_loss: 0.6596 - val_accuracy: 0.5910\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.5847 - val_loss: 0.6630 - val_accuracy: 0.5840\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5825 - val_loss: 0.6599 - val_accuracy: 0.5875\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.5811 - val_loss: 0.6822 - val_accuracy: 0.5513\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5824 - val_loss: 0.6627 - val_accuracy: 0.5896\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5899 - val_loss: 0.6568 - val_accuracy: 0.5962\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5491 - val_loss: 0.6899 - val_accuracy: 0.5333\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6743 - accuracy: 0.5673 - val_loss: 0.6609 - val_accuracy: 0.5901\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6616 - accuracy: 0.5908 - val_loss: 0.6618 - val_accuracy: 0.5857\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6613 - accuracy: 0.5907 - val_loss: 0.6691 - val_accuracy: 0.5678\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5583 - val_loss: 0.6690 - val_accuracy: 0.5763\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5842 - val_loss: 0.6537 - val_accuracy: 0.5950\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.5924 - val_loss: 0.6530 - val_accuracy: 0.5993\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6591 - accuracy: 0.5910 - val_loss: 0.6524 - val_accuracy: 0.6004\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.5921 - val_loss: 0.6643 - val_accuracy: 0.5833\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5733 - val_loss: 0.6697 - val_accuracy: 0.5784\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6650 - accuracy: 0.5848 - val_loss: 0.6592 - val_accuracy: 0.5948\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6603 - accuracy: 0.5939 - val_loss: 0.6570 - val_accuracy: 0.5978\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6570 - accuracy: 0.5968 - val_loss: 0.6534 - val_accuracy: 0.5994\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6551 - accuracy: 0.5985 - val_loss: 0.6526 - val_accuracy: 0.6001\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6573 - accuracy: 0.5953 - val_loss: 0.6592 - val_accuracy: 0.5965\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6013 - val_loss: 0.6523 - val_accuracy: 0.5998\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6021 - val_loss: 0.6601 - val_accuracy: 0.5940\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6012 - val_loss: 0.6487 - val_accuracy: 0.6073\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6515 - accuracy: 0.6037 - val_loss: 0.6496 - val_accuracy: 0.6099\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5729 - val_loss: 0.6888 - val_accuracy: 0.5258\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6636 - accuracy: 0.5841 - val_loss: 0.6481 - val_accuracy: 0.6069\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6019 - val_loss: 0.6444 - val_accuracy: 0.6120\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6508 - accuracy: 0.6051 - val_loss: 0.6490 - val_accuracy: 0.6072\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6498 - accuracy: 0.6070 - val_loss: 0.6664 - val_accuracy: 0.5897\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6494 - accuracy: 0.6068 - val_loss: 0.6518 - val_accuracy: 0.6051\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.6074 - val_loss: 0.6418 - val_accuracy: 0.6160\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6512 - accuracy: 0.6069 - val_loss: 0.6495 - val_accuracy: 0.6093\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6114 - val_loss: 0.6420 - val_accuracy: 0.6158\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6475 - accuracy: 0.6103 - val_loss: 0.6418 - val_accuracy: 0.6179\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.5868 - val_loss: 0.6566 - val_accuracy: 0.5995\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.5995 - val_loss: 0.6529 - val_accuracy: 0.6004\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6515 - accuracy: 0.6033 - val_loss: 0.6478 - val_accuracy: 0.6065\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6047 - val_loss: 0.6502 - val_accuracy: 0.6037\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6489 - accuracy: 0.6076 - val_loss: 0.6446 - val_accuracy: 0.6114\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6488 - accuracy: 0.6072 - val_loss: 0.6437 - val_accuracy: 0.6135\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6505 - accuracy: 0.6068 - val_loss: 0.6547 - val_accuracy: 0.6015\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5168 - val_loss: 0.6966 - val_accuracy: 0.5019\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5048 - val_loss: 0.6946 - val_accuracy: 0.5003\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5085 - val_loss: 0.6941 - val_accuracy: 0.5045\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_32\n",
      "cannot prune layer q_activation_32\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_33\n",
      "cannot prune layer q_activation_33\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6912 - accuracy: 0.5232 - val_loss: 0.7012 - val_accuracy: 0.4972\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5682 - val_loss: 0.7101 - val_accuracy: 0.5083\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.5116 - val_loss: 0.6981 - val_accuracy: 0.5120\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5186 - val_loss: 0.6937 - val_accuracy: 0.5208\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5176 - val_loss: 0.7109 - val_accuracy: 0.4992\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5156 - val_loss: 0.7046 - val_accuracy: 0.5093\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5185 - val_loss: 0.6920 - val_accuracy: 0.5270\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5314 - val_loss: 0.6969 - val_accuracy: 0.5100\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5431 - val_loss: 0.6846 - val_accuracy: 0.5538\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6819 - accuracy: 0.5568 - val_loss: 0.6779 - val_accuracy: 0.5663\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5705 - val_loss: 0.6725 - val_accuracy: 0.5763\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.5768 - val_loss: 0.6669 - val_accuracy: 0.5869\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6688 - accuracy: 0.5820 - val_loss: 0.6631 - val_accuracy: 0.5905\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6682 - accuracy: 0.5847 - val_loss: 0.6625 - val_accuracy: 0.5897\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5861 - val_loss: 0.6605 - val_accuracy: 0.5919\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5888 - val_loss: 0.6585 - val_accuracy: 0.5947\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5722 - val_loss: 0.6607 - val_accuracy: 0.5961\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5919 - val_loss: 0.6594 - val_accuracy: 0.5964\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6659 - accuracy: 0.5893 - val_loss: 0.6702 - val_accuracy: 0.5754\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5900 - val_loss: 0.6611 - val_accuracy: 0.5920\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6652 - accuracy: 0.5911 - val_loss: 0.6612 - val_accuracy: 0.5966\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6624 - accuracy: 0.5944 - val_loss: 0.6574 - val_accuracy: 0.5993\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6631 - accuracy: 0.5925 - val_loss: 0.6563 - val_accuracy: 0.6007\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6609 - accuracy: 0.5962 - val_loss: 0.6629 - val_accuracy: 0.6024\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5957 - val_loss: 0.6556 - val_accuracy: 0.6035\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5961 - val_loss: 0.6576 - val_accuracy: 0.5987\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6683 - accuracy: 0.5856 - val_loss: 0.6654 - val_accuracy: 0.5875\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6616 - accuracy: 0.5956 - val_loss: 0.6570 - val_accuracy: 0.6001\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5968 - val_loss: 0.6560 - val_accuracy: 0.6017\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6606 - accuracy: 0.5976 - val_loss: 0.6558 - val_accuracy: 0.6024\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6617 - accuracy: 0.5957 - val_loss: 0.6563 - val_accuracy: 0.6030\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6589 - accuracy: 0.5994 - val_loss: 0.6544 - val_accuracy: 0.6064\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.6001 - val_loss: 0.6526 - val_accuracy: 0.6085\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6605 - accuracy: 0.5984 - val_loss: 0.6608 - val_accuracy: 0.5935\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6599 - accuracy: 0.5980 - val_loss: 0.6533 - val_accuracy: 0.6045\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6582 - accuracy: 0.5986 - val_loss: 0.6516 - val_accuracy: 0.6092\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6594 - accuracy: 0.5997 - val_loss: 0.6583 - val_accuracy: 0.6032\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5985 - val_loss: 0.6619 - val_accuracy: 0.6037\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6589 - accuracy: 0.5997 - val_loss: 0.6552 - val_accuracy: 0.6009\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5998 - val_loss: 0.6515 - val_accuracy: 0.6078\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.6002 - val_loss: 0.6542 - val_accuracy: 0.6039\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.6016 - val_loss: 0.6508 - val_accuracy: 0.6079\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6578 - accuracy: 0.6015 - val_loss: 0.6526 - val_accuracy: 0.6057\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6573 - accuracy: 0.6023 - val_loss: 0.6534 - val_accuracy: 0.6035\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5952 - val_loss: 0.6542 - val_accuracy: 0.6028\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6603 - accuracy: 0.5962 - val_loss: 0.6560 - val_accuracy: 0.6030\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.6008 - val_loss: 0.6526 - val_accuracy: 0.6046\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6554 - accuracy: 0.6019 - val_loss: 0.6508 - val_accuracy: 0.6075\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.6026 - val_loss: 0.6511 - val_accuracy: 0.6077\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.6033 - val_loss: 0.6533 - val_accuracy: 0.6093\n",
      "1714/1714 [==============================] - 4s 1ms/step\n",
      "[[0.46138406]\n",
      " [0.6879751 ]\n",
      " [0.34491414]\n",
      " [0.51884025]\n",
      " [0.25803274]\n",
      " [0.46989125]\n",
      " [0.34973258]\n",
      " [0.32250845]\n",
      " [0.44384003]\n",
      " [0.6836106 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_34 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_35 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_34 is normal keras bn layer\n",
      "q_activation_34      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_35 is normal keras bn layer\n",
      "q_activation_35      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 2.7148 - accuracy: 0.4979 - val_loss: 2.2982 - val_accuracy: 0.5001\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 2.0114 - accuracy: 0.4980 - val_loss: 1.8685 - val_accuracy: 0.5004\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.7553 - accuracy: 0.4986 - val_loss: 1.6900 - val_accuracy: 0.5051\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.5454 - accuracy: 0.5001 - val_loss: 1.4310 - val_accuracy: 0.5034\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.3510 - accuracy: 0.5008 - val_loss: 1.1982 - val_accuracy: 0.5048\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.1064 - accuracy: 0.4995 - val_loss: 0.9319 - val_accuracy: 0.5024\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.8665 - accuracy: 0.4991 - val_loss: 0.7940 - val_accuracy: 0.5013\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7820 - accuracy: 0.4985 - val_loss: 0.7505 - val_accuracy: 0.5004\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7471 - accuracy: 0.4988 - val_loss: 0.7316 - val_accuracy: 0.5013\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7289 - accuracy: 0.4999 - val_loss: 0.7215 - val_accuracy: 0.5037\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7211 - accuracy: 0.5004 - val_loss: 0.7163 - val_accuracy: 0.5043\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7162 - accuracy: 0.5003 - val_loss: 0.7125 - val_accuracy: 0.5059\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7125 - accuracy: 0.5010 - val_loss: 0.7095 - val_accuracy: 0.5050\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7096 - accuracy: 0.5005 - val_loss: 0.7061 - val_accuracy: 0.5032\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7065 - accuracy: 0.5012 - val_loss: 0.7044 - val_accuracy: 0.5044\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7044 - accuracy: 0.5012 - val_loss: 0.7025 - val_accuracy: 0.5043\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7025 - accuracy: 0.5020 - val_loss: 0.7010 - val_accuracy: 0.5032\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7013 - accuracy: 0.5018 - val_loss: 0.7025 - val_accuracy: 0.5021\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6997 - accuracy: 0.5024 - val_loss: 0.6991 - val_accuracy: 0.5022\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6984 - accuracy: 0.5020 - val_loss: 0.6981 - val_accuracy: 0.5029\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6972 - accuracy: 0.5036 - val_loss: 0.6972 - val_accuracy: 0.5044\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6966 - accuracy: 0.5037 - val_loss: 0.6966 - val_accuracy: 0.5046\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5041 - val_loss: 0.6959 - val_accuracy: 0.5047\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5058 - val_loss: 0.6955 - val_accuracy: 0.5027\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5052 - val_loss: 0.6950 - val_accuracy: 0.5024\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5049 - val_loss: 0.6954 - val_accuracy: 0.5018\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5056 - val_loss: 0.6946 - val_accuracy: 0.5024\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5068 - val_loss: 0.6946 - val_accuracy: 0.5038\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5051 - val_loss: 0.6945 - val_accuracy: 0.5035\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5067 - val_loss: 0.6943 - val_accuracy: 0.5015\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5046\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6943 - val_accuracy: 0.5046\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5065 - val_loss: 0.6942 - val_accuracy: 0.5059\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5073 - val_loss: 0.6941 - val_accuracy: 0.5053\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5085 - val_loss: 0.6939 - val_accuracy: 0.5037\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5079 - val_loss: 0.6938 - val_accuracy: 0.5052\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5086 - val_loss: 0.6938 - val_accuracy: 0.5035\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5102 - val_loss: 0.6937 - val_accuracy: 0.5060\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6938 - val_accuracy: 0.5050\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6935 - val_accuracy: 0.5075\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5106 - val_loss: 0.6936 - val_accuracy: 0.5066\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5112 - val_loss: 0.6935 - val_accuracy: 0.5080\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6933 - val_accuracy: 0.5086\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5124 - val_loss: 0.6935 - val_accuracy: 0.5054\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.5081\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5119 - val_loss: 0.6934 - val_accuracy: 0.5080\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5123 - val_loss: 0.6933 - val_accuracy: 0.5080\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5131 - val_loss: 0.6933 - val_accuracy: 0.5089\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6932 - val_accuracy: 0.5088\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6932 - val_accuracy: 0.5082\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5130 - val_loss: 0.6939 - val_accuracy: 0.5093\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5135 - val_loss: 0.6934 - val_accuracy: 0.5095\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5129 - val_loss: 0.6932 - val_accuracy: 0.5094\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5130 - val_loss: 0.6936 - val_accuracy: 0.5088\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5121 - val_loss: 0.7161 - val_accuracy: 0.5103\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5126 - val_loss: 0.6932 - val_accuracy: 0.5098\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5130 - val_loss: 0.6934 - val_accuracy: 0.5105\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5116 - val_loss: 0.6938 - val_accuracy: 0.5095\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.5106\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5109 - val_loss: 0.7097 - val_accuracy: 0.5108\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5118 - val_loss: 0.6939 - val_accuracy: 0.5106\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5131 - val_loss: 0.6946 - val_accuracy: 0.5096\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5123 - val_loss: 0.6943 - val_accuracy: 0.5116\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5123 - val_loss: 0.7075 - val_accuracy: 0.5126\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5135 - val_loss: 0.7053 - val_accuracy: 0.5134\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5132 - val_loss: 0.6945 - val_accuracy: 0.5097\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5142 - val_loss: 0.6945 - val_accuracy: 0.5126\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5151 - val_loss: 0.7048 - val_accuracy: 0.5133\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5134 - val_loss: 0.6943 - val_accuracy: 0.5135\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5150 - val_loss: 0.6943 - val_accuracy: 0.5160\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5143 - val_loss: 0.6945 - val_accuracy: 0.5141\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5173 - val_loss: 0.6943 - val_accuracy: 0.5119\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5167 - val_loss: 0.7000 - val_accuracy: 0.5189\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_34\n",
      "cannot prune layer q_activation_34\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_35\n",
      "cannot prune layer q_activation_35\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6944 - accuracy: 0.5125 - val_loss: 0.6990 - val_accuracy: 0.5012\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6961 - accuracy: 0.5080 - val_loss: 0.6988 - val_accuracy: 0.5024\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6985 - accuracy: 0.5052 - val_loss: 0.6978 - val_accuracy: 0.5039\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6993 - accuracy: 0.5052 - val_loss: 0.7042 - val_accuracy: 0.5034\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6987 - accuracy: 0.5053 - val_loss: 0.6962 - val_accuracy: 0.5050\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6979 - accuracy: 0.5044 - val_loss: 0.6948 - val_accuracy: 0.5059\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5036 - val_loss: 0.6940 - val_accuracy: 0.5054\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5052 - val_loss: 0.6946 - val_accuracy: 0.5076\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5048 - val_loss: 0.6940 - val_accuracy: 0.5076\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5060 - val_loss: 0.6937 - val_accuracy: 0.5071\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5052 - val_loss: 0.6934 - val_accuracy: 0.5079\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5066 - val_loss: 0.6934 - val_accuracy: 0.5068\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5068 - val_loss: 0.6933 - val_accuracy: 0.5080\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5071 - val_loss: 0.6932 - val_accuracy: 0.5070\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6943 - accuracy: 0.5074 - val_loss: 0.6930 - val_accuracy: 0.5090\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5073 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5073 - val_loss: 0.6930 - val_accuracy: 0.5084\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5080 - val_loss: 0.6931 - val_accuracy: 0.5071\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5086 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5087\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5118 - val_loss: 0.6930 - val_accuracy: 0.5069\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6927 - val_accuracy: 0.5099\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5110\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6927 - val_accuracy: 0.5097\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6928 - val_accuracy: 0.5091\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.5115\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6924 - val_accuracy: 0.5130\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.7097 - val_accuracy: 0.5065\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5155 - val_loss: 0.6924 - val_accuracy: 0.5120\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5129\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5144 - val_loss: 0.6924 - val_accuracy: 0.5116\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5125\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5142\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5165 - val_loss: 0.6922 - val_accuracy: 0.5155\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5177 - val_loss: 0.6923 - val_accuracy: 0.5150\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5182 - val_loss: 0.6924 - val_accuracy: 0.5155\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5185 - val_loss: 0.6924 - val_accuracy: 0.5173\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5190 - val_loss: 0.6924 - val_accuracy: 0.5151\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5157\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5190 - val_loss: 0.6922 - val_accuracy: 0.5184\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5191 - val_loss: 0.6924 - val_accuracy: 0.5169\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5186\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5197 - val_loss: 0.6921 - val_accuracy: 0.5194\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5209 - val_loss: 0.6921 - val_accuracy: 0.5189\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5215 - val_loss: 0.6921 - val_accuracy: 0.5198\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5220 - val_loss: 0.6921 - val_accuracy: 0.5194\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5215 - val_loss: 0.6920 - val_accuracy: 0.5188\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5081399 ]\n",
      " [0.5081399 ]\n",
      " [0.48060465]\n",
      " [0.5081399 ]\n",
      " [0.4789773 ]\n",
      " [0.47483745]\n",
      " [0.4809744 ]\n",
      " [0.5151807 ]\n",
      " [0.5112333 ]\n",
      " [0.47330087]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = hyperparameter_search(data, HYPERPARAMETERS, param_grid, result_file=SAVE_FILE)\n",
    "    send_email_notification(\"All done with hyperparameter search\", 'Done!')\n",
    "except Exception as e:\n",
    "    print(\"Error encountered:\", e)\n",
    "    send_email_notification(\"Hyperparameter search ran into an error\", 'Go fix it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE REFORMATTING\n",
    "# (RUN LATER WHEN THEY ARENT BEING WRITTEN TO)\n",
    "\n",
    "def reformat_hyperparameter_results(input_file, output_file):\n",
    "    # Read the original JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process and format the metrics\n",
    "    for key, value in data.items():\n",
    "        if \"metrics\" in value:\n",
    "            value[\"metrics\"] = format_metrics(value[\"metrics\"])\n",
    "\n",
    "    # Write the updated data to the new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Define input and output file names\n",
    "input_file = 'one_layerDNN_results.json'\n",
    "output_file = 'one_layerDNN_results.json'\n",
    "\n",
    "# Call the function to reformat the JSON data\n",
    "reformat_hyperparameter_results(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Read / Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 8)                 32        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_output_sigmoid (QDen  (None, 1)                 9         \n",
      " se)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "batch_normalization  is normal keras bn layer\n",
      "q_activation         quantized_relu(6,0)\n",
      "dense_output_sigmoid u=1 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 0 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.1534 - accuracy: 0.5018 - val_loss: 0.7326 - val_accuracy: 0.5053\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.5026 - val_loss: 0.7093 - val_accuracy: 0.5014\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7034 - accuracy: 0.5018 - val_loss: 0.7003 - val_accuracy: 0.5030\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7001 - accuracy: 0.5022 - val_loss: 0.7378 - val_accuracy: 0.5022\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6995 - accuracy: 0.5028 - val_loss: 0.6996 - val_accuracy: 0.5012\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7000 - accuracy: 0.5013 - val_loss: 0.6978 - val_accuracy: 0.5042\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7048 - accuracy: 0.5022 - val_loss: 0.7210 - val_accuracy: 0.4990\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7066 - accuracy: 0.5030 - val_loss: 0.6950 - val_accuracy: 0.5033\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5048 - val_loss: 0.6953 - val_accuracy: 0.4993\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5043 - val_loss: 0.6947 - val_accuracy: 0.5019\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5037 - val_loss: 0.6943 - val_accuracy: 0.5018\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5052 - val_loss: 0.6937 - val_accuracy: 0.5027\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5045 - val_loss: 0.6941 - val_accuracy: 0.4999\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.5052 - val_loss: 0.6946 - val_accuracy: 0.5018\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5040 - val_loss: 0.6949 - val_accuracy: 0.4997\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5041 - val_loss: 0.6943 - val_accuracy: 0.5039\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5056 - val_loss: 0.6991 - val_accuracy: 0.5049\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6970 - accuracy: 0.5028 - val_loss: 0.6999 - val_accuracy: 0.5040\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5050 - val_loss: 0.6949 - val_accuracy: 0.5056\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5054 - val_loss: 0.6998 - val_accuracy: 0.5024\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.5049 - val_loss: 0.7035 - val_accuracy: 0.5014\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.5007 - val_loss: 0.6960 - val_accuracy: 0.5004\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7039 - accuracy: 0.5006 - val_loss: 0.6981 - val_accuracy: 0.5016\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7049 - accuracy: 0.5007 - val_loss: 0.7108 - val_accuracy: 0.5016\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.5024 - val_loss: 0.7086 - val_accuracy: 0.5017\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7019 - accuracy: 0.5020 - val_loss: 0.7145 - val_accuracy: 0.5014\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7015 - accuracy: 0.5046 - val_loss: 0.7022 - val_accuracy: 0.5020\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6993 - accuracy: 0.5048 - val_loss: 0.6945 - val_accuracy: 0.5063\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6974 - accuracy: 0.5061 - val_loss: 0.6954 - val_accuracy: 0.5053\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7003 - accuracy: 0.5052 - val_loss: 0.6943 - val_accuracy: 0.5046\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7004 - accuracy: 0.5059 - val_loss: 0.6939 - val_accuracy: 0.5070\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6994 - accuracy: 0.5077 - val_loss: 0.6931 - val_accuracy: 0.5055\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5070 - val_loss: 0.6928 - val_accuracy: 0.5042\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5087 - val_loss: 0.6933 - val_accuracy: 0.5070\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5093 - val_loss: 0.6930 - val_accuracy: 0.5098\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5091 - val_loss: 0.6931 - val_accuracy: 0.5082\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6929 - val_accuracy: 0.5071\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5081 - val_loss: 0.6932 - val_accuracy: 0.5061\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5081 - val_loss: 0.6939 - val_accuracy: 0.5056\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5085 - val_loss: 0.6927 - val_accuracy: 0.5048\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6936 - val_accuracy: 0.5035\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6928 - val_accuracy: 0.5051\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.5108\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6927 - val_accuracy: 0.5108\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.5086\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5077\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5091 - val_loss: 0.6930 - val_accuracy: 0.5074\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5103 - val_loss: 0.6926 - val_accuracy: 0.5071\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5093 - val_loss: 0.6932 - val_accuracy: 0.5089\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5101 - val_loss: 0.6927 - val_accuracy: 0.5040\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5097 - val_loss: 0.6934 - val_accuracy: 0.5085\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5102 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5084\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5105 - val_loss: 0.6936 - val_accuracy: 0.5038\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5081 - val_loss: 0.6928 - val_accuracy: 0.5060\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5068 - val_loss: 0.6928 - val_accuracy: 0.5057\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5059\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5054\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5086 - val_loss: 0.7243 - val_accuracy: 0.4989\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5069 - val_loss: 0.6933 - val_accuracy: 0.5033\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5066 - val_loss: 0.6930 - val_accuracy: 0.5069\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5075 - val_loss: 0.6933 - val_accuracy: 0.5035\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5054 - val_loss: 0.6928 - val_accuracy: 0.5050\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5063 - val_loss: 0.6935 - val_accuracy: 0.5041\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.5059 - val_loss: 0.6926 - val_accuracy: 0.5079\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5074 - val_loss: 0.6937 - val_accuracy: 0.5018\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.5068 - val_loss: 0.6931 - val_accuracy: 0.5050\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5080 - val_loss: 0.6928 - val_accuracy: 0.5067\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization\n",
      "cannot prune layer q_activation\n",
      "pruning layer dense_output_sigmoid\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6925 - accuracy: 0.5099 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5109 - val_loss: 0.6926 - val_accuracy: 0.5061\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5063\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5091 - val_loss: 0.6926 - val_accuracy: 0.5066\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6926 - val_accuracy: 0.5065\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5035\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5092 - val_loss: 0.6927 - val_accuracy: 0.5032\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5099 - val_loss: 0.6927 - val_accuracy: 0.5059\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5098 - val_loss: 0.6929 - val_accuracy: 0.5031\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5083 - val_loss: 0.6926 - val_accuracy: 0.5059\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5094 - val_loss: 0.6929 - val_accuracy: 0.5058\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5091 - val_loss: 0.6928 - val_accuracy: 0.5044\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5077 - val_loss: 0.6927 - val_accuracy: 0.5075\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5083 - val_loss: 0.6927 - val_accuracy: 0.5058\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5080 - val_loss: 0.6929 - val_accuracy: 0.5054\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5092 - val_loss: 0.6931 - val_accuracy: 0.5057\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5085 - val_loss: 0.6929 - val_accuracy: 0.5055\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.5068 - val_loss: 0.6930 - val_accuracy: 0.5041\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.5078 - val_loss: 0.6928 - val_accuracy: 0.5049\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5067 - val_loss: 0.6928 - val_accuracy: 0.5039\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5058 - val_loss: 0.6929 - val_accuracy: 0.5027\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5072 - val_loss: 0.6928 - val_accuracy: 0.5051\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5068 - val_loss: 0.6929 - val_accuracy: 0.5060\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5081 - val_loss: 0.6932 - val_accuracy: 0.5039\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5072 - val_loss: 0.7198 - val_accuracy: 0.4990\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5050 - val_loss: 0.6937 - val_accuracy: 0.5033\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6972 - accuracy: 0.5050 - val_loss: 0.7170 - val_accuracy: 0.4989\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5049 - val_loss: 0.7167 - val_accuracy: 0.4991\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6970 - accuracy: 0.5042 - val_loss: 0.6941 - val_accuracy: 0.5021\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6973 - accuracy: 0.5047 - val_loss: 0.6978 - val_accuracy: 0.5029\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.5042 - val_loss: 0.7127 - val_accuracy: 0.4993\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6980 - accuracy: 0.5026 - val_loss: 0.6938 - val_accuracy: 0.5026\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6981 - accuracy: 0.5027 - val_loss: 0.7109 - val_accuracy: 0.4995\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6990 - accuracy: 0.5024 - val_loss: 0.7111 - val_accuracy: 0.4995\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6987 - accuracy: 0.5025 - val_loss: 0.7079 - val_accuracy: 0.4994\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6983 - accuracy: 0.5030 - val_loss: 0.6946 - val_accuracy: 0.5019\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6989 - accuracy: 0.5018 - val_loss: 0.6957 - val_accuracy: 0.5024\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5018 - val_loss: 0.6959 - val_accuracy: 0.5020\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5017 - val_loss: 0.6948 - val_accuracy: 0.5031\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.5032 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6986 - accuracy: 0.5026 - val_loss: 0.6944 - val_accuracy: 0.5032\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6986 - accuracy: 0.5026 - val_loss: 0.6938 - val_accuracy: 0.5039\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6990 - accuracy: 0.5012 - val_loss: 0.6968 - val_accuracy: 0.5023\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6985 - accuracy: 0.5024 - val_loss: 0.6940 - val_accuracy: 0.5030\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6989 - accuracy: 0.5014 - val_loss: 0.6953 - val_accuracy: 0.5026\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5015 - val_loss: 0.6953 - val_accuracy: 0.5030\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.5013 - val_loss: 0.6955 - val_accuracy: 0.5025\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5017 - val_loss: 0.6948 - val_accuracy: 0.5026\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5021 - val_loss: 0.6959 - val_accuracy: 0.5027\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6994 - accuracy: 0.5011 - val_loss: 0.7106 - val_accuracy: 0.4995\n"
     ]
    }
   ],
   "source": [
    "model, train_metrics = train_model(data, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5966797 ]\n",
      " [0.59375   ]\n",
      " [0.5957031 ]\n",
      " [0.5966797 ]\n",
      " [0.58984375]\n",
      " [0.5908203 ]\n",
      " [0.609375  ]\n",
      " [0.60839844]\n",
      " [0.6044922 ]\n",
      " [0.58203125]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAAIhCAYAAAABw3F3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABLyElEQVR4nO3df3zP9f7/8fvbbG8z9s6s/SqEEE1hxFZCGDI/jopaZ1lpKuIsUx05Rb8sKiSFVH6lVqciRTt0/CjZ/FhWfqVfhNPW0EyG/fL6/tHX+9PbRht72o/37Xq5vC6X9no938/38/U6l855nPvz9Xy+bZZlWQIAAAAMqFHRAwAAAED1RbEJAAAAYyg2AQAAYAzFJgAAAIyh2AQAAIAxFJsAAAAwhmITAAAAxlBsAgAAwBiKTQAAABhDsQlUAd98843uvvtuNW7cWLVq1VKdOnXUrl07TZkyRb/99pvR7966dau6dOkih8Mhm82m6dOnl/t32Gw2TZw4sdz7/Svz58+XzWaTzWbT2rVri123LEtXXnmlbDabunbtel7f8eqrr2r+/Pll+szatWvPOiYAqGpqVvQAAJzb3LlzNWLECLVo0UIPP/ywWrVqpYKCAm3ZskWzZ89WSkqKlixZYuz777nnHuXm5iopKUn16tXTFVdcUe7fkZKSossvv7zc+y2tunXr6o033ihWUK5bt04//vij6tate959v/rqq/L391dsbGypP9OuXTulpKSoVatW5/29AFBZUGwClVhKSooeeOAB9ezZU0uXLpXdbnde69mzpxISEpScnGx0DNu3b1dcXJz69Olj7Ds6depkrO/SGDJkiBYvXqxXXnlFvr6+zvNvvPGGwsPDdfTo0YsyjoKCAtlsNvn6+lb4MwGA8sI0OlCJTZo0STabTa+99ppLoXmal5eX+vfv7/z71KlTmjJliq666irZ7XYFBATorrvu0oEDB1w+17VrV4WGhmrz5s3q3LmzateurSZNmui5557TqVOnJP3fFHNhYaFmzZrlnG6WpIkTJzr/+c9Of2bv3r3Oc6tXr1bXrl1Vv359eXt7q2HDhrrlllt0/PhxZ5uSptG3b9+uAQMGqF69eqpVq5batGmjBQsWuLQ5Pd38zjvvaPz48QoJCZGvr6969Oih3bt3l+4hS7rjjjskSe+8847zXE5Ojj744APdc889JX7mySefVMeOHeXn5ydfX1+1a9dOb7zxhizLcra54oortGPHDq1bt875/E4nw6fHvmjRIiUkJOiyyy6T3W7XDz/8UGwa/dChQ2rQoIEiIiJUUFDg7H/nzp3y8fFRTExMqe8VAC42ik2gkioqKtLq1asVFhamBg0alOozDzzwgB599FH17NlTy5Yt09NPP63k5GRFRETo0KFDLm0zMzN155136u9//7uWLVumPn36aNy4cXrrrbckSX379lVKSook6dZbb1VKSorz79Lau3ev+vbtKy8vL7355ptKTk7Wc889Jx8fH+Xn55/1c7t371ZERIR27NihGTNm6MMPP1SrVq0UGxurKVOmFGv/2GOP6eeff9brr7+u1157Td9//7369eunoqKiUo3T19dXt956q958803nuXfeeUc1atTQkCFDznpv9913n9577z19+OGHGjRokEaNGqWnn37a2WbJkiVq0qSJ2rZt63x+Z77yMG7cOO3bt0+zZ8/Wxx9/rICAgGLf5e/vr6SkJG3evFmPPvqoJOn48eO67bbb1LBhQ82ePbtU9wkAFcICUCllZmZakqzbb7+9VO137dplSbJGjBjhcn7jxo2WJOuxxx5znuvSpYslydq4caNL21atWlm9evVyOSfJGjlypMu5CRMmWCX918e8efMsSdaePXssy7Ks999/35Jkpaenn3PskqwJEyY4/7799tstu91u7du3z6Vdnz59rNq1a1tHjhyxLMuy1qxZY0mybr75Zpd27733niXJSklJOef3nh7v5s2bnX1t377dsizL6tChgxUbG2tZlmVdffXVVpcuXc7aT1FRkVVQUGA99dRTVv369a1Tp045r53ts6e/78YbbzzrtTVr1ricnzx5siXJWrJkiTV06FDL29vb+uabb855jwBQ0Ug2gWpizZo1klRsIcp1112nli1b6r///a/L+aCgIF133XUu56655hr9/PPP5TamNm3ayMvLS8OHD9eCBQv0008/lepzq1evVvfu3YslurGxsTp+/HixhPXPrxJIf9yHpDLdS5cuXdS0aVO9+eab2rZtmzZv3nzWKfTTY+zRo4ccDoc8PDzk6empJ554QocPH1ZWVlapv/eWW24pdduHH35Yffv21R133KEFCxbo5ZdfVuvWrUv9eQCoCBSbQCXl7++v2rVra8+ePaVqf/jwYUlScHBwsWshISHO66fVr1+/WDu73a4TJ06cx2hL1rRpU3322WcKCAjQyJEj1bRpUzVt2lQvvfTSOT93+PDhs97H6et/dua9nH6/tSz3YrPZdPfdd+utt97S7Nmz1bx5c3Xu3LnEtps2bVJkZKSkP3YL+PLLL7V582aNHz++zN9b0n2ea4yxsbE6efKkgoKCeFcTQJVAsQlUUh4eHurevbvS0tKKLfApyemCKyMjo9i1X375Rf7+/uU2tlq1akmS8vLyXM6f+V6oJHXu3Fkff/yxcnJylJqaqvDwcMXHxyspKems/devX/+s9yGpXO/lz2JjY3Xo0CHNnj1bd99991nbJSUlydPTU5988okGDx6siIgItW/f/ry+s6SFVmeTkZGhkSNHqk2bNjp8+LDGjh17Xt8JABcTxSZQiY0bN06WZSkuLq7EBTUFBQX6+OOPJUk33XSTJDkX+Jy2efNm7dq1S927dy+3cZ1eUf3NN9+4nD89lpJ4eHioY8eOeuWVVyRJX3311Vnbdu/eXatXr3YWl6ctXLhQtWvXNrYt0GWXXaaHH35Y/fr109ChQ8/azmazqWbNmvLw8HCeO3HihBYtWlSsbXmlxUVFRbrjjjtks9n06aefKjExUS+//LI+/PDDC+4bAExin02gEgsPD9esWbM0YsQIhYWF6YEHHtDVV1+tgoICbd26Va+99ppCQ0PVr18/tWjRQsOHD9fLL7+sGjVqqE+fPtq7d68ef/xxNWjQQA899FC5jevmm2+Wn5+fhg0bpqeeeko1a9bU/PnztX//fpd2s2fP1urVq9W3b181bNhQJ0+edK747tGjx1n7nzBhgj755BN169ZNTzzxhPz8/LR48WItX75cU6ZMkcPhKLd7OdNzzz33l2369u2rqVOnKjo6WsOHD9fhw4f1wgsvlLg9VevWrZWUlKR3331XTZo0Ua1atc7rPcsJEyboiy++0MqVKxUUFKSEhAStW7dOw4YNU9u2bdW4ceMy9wkAFwPFJlDJxcXF6brrrtO0adM0efJkZWZmytPTU82bN1d0dLQefPBBZ9tZs2apadOmeuONN/TKK6/I4XCod+/eSkxMLPEdzfPl6+ur5ORkxcfH6+9//7suueQS3XvvverTp4/uvfdeZ7s2bdpo5cqVmjBhgjIzM1WnTh2FhoZq2bJlznceS9KiRQtt2LBBjz32mEaOHKkTJ06oZcuWmjdvXpl+iceUm266SW+++aYmT56sfv366bLLLlNcXJwCAgI0bNgwl7ZPPvmkMjIyFBcXp99//12NGjVy2Ye0NFatWqXExEQ9/vjjLgn1/Pnz1bZtWw0ZMkTr16+Xl5dXedweAJQrm2X9aQdiAAAAoBzxziYAAACModgEAACAMRSbAAAAMIZiEwAAAMZQbAIAAMAYik0AAAAYQ7EJAAAAY6rppu7fVfQAABji0+jpih4CAENyfy7+k68Xi3fDO4z1fWLfO8b6rgpINgEAAGBMNU02AQAASs9mI38zhWITAAC4PRuTvcbwZAEAAGAMySYAAHB7TKObw5MFAACAMSSbAADA7ZFsmsOTBQAAgDEkmwAAwO3ZbLaKHkK1RbIJAAAAY0g2AQAAyN+ModgEAABujwVC5vBkAQAAYAzJJgAAcHskm+bwZAEAAGAMySYAAHB7NvI3Y3iyAAAAMIZkEwAAuD3e2TSHJwsAAABjSDYBAIDbI9k0h2ITAAC4PYpNc3iyAAAAMIZkEwAAuD2bbBU9hGqLZBMAAADGkGwCAAC3xzub5vBkAQAAYAzJJgAAcHskm+bwZAEAAGAMySYAAHB7JJvmUGwCAAAw2WsMTxYAAADGkGwCAAC3xzS6OTxZAAAAGEOyCQAA3B7Jpjk8WQAAABhDsgkAANyejfzNGJ4sAAAAjKHYBAAAbs9mq2HsKIvExER16NBBdevWVUBAgAYOHKjdu3e7tImNjZXNZnM5OnXq5NImLy9Po0aNkr+/v3x8fNS/f38dOHDApU12drZiYmLkcDjkcDgUExOjI0eOuLTZt2+f+vXrJx8fH/n7+2v06NHKz88v0z1RbAIAALd3ZvFWnkdZrFu3TiNHjlRqaqpWrVqlwsJCRUZGKjc316Vd7969lZGR4TxWrFjhcj0+Pl5LlixRUlKS1q9fr2PHjikqKkpFRUXONtHR0UpPT1dycrKSk5OVnp6umJgY5/WioiL17dtXubm5Wr9+vZKSkvTBBx8oISGhbM/WsiyrTJ+oEr6r6AEAMMSn0dMVPQQAhuT+vKjCvrvBNU8Z6/uHzY8qLy/P5Zzdbpfdbv/Lzx48eFABAQFat26dbrzxRkl/JJtHjhzR0qVLS/xMTk6OLr30Ui1atEhDhgyRJP3yyy9q0KCBVqxYoV69emnXrl1q1aqVUlNT1bFjR0lSamqqwsPD9e2336pFixb69NNPFRUVpf379yskJESSlJSUpNjYWGVlZcnX17dU90+yCQAA3J7JafTExETnVPXpIzExsVTjysnJkST5+fm5nF+7dq0CAgLUvHlzxcXFKSsry3ktLS1NBQUFioyMdJ4LCQlRaGioNmzYIElKSUmRw+FwFpqS1KlTJzkcDpc2oaGhzkJTknr16qW8vDylpaWV+tmyGh0AAMCgcePGacyYMS7nSpNqWpalMWPG6IYbblBoaKjzfJ8+fXTbbbepUaNG2rNnjx5//HHddNNNSktLk91uV2Zmpry8vFSvXj2X/gIDA5WZmSlJyszMVEBAQLHvDAgIcGkTGBjocr1evXry8vJytikNik0AAOD2TG59VNop8zM9+OCD+uabb7R+/XqX86enxiUpNDRU7du3V6NGjbR8+XINGjTorP1ZluXyDmlJ75OeT5u/wjQ6AABAJTNq1CgtW7ZMa9as0eWXX37OtsHBwWrUqJG+//57SVJQUJDy8/OVnZ3t0i4rK8uZVAYFBenXX38t1tfBgwdd2pyZYGZnZ6ugoKBY4nkuFJsAAMDtVZatjyzL0oMPPqgPP/xQq1evVuPGjf/yM4cPH9b+/fsVHBwsSQoLC5Onp6dWrVrlbJORkaHt27crIiJCkhQeHq6cnBxt2rTJ2Wbjxo3KyclxabN9+3ZlZGQ426xcuVJ2u11hYWGlviem0QEAACqJkSNH6u2339ZHH32kunXrOpNFh8Mhb29vHTt2TBMnTtQtt9yi4OBg7d27V4899pj8/f31t7/9zdl22LBhSkhIUP369eXn56exY8eqdevW6tGjhySpZcuW6t27t+Li4jRnzhxJ0vDhwxUVFaUWLVpIkiIjI9WqVSvFxMTo+eef12+//aaxY8cqLi6u1CvRJYpNAACAMieQpsyaNUuS1LVrV5fz8+bNU2xsrDw8PLRt2zYtXLhQR44cUXBwsLp166Z3331XdevWdbafNm2aatasqcGDB+vEiRPq3r275s+fLw8PD2ebxYsXa/To0c5V6/3799fMmTOd1z08PLR8+XKNGDFC119/vby9vRUdHa0XXnihTPfEPpsAqhT22QSqr4rcZ7NJ27IVUGXx09axxvquCipHGQ8AAIBqiWl0AACASjKNXh3xZAEAAGAMySYAAHB7lWWBUHXEkwUAAIAxJJsAAMDtleXnF1E2JJsAAAAwhmQTAAC4PRv5mzEUmwAAwO2xQMgcniwAAACMIdkEAABggZAxJJsAAAAwhmQTAACA+M0YHi0AAACMIdkEAADgnU1jSDYBAABgDMkmAAAAyaYxFJsAAADM9RrDowUAAIAxJJsAAMDtWUyjG0OyCQAAAGNINgEAAAg2jSHZBAAAgDEkmwAAADWINk0h2QQAAIAxJJsAAACsRjeGZBMAAADGkGwCAAAQbBpDsQkAAMACIWOYRgcAAIAxJJsAAAAsEDKGZBMAAADGkGwCAAAQbBpDsgkAAABjSDYBAABYjW4MySYAAACMIdkEAAAg2DSGYhMAALg9i62PjGEaHQAAAMaQbAIAALBAyBiSTQAAABhDsgkAAECwaQzJJgAAAIwh2QQAAGA1ujEkmwAAADCGZBMAAIDV6MZQbAIAAFBrGsM0OgAAAIwh2QQAAGCBkDEkmwAAADCGZBMAAIBk0xiSTQAAABhDsgkAAED8ZgyPFgAAAMaQbAIAAPDOpjEUmwAAANSaxjCNDgAAAGNINgEAgNuz+G10Y0g2AQAAYAzJJgAAAAuEjCHZBAAAgDEkm6gSjh07rpdeWqzPPkvR4cM5atWqiR57LE7XXNO8WNsnnpipd9/9j8aNu1exsQMqYLSAexo7op/6926v5k2DdfJkgVLTvtfjzyXp+58ynW1yf15U4mfHT3pH0+esUD2Hj8aPGaTunVvr8hA/Hf7td3288is9/eL7Ovr7CWf7915/SNe0aqhL6/vqyNHjWrN+u/6V+K4ys44421weUl/Tnh6qLhGtdOJkvt77KEWPPfu2CgqKjD0DVGEEm8ZQbKJK+Ne/Xtb33/+sKVPGKCDAT8uWrdXddz+uFSteVWBgfWe7zz5L0ddff6eAAL8KHC3gnm7oeJVeW/iZ0r7+STVremjCw7dq2aJHFdbjnzp+Ik+S1KT9gy6fiex6jV6dcq+WrtgsSQoOrKfgwHp67Nl39O33/1PDy/310rOxCg68RH9/4GXn5z5P2aXnX1mmzKwjCgny06Txd2jx7NHqPugpSVKNGjZ9OC9Bh377XT1vfVp+l9TRa1OHy2aTxk4oueAFYIbNsiyrogdR/r6r6AGgHJ08mad27Qbr1Vf/pa5dOzjPDxgwWl27dtBDD8VIkn799bBuuy1Bb7zxpO677ynddVd/ks1qyKfR0xU9BJSSv19d/bz1VUXe9oy+3LS7xDZJr8Wrbp1a6hv93Fn7+dvN1+mN6ffr0pb3qqjoVIltbu7RVu/OjVe9ZveosLBIkV2v0ftvJqh5p384085b+3XSnBfidEXYSP1+7OQF3x/K39mS74uhaUySsb5/XHS7sb6rggpNNg8cOKBZs2Zpw4YNyszMlM1mU2BgoCIiInT//ferQYMGFTk8VBKFhUUqKjolu93L5XytWl766qudkqRTp07p4YenatiwQWrWrFFFDBPAGXzrekuSso/klng9wN9XvW+6VsMTXjt3P77eOnrsxFkLzXoOHw0ZGKHUtO9VWPjHFPl17a7Uzt0HXKbVP1v3jWrV8lLb1o31ecqu87gjVGssEDKmworN9evXq0+fPmrQoIEiIyMVGRkpy7KUlZWlpUuX6uWXX9ann36q66+//pz95OXlKS8vz+Wc3Z5frDBB1VWnTm21bXuVXn01SU2aXC5//0v0ySef6+uvv1OjRiGSpLlzP1DNmjV01139Kni0AE577vE79eWm3dr53YESr995S2f9nntSHyVvOWsffpfU0T9HDdSbb68pdu3pfw7RfUN7yqe2XRu/+l633j3VeS3w0kuUdSjHpf2Ro8eVl1egwEsd53lHAM5HhRWbDz30kO69915NmzbtrNfj4+O1efPmc/aTmJioJ5980uXchAkPauLEUeU2VlS8KVPG6LHHXtKNN8bKw6OGWrVqqqioLtq580dt3/6DFi5cpg8/nC4b/88UqBSmPj1UoVc1UI9bz/7aQ8zgG/Xu0g3Kyyso8XrdOrX0wbwEffvD/zRp+pJi16fPWa4F765Tw8v8NS5+oOZOu0+33P2i83pJb4nZbDZVx5fHUA74nw9jKqzY3L59u956662zXr/vvvs0e/bsv+xn3LhxGjNmjMs5u33fBY8PlUvDhsF6663ndPz4SR07dlwBAX6Kj5+syy8P1JYtO3T4cI66dbvH2b6o6JQmT35TCxcu0+rVb1TgyAH388KTMerbo60iBz+rXzKzS2wT0aG5WlwZoqEPvlLi9To+tbR04SPKPX5Stw9/yTk9/meHs4/pcPYx/bAnU9/+8D99v3GGrmt3pTZ99YN+PXhEHdo0dWl/iW9teXnVLJZ4AjCrworN4OBgbdiwQS1atCjxekpKioKDg/+yH7vdLrvdfsZZptCrq9q1a6l27VrKyTmm9eu36uGHYxUZGaGIiDYu7YYNe0IDBnTToEE9KmaggJt68am71L9XmHoPmaSf9x88a7uhQ7rqq29+0rZdxcOBunVq6aNFjygvr1C3DZt21uTzz07Pati9/viftU1f/aBHHhygoACHMrP+KC6739haJ0/ma+u2Pedza6ju+LlKYyqs2Bw7dqzuv/9+paWlqWfPngoMDJTNZlNmZqZWrVql119/XdOnT6+o4aGS+eKLr2RZlho3vkz79mVoypR5atz4Mg0a1EOenjVVr56vS3tPz5ry96+nJk0ur6ARA+5n2jNDNbh/uIbETdex3JPOdyNzjh7XyT8VjHXr1NLf+l6ncc+8XayPOj61tGzRo6rt7aVh/5gt37rezoVGBw8f1alTlsKubaL2bZooZfN3ys7JVeOGAfrXmFv0495ftfGrHyRJn32+Td9+/z+9Pu1+jZ+UpHqX+GjS+Ds0L2ktK9GBi6zCis0RI0aofv36mjZtmubMmaOioj+mSDw8PBQWFqaFCxdq8ODBFTU8VDK//56rqVMXKjPzkC65pK4iIyP00EMx8vRkq1igshge88dMwn/eG+9y/r6E1/TW+184/761X7hsNunfy1KK9dG29RW6rt2VkqTtX7zocq3l9Q9p34FDOnkyXwN6d9D4hwbJx9uuzIM5WrX2Gw198BXl5xdKkk6dsjTo7hc1/Zmh+uyDx/+0qfs75XrPqEZINo2pFPtsFhQU6NChQ5Ikf39/eXp6XmCP7LMJVFfsswlUXxW6z+awfxvr+8c3bjPWd1VQKWIhT0/PUr2fCQAAYIJFsGlMjYoeAAAAQIWrYTN3lEFiYqI6dOigunXrKiAgQAMHDtTu3a6/wGVZliZOnKiQkBB5e3ura9eu2rFjh0ubvLw8jRo1Sv7+/vLx8VH//v114IDrnrfZ2dmKiYmRw+GQw+FQTEyMjhw54tJm37596tevn3x8fOTv76/Ro0crPz+/TPdEsQkAAFBJrFu3TiNHjlRqaqpWrVqlwsJCRUZGKjf3/36Ja8qUKZo6dapmzpypzZs3KygoSD179tTvv//ubBMfH68lS5YoKSlJ69ev17FjxxQVFeVcIyNJ0dHRSk9PV3JyspKTk5Wenq6YmBjn9aKiIvXt21e5ublav369kpKS9MEHHyghIaFM91Qp3tksf7yzCVRXvLMJVF8V+c5mk/s+MNb3T3NuOe/PHjx4UAEBAVq3bp1uvPFGWZalkJAQxcfH69FHH5X0R4oZGBioyZMn67777lNOTo4uvfRSLVq0SEOGDJEk/fLLL2rQoIFWrFihXr16adeuXWrVqpVSU1PVsWNHSVJqaqrCw8P17bffqkWLFvr0008VFRWl/fv3KyTkj1/sS0pKUmxsrLKysuTr61vyoM9AsgkAAGBQXl6ejh496nKc+VPbZ5OT88c+sX5+fpKkPXv2KDMzU5GRkc42drtdXbp00YYNGyRJaWlpKigocGkTEhKi0NBQZ5uUlBQ5HA5noSlJnTp1ksPhcGkTGhrqLDQlqVevXsrLy1NaWlqp759iEwAAwOA7m4mJic73Ik8fiYmJfzkky7I0ZswY3XDDDQoNDZUkZWZmSpICAwNd2gYGBjqvZWZmysvLS/Xq1Ttnm4CAgGLfGRAQ4NLmzO+pV6+evLy8nG1Ko1KsRgcAAKiuSv5p7TN//bC4Bx98UN98843Wr19f7NrpX806zbKsYufOdGabktqfT5u/QrIJAABQw9xht9vl6+vrcvxVsTlq1CgtW7ZMa9as0eWX/9+v4QUFBUlSsWQxKyvLmUIGBQUpPz9f2dnZ52zz66+/FvvegwcPurQ583uys7NVUFBQLPE8F4pNAACASsKyLD344IP68MMPtXr1ajVu3NjleuPGjRUUFKRVq1Y5z+Xn52vdunWKiIiQJIWFhcnT09OlTUZGhrZv3+5sEx4erpycHG3atMnZZuPGjcrJyXFps337dmVkZDjbrFy5Una7XWFhYaW+J6bRAQAAyjAtbNLIkSP19ttv66OPPlLdunWdyaLD4ZC3t7dsNpvi4+M1adIkNWvWTM2aNdOkSZNUu3ZtRUdHO9sOGzZMCQkJql+/vvz8/DR27Fi1bt1aPXr88bOyLVu2VO/evRUXF6c5c+ZIkoYPH66oqCi1aNFCkhQZGalWrVopJiZGzz//vH777TeNHTtWcXFxpV6JLlFsAgAAVJrfRp81a5YkqWvXri7n582bp9jYWEnSI488ohMnTmjEiBHKzs5Wx44dtXLlStWtW9fZftq0aapZs6YGDx6sEydOqHv37po/f748PDycbRYvXqzRo0c7V633799fM2fOdF738PDQ8uXLNWLECF1//fXy9vZWdHS0XnjhhTLdE/tsAqhS2GcTqL4qdJ/N0UuN9f3TjIHG+q4KSDYBAIDbsyrJNHp1xAIhAAAAGEOyCQAAQPxmDI8WAAAAxpBsAgAAVJLV6NURySYAAACMIdkEAABgNboxFJsAAABMoxvDNDoAAACMIdkEAAAg2DSGZBMAAADGkGwCAAC3Z/HOpjEkmwAAADCGZBMAAIBk0xiSTQAAABhDsgkAAMCm7saQbAIAAMAYkk0AAADiN2MoNgEAAJhGN4Y6HgAAAMaQbAIAALD1kTEkmwAAADCGZBMAAIBk0xiSTQAAABhDsgkAANyexWp0Y0g2AQAAYAzJJgAAAPGbMRSbAAAATKMbQx0PAAAAY0g2AQAA2PrIGJJNAAAAGEOyCQAAQLJpDMkmAAAAjCHZBAAAINg0hmQTAAAAxpBsAgAAt2fxzqYxFJsAAABs6m4M0+gAAAAwhmQTAACAaXRjSDYBAABgDMkmAAAAwaYxJJsAAAAwhmQTAAC4vRrEb8bwaAEAAGAMySYAAHB7bLNpDsUmAABwexSb5jCNDgAAAGNINgEAgNuzEW0aQ7IJAAAAY0g2AQCA2yPYNIdkEwAAAMaQbAIAALdHsmkOySYAAACMIdkEAABuz0b8ZgzFJgAAcHtMo5tDHQ8AAABjSDYBAIDbq0GyaQzJJgAAAIwh2QQAAG6PdzbNIdkEAACAMSSbAADA7ZFsmkOyCQAAAGMuuNgsKipSenq6srOzy2M8AAAAF53NZjN2uLsyF5vx8fF64403JP1RaHbp0kXt2rVTgwYNtHbt2vIeHwAAgHG2GuYOd1fmR/D+++/r2muvlSR9/PHH2rNnj7799lvFx8dr/Pjx5T5AAAAAVF1lLjYPHTqkoKAgSdKKFSt02223qXnz5ho2bJi2bdtW7gMEAAAwzWYzd7i7MhebgYGB2rlzp4qKipScnKwePXpIko4fPy4PD49yHyAAAACqrjJvfXT33Xdr8ODBCg4Ols1mU8+ePSVJGzdu1FVXXVXuAwQAADCNBNKcMhebEydOVGhoqPbv36/bbrtNdrtdkuTh4aF//vOf5T5AAAAAVF3ntan7rbfeWuzc0KFDL3gwAAAAFYFk05xSFZszZswodYejR48+78EAAACgeilVsTlt2rRSdWaz2Sg2AQBAlVODZNOYUhWbe/bsMT0OAACACsM0ujnnva99fn6+du/ercLCwvIcDwAAgFv7/PPP1a9fP4WEhMhms2np0qUu12NjY4v9JGanTp1c2uTl5WnUqFHy9/eXj4+P+vfvrwMHDri0yc7OVkxMjBwOhxwOh2JiYnTkyBGXNvv27VO/fv3k4+Mjf39/jR49Wvn5+WW6nzIXm8ePH9ewYcNUu3ZtXX311dq3b5+kP97VfO6558raHQAAQIWrTJu65+bm6tprr9XMmTPP2qZ3797KyMhwHitWrHC5Hh8fryVLligpKUnr16/XsWPHFBUVpaKiImeb6OhopaenKzk5WcnJyUpPT1dMTIzzelFRkfr27avc3FytX79eSUlJ+uCDD5SQkFCm+ynzavRx48bp66+/1tq1a9W7d2/n+R49emjChAlsfwQAAHAB+vTpoz59+pyzjd1ud/6i45lycnL0xhtvaNGiRc4f33nrrbfUoEEDffbZZ+rVq5d27dql5ORkpaamqmPHjpKkuXPnKjw8XLt371aLFi20cuVK7dy5U/v371dISIgk6cUXX1RsbKyeffZZ+fr6lup+ypxsLl26VDNnztQNN9wg25/K9VatWunHH38sa3cAAAAVzlbDZuzIy8vT0aNHXY68vLwLGu/atWsVEBCg5s2bKy4uTllZWc5raWlpKigoUGRkpPNcSEiIQkNDtWHDBklSSkqKHA6Hs9CUpE6dOsnhcLi0CQ0NdRaaktSrVy/l5eUpLS2t1GMtc7F58OBBBQQEFDufm5vrUnwCAABASkxMdL4XefpITEw87/769OmjxYsXa/Xq1XrxxRe1efNm3XTTTc4CNjMzU15eXqpXr57L5wIDA5WZmelsU1I9FxAQ4NImMDDQ5Xq9evXk5eXlbFMaZZ5G79Chg5YvX65Ro0ZJkrPAPB29AgAAVDUm87Jx48ZpzJgxLudO/wLj+RgyZIjzn0NDQ9W+fXs1atRIy5cv16BBg876OcuyXILBkkLC82nzV8pcbCYmJqp3797auXOnCgsL9dJLL2nHjh1KSUnRunXrytodAABAtWa32y+ouPwrwcHBatSokb7//ntJUlBQkPLz85Wdne2SbmZlZSkiIsLZ5tdffy3W18GDB51pZlBQkDZu3OhyPTs7WwUFBcUSz3Mp8zR6RESEvvzySx0/flxNmzbVypUrFRgYqJSUFIWFhZW1OwAAgApXmVajl9Xhw4e1f/9+BQcHS5LCwsLk6empVatWOdtkZGRo+/btzmIzPDxcOTk52rRpk7PNxo0blZOT49Jm+/btysjIcLZZuXKl7HZ7mWq+8/pt9NatW2vBggXn81EAAIBKpzItOzl27Jh++OEH59979uxRenq6/Pz85Ofnp4kTJ+qWW25RcHCw9u7dq8cee0z+/v7629/+JklyOBwaNmyYEhISVL9+ffn5+Wns2LFq3bq1c3V6y5Yt1bt3b8XFxWnOnDmSpOHDhysqKkotWrSQJEVGRqpVq1aKiYnR888/r99++01jx45VXFxcqVeiS+dZbBYVFWnJkiXatWuXbDabWrZsqQEDBqhmzfPqDgAAAP/fli1b1K1bN+ffp9/3HDp0qGbNmqVt27Zp4cKFOnLkiIKDg9WtWze9++67qlu3rvMz06ZNU82aNTV48GCdOHFC3bt31/z58+Xh4eFss3jxYo0ePdq5ar1///4ue3t6eHho+fLlGjFihK6//np5e3srOjpaL7zwQpnux2ZZllWWD2zfvl0DBgxQZmams/L97rvvdOmll2rZsmVq3bp1mQZgxncVPQAAhvg0erqihwDAkNyfF1XYd0d8uN5Y3xsG3WCs76qgzO9s3nvvvbr66qt14MABffXVV/rqq6+0f/9+XXPNNRo+fLiJMQIAAKCKKvO899dff60tW7a4rG6qV6+enn32WXXo0KFcBwcAAHAxVKZ3NqubMiebLVq0KHGpfFZWlq688spyGRQAAACqh1Ilm0ePHnX+86RJkzR69GhNnDhRnTp1kiSlpqbqqaee0uTJk82MEgAAwCBbmeM3lFapis1LLrnEZad4y7I0ePBg57nTa4z69eunoqIiA8MEAABAVVSqYnPNmjWmxwEAAFBheGfTnFIVm126dDE9DgAAAFRD570L+/Hjx7Vv3z7l5+e7nL/mmmsueFAAAAAXk41o05gyF5sHDx7U3XffrU8//bTE67yzCQAAqhpqTXPKvPYqPj5e2dnZSk1Nlbe3t5KTk7VgwQI1a9ZMy5YtMzFGAAAAVFFlTjZXr16tjz76SB06dFCNGjXUqFEj9ezZU76+vkpMTFTfvn1NjBMAAMAYkk1zypxs5ubmKiAgQJLk5+engwcPSpJat26tr776qnxHBwAAgCrtvH5BaPfu3ZKkNm3aaM6cOfrf//6n2bNnKzg4uNwHCAAAYJrNZu5wd2WeRo+Pj1dGRoYkacKECerVq5cWL14sLy8vzZ8/v7zHBwAAgCqszMXmnXfe6fzntm3bau/evfr222/VsGFD+fv7l+vgAOBMp6zCih4CgGqoBgmkMee9z+ZptWvXVrt27cpjLAAAAKhmSlVsjhkzptQdTp069bwHAwAAUBFINs0pVbG5devWUnXG7vsAAKAqqmGzKnoI1Vapis01a9aYHgcAAACqoQt+ZxMAAKCqYxrdnDLvswkAAACUFskmAABwe6Rv5vBsAQAAYAzJJgAAcHusRjfnvJLNRYsW6frrr1dISIh+/vlnSdL06dP10UcflevgAAAAULWVudicNWuWxowZo5tvvllHjhxRUVGRJOmSSy7R9OnTy3t8AAAAxtWwmTvcXZmLzZdffllz587V+PHj5eHh4Tzfvn17bdu2rVwHBwAAcDHUMHi4uzI/gz179qht27bFztvtduXm5pbLoAAAAFA9lLnYbNy4sdLT04ud//TTT9WqVavyGBMAAMBFxTS6OWVejf7www9r5MiROnnypCzL0qZNm/TOO+8oMTFRr7/+uokxAgAAoIoqc7F59913q7CwUI888oiOHz+u6OhoXXbZZXrppZd0++23mxgjAACAUTa2PjLmvPbZjIuLU1xcnA4dOqRTp04pICCgvMcFAACAauCCNnX39/cvr3EAAABUGN6tNKfMxWbjxo1ls539P5GffvrpggYEAACA6qPMxWZ8fLzL3wUFBdq6dauSk5P18MMPl9e4AAAALhr2wzSnzMXmP/7xjxLPv/LKK9qyZcsFDwgAAOBi47fRzSm3Qr5Pnz764IMPyqs7AAAAVAMXtEDoz95//335+fmVV3cAAAAXDQuEzClzsdm2bVuXBUKWZSkzM1MHDx7Uq6++Wq6DAwAAQNVW5mJz4MCBLn/XqFFDl156qbp27aqrrrqqvMYFAABw0bBAyJwyFZuFhYW64oor1KtXLwUFBZkaEwAAAKqJMhXyNWvW1AMPPKC8vDxT4wEAALjoatjMHe6uzKlxx44dtXXrVhNjAQAAQDVT5nc2R4wYoYSEBB04cEBhYWHy8fFxuX7NNdeU2+AAAAAuBvbZNKfUxeY999yj6dOna8iQIZKk0aNHO6/ZbDZZliWbzaaioqLyHyUAAIBBTHebU+pic8GCBXruuee0Z88ek+MBAABANVLqYtOy/oiXGzVqZGwwAAAAFYGtj8wp07P982buAAAAwF8p0wKh5s2b/2XB+dtvv13QgAAAAC42FgiZU6Zi88knn5TD4TA1FgAAAFQzZSo2b7/9dgUEBJgaCwAAQIVgNbo5pX5nk/c1AQAAUFZlXo0OAABQ3ZBsmlPqYvPUqVMmxwEAAFBh2PrIHJ4tAAAAjCnzb6MDAABUN2x9ZA7JJgAAAIwh2QQAAG6PBULmkGwCAADAGJJNAADg9kjfzOHZAgAAwBiSTQAA4PZ4Z9Mcik0AAOD2bGx9ZAzT6AAAADCGZBMAALg9ptHNIdkEAACAMSSbAADA7ZG+mcOzBQAAgDEkmwAAwO3VYDW6MSSbAAAAMIZkEwAAuD1Wo5tDsQkAANwexaY5TKMDAADAGIpNAADg9jwMHmX1+eefq1+/fgoJCZHNZtPSpUtdrluWpYkTJyokJETe3t7q2rWrduzY4dImLy9Po0aNkr+/v3x8fNS/f38dOHDApU12drZiYmLkcDjkcDgUExOjI0eOuLTZt2+f+vXrJx8fH/n7+2v06NHKz88v0/1QbAIAAFQiubm5uvbaazVz5swSr0+ZMkVTp07VzJkztXnzZgUFBalnz576/fffnW3i4+O1ZMkSJSUlaf369Tp27JiioqJUVFTkbBMdHa309HQlJycrOTlZ6enpiomJcV4vKipS3759lZubq/Xr1yspKUkffPCBEhISynQ/NsuyquFa/+8qegAADPFuOKGihwDAkBP73qmw756UvspY34+16Xnen7XZbFqyZIkGDhwo6Y9UMyQkRPHx8Xr00Ucl/ZFiBgYGavLkybrvvvuUk5OjSy+9VIsWLdKQIUMkSb/88osaNGigFStWqFevXtq1a5datWql1NRUdezYUZKUmpqq8PBwffvtt2rRooU+/fRTRUVFaf/+/QoJCZEkJSUlKTY2VllZWfL19S3VPZBsAgAAGJSXl6ejR4+6HHl5eefV1549e5SZmanIyEjnObvdri5dumjDhg2SpLS0NBUUFLi0CQkJUWhoqLNNSkqKHA6Hs9CUpE6dOsnhcLi0CQ0NdRaaktSrVy/l5eUpLS2t1GOm2AQAAG6vhs3ckZiY6Hwv8vSRmJh4XuPMzMyUJAUGBrqcDwwMdF7LzMyUl5eX6tWrd842AQEBxfoPCAhwaXPm99SrV09eXl7ONqXB1kcAAAAGjRs3TmPGjHE5Z7fbL6hPm811rybLsoqdO9OZbUpqfz5t/grJJgAAcHsmk0273S5fX1+X43yLzaCgIEkqlixmZWU5U8igoCDl5+crOzv7nG1+/fXXYv0fPHjQpc2Z35Odna2CgoJiiee5UGwCAAC352Ezd5Snxo0bKygoSKtW/d+Cpvz8fK1bt04RERGSpLCwMHl6erq0ycjI0Pbt251twsPDlZOTo02bNjnbbNy4UTk5OS5ttm/froyMDGeblStXym63KywsrNRjZhodAACgEjl27Jh++OEH59979uxRenq6/Pz81LBhQ8XHx2vSpElq1qyZmjVrpkmTJql27dqKjo6WJDkcDg0bNkwJCQmqX7++/Pz8NHbsWLVu3Vo9evSQJLVs2VK9e/dWXFyc5syZI0kaPny4oqKi1KJFC0lSZGSkWrVqpZiYGD3//PP67bffNHbsWMXFxZV6JbpEsQkAAFCpfq5yy5Yt6tatm/Pv0+97Dh06VPPnz9cjjzyiEydOaMSIEcrOzlbHjh21cuVK1a1b1/mZadOmqWbNmho8eLBOnDih7t27a/78+fLw+L9t5hcvXqzRo0c7V63379/fZW9PDw8PLV++XCNGjND1118vb29vRUdH64UXXijT/bDPJoAqhX02geqrIvfZnLbd3D6bD4We/z6b1QHJJgAAcHs1bNUwe6skWCAEAAAAY0g2AQCA26tM72xWNySbAAAAMIZkEwAAuD2Pv26C80SyCQAAAGNINgEAgNvjnU1zKDYBAIDbY+sjc5hGBwAAgDEkmwAAwO15MI1uDMkmAAAAjCHZBAAAbo8FQuaQbAIAAMAYkk0AAOD2SDbNIdkEAACAMSSbAADA7ZFsmkOxCQAA3J4Hm7obwzQ6AAAAjCHZBAAAbo/0zRyeLQAAAIwh2QQAAG6PBULmkGwCAADAGJJNAADg9kg2zSHZBAAAgDEkmwAAwO2xz6Y5FJsAAMDtMY1uDtPoAAAAMIZkEwAAuD2STXNINgEAAGAMySYAAHB7JJvmkGwCAADAGJJNAADg9jxINo0h2QQAAIAxJJsAAMDt1WBTd2MoNgEAgNtjqtccni0AAACMIdkEAABuj62PzCHZBAAAgDEkmwAAwO2x9ZE5JJsAAAAwhmQTVc6cOf/W1KkLdddd/TV+fJwKCgo1ffpb+vzzLdq/P1N16vgoIuJaJSQMVWBg/YoeLuA2xo4coIG9O6h50xCdOJmvjWnfaXziO/r+pwxnmxP73inxs489u1jT5nxS7PzSBY+qV7c2Gnzvi/p45RZJUudOLbXyvSdK7OeGqPFK++Yn/f3WGzV36gMltmnY9j4dPHy0rLeHao6tj8yh2ESV8s033+ndd5PVosUVznMnT+Zp584f9cADQ3TVVY119OgxTZr0uh544Bl9+OG0ihss4GY6d2yp2QtWKu2bn1TTo4YmPjJEn7w1Tm27P6zjJ/IkSVeE3e/ymciubTT7+eFa8ummYv2NGtZHllW8AEhN+65YP0+MHaybrg9V2jc/SZLe/zhFq9Z97dLmtRcfUC27J4UmcJFRbKLKyM09oYcfflHPPDNKs2a96zxft66P5s172qXtv/41XLfdlqBffslSSEjAxR4q4JYG3PWcy9/3JczW/vTX1LZ1Y3256VtJ0q8Hc1za9IsM07qUndq7L8vlfOuWDTU6rq9u6Ddee9Nmu1wrKChy6admTQ/17RGm2Qv+4zx3Mq9AJ//Uxt+vrrpGXK37H5lzYTeJaovV6ObwziaqjKeemq0uXdorIqLNX7Y9duy4bDabfH3rmB8YgBL51q0tSco+cqzE6wH+DvW+qa0WJK1xOe9dy0sLZo7SQ4/PK1acliSqZ5j8/erqrX+vO2ubO2+5UcdP5GnJ8o1luAO4kxo2c4e7q9TF5v79+3XPPfecs01eXp6OHj3qcuTl5V+kEeJiWb78c+3c+aMSEob+Zdu8vHy98MICRUV1UZ06tS/C6ACUZPITMfpy07fa+d2BEq///dYb9XvuSS1N3uxyfsqEGKVu+U6frEor1fcMHdJVq9Z9rQMZv521zV1DuurdjzboZF5B6W8AQLmo1MXmb7/9pgULFpyzTWJiohwOh8uRmMg0SXWSkXFQzz47V88/nyC73eucbQsKCvXQQ1NkWac0cWLJiwMAmDft6bvV+qqGGvrgy2dtc9fgLnp3yZfK+1MB2LdnmLpGXK2Hn1xYqu+5LMhPPbtcqwXvrj1rm47tmqlV88u14N01Z20D1DB4uLsKfWdz2bJl57z+008//WUf48aN05gxY1zO2e37LmhcqFx27PhBhw8f0aBB8c5zRUWntHnzDi1e/Im2bftQHh4eKigoVHz8ZB048KsWLHiWVBOoIFOfjFVUzzD1uO1J/S+z5LTx+utaqMWVlylm5AyX810jrlaTRoHK3P6Gy/l35jykLzd9q15DXN/PjhncRYezfz9nChp7ezelb9+rrdv2nOcdAbgQFVpsDhw4UDabrcTVhqfZbOd+2cFut8tut59x9tzpF6qWTp2u1ccfz3Q5N27cdDVpcrni4m51KTR//vkXLVw4SfXq+VbQaAH3Nu2pWPXv3UGRg5/Wz/sPnrXd0CHdlPbNT9q2yzUceOHVjzTvndUu59I+e16PPLVQyz/7qlg/dw3uorc/+EKFhUUlfo9PbbtuieqkJyYnncfdwJ38RbmBC1ChxWZwcLBeeeUVDRw4sMTr6enpCgsLu7iDQqVTp05tNW/eyOVc7dq1dMklvmrevJEKC4s0evRz2rnzR82Z84SKik7p4MFsSZLDUUdeXp4VMWzA7Ux/5h4NGRCh2+59UcdyTyjwUockKefocZd3JevW8dagvh31z2cWF+vj14M5JS4K2v+/w8WK167XX63GDQM1/xzT47f2C1fNmh5KWvrl+d4WgAtUocVmWFiYvvrqq7MWm3+VegKSlJl5SKtX/7HCdMCA0S7XFi6cpI4dW1fEsAC3c99dPSVJq/7tuuF63JhZeuv9z51/39Y/XDabTe99dGEFYOyQbkrZslu7f/jlnG0++nSTjuTkXtB3ofoj2DTHZlVgNffFF18oNzdXvXv3LvF6bm6utmzZoi5dupSx5+8ufHAAKiXvhhMqeggADDnbL0xdDJsPLjfWd4dL+xrruyqo0GSzc+fO57zu4+NzHoUmAABA2fDOpjn8ghAAAHB7bFFkDs8WAAAAxpBsAgAAt2ezsSDZFJJNAAAAGEOyCQAA3B7rg8wh2QQAAIAxJJsAAMDtsfWROSSbAAAAMIZkEwAAuD2CTXMoNgEAgNurQbVpDNPoAAAAMIZkEwAAuD2CTXNINgEAAGAMySYAAHB7bH1kDskmAAAAjCHZBAAAbo9g0xySTQAAABhDsgkAANweyaY5FJsAAMDtsam7OUyjAwAAwBiSTQAA4PYINs0h2QQAAKgkJk6cKJvN5nIEBQU5r1uWpYkTJyokJETe3t7q2rWrduzY4dJHXl6eRo0aJX9/f/n4+Kh///46cOCAS5vs7GzFxMTI4XDI4XAoJiZGR44cMXJPFJsAAMDt2WyWsaOsrr76amVkZDiPbdu2Oa9NmTJFU6dO1cyZM7V582YFBQWpZ8+e+v33351t4uPjtWTJEiUlJWn9+vU6duyYoqKiVFRU5GwTHR2t9PR0JScnKzk5Wenp6YqJibmwh3gWTKMDAABUIjVr1nRJM0+zLEvTp0/X+PHjNWjQIEnSggULFBgYqLffflv33XefcnJy9MYbb2jRokXq0aOHJOmtt95SgwYN9Nlnn6lXr17atWuXkpOTlZqaqo4dO0qS5s6dq/DwcO3evVstWrQo1/sh2QQAAG7PZvDIy8vT0aNHXY68vLyzjuX7779XSEiIGjdurNtvv10//fSTJGnPnj3KzMxUZGSks63dbleXLl20YcMGSVJaWpoKCgpc2oSEhCg0NNTZJiUlRQ6Hw1loSlKnTp3kcDicbcoTxSYAAIBBiYmJzncjTx+JiYkltu3YsaMWLlyo//znP5o7d64yMzMVERGhw4cPKzMzU5IUGBjo8pnAwEDntczMTHl5ealevXrnbBMQEFDsuwMCApxtyhPT6AAAwO3ZDC5HHzdunMaMGeNyzm63l9i2T58+zn9u3bq1wsPD1bRpUy1YsECdOnX6/2N1HaxlWcXOnenMNiW1L00/54NkEwAAwCC73S5fX1+X42zF5pl8fHzUunVrff/99873OM9MH7OyspxpZ1BQkPLz85WdnX3ONr/++mux7zp48GCx1LQ8UGwCAAC3V8PgcSHy8vK0a9cuBQcHq3HjxgoKCtKqVauc1/Pz87Vu3TpFRERIksLCwuTp6enSJiMjQ9u3b3e2CQ8PV05OjjZt2uRss3HjRuXk5DjblCem0QEAgNszOY1eFmPHjlW/fv3UsGFDZWVl6ZlnntHRo0c1dOhQ2Ww2xcfHa9KkSWrWrJmaNWumSZMmqXbt2oqOjpYkORwODRs2TAkJCapfv778/Pw0duxYtW7d2rk6vWXLlurdu7fi4uI0Z84cSdLw4cMVFRVV7ivRJYpNAACASuPAgQO64447dOjQIV166aXq1KmTUlNT1ahRI0nSI488ohMnTmjEiBHKzs5Wx44dtXLlStWtW9fZx7Rp01SzZk0NHjxYJ06cUPfu3TV//nx5eHg42yxevFijR492rlrv37+/Zs6caeSebJZllX230Urvu4oeAABDvBtOqOghADDkxL53Kuy79x372FjfDev0M9Z3VcA7mwAAADCGaXQAAOD2Kss7m9URySYAAACMIdkEAABuj2DTHJJNAAAAGEOyCQAA3F4Nok1jKDYBAIDbo9Y0h2l0AAAAGEOyCQAA3J7NVg1/46aSINkEAACAMSSbAADA7fHOpjkkmwAAADCGZBMAALg9fq7SHJJNAAAAGEOyCQAA3B7BpjkUmwAAwO0x1WsOzxYAAADGkGwCAAC3xwIhc0g2AQAAYAzJJgAAAEuEjCHZBAAAgDEkmwAAwO3ZSDaNIdkEAACAMSSbAADA7dls5G+mUGwCAAAwjW4MZTwAAACMIdkEAABujwVC5pBsAgAAwBiSTQAAAJJNY0g2AQAAYAzJJgAAcHtsfWQOTxYAAADGkGwCAADwzqYxFJsAAMDtsfWROUyjAwAAwBiSTQAA4PZINs0h2QQAAIAxJJsAAADkb8bwZAEAAGAMySYAAHB7NhvvbJpCsgkAAABjSDYBAABYjW4MxSYAAHB7bH1kDtPoAAAAMIZkEwAAgPzNGJ4sAAAAjCHZBAAAbo93Ns0h2QQAAIAxJJsAAMDtsam7OSSbAAAAMIZkEwAAgHc2jaHYBAAAbs/GZK8xPFkAAAAYQ7IJAADANLoxJJsAAAAwhmQTAAC4PbY+ModkEwAAAMaQbAIAAPDOpjEkmwAAADCGZBMAALg99tk0h2ITAACAaXRjKOMBAABgDMkmAABwezaSTWNINgEAAGAMySYAAHB7bOpuDskmAAAAjCHZBAAAIH8zhicLAAAAY0g2AQCA22M1ujkkmwAAADCGZBMAAIBk0xiKTQAA4PbY+sgcptEBAABgDMkmAAAA+ZsxPFkAAAAYQ7IJAADcHlsfmUOyCQAAAGNslmVZFT0I4Hzl5eUpMTFR48aNk91ur+jhAChH/PsNVA8Um6jSjh49KofDoZycHPn6+lb0cACUI/79BqoHptEBAABgDMUmAAAAjKHYBAAAgDEUm6jS7Ha7JkyYwOIBoBri32+gemCBEAAAAIwh2QQAAIAxFJsAAAAwhmITAAAAxlBsAgAAwBiKTVRpr776qho3bqxatWopLCxMX3zxRUUPCcAF+vzzz9WvXz+FhITIZrNp6dKlFT0kABeAYhNV1rvvvqv4+HiNHz9eW7duVefOndWnTx/t27evoocG4ALk5ubq2muv1cyZMyt6KADKAVsfocrq2LGj2rVrp1mzZjnPtWzZUgMHDlRiYmIFjgxAebHZbFqyZIkGDhxY0UMBcJ5INlEl5efnKy0tTZGRkS7nIyMjtWHDhgoaFQAAOBPFJqqkQ4cOqaioSIGBgS7nAwMDlZmZWUGjAgAAZ6LYRJVms9lc/rYsq9g5AABQcSg2USX5+/vLw8OjWIqZlZVVLO0EAAAVh2ITVZKXl5fCwsK0atUql/OrVq1SREREBY0KAACcqWZFDwA4X2PGjFFMTIzat2+v8PBwvfbaa9q3b5/uv//+ih4agAtw7Ngx/fDDD86/9+zZo/T0dPn5+alhw4YVODIA54Otj1Clvfrqq5oyZYoyMjIUGhqqadOm6cYbb6zoYQG4AGvXrlW3bt2KnR86dKjmz59/8QcE4IJQbAIAAMAY3tkEAACAMRSbAAAAMIZiEwAAAMZQbAIAAMAYik0AAAAYQ7EJAAAAYyg2AQAAYAzFJgAAAIyh2ARwwSZOnKg2bdo4/46NjdXAgQMv+jj27t0rm82m9PT0s7a54oorNH369FL3OX/+fF1yySUXPDabzaalS5decD8AUNVQbALVVGxsrGw2m2w2mzw9PdWkSRONHTtWubm5xr/7pZdeKvXPCpamQAQAVF01K3oAAMzp3bu35s2bp4KCAn3xxRe69957lZubq1mzZhVrW1BQIE9Pz3L5XofDUS79AACqPpJNoBqz2+0KCgpSgwYNFB0drTvvvNM5lXt66vvNN99UkyZNZLfbZVmWcnJyNHz4cAUEBMjX11c33XSTvv76a5d+n3vuOQUGBqpu3boaNmyYTp486XL9zGn0U6dOafLkybryyitlt9vVsGFDPfvss5Kkxo0bS5Latm0rm82mrl27Oj83b948tWzZUrVq1dJVV12lV1991eV7Nm3apLZt26pWrVpq3769tm7dWuZnNHXqVLVu3Vo+Pj5q0KCBRowYoWPHjhVrt3TpUjVv3ly1atVSz549tX//fpfrH3/8scLCwlSrVi01adJETz75pAoLC0v8zvz8fD344IMKDg5WrVq1dMUVVygxMbHMYweAqoBkE3Aj3t7eKigocP79ww8/6L333tMHH3wgDw8PSVLfvn3l5+enFStWyOFwaM6cOerevbu+++47+fn56b333tOECRP0yiuvqHPnzlq0aJFmzJihJk2anPV7x40bp7lz52ratGm64YYblJGRoW+//VbSHwXjddddp88++0xXX321vLy8JElz587VhAkTNHPmTLVt21Zbt25VXFycfHx8NHToUOXm5ioqKko33XST3nrrLe3Zs0f/+Mc/yvxMatSooRkzZuiKK67Qnj17NGLECD3yyCMuhe3x48f17LPPasGCBfLy8tKIESN0++2368svv5Qk/ec//9Hf//53zZgxQ507d9aPP/6o4cOHS5ImTJhQ7DtnzJihZcuW6b333lPDhg21f//+YsUrAFQbFoBqaejQodaAAQOcf2/cuNGqX7++NXjwYMuyLGvChAmWp6enlZWV5Wzz3//+1/L19bVOnjzp0lfTpk2tOXPmWJZlWeHh4db999/vcr1jx47WtddeW+J3Hz161LLb7dbcuXNLHOeePXssSdbWrVtdzjdo0MB6++23Xc49/fTTVnh4uGVZljVnzhzLz8/Pys3NdV6fNWtWiX39WaNGjaxp06ad9fp7771n1a9f3/n3vHnzLElWamqq89yuXbssSdbGjRsty7Kszp07W5MmTXLpZ9GiRVZwcLDzb0nWkiVLLMuyrFGjRlk33XSTderUqbOOAwCqC5JNoBr75JNPVKdOHRUWFqqgoEADBgzQyy+/7LzeqFEjXXrppc6/09LSdOzYMdWvX9+lnxMnTujHH3+UJO3atUv333+/y/Xw8HCtWbOmxDHs2rVLeXl56t69e6nHffDgQe3fv1/Dhg1TXFyc83xhYaHzfdBdu3bp2muvVe3atV3GUVZr1qzRpEmTtHPnTh09elSFhYU6efKkcnNz5ePjI0mqWbOm2rdv7/zMVVddpUsuuUS7du3Sddddp7S0NG3evNn5aoAkFRUV6eTJkzp+/LjLGKU/XjPo2bOnWrRood69eysqKkqRkZFlHjsAVAUUm0A11q1bN82aNUuenp4KCQkptgDodDF12qlTpxQcHKy1a9cW6+t8t//x9vYu82dOnTol6Y+p9I4dO7pcOz3db1nWeY3nz37++WfdfPPNuv/++/X000/Lz89P69ev17Bhw1xeN5D+2LroTKfPnTp1Sk8++aQGDRpUrE2tWrWKnWvXrp327NmjTz/9VJ999pkGDx6sHj166P3337/gewKAyoZiE6jGfHx8dOWVV5a6fbt27ZSZmamaNWvqiiuuKLFNy5YtlZqaqrvuust5LjU19ax9NmvWTN7e3vrvf/+re++9t9j10+9oFhUVOc8FBgbqsssu008//aQ777yzxH5btWqlRYsW6cSJE86C9lzjKMmWLVtUWFioF198UTVq/LFe8r333ivWrrCwUFu2bNF1110nSdq9e7eOHDmiq666StIfz2337t1leta+vr4aMmSIhgwZoltvvVW9e/fWb7/9Jj8/vzLdAwBUdhSbAJx69Oih8PBwDRw4UJMnT1aLFi30yy+/aMWKFRo4cKDat2+vf/zjHxo6dKjat2+vG264QYsXL9aOHTvOukCoVq1aevTRR/XII4/Iy8tL119/vQ4ePKgdO3Zo2LBhCggIkLe3t5KTk3X55ZerVq1acjgcmjhxokaPHi1fX1/16dNHeXl52rJli7KzszVmzBhFR0dr/PjxGjZsmP71r39p7969euGFF8p0v02bNlVhYaFefvll9evXT19++aVmz55drJ2np6dGjRqlGTNmyNPTUw8++KA6derkLD6feOIJRUVFqUGDBrrttttUo0YNffPNN9q2bZueeeaZYv1NmzZNwcHBatOmjWrUqKF///vfCgoKKpfN4wGgsmHrIwBONptNK1as0I033qh77rlHzZs31+233669e/cqMDBQkjRkyBA98cQTevTRRxUWFqaff/5ZDzzwwDn7ffzxx5WQkKAnnnhCLVu21JAhQ5SVlSXpj/chZ8yYoTlz5igkJEQDBgyQJN177716/fXXNX/+fLVu3VpdunTR/PnznVsl1alTRx9//LF27typtm3bavz48Zo8eXKZ7rdNmzaaOnWqJk+erNDQUC1evLjELYhq166tRx99VNHR0QoPD5e3t7eSkpKc13v16qVPPvlEq1atUocOHdSpUydNnTpVjRo1KvF769Spo8mTJ6t9+/bq0KGD9u7dqxUrVjjTVQCoTmxWebz4BAAAAJSA/xsNAAAAYyg2AQAAYAzFJgAAAIyh2AQAAIAxFJsAAAAwhmITAAAAxlBsAgAAwBiKTQAAABhDsQkAAABjKDYBAABgDMUmAAAAjPl/4uV5PzPKHu4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8801555289073004, 0.1467208315034402), (0.8982521167193576, 0.12651881130142), (0.9330280896834914, 0.08907919777484995), (0.9595552163959447, 0.058263797394232174), (0.9798684545223301, 0.031766944810423074), (0.9901522584396235, 0.015480895915678524), (0.9951306370144264, 0.008454106280193236), (0.9990188596969367, 0.0024154589371980675)]\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_batch_norm(dense_layer, bn_layer):\n",
    "    W, b = dense_layer.get_weights()\n",
    "    gamma, beta, moving_mean, moving_var = bn_layer.get_weights()\n",
    "\n",
    "    epsilon = bn_layer.epsilon\n",
    "    std = np.sqrt(moving_var + epsilon)\n",
    "    new_W = gamma / std * W\n",
    "    new_b = gamma / std * (b - moving_mean) + beta\n",
    "\n",
    "    return new_W, new_b\n",
    "\n",
    "def create_folded_model(original_model): # Fold batch normalization layers into dense layers\n",
    "    inputs = original_model.input\n",
    "    x = inputs\n",
    "    new_layers = []\n",
    "\n",
    "    for layer in original_model.layers:\n",
    "        if isinstance(layer, QDense):\n",
    "            next_layer = new_layers[-1] if new_layers else inputs\n",
    "            if isinstance(next_layer, BatchNormalization):\n",
    "                # Fold the BatchNormalization into the previous Dense layer\n",
    "                new_W, new_b = fold_batch_norm(layer, next_layer)\n",
    "                x = QDense(layer.units, weights=[new_W, new_b], kernel_quantizer=layer.kernel_quantizer, bias_quantizer=layer.bias_quantizer)(x)\n",
    "                new_layers.pop()  # Remove the BatchNormalization layer\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        elif not isinstance(layer, BatchNormalization):\n",
    "            x = layer(x)\n",
    "        new_layers.append(x)\n",
    "\n",
    "    outputs = x\n",
    "\n",
    "    new_model = Model(inputs, outputs)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_1\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_timed_input (InputLayer)  multiple                     0         ['y_timed_input[0][0]']       \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 8)                    848       ['y_timed_input[1][0]']       \n",
      "                                                                                                  \n",
      " q_activation (QActivation)  (None, 8)                    0         ['dense1[2][0]']              \n",
      "                                                                                                  \n",
      " dense_output_sigmoid (QDen  (None, 1)                    9         ['q_activation[3][0]']        \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 857 (3.35 KB)\n",
      "Trainable params: 857 (3.35 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Create the folded model\n",
    "new_model = create_folded_model(model)\n",
    "\n",
    "# Verify the new model\n",
    "new_model.summary()\n",
    "\n",
    "new_model.save(f'./DNN_L1_S8_best_performance_single_quant_folded_nosigmoid.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the model when done\n",
    "model.save(f'./DNN_L3_S32_best_performance_quant.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 64)                6784      \n",
      "                                                                 \n",
      " q_activation_6 (QActivatio  (None, 64)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " q_activation_7 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense3 (QDense)             (None, 16)                528       \n",
      "                                                                 \n",
      " q_activation_8 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 16)                64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 51        \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9891 (38.64 KB)\n",
      "Trainable params: 9667 (37.76 KB)\n",
      "Non-trainable params: 224 (896.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load in a saved model from the h5 file\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "loaded_model = load_model('./DNN_L3_S64_best_performance.h5', custom_objects=co)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m(data, loaded_model)\n\u001b[1;32m      2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m getTargetMetrics(test_results)\n\u001b[1;32m      3\u001b[0m displayPerformance(data, test_results, metrics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, loaded_model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write input data to file\n",
    "with open('DNN_hp_input_features.dat', 'w') as file:\n",
    "    for row in input_train_data_combined:\n",
    "        line = ' '.join(map(str, row))  # Convert each number to string and join with space\n",
    "        file.write(line + '\\n')\n",
    "# Write target data to file\n",
    "with open('./DNN_hp_predictions_small.dat', 'w') as file:\n",
    "    for score in target_test_data_coded:\n",
    "        file.write(str(score[0]) + '\\n')  # Convert number to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set display by time slice\n",
    "def display_dataset(input_dataset, target_dataset, i, gif=False):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset[i]\n",
    "    target_datapoint = target_dataset[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if input_datapoint.shape != (20, 13, 21):\n",
    "        raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point\")\n",
    "    \n",
    "    for i in range(8):\n",
    "        for j in range(13):\n",
    "            for k in range(21):\n",
    "                print(input_datapoint[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"--------\", i)\n",
    "\n",
    "    # Extracting the transverse momentum (pt) from the target_data dataset\n",
    "    pt = target_datapoint[8]  # Assuming the 9th variable is at index 8\n",
    "\n",
    "    fig, ax_main = plt.subplots(figsize=(8, 6))\n",
    "    divider = make_axes_locatable(ax_main)\n",
    "\n",
    "    # Add row sum plot as an inset to the main plot\n",
    "    ax_row = divider.append_axes(\"right\", size=\"20%\", pad=0.4)\n",
    "\n",
    "    # Add column sum plot below the main plot\n",
    "    ax_column = divider.append_axes(\"bottom\", size=\"20%\", pad=0.5)\n",
    "\n",
    "    # Initial plot\n",
    "    im = ax_main.imshow(input_datapoint[0, :, :], cmap='plasma')\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[2]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "    # Function to update the animation\n",
    "    def update(t):\n",
    "        # Update main plot\n",
    "        data = input_datapoint[t, :, :]\n",
    "        im.set_data(data)\n",
    "\n",
    "        # Update row sum plot\n",
    "        ax_row.clear()\n",
    "        ax_row.barh(np.arange(data.shape[0]), np.sum(data, axis=1), color='red')\n",
    "        ax_row.set_ylim(0, data.shape[0]-1)\n",
    "        ax_row.set_yticks(np.arange(data.shape[0]))\n",
    "        ax_row.set_xlim(np.min(input_datapoint[:, :, :].sum(axis=2)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=2)) * 1.1)\n",
    "        ax_row.set_xlabel(\"Row Sum\")\n",
    "\n",
    "        # Update column sum plot\n",
    "        ax_column.clear()\n",
    "        ax_column.bar(np.arange(data.shape[1]), np.sum(data, axis=0), color='blue')\n",
    "        ax_column.set_xlim(0, data.shape[1]-1)\n",
    "        ax_column.set_xticks(np.arange(data.shape[1]))\n",
    "        ax_column.set_ylim(np.min(input_datapoint[:, :, :].sum(axis=1)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=1)) * 1.1)\n",
    "        ax_column.set_ylabel(\"Column Sum\")\n",
    "\n",
    "        # Update labels and grid\n",
    "        ax_main.set_xlabel(\"X Position\")\n",
    "        ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "        # Update title for the entire figure\n",
    "        fig.suptitle(f\"Timestep: {t+1} | Data Point: {i} | pt: {pt:.2f} GeV\")\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=20, repeat=True)\n",
    "\n",
    "    gif_path = f\"data_point.gif\"\n",
    "    if gif:\n",
    "        # Save the animation as a GIF\n",
    "        writer = PillowWriter(fps=1000 // FRAME_TIME)\n",
    "        ani.save(gif_path, writer=writer)\n",
    "\n",
    "    plt.close()\n",
    "    return display(HTML(ani.to_jshtml())), gif_path\n",
    "\n",
    "def display_model_IO(input_dataset_combined, target_dataset_coded, i):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset_combined[i]\n",
    "    target_datapoint = target_dataset_coded[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        if input_datapoint.shape != (NUM_TIME_SLICES * 13 + 1,):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 2\")\n",
    "        input_datapoint = input_datapoint[:-1].reshape(NUM_TIME_SLICES, 13)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        if input_datapoint[0].shape != (NUM_TIME_SLICES, 13):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 3\")\n",
    "        input_datapoint = input_datapoint[0]\n",
    "        \n",
    "    # Extracting the label from the target datapoint\n",
    "    if target_datapoint[0] == 1:\n",
    "        label = f\"High p_t (over {TEST_PT_THRESHOLD} GeV)\"\n",
    "    elif target_datapoint[1] == 1:\n",
    "        label = f\"low p_t and negative charge\"\n",
    "    elif target_datapoint[2] == 1:\n",
    "        label = f\"low p_t and positive charge\"\n",
    "    else: \n",
    "        raise ValueError(\"Invalid labelling for the target data point\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax_main = plt.subplots(figsize=(4,4))\n",
    "    print(input_datapoint.shape)\n",
    "    print(input_datapoint)\n",
    "    im = ax_main.imshow(input_datapoint.T, cmap='coolwarm_r', vmin=-1, vmax=1)\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[0]))\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Update labels and grid\n",
    "    ax_main.set_xlabel(\"Time Slice\")\n",
    "    ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "\n",
    "    # Update title for the entire figure\n",
    "    fig.suptitle(f\"Data Point: {i} | label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_model_IO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rand_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m FRAME_TIME \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m  \u001b[38;5;66;03m# milliseconds between frames\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdisplay_model_IO\u001b[49m(input_data_combined_example, target_data_coded_example, rand_idx)\n\u001b[1;32m      5\u001b[0m animation, gif \u001b[38;5;241m=\u001b[39m display_dataset(input_data_example, target_data_example, rand_idx, gif\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_model_IO' is not defined"
     ]
    }
   ],
   "source": [
    "# DATASET DISPLAY\n",
    "rand_idx = random.randint(0, 100)\n",
    "FRAME_TIME = 120  # milliseconds between frames\n",
    "display_model_IO(input_data_combined_example, target_data_coded_example, rand_idx)\n",
    "animation, gif = display_dataset(input_data_example, target_data_example, rand_idx, gif=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import os\n",
    "os.environ['XILINX_HLS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis_HLS/2023.1'\n",
    "os.environ['XILINX_VIVADO'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vivado/2023.1'\n",
    "os.environ['XILINX_VITIS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis/2023.1'\n",
    "os.environ['XILINX_AP_INCLUDE'] = '/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/HLS_arbitrary_Precision_Types/include'\n",
    "os.environ['PATH'] = os.environ['XILINX_HLS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VITIS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_AP_INCLUDE'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "strip_model = strip_pruning(qmodel_pruned)\n",
    "hls_config = hls4ml.utils.config_from_keras_model(strip_model , granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    print(Layer)\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "# If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "hls_config['LayerName']['output_sigmoid']['Strategy'] = 'Stable'\n",
    "hls_config['LayerName']['output_sigmoid']['Precision'] = 'ap_fixed<32,8,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vitis')\n",
    "\n",
    "cfg['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg['HLSConfig'] = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'cnn_debug/'\n",
    "cfg['XilinxPart'] = 'xcku040-ffva1156-2-e'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()\n",
    "#hls_model.profile()\n",
    "hls_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in qmodel_pruned.layers:\n",
    "    for i, w in enumerate(layer.weights):\n",
    "        try:\n",
    "            print(\"weight is\", w.numpy(), \"for layer number\", i)  # TF 2.x\n",
    "        except Exception:\n",
    "            print(\"weight is\", layer.get_weights()[i], \"for layer number\", i) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Smart_Pixel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
