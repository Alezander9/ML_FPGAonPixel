{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 11:01:52.723553: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-17 11:01:52.810104: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-17 11:01:52.811104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 11:01:55.282031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.13.1\n",
      "keras version: 2.13.1\n",
      "qkeras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "# Machine Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, ReLU, Dropout, BatchNormalization, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow_model_optimization\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "import keras\n",
    "print(\"keras version:\",keras.__version__)\n",
    "import qkeras\n",
    "from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm\n",
    "from qkeras import quantized_relu, quantized_bits\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "print(\"qkeras version:\",keras.__version__)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Display and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Data management\n",
    "import psutil\n",
    "import h5py\n",
    "# Memory management\n",
    "import gc\n",
    "# Notifications\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def send_email_notification(subject, content):\n",
    "    sender_email = os.getenv('EMAIL_USER')\n",
    "    receiver_email = \"alexander.j.yue@gmail.com\"\n",
    "    password = os.getenv('EMAIL_PASS')\n",
    "\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = receiver_email\n",
    "    message[\"Subject\"] = subject\n",
    "    body = content\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message.as_string())\n",
    "\n",
    "# Memory monitoring functions\n",
    "def print_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory usage percentage: {memory.percent}%\")\n",
    "\n",
    "def print_cpu_usage():\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pixel cluster to transverse momentum dataset into the input_data and target_data\n",
    "def load_combine_shuffle_data_optimized_hdf5():\n",
    "    # Load the dataset from Kenny's computer\n",
    "    with h5py.File('/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/fl32_data_v3.hdf5', 'r') as h5f:\n",
    "        combined_input = None\n",
    "        combined_target = None\n",
    "\n",
    "        for data_type in ['sig', 'bkg']:\n",
    "            # Construct dataset names\n",
    "            input_dataset_name = f'{data_type}_input'\n",
    "            target_dataset_name = f'{data_type}_target'\n",
    "\n",
    "            # Check if the dataset exists and load data sequentially\n",
    "            if input_dataset_name in h5f and target_dataset_name in h5f:\n",
    "                input_data = h5f[input_dataset_name][:].astype(np.float32)\n",
    "                target_data = h5f[target_dataset_name][:].astype(np.float32)\n",
    "\n",
    "                if combined_input is None:\n",
    "                    combined_input = input_data\n",
    "                    combined_target = target_data\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "                else:\n",
    "                    print_memory_usage()\n",
    "                    combined_input = np.vstack((combined_input, input_data))\n",
    "                    combined_target = np.vstack((combined_target, target_data))\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "            else:\n",
    "                print(f\"Dataset {input_dataset_name} or {target_dataset_name} not found.\")\n",
    "\n",
    "        # Shuffling\n",
    "        indices = np.arange(combined_input.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        combined_input = combined_input[indices]\n",
    "        combined_target = combined_target[indices]\n",
    "\n",
    "        return combined_input, combined_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load dataset into memory\n",
    "    input_data, target_data = load_combine_shuffle_data_optimized_hdf5()\n",
    "    # Format the dataset into a 20x13x21 tensor (time, y, x)\n",
    "    input_data = input_data.reshape(input_data.shape[0],20,13,21)\n",
    "    return input_data, target_data\n",
    "\n",
    "def process_dataset(input_data, target_data, hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    TRAIN_PT_THRESHOLD = hyperparams[\"TRAIN_PT_THRESHOLD\"]\n",
    "    TEST_PT_THRESHOLD = hyperparams[\"TEST_PT_THRESHOLD\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "\n",
    "    # Split 80% of data into training data, 10% for validation data and 10% for testing data\n",
    "    input_train_data, input_temp, target_train_data, target_temp = \\\n",
    "    train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "    del input_data\n",
    "    del target_data\n",
    "    gc.collect()\n",
    "    input_validate_data, input_test_data, target_validate_data, target_test_data = \\\n",
    "    train_test_split(input_temp, target_temp, test_size=0.5, random_state=42)\n",
    "    del input_temp\n",
    "    del target_temp\n",
    "    gc.collect()\n",
    "\n",
    "    # Save some data for displaying\n",
    "    input_data_example = input_test_data[0:100,:]\n",
    "    target_data_example = target_test_data[0:100,:]\n",
    "\n",
    "    # Fit the scalers on the training data to it all scales the exact same\n",
    "    input_scaler = StandardScaler()\n",
    "    input_scaler.fit(input_train_data[:, :NUM_TIME_SLICES, :, :].reshape(-1,8*13))\n",
    "    y0_scaler = StandardScaler()\n",
    "    y0_scaler.fit(target_train_data[:,7].reshape(-1, 1))\n",
    "\n",
    "    # Process the data into input shape and labels for training\n",
    "    def process_data(input_data, target_data, pt_threshold):\n",
    "        if input_data.shape[1:] == (20, 13, 21) and target_data.shape[1:] == (13, ):\n",
    "\n",
    "            # Truncate down to first time slices\n",
    "            input_data = input_data[:, :NUM_TIME_SLICES, :, :]\n",
    "\n",
    "            # sum over the x axis to turn the input data into a 2D NUM_TIME_SLICES x 13 tensor (time, y)\n",
    "            input_data = np.sum(input_data, axis=3)\n",
    "\n",
    "            if OUTPUT == \"SOFTMAX\" or OUTPUT == \"LINEAR\":\n",
    "                # Encode the target data into one_hot encoding\n",
    "                one_hot = np.zeros((target_data.shape[0], 3))\n",
    "                # Assign 1 for p_t > pt_threshold in GeV, for low p_t put 1 in slot 2 for negative and a 1 in slot 3 for positive\n",
    "                one_hot[np.abs(target_data[:, 8]) >= pt_threshold, 0] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] > 0), 1] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] < 0), 2] = 1\n",
    "                label_data = one_hot\n",
    "            # elif OUTPUT == \"ARGMAX\": # DOES NOT WORK \n",
    "            #     label_data = np.argmax(one_hot, axis=1).astype(np.int64)\n",
    "            #     print(\"one hot is \", one_hot)\n",
    "            elif OUTPUT == \"SINGLE\":\n",
    "            # Binary labels for 0th category\n",
    "                label_data = (np.abs(target_data[:, 8]) >= pt_threshold).astype(np.int64)\n",
    "\n",
    "            # Flatten the input data\n",
    "            input_data = input_data.reshape(-1,NUM_TIME_SLICES*13)\n",
    "\n",
    "            # Normalize the input data to have mean| 0 and std 1\n",
    "            \n",
    "            input_data = input_scaler.transform(input_data)\n",
    "            # # Replace all values < 1 with 1 so they log to 0\n",
    "            # input_data = np.where(np.abs(input_data) < 1.0, 1.0, input_data)\n",
    "            # # Apply logarithmic scaling\n",
    "            # input_data = np.log(np.abs(input_data)) * np.sign(input_data)\n",
    "            # # Min-max normalization (global)\n",
    "            # min_val = np.min(input_data)\n",
    "            # max_val = np.max(input_data)\n",
    "            # print(f\"max of log of data is {max_val} and min is {min_val}\")\n",
    "            # input_data = (input_data) / np.max([max_val,min_val])\n",
    "\n",
    "            # Get the y_0 data\n",
    "            y0_data = target_data[:,7].reshape(-1, 1)\n",
    "            y0_data = y0_scaler.transform(y0_data)\n",
    "            # Combine with input data\n",
    "            if (MODEL_TYPE == \"DNN\"):\n",
    "                # For DNN we concatenate in the y_0 data\n",
    "                input_data_combined = np.hstack((input_data, y0_data))\n",
    "\n",
    "                \n",
    "            elif (MODEL_TYPE == \"CNN\"):\n",
    "                # Reshape data into a matrix for the convolutions\n",
    "                input_data= input_data.reshape(-1, NUM_TIME_SLICES, 13)\n",
    "                # Package with the y_0 data to be added later\n",
    "                input_data_combined = [input_data, y0_data]\n",
    "\n",
    "            return input_data_combined, label_data\n",
    "        else:\n",
    "            raise ValueError(\"Wrong array shape!\")\n",
    "\n",
    "    # Apply data processing to our datasets\n",
    "    input_train_data_combined, target_train_data_coded = process_data(input_train_data, target_train_data, TRAIN_PT_THRESHOLD)\n",
    "    input_validate_data_combined, target_validate_data_coded = process_data(input_validate_data, target_validate_data, TRAIN_PT_THRESHOLD)\n",
    "    input_test_data_combined, target_test_data_coded = process_data(input_test_data, target_test_data, TEST_PT_THRESHOLD)\n",
    "\n",
    "    # Save some data for displaying\n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        input_data_combined_example = input_test_data_combined[0:100,:]\n",
    "        if OUTPUT == \"ARGMAX\" or OUTPUT == \"SINGLE\":\n",
    "            target_data_coded_example = target_test_data_coded[0:100]\n",
    "        else:\n",
    "            target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    elif MODEL_TYPE == \"CNN\":\n",
    "        input_data_combined_example = np.hstack((input_test_data_combined[0][0:100,:].reshape(100, -1), input_test_data_combined[1][0:100,:]))\n",
    "        target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    processed_dataset = {\n",
    "        \"input_train_data_combined\": input_train_data_combined,\n",
    "        \"target_train_data_coded\": target_train_data_coded,\n",
    "        \"input_validate_data_combined\": input_validate_data_combined,\n",
    "        \"target_validate_data_coded\": target_validate_data_coded,\n",
    "        \"input_test_data_combined\": input_test_data_combined,\n",
    "        \"target_test_data_coded\": target_test_data_coded,\n",
    "\n",
    "        \"input_data_example\": input_data_example,\n",
    "        \"target_data_example\": target_data_example,\n",
    "        \"input_data_combined_example\": input_data_combined_example,\n",
    "        \"target_data_coded_example\": target_data_coded_example,\n",
    "    }\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class ArgmaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ArgmaxLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(tf.argmax(inputs, axis=-1), dtype=tf.int64)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ArgmaxLayer, self).get_config()\n",
    "        return config\n",
    "    \n",
    "def qDNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    DNN_LAYERS = hyperparams[\"DNN_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    y_timed_input = Input(shape=(NUM_TIME_SLICES*13 + 1,), name='y_timed_input')\n",
    "    layer = y_timed_input\n",
    "    \n",
    "    for i, size in enumerate(DNN_LAYERS):\n",
    "        layer = QDense(size, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name=f'dense{i+1}')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS))(layer)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = ArgmaxLayer(name='output_argmax')(output)\n",
    "        print(f\"Argmax output dtype: {output.dtype}\")\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        output = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS), \n",
    "                bias_quantizer=quantized_bits(BIAS_BITS), \n",
    "                activation='sigmoid', \n",
    "                name='dense_output')(layer)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported output type\")\n",
    "   \n",
    "    model = Model(inputs=y_timed_input, outputs=output)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "              loss='mse', \n",
    "              metrics=[Precision()])\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def qCNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    CONV_LAYER_DEPTHS = hyperparams[\"CONV_LAYER_DEPTHS\"]\n",
    "    CONV_LAYER_KERNELS = hyperparams[\"CONV_LAYER_KERNELS\"]\n",
    "    CONV_LAYER_STRIDES = hyperparams[\"CONV_LAYER_STRIDES\"]\n",
    "    MAX_POOLING_SIZE = hyperparams[\"MAX_POOLING_SIZE\"]\n",
    "    FLATTENED_LAYERS = hyperparams[\"FLATTENED_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    INTEGER_BITS = hyperparams[\"INTEGER_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "\n",
    "    y_profile_input = Input(shape=(NUM_TIME_SLICES, 13, 1), name='y_profile_input')  # Adjust the shape based on your input\n",
    "    layer = y_profile_input\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(len(CONV_LAYER_DEPTHS)):\n",
    "        layer = QConv2D(\n",
    "        CONV_LAYER_DEPTHS[i],\n",
    "        kernel_size=CONV_LAYER_KERNELS[i],\n",
    "        strides=CONV_LAYER_STRIDES[i],\n",
    "        kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        padding='same',\n",
    "        use_bias=True,\n",
    "        name=f'conv{i+1}'\n",
    "        )(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS), name=f'relu{i+1}')(layer)\n",
    "        layer = MaxPooling2D(pool_size=MAX_POOLING_SIZE, name=f'maxpool{i+1}')(layer)\n",
    "\n",
    "    # Flatten the output to feed into a dense layer\n",
    "    layer = Flatten(name='flattened')(layer)\n",
    "\n",
    "    # Flatten and concatenate with y0 input\n",
    "    y0_input = Input(shape=(1,), name='y0_input')\n",
    "    layer = Concatenate(name='concat')([layer, y0_input])\n",
    "\n",
    "    # Post-flattening dense layers\n",
    "    for i in range(len(FLATTENED_LAYERS)):\n",
    "        layer = QDense(FLATTENED_LAYERS[i], kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name=f'dense{i+1}')(layer)\n",
    "        layer = QActivation(quantized_relu(10), name=f'relu{len(CONV_LAYER_DEPTHS)+i+1}')(layer)\n",
    "\n",
    "    # Output layer (adjust based on your classification problem)\n",
    "    output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    # layer = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0), \n",
    "    #                bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name='output_dense')(layer)\n",
    "    # output = Activation(\"sigmoid\", name='output_sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=[y_profile_input, y0_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy']) # loss='binary_crossentropy'\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Pruning the model\n",
    "def pruneFunction(layer, train_data_size, hyperparams):\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    FINAL_SPARSITY = hyperparams[\"FINAL_SPARSITY\"]\n",
    "    PRUNE_START_EPOCH = hyperparams[\"PRUNE_START_EPOCH\"]\n",
    "    NUM_PRUNE_EPOCHS = hyperparams[\"NUM_PRUNE_EPOCHS\"]\n",
    "\n",
    "    steps_per_epoch = train_data_size // BATCH_SIZE #input_train_data_combined.shape[0]\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.0,\n",
    "            final_sparsity=FINAL_SPARSITY,\n",
    "            begin_step=steps_per_epoch * PRUNE_START_EPOCH,\n",
    "            end_step=steps_per_epoch * (PRUNE_START_EPOCH + NUM_PRUNE_EPOCHS),\n",
    "            frequency=steps_per_epoch # prune after every epoch\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "    if isinstance(layer, QDense):\n",
    "        if layer.name != 'output_softmax' and layer.name != 'dense2':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        elif layer.name != 'output_softmax' and layer.name != 'dense1':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        else:\n",
    "            print(f\"cannot prune layer {layer.name}\")\n",
    "            return layer\n",
    "\n",
    "    else:\n",
    "        print(f\"cannot prune layer {layer.name}\")\n",
    "        return layer\n",
    "    \n",
    "def pruneFunctionWrapper(train_data_size, hyperparams):\n",
    "    def wrapper(layer):\n",
    "        return pruneFunction(layer, train_data_size, hyperparams)\n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights = layer.get_weights()[0]\n",
    "            total_params += weights.size\n",
    "            zero_params += np.sum(weights == 0)\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, hyperparams):\n",
    "    input_train_data_combined = data[\"input_train_data_combined\"]\n",
    "    target_train_data_coded = data[\"target_train_data_coded\"]\n",
    "    input_validate_data_combined = data[\"input_validate_data_combined\"]\n",
    "    target_validate_data_coded = data[\"target_validate_data_coded\"]\n",
    "\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    PATIENCE = hyperparams[\"PATIENCE\"]\n",
    "    EPOCHS = hyperparams[\"EPOCHS\"]\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    POST_PRUNE_EPOCHS = hyperparams[\"POST_PRUNE_EPOCHS\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define the model\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        model = qDNNmodel(hyperparams)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        model = qCNNmodel(hyperparams)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported model type\")\n",
    "\n",
    "    model.summary()\n",
    "    print_qmodel_summary(model)\n",
    "    print(f\"Initial Sparsity: {calculate_sparsity(model) * 100:.2f}%\")\n",
    "\n",
    "    train_metrics = {}\n",
    "\n",
    "    # Train the model\n",
    "    earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=PATIENCE, restore_best_weights=True)\n",
    "    print(\"shape 12323 is \", target_train_data_coded.shape, \"data is like\", target_validate_data_coded[1:5])\n",
    "    history = model.fit(\n",
    "        input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "        validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[earlyStop_callback]\n",
    "    )\n",
    "    # Best at this step val_loss 0.7085\n",
    "    train_metrics[\"val_loss\"] = history.history['val_loss'][-1]\n",
    "\n",
    "    # Prune the DNN model \n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        model_pruned = keras.models.clone_model(model, clone_function=pruneFunctionWrapper(input_train_data_combined.shape[0], hyperparams))\n",
    "        model_pruned.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Re-train the pruned model\n",
    "        history = model_pruned.fit(\n",
    "            input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "            validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "            epochs=POST_PRUNE_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks = [pruning_callbacks.UpdatePruningStep()]\n",
    "        ) \n",
    "        # best at this step val_loss 0.4314\n",
    "\n",
    "        model = strip_pruning(model_pruned)\n",
    "        # train_metrics[\"pruned_sparsity\"] = calculate_sparsity(model)\n",
    "\n",
    "    try:\n",
    "        train_metrics[\"pruned_val_loss\"] = history.history['val_loss'][-1]\n",
    "    except:\n",
    "        print(\"Error: no post-pruning val_loss found\")\n",
    "        train_metrics[\"pruned_val_loss\"] = train_metrics[\"val_loss\"]\n",
    "\n",
    "    return model, train_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data, model, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    input_test_data_combined = data[\"input_test_data_combined\"]\n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "\n",
    "    \n",
    "    # Test the model at threshold 0.5\n",
    "    predictions = model.predict(input_test_data_combined)\n",
    "    print(predictions[:10, :])\n",
    "    predictions_prob = predictions[:,0]\n",
    "    predictions_labels = (predictions_prob >= 0.5).astype(int).flatten()\n",
    "\n",
    "    # Test the model at different thresholds\n",
    "    thresholds = np.linspace(0.0, 1.0, 1000)\n",
    "    signal_efficiencies = []\n",
    "    background_rejections = []\n",
    "    max_sum_se = 0\n",
    "    max_sum_br = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # predicted_class = ((predictions_prob[:, 0] + threshold > predictions_prob[:, 1]) & (predictions_prob[:, 0] + threshold > predictions_prob[:, 2])).astype(int)\n",
    "        predicted_class = (predictions_prob > threshold).astype(int)\n",
    "        # Compute confusion matrix\n",
    "        if OUTPUT == \"SINGLE\":\n",
    "            cm = confusion_matrix(target_test_data_coded[:], predicted_class)\n",
    "        else:\n",
    "            cm = confusion_matrix(target_test_data_coded[:, 0], predicted_class)\n",
    "\n",
    "        # Calculate signal efficiency and background rejection\n",
    "        signal_efficiency = cm[1, 1] / np.sum(cm[1, :])\n",
    "        background_rejection = cm[0, 0] / np.sum(cm[0, :])\n",
    "\n",
    "        # Store metrics\n",
    "        signal_efficiencies.append(signal_efficiency)\n",
    "        background_rejections.append(background_rejection)\n",
    "\n",
    "        # get maximum added score\n",
    "        if signal_efficiency + background_rejection > max_sum_se + max_sum_br:\n",
    "            max_sum_se = signal_efficiency\n",
    "            max_sum_br = background_rejection\n",
    "    \n",
    "    test_results = {\n",
    "        \"predictions_prob\": predictions_prob,\n",
    "        \"predictions_labels\": predictions_labels,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"signal_efficiencies\": signal_efficiencies,\n",
    "        \"background_rejections\": background_rejections,\n",
    "        \"max_sum_se\": max_sum_se,\n",
    "        \"max_sum_br\": max_sum_br,\n",
    "    }\n",
    "\n",
    "    return test_results\n",
    "\n",
    "def ShowConfusionMatrix(data, test_results):\n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "    predictions_labels = test_results[\"predictions_labels\"]\n",
    "\n",
    "    cm = confusion_matrix(target_test_data_coded[:,0], predictions_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='YlGnBu')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def showMetricsByThreshold(test_results):\n",
    "    thresholds = test_results[\"thresholds\"]\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, signal_efficiencies, label='Signal Efficiency')\n",
    "    plt.plot(thresholds, background_rejections, label='Background Rejection')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Effect of Threshold on Signal Efficiency and Background Rejection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def showEfficiencyVSRejection(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(signal_efficiencies, background_rejections, marker='o')\n",
    "    plt.xlabel('Signal Efficiency')\n",
    "    plt.ylabel('Background Rejection')\n",
    "    plt.title('Background Rejection vs. Signal Efficiency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def find_closest(sorted_array, value):\n",
    "    # Ensure the array is a NumPy array\n",
    "    sorted_array = np.array(sorted_array)\n",
    "    # Compute the absolute difference\n",
    "    abs_diff = np.abs(sorted_array - value)\n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = np.argmin(abs_diff)\n",
    "    return closest_index\n",
    "\n",
    "def getTargetMetrics(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    target_efficiencies = [0.873, 0.90, 0.93, 0.96, 0.98, 0.99, 0.995, 0.999]\n",
    "    metrics = []\n",
    "    for target in target_efficiencies:\n",
    "        index = find_closest(signal_efficiencies, target)\n",
    "        metrics.append((signal_efficiencies[index], background_rejections[index]))\n",
    "        # print(f\"Signal Efficiency: {signal_efficiencies[index]*100:.1f}%,\",f\"Background Rejections: {background_rejections[index]*100:.1f}%\")\n",
    "    return metrics\n",
    "\n",
    "def displayPerformance(data, test_results, metrics):\n",
    "    ShowConfusionMatrix(data, test_results)\n",
    "    showMetricsByThreshold(test_results)\n",
    "    showEfficiencyVSRejection(test_results)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(metrics):\n",
    "    # Convert metrics list of tuples to a formatted string\n",
    "    return \", \".join([f\"({m1:.2f}, {m2:.2f})\" for m1, m2 in metrics])\n",
    "\n",
    "def hyperparameter_search(data, base_hyperparams, param_grid, result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file if it exists\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        hyperparams = dict(zip(keys, v))\n",
    "        # Update base hyperparameters with the current set\n",
    "        current_hyperparams = base_hyperparams.copy()\n",
    "        current_hyperparams.update(hyperparams)\n",
    "\n",
    "        # Convert hyperparameters to a string for use as a dictionary key\n",
    "        hyperparams_str = json.dumps(current_hyperparams, sort_keys=True)\n",
    "\n",
    "        # Check if these hyperparameters have been tried before\n",
    "        if hyperparams_str in all_results:\n",
    "            print(f\"Skipping already tested hyperparameters: {current_hyperparams}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Testing hyperparameters: {current_hyperparams}\")\n",
    "\n",
    "        # Train the model\n",
    "        model, train_metrics = train_model(data, current_hyperparams)\n",
    "\n",
    "        # Test the model\n",
    "        test_results = test_model(data, model, current_hyperparams)\n",
    "        metrics = format_metrics(getTargetMetrics(test_results))\n",
    "        \n",
    "        test_scores = {\n",
    "            \"max_sum_se\": test_results[\"max_sum_se\"],\n",
    "            \"max_sum_br\": test_results[\"max_sum_br\"],\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        # Add all keys and values from train_metrics into test_scores\n",
    "        test_scores.update(train_metrics)\n",
    "\n",
    "        # Save the results to the file\n",
    "        all_results[hyperparams_str] = test_scores\n",
    "        with open(result_file, 'w') as file:\n",
    "            json.dump(all_results, file, indent=4)\n",
    "\n",
    "\n",
    "        # If new best found, email alex\n",
    "        if test_scores[\"pruned_val_loss\"] < find_min_pruned_val_loss(result_file):\n",
    "\n",
    "            # Format metrics for the email\n",
    "            metrics_str = \", \".join([f\"({m1:.3f}, {m2:.3f})\" for m1, m2 in test_scores[\"metrics\"]])\n",
    "\n",
    "            # email results\n",
    "            model_name = \"undefined\"\n",
    "            if (current_hyperparams[\"MODEL_TYPE\"] == \"DNN\"):\n",
    "                model_name = \"Dean\"\n",
    "            elif (current_hyperparams[\"MODEL_TYPE\"] == \"CNN\"):\n",
    "                model_name = \"Connor\"\n",
    "            send_email_notification(\"ML Training Report\", \n",
    "                f' \\\n",
    "                Your model {model_name} has finished training. \\\n",
    "                He got a grade of {test_scores[\"max_sum_se\"]*100:.1f}% in SE and {test_scores[\"max_sum_br\"]*100:.1f}% in BR. \\\n",
    "                New lowest validated loss: {test_scores[\"pruned_val_loss\"]} \\n \\\n",
    "                All metrics: {metrics_str} \\n \\\n",
    "                Hyperparams: {hyperparams_str} \\\n",
    "                ')\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def find_min_pruned_val_loss(result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        print(f\"No results found in {result_file}\")\n",
    "        return None\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    min_hyperparams = None\n",
    "\n",
    "    # Iterate through the results to find the minimum pruned_val_loss\n",
    "    for hyperparams_str, results in all_results.items():\n",
    "        if \"pruned_val_loss\" in results:\n",
    "            pruned_val_loss = results[\"pruned_val_loss\"]\n",
    "            if pruned_val_loss < min_loss:\n",
    "                min_loss = pruned_val_loss\n",
    "                min_hyperparams = hyperparams_str\n",
    "\n",
    "    # Print the hyperparameters with the minimum pruned_val_loss\n",
    "    if min_hyperparams is not None:\n",
    "        print(f\"Hyperparameters with minimum pruned_val_loss: {min_hyperparams}\")\n",
    "        print(f\"Minimum pruned_val_loss: {min_loss}\")\n",
    "    else:\n",
    "        print(\"No entry with pruned_val_loss found\")\n",
    "\n",
    "    return min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.487087607383728"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    # Model Type\n",
    "    \"MODEL_TYPE\": \"DNN\",  # DNN or CNN\n",
    "    # Input format\n",
    "    \"NUM_TIME_SLICES\": 8,\n",
    "    \"TRAIN_PT_THRESHOLD\": 2,  # in GeV\n",
    "    \"TEST_PT_THRESHOLD\": 2,  # in GeV\n",
    "    # DNN model mormat\n",
    "    \"DNN_LAYERS\": [72],\n",
    "    # CNN model format\n",
    "    \"CONV_LAYER_DEPTHS\": [4, 7],\n",
    "    \"CONV_LAYER_KERNELS\": [(3, 3), (3, 3)],\n",
    "    \"CONV_LAYER_STRIDES\": [(1, 1), (1, 1)],\n",
    "    \"FLATTENED_LAYERS\": [7],\n",
    "    \"MAX_POOLING_SIZE\": (2, 2),\n",
    "    # Output function\n",
    "    \"OUTPUT\": \"SINGLE\", # SOFTMAX or ARGMAX (not working) or LINEAR or SINGLE\n",
    "    # Model quantization\n",
    "    \"WEIGHTS_BITS\": 4,\n",
    "    \"BIAS_BITS\": 4,\n",
    "    \"ACTIVATION_BITS\": 6,\n",
    "    \"INTEGER_BITS\": 2,\n",
    "    # Training\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"BATCH_SIZE\": 1024,  # Number of samples per gradient update\n",
    "    \"EPOCHS\": 100,  # Number of epochs to train\n",
    "    \"PATIENCE\": 20,  # Stop after this number of epochs without improvement\n",
    "    # Pruning\n",
    "    \"PRUNE_START_EPOCH\": 0,  # Number of epochs before pruning\n",
    "    \"NUM_PRUNE_EPOCHS\": 10,\n",
    "    \"FINAL_SPARSITY\": 0.0,\n",
    "    \"POST_PRUNE_EPOCHS\": 50,\n",
    "}\n",
    "\n",
    "SAVE_FILE = \"one_layer\"+HYPERPARAMETERS[\"MODEL_TYPE\"]+\"_results.json\"\n",
    "\n",
    "param_grid = {\n",
    "    \"DNN_LAYERS\": [[72], [64], [48], [32], [24], [16], [8]],\n",
    "    \"FINAL_SPARSITY\": [0.35], \n",
    "}\n",
    "\n",
    "find_min_pruned_val_loss(result_file=(SAVE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 376.23 GB\n",
      "Available memory: 289.05 GB\n",
      "Used memory: 76.45 GB\n",
      "Memory usage percentage: 23.2%\n",
      "Total memory: 376.23 GB\n",
      "Available memory: 277.77 GB\n",
      "Used memory: 87.74 GB\n",
      "Memory usage percentage: 26.2%\n"
     ]
    }
   ],
   "source": [
    "input_data, target_data = load_dataset()\n",
    "data = process_dataset(input_data, target_data, HYPERPARAMETERS) # Depends only on: NUM_TIME_SLICES MODEL_TYPE TRAIN_PT_THRESHOLD TEST_PT_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already tested hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 4, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 4}\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 4, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 35}\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/4\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 1.0407 - accuracy: 0.4992 - val_loss: 1.0400 - val_accuracy: 0.4996\n",
      "Epoch 2/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 1.0396 - accuracy: 0.5003 - val_loss: 1.0400 - val_accuracy: 0.4996\n",
      "Epoch 3/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 1.0396 - accuracy: 0.5003 - val_loss: 1.0404 - val_accuracy: 0.4996\n",
      "Epoch 4/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 1.0397 - accuracy: 0.5003 - val_loss: 1.0401 - val_accuracy: 0.4996\n",
      "1714/1714 [==============================] - 5s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 4, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 4, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.8504971265792847\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 4, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/4\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9007 - accuracy: 0.5801 - val_loss: 0.8578 - val_accuracy: 0.6133\n",
      "Epoch 2/4\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8401 - accuracy: 0.6222 - val_loss: 0.8266 - val_accuracy: 0.6328\n",
      "Epoch 3/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8188 - accuracy: 0.6316 - val_loss: 0.8061 - val_accuracy: 0.6416\n",
      "Epoch 4/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8057 - accuracy: 0.6385 - val_loss: 0.7960 - val_accuracy: 0.6473\n",
      "1714/1714 [==============================] - 5s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 4, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7960143089294434\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 4, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 100}\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/4\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9564 - accuracy: 0.5320 - val_loss: 0.9137 - val_accuracy: 0.5746\n",
      "Epoch 2/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8879 - accuracy: 0.5937 - val_loss: 0.8883 - val_accuracy: 0.5922\n",
      "Epoch 3/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8710 - accuracy: 0.6086 - val_loss: 0.8660 - val_accuracy: 0.6120\n",
      "Epoch 4/4\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8631 - accuracy: 0.6150 - val_loss: 0.8624 - val_accuracy: 0.6168\n",
      "1714/1714 [==============================] - 5s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 4, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7960143089294434\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 50, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 4}\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/50\n",
      "857/857 [==============================] - 14s 13ms/step - loss: 1.0075 - accuracy: 0.4992 - val_loss: 0.9519 - val_accuracy: 0.4996\n",
      "Epoch 2/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9429 - accuracy: 0.5003 - val_loss: 0.9408 - val_accuracy: 0.4996\n",
      "Epoch 3/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9349 - accuracy: 0.5003 - val_loss: 0.9344 - val_accuracy: 0.4996\n",
      "Epoch 4/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9311 - accuracy: 0.5003 - val_loss: 0.9287 - val_accuracy: 0.4996\n",
      "Epoch 5/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9292 - accuracy: 0.5003 - val_loss: 0.9266 - val_accuracy: 0.4996\n",
      "Epoch 6/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9273 - accuracy: 0.5003 - val_loss: 0.9374 - val_accuracy: 0.4996\n",
      "Epoch 7/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9261 - accuracy: 0.5003 - val_loss: 0.9251 - val_accuracy: 0.4996\n",
      "Epoch 8/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9248 - accuracy: 0.5003 - val_loss: 0.9233 - val_accuracy: 0.4996\n",
      "Epoch 9/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9242 - accuracy: 0.5003 - val_loss: 0.9237 - val_accuracy: 0.4996\n",
      "Epoch 10/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9238 - accuracy: 0.5003 - val_loss: 0.9297 - val_accuracy: 0.4996\n",
      "Epoch 11/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9228 - accuracy: 0.5003 - val_loss: 0.9248 - val_accuracy: 0.4996\n",
      "Epoch 12/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9224 - accuracy: 0.5003 - val_loss: 0.9227 - val_accuracy: 0.4996\n",
      "Epoch 13/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9225 - accuracy: 0.5003 - val_loss: 0.9210 - val_accuracy: 0.4996\n",
      "Epoch 14/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9220 - accuracy: 0.5003 - val_loss: 0.9228 - val_accuracy: 0.4996\n",
      "Epoch 15/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9224 - accuracy: 0.5003 - val_loss: 0.9245 - val_accuracy: 0.4996\n",
      "Epoch 16/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9215 - accuracy: 0.5003 - val_loss: 0.9204 - val_accuracy: 0.4996\n",
      "Epoch 17/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9214 - accuracy: 0.5003 - val_loss: 0.9205 - val_accuracy: 0.4996\n",
      "Epoch 18/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9208 - accuracy: 0.5003 - val_loss: 0.9242 - val_accuracy: 0.4996\n",
      "Epoch 19/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9218 - accuracy: 0.5003 - val_loss: 0.9199 - val_accuracy: 0.4996\n",
      "Epoch 20/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9207 - accuracy: 0.5003 - val_loss: 0.9233 - val_accuracy: 0.4996\n",
      "Epoch 21/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9206 - accuracy: 0.5003 - val_loss: 0.9197 - val_accuracy: 0.4996\n",
      "Epoch 22/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9208 - accuracy: 0.5003 - val_loss: 0.9192 - val_accuracy: 0.4996\n",
      "Epoch 23/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9199 - accuracy: 0.5003 - val_loss: 0.9329 - val_accuracy: 0.4996\n",
      "Epoch 24/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9209 - accuracy: 0.5003 - val_loss: 0.9190 - val_accuracy: 0.4996\n",
      "Epoch 25/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9203 - accuracy: 0.5003 - val_loss: 0.9192 - val_accuracy: 0.4996\n",
      "Epoch 26/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9205 - accuracy: 0.5003 - val_loss: 0.9209 - val_accuracy: 0.4996\n",
      "Epoch 27/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9204 - accuracy: 0.5003 - val_loss: 0.9191 - val_accuracy: 0.4996\n",
      "Epoch 28/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9206 - accuracy: 0.5003 - val_loss: 0.9188 - val_accuracy: 0.4996\n",
      "Epoch 29/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9197 - accuracy: 0.5003 - val_loss: 0.9222 - val_accuracy: 0.4996\n",
      "Epoch 30/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9197 - accuracy: 0.5003 - val_loss: 0.9202 - val_accuracy: 0.4996\n",
      "Epoch 31/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9202 - accuracy: 0.5003 - val_loss: 0.9187 - val_accuracy: 0.4996\n",
      "Epoch 32/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9195 - accuracy: 0.5003 - val_loss: 0.9191 - val_accuracy: 0.4996\n",
      "Epoch 33/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9197 - accuracy: 0.5003 - val_loss: 0.9219 - val_accuracy: 0.4996\n",
      "Epoch 34/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9201 - accuracy: 0.5003 - val_loss: 0.9200 - val_accuracy: 0.4996\n",
      "Epoch 35/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9202 - accuracy: 0.5003 - val_loss: 0.9209 - val_accuracy: 0.4996\n",
      "Epoch 36/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9197 - accuracy: 0.5003 - val_loss: 0.9189 - val_accuracy: 0.4996\n",
      "Epoch 37/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9198 - accuracy: 0.5003 - val_loss: 0.9201 - val_accuracy: 0.4996\n",
      "Epoch 38/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9197 - accuracy: 0.5003 - val_loss: 0.9251 - val_accuracy: 0.4996\n",
      "Epoch 39/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9059 - accuracy: 0.5255 - val_loss: 0.8915 - val_accuracy: 0.5939\n",
      "Epoch 40/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8478 - accuracy: 0.6280 - val_loss: 0.8414 - val_accuracy: 0.6341\n",
      "Epoch 41/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8404 - accuracy: 0.6338 - val_loss: 0.8411 - val_accuracy: 0.6332\n",
      "Epoch 42/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8391 - accuracy: 0.6350 - val_loss: 0.8371 - val_accuracy: 0.6385\n",
      "Epoch 43/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8375 - accuracy: 0.6372 - val_loss: 0.8332 - val_accuracy: 0.6426\n",
      "Epoch 44/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8366 - accuracy: 0.6385 - val_loss: 0.8398 - val_accuracy: 0.6357\n",
      "Epoch 45/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8362 - accuracy: 0.6382 - val_loss: 0.8375 - val_accuracy: 0.6361\n",
      "Epoch 46/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8362 - accuracy: 0.6378 - val_loss: 0.8327 - val_accuracy: 0.6426\n",
      "Epoch 47/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8351 - accuracy: 0.6385 - val_loss: 0.8374 - val_accuracy: 0.6359\n",
      "Epoch 48/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8354 - accuracy: 0.6383 - val_loss: 0.8370 - val_accuracy: 0.6377\n",
      "Epoch 49/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8359 - accuracy: 0.6383 - val_loss: 0.8386 - val_accuracy: 0.6363\n",
      "Epoch 50/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8360 - accuracy: 0.6383 - val_loss: 0.8442 - val_accuracy: 0.6315\n",
      "1714/1714 [==============================] - 5s 3ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 4, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7960143089294434\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 50, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 35}\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/50\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9219 - accuracy: 0.5619 - val_loss: 0.8789 - val_accuracy: 0.5920\n",
      "Epoch 2/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8650 - accuracy: 0.6019 - val_loss: 0.8540 - val_accuracy: 0.6073\n",
      "Epoch 3/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8483 - accuracy: 0.6112 - val_loss: 0.8585 - val_accuracy: 0.6064\n",
      "Epoch 4/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8377 - accuracy: 0.6175 - val_loss: 0.8434 - val_accuracy: 0.6125\n",
      "Epoch 5/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8290 - accuracy: 0.6215 - val_loss: 0.8267 - val_accuracy: 0.6232\n",
      "Epoch 6/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8205 - accuracy: 0.6264 - val_loss: 0.8167 - val_accuracy: 0.6278\n",
      "Epoch 7/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8157 - accuracy: 0.6292 - val_loss: 0.8148 - val_accuracy: 0.6287\n",
      "Epoch 8/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8132 - accuracy: 0.6303 - val_loss: 0.8223 - val_accuracy: 0.6229\n",
      "Epoch 9/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8109 - accuracy: 0.6320 - val_loss: 0.8081 - val_accuracy: 0.6329\n",
      "Epoch 10/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8079 - accuracy: 0.6331 - val_loss: 0.8038 - val_accuracy: 0.6374\n",
      "Epoch 11/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8057 - accuracy: 0.6350 - val_loss: 0.8409 - val_accuracy: 0.6010\n",
      "Epoch 12/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8037 - accuracy: 0.6357 - val_loss: 0.8055 - val_accuracy: 0.6337\n",
      "Epoch 13/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8029 - accuracy: 0.6367 - val_loss: 0.8121 - val_accuracy: 0.6291\n",
      "Epoch 14/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8019 - accuracy: 0.6369 - val_loss: 0.7928 - val_accuracy: 0.6443\n",
      "Epoch 15/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7988 - accuracy: 0.6393 - val_loss: 0.8057 - val_accuracy: 0.6331\n",
      "Epoch 16/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7967 - accuracy: 0.6407 - val_loss: 0.8207 - val_accuracy: 0.6216\n",
      "Epoch 17/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7982 - accuracy: 0.6400 - val_loss: 0.8100 - val_accuracy: 0.6304\n",
      "Epoch 18/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7946 - accuracy: 0.6416 - val_loss: 0.8002 - val_accuracy: 0.6380\n",
      "Epoch 19/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7947 - accuracy: 0.6417 - val_loss: 0.7924 - val_accuracy: 0.6432\n",
      "Epoch 20/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7925 - accuracy: 0.6431 - val_loss: 0.7890 - val_accuracy: 0.6463\n",
      "Epoch 21/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7925 - accuracy: 0.6429 - val_loss: 0.8154 - val_accuracy: 0.6256\n",
      "Epoch 22/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7917 - accuracy: 0.6429 - val_loss: 0.7910 - val_accuracy: 0.6465\n",
      "Epoch 23/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7899 - accuracy: 0.6447 - val_loss: 0.7872 - val_accuracy: 0.6467\n",
      "Epoch 24/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7895 - accuracy: 0.6445 - val_loss: 0.7903 - val_accuracy: 0.6423\n",
      "Epoch 25/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7869 - accuracy: 0.6468 - val_loss: 0.8173 - val_accuracy: 0.6237\n",
      "Epoch 26/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7889 - accuracy: 0.6445 - val_loss: 0.7859 - val_accuracy: 0.6463\n",
      "Epoch 27/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7885 - accuracy: 0.6445 - val_loss: 0.7865 - val_accuracy: 0.6531\n",
      "Epoch 28/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7871 - accuracy: 0.6459 - val_loss: 0.7901 - val_accuracy: 0.6413\n",
      "Epoch 29/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7871 - accuracy: 0.6462 - val_loss: 0.7837 - val_accuracy: 0.6482\n",
      "Epoch 30/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7874 - accuracy: 0.6457 - val_loss: 0.7914 - val_accuracy: 0.6432\n",
      "Epoch 31/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7846 - accuracy: 0.6476 - val_loss: 0.7881 - val_accuracy: 0.6419\n",
      "Epoch 32/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7847 - accuracy: 0.6473 - val_loss: 0.7799 - val_accuracy: 0.6493\n",
      "Epoch 33/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7853 - accuracy: 0.6472 - val_loss: 0.7827 - val_accuracy: 0.6481\n",
      "Epoch 34/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7854 - accuracy: 0.6458 - val_loss: 0.7815 - val_accuracy: 0.6471\n",
      "Epoch 35/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7849 - accuracy: 0.6475 - val_loss: 0.7854 - val_accuracy: 0.6461\n",
      "Epoch 36/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7837 - accuracy: 0.6474 - val_loss: 0.7877 - val_accuracy: 0.6411\n",
      "Epoch 37/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7835 - accuracy: 0.6484 - val_loss: 0.7882 - val_accuracy: 0.6422\n",
      "Epoch 38/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7823 - accuracy: 0.6487 - val_loss: 0.7829 - val_accuracy: 0.6504\n",
      "Epoch 39/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7832 - accuracy: 0.6478 - val_loss: 0.7771 - val_accuracy: 0.6530\n",
      "Epoch 40/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7856 - accuracy: 0.6467 - val_loss: 0.7881 - val_accuracy: 0.6456\n",
      "Epoch 41/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7827 - accuracy: 0.6483 - val_loss: 0.7957 - val_accuracy: 0.6395\n",
      "Epoch 42/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7830 - accuracy: 0.6481 - val_loss: 0.7782 - val_accuracy: 0.6512\n",
      "Epoch 43/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7818 - accuracy: 0.6491 - val_loss: 0.7824 - val_accuracy: 0.6493\n",
      "Epoch 44/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7813 - accuracy: 0.6486 - val_loss: 0.7881 - val_accuracy: 0.6405\n",
      "Epoch 45/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7807 - accuracy: 0.6496 - val_loss: 0.7743 - val_accuracy: 0.6525\n",
      "Epoch 46/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7816 - accuracy: 0.6485 - val_loss: 0.7875 - val_accuracy: 0.6463\n",
      "Epoch 47/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7804 - accuracy: 0.6495 - val_loss: 0.7885 - val_accuracy: 0.6415\n",
      "Epoch 48/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7796 - accuracy: 0.6502 - val_loss: 0.7852 - val_accuracy: 0.6453\n",
      "Epoch 49/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7797 - accuracy: 0.6499 - val_loss: 0.8235 - val_accuracy: 0.6146\n",
      "Epoch 50/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7797 - accuracy: 0.6495 - val_loss: 0.7755 - val_accuracy: 0.6517\n",
      "1714/1714 [==============================] - 5s 3ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 50, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/50\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9185 - accuracy: 0.5651 - val_loss: 0.8781 - val_accuracy: 0.5972\n",
      "Epoch 2/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8506 - accuracy: 0.6156 - val_loss: 0.8334 - val_accuracy: 0.6284\n",
      "Epoch 3/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8350 - accuracy: 0.6250 - val_loss: 0.8269 - val_accuracy: 0.6333\n",
      "Epoch 4/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8280 - accuracy: 0.6302 - val_loss: 0.8331 - val_accuracy: 0.6239\n",
      "Epoch 5/50\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8246 - accuracy: 0.6319 - val_loss: 0.8217 - val_accuracy: 0.6336\n",
      "Epoch 6/50\n",
      "857/857 [==============================] - 19s 22ms/step - loss: 0.8216 - accuracy: 0.6341 - val_loss: 0.8163 - val_accuracy: 0.6390\n",
      "Epoch 7/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8191 - accuracy: 0.6359 - val_loss: 0.8169 - val_accuracy: 0.6388\n",
      "Epoch 8/50\n",
      "857/857 [==============================] - 14s 17ms/step - loss: 0.8177 - accuracy: 0.6366 - val_loss: 0.8154 - val_accuracy: 0.6394\n",
      "Epoch 9/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8166 - accuracy: 0.6373 - val_loss: 0.8212 - val_accuracy: 0.6343\n",
      "Epoch 10/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8154 - accuracy: 0.6379 - val_loss: 0.8087 - val_accuracy: 0.6438\n",
      "Epoch 11/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8138 - accuracy: 0.6398 - val_loss: 0.8204 - val_accuracy: 0.6343\n",
      "Epoch 12/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8126 - accuracy: 0.6403 - val_loss: 0.8148 - val_accuracy: 0.6373\n",
      "Epoch 13/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8124 - accuracy: 0.6410 - val_loss: 0.8077 - val_accuracy: 0.6451\n",
      "Epoch 14/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8131 - accuracy: 0.6404 - val_loss: 0.8083 - val_accuracy: 0.6432\n",
      "Epoch 15/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8112 - accuracy: 0.6410 - val_loss: 0.8148 - val_accuracy: 0.6377\n",
      "Epoch 16/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8100 - accuracy: 0.6412 - val_loss: 0.8195 - val_accuracy: 0.6362\n",
      "Epoch 17/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8099 - accuracy: 0.6412 - val_loss: 0.8190 - val_accuracy: 0.6347\n",
      "Epoch 18/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8099 - accuracy: 0.6423 - val_loss: 0.8067 - val_accuracy: 0.6438\n",
      "Epoch 19/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8091 - accuracy: 0.6425 - val_loss: 0.8061 - val_accuracy: 0.6456\n",
      "Epoch 20/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8088 - accuracy: 0.6426 - val_loss: 0.8098 - val_accuracy: 0.6398\n",
      "Epoch 21/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8085 - accuracy: 0.6428 - val_loss: 0.8156 - val_accuracy: 0.6373\n",
      "Epoch 22/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8068 - accuracy: 0.6439 - val_loss: 0.8153 - val_accuracy: 0.6420\n",
      "Epoch 23/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8071 - accuracy: 0.6434 - val_loss: 0.8091 - val_accuracy: 0.6453\n",
      "Epoch 24/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8075 - accuracy: 0.6437 - val_loss: 0.8062 - val_accuracy: 0.6443\n",
      "Epoch 25/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8064 - accuracy: 0.6446 - val_loss: 0.8318 - val_accuracy: 0.6188\n",
      "Epoch 26/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8064 - accuracy: 0.6447 - val_loss: 0.8105 - val_accuracy: 0.6418\n",
      "Epoch 27/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8065 - accuracy: 0.6445 - val_loss: 0.8275 - val_accuracy: 0.6266\n",
      "Epoch 28/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8068 - accuracy: 0.6437 - val_loss: 0.8120 - val_accuracy: 0.6404\n",
      "Epoch 29/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8065 - accuracy: 0.6445 - val_loss: 0.8031 - val_accuracy: 0.6465\n",
      "Epoch 30/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8051 - accuracy: 0.6455 - val_loss: 0.8018 - val_accuracy: 0.6486\n",
      "Epoch 31/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8057 - accuracy: 0.6443 - val_loss: 0.8093 - val_accuracy: 0.6412\n",
      "Epoch 32/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8047 - accuracy: 0.6454 - val_loss: 0.8021 - val_accuracy: 0.6478\n",
      "Epoch 33/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8049 - accuracy: 0.6459 - val_loss: 0.8087 - val_accuracy: 0.6383\n",
      "Epoch 34/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8045 - accuracy: 0.6458 - val_loss: 0.8119 - val_accuracy: 0.6349\n",
      "Epoch 35/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8047 - accuracy: 0.6456 - val_loss: 0.8020 - val_accuracy: 0.6484\n",
      "Epoch 36/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8051 - accuracy: 0.6451 - val_loss: 0.8049 - val_accuracy: 0.6484\n",
      "Epoch 37/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8041 - accuracy: 0.6467 - val_loss: 0.8016 - val_accuracy: 0.6516\n",
      "Epoch 38/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8048 - accuracy: 0.6455 - val_loss: 0.8118 - val_accuracy: 0.6422\n",
      "Epoch 39/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8038 - accuracy: 0.6465 - val_loss: 0.8044 - val_accuracy: 0.6470\n",
      "Epoch 40/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8032 - accuracy: 0.6464 - val_loss: 0.8103 - val_accuracy: 0.6388\n",
      "Epoch 41/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8042 - accuracy: 0.6464 - val_loss: 0.8010 - val_accuracy: 0.6485\n",
      "Epoch 42/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8034 - accuracy: 0.6470 - val_loss: 0.8000 - val_accuracy: 0.6522\n",
      "Epoch 43/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8037 - accuracy: 0.6461 - val_loss: 0.8035 - val_accuracy: 0.6439\n",
      "Epoch 44/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8042 - accuracy: 0.6458 - val_loss: 0.8084 - val_accuracy: 0.6423\n",
      "Epoch 45/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8040 - accuracy: 0.6456 - val_loss: 0.8058 - val_accuracy: 0.6484\n",
      "Epoch 46/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8036 - accuracy: 0.6467 - val_loss: 0.7990 - val_accuracy: 0.6524\n",
      "Epoch 47/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8027 - accuracy: 0.6474 - val_loss: 0.8092 - val_accuracy: 0.6392\n",
      "Epoch 48/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8041 - accuracy: 0.6454 - val_loss: 0.8091 - val_accuracy: 0.6462\n",
      "Epoch 49/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8029 - accuracy: 0.6464 - val_loss: 0.8076 - val_accuracy: 0.6469\n",
      "Epoch 50/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8028 - accuracy: 0.6469 - val_loss: 0.8017 - val_accuracy: 0.6483\n",
      "1714/1714 [==============================] - 5s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 50, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 100}\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/50\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 1.0263 - accuracy: 0.5006 - val_loss: 0.9279 - val_accuracy: 0.5455\n",
      "Epoch 2/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8858 - accuracy: 0.5918 - val_loss: 0.8632 - val_accuracy: 0.6081\n",
      "Epoch 3/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8536 - accuracy: 0.6132 - val_loss: 0.8449 - val_accuracy: 0.6168\n",
      "Epoch 4/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8429 - accuracy: 0.6194 - val_loss: 0.8455 - val_accuracy: 0.6141\n",
      "Epoch 5/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8361 - accuracy: 0.6243 - val_loss: 0.8404 - val_accuracy: 0.6179\n",
      "Epoch 6/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8357 - accuracy: 0.6236 - val_loss: 0.8396 - val_accuracy: 0.6188\n",
      "Epoch 7/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8311 - accuracy: 0.6269 - val_loss: 0.8300 - val_accuracy: 0.6267\n",
      "Epoch 8/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8301 - accuracy: 0.6276 - val_loss: 0.8271 - val_accuracy: 0.6281\n",
      "Epoch 9/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8282 - accuracy: 0.6292 - val_loss: 0.8302 - val_accuracy: 0.6258\n",
      "Epoch 10/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8271 - accuracy: 0.6297 - val_loss: 0.8226 - val_accuracy: 0.6322\n",
      "Epoch 11/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8257 - accuracy: 0.6305 - val_loss: 0.8248 - val_accuracy: 0.6278\n",
      "Epoch 12/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8247 - accuracy: 0.6308 - val_loss: 0.8265 - val_accuracy: 0.6229\n",
      "Epoch 13/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8227 - accuracy: 0.6327 - val_loss: 0.8215 - val_accuracy: 0.6348\n",
      "Epoch 14/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8216 - accuracy: 0.6332 - val_loss: 0.8225 - val_accuracy: 0.6296\n",
      "Epoch 15/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8213 - accuracy: 0.6337 - val_loss: 0.8242 - val_accuracy: 0.6322\n",
      "Epoch 16/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8199 - accuracy: 0.6351 - val_loss: 0.8204 - val_accuracy: 0.6314\n",
      "Epoch 17/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8188 - accuracy: 0.6358 - val_loss: 0.8146 - val_accuracy: 0.6372\n",
      "Epoch 18/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8172 - accuracy: 0.6366 - val_loss: 0.8174 - val_accuracy: 0.6358\n",
      "Epoch 19/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8177 - accuracy: 0.6360 - val_loss: 0.8139 - val_accuracy: 0.6396\n",
      "Epoch 20/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8155 - accuracy: 0.6373 - val_loss: 0.8357 - val_accuracy: 0.6221\n",
      "Epoch 21/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8143 - accuracy: 0.6389 - val_loss: 0.8147 - val_accuracy: 0.6349\n",
      "Epoch 22/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8146 - accuracy: 0.6383 - val_loss: 0.8148 - val_accuracy: 0.6384\n",
      "Epoch 23/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8134 - accuracy: 0.6388 - val_loss: 0.8120 - val_accuracy: 0.6404\n",
      "Epoch 24/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8123 - accuracy: 0.6396 - val_loss: 0.8147 - val_accuracy: 0.6392\n",
      "Epoch 25/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8117 - accuracy: 0.6403 - val_loss: 0.8167 - val_accuracy: 0.6342\n",
      "Epoch 26/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8113 - accuracy: 0.6409 - val_loss: 0.8132 - val_accuracy: 0.6409\n",
      "Epoch 27/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8114 - accuracy: 0.6403 - val_loss: 0.8082 - val_accuracy: 0.6446\n",
      "Epoch 28/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8099 - accuracy: 0.6416 - val_loss: 0.8114 - val_accuracy: 0.6401\n",
      "Epoch 29/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8102 - accuracy: 0.6416 - val_loss: 0.8204 - val_accuracy: 0.6300\n",
      "Epoch 30/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8092 - accuracy: 0.6419 - val_loss: 0.8078 - val_accuracy: 0.6447\n",
      "Epoch 31/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8089 - accuracy: 0.6417 - val_loss: 0.8154 - val_accuracy: 0.6368\n",
      "Epoch 32/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8091 - accuracy: 0.6417 - val_loss: 0.8095 - val_accuracy: 0.6383\n",
      "Epoch 33/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8081 - accuracy: 0.6424 - val_loss: 0.8126 - val_accuracy: 0.6375\n",
      "Epoch 34/50\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8075 - accuracy: 0.6427 - val_loss: 0.8076 - val_accuracy: 0.6441\n",
      "Epoch 35/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8072 - accuracy: 0.6432 - val_loss: 0.8054 - val_accuracy: 0.6448\n",
      "Epoch 36/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8063 - accuracy: 0.6439 - val_loss: 0.8104 - val_accuracy: 0.6418\n",
      "Epoch 37/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8068 - accuracy: 0.6437 - val_loss: 0.8058 - val_accuracy: 0.6456\n",
      "Epoch 38/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8063 - accuracy: 0.6440 - val_loss: 0.8058 - val_accuracy: 0.6455\n",
      "Epoch 39/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8059 - accuracy: 0.6443 - val_loss: 0.8068 - val_accuracy: 0.6442\n",
      "Epoch 40/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8054 - accuracy: 0.6451 - val_loss: 0.8212 - val_accuracy: 0.6294\n",
      "Epoch 41/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8058 - accuracy: 0.6440 - val_loss: 0.8130 - val_accuracy: 0.6347\n",
      "Epoch 42/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8052 - accuracy: 0.6446 - val_loss: 0.8031 - val_accuracy: 0.6448\n",
      "Epoch 43/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8050 - accuracy: 0.6452 - val_loss: 0.8037 - val_accuracy: 0.6469\n",
      "Epoch 44/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8053 - accuracy: 0.6445 - val_loss: 0.8011 - val_accuracy: 0.6489\n",
      "Epoch 45/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8047 - accuracy: 0.6445 - val_loss: 0.8049 - val_accuracy: 0.6443\n",
      "Epoch 46/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8034 - accuracy: 0.6460 - val_loss: 0.8083 - val_accuracy: 0.6456\n",
      "Epoch 47/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8039 - accuracy: 0.6458 - val_loss: 0.8080 - val_accuracy: 0.6401\n",
      "Epoch 48/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8040 - accuracy: 0.6452 - val_loss: 0.8252 - val_accuracy: 0.6277\n",
      "Epoch 49/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8044 - accuracy: 0.6443 - val_loss: 0.8114 - val_accuracy: 0.6414\n",
      "Epoch 50/50\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8046 - accuracy: 0.6451 - val_loss: 0.8068 - val_accuracy: 0.6436\n",
      "1714/1714 [==============================] - 5s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 4}\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9590 - accuracy: 0.5002 - val_loss: 0.9352 - val_accuracy: 0.4996\n",
      "Epoch 2/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9331 - accuracy: 0.5003 - val_loss: 0.9300 - val_accuracy: 0.4996\n",
      "Epoch 3/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9296 - accuracy: 0.5003 - val_loss: 0.9308 - val_accuracy: 0.4996\n",
      "Epoch 4/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9267 - accuracy: 0.5003 - val_loss: 0.9254 - val_accuracy: 0.4996\n",
      "Epoch 5/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9254 - accuracy: 0.5003 - val_loss: 0.9240 - val_accuracy: 0.4996\n",
      "Epoch 6/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9238 - accuracy: 0.5003 - val_loss: 0.9236 - val_accuracy: 0.4996\n",
      "Epoch 7/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9230 - accuracy: 0.5003 - val_loss: 0.9274 - val_accuracy: 0.4996\n",
      "Epoch 8/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9216 - accuracy: 0.5003 - val_loss: 0.9205 - val_accuracy: 0.4996\n",
      "Epoch 9/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9210 - accuracy: 0.5003 - val_loss: 0.9250 - val_accuracy: 0.4996\n",
      "Epoch 10/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9201 - accuracy: 0.5003 - val_loss: 0.9203 - val_accuracy: 0.4996\n",
      "Epoch 11/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9198 - accuracy: 0.5003 - val_loss: 0.9222 - val_accuracy: 0.4996\n",
      "Epoch 12/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9196 - accuracy: 0.5003 - val_loss: 0.9190 - val_accuracy: 0.4996\n",
      "Epoch 13/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9193 - accuracy: 0.5003 - val_loss: 0.9177 - val_accuracy: 0.4996\n",
      "Epoch 14/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9193 - accuracy: 0.5003 - val_loss: 0.9202 - val_accuracy: 0.4996\n",
      "Epoch 15/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9186 - accuracy: 0.5003 - val_loss: 0.9185 - val_accuracy: 0.4996\n",
      "Epoch 16/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9185 - accuracy: 0.5003 - val_loss: 0.9208 - val_accuracy: 0.4996\n",
      "Epoch 17/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9191 - accuracy: 0.5003 - val_loss: 0.9190 - val_accuracy: 0.4996\n",
      "Epoch 18/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9183 - accuracy: 0.5003 - val_loss: 0.9182 - val_accuracy: 0.4996\n",
      "Epoch 19/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9182 - accuracy: 0.5003 - val_loss: 0.9171 - val_accuracy: 0.4996\n",
      "Epoch 20/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9179 - accuracy: 0.5003 - val_loss: 0.9183 - val_accuracy: 0.4996\n",
      "Epoch 21/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9180 - accuracy: 0.5003 - val_loss: 0.9181 - val_accuracy: 0.4996\n",
      "Epoch 22/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9182 - accuracy: 0.5003 - val_loss: 0.9181 - val_accuracy: 0.4996\n",
      "Epoch 23/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9175 - accuracy: 0.5003 - val_loss: 0.9175 - val_accuracy: 0.4996\n",
      "Epoch 24/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9174 - accuracy: 0.5003 - val_loss: 0.9174 - val_accuracy: 0.4996\n",
      "Epoch 25/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9173 - accuracy: 0.5003 - val_loss: 0.9186 - val_accuracy: 0.4996\n",
      "Epoch 26/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9171 - accuracy: 0.5003 - val_loss: 0.9188 - val_accuracy: 0.4996\n",
      "Epoch 27/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9170 - accuracy: 0.5003 - val_loss: 0.9179 - val_accuracy: 0.4996\n",
      "Epoch 28/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9170 - accuracy: 0.5003 - val_loss: 0.9179 - val_accuracy: 0.4996\n",
      "Epoch 29/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9170 - accuracy: 0.5003 - val_loss: 0.9212 - val_accuracy: 0.4996\n",
      "Epoch 30/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9167 - accuracy: 0.5003 - val_loss: 0.9171 - val_accuracy: 0.4996\n",
      "Epoch 31/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9172 - accuracy: 0.5003 - val_loss: 0.9166 - val_accuracy: 0.4996\n",
      "Epoch 32/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9165 - accuracy: 0.5003 - val_loss: 0.9181 - val_accuracy: 0.4996\n",
      "Epoch 33/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9168 - accuracy: 0.5003 - val_loss: 0.9157 - val_accuracy: 0.4996\n",
      "Epoch 34/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9165 - accuracy: 0.5003 - val_loss: 0.9247 - val_accuracy: 0.4996\n",
      "Epoch 35/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9167 - accuracy: 0.5003 - val_loss: 0.9248 - val_accuracy: 0.4996\n",
      "Epoch 36/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9168 - accuracy: 0.5003 - val_loss: 0.9156 - val_accuracy: 0.4996\n",
      "Epoch 37/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9162 - accuracy: 0.5003 - val_loss: 0.9187 - val_accuracy: 0.4996\n",
      "Epoch 38/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9165 - accuracy: 0.5003 - val_loss: 0.9161 - val_accuracy: 0.4996\n",
      "Epoch 39/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9163 - accuracy: 0.5003 - val_loss: 0.9155 - val_accuracy: 0.4996\n",
      "Epoch 40/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9165 - accuracy: 0.5003 - val_loss: 0.9175 - val_accuracy: 0.4996\n",
      "Epoch 41/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9165 - accuracy: 0.5003 - val_loss: 0.9156 - val_accuracy: 0.4996\n",
      "Epoch 42/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9157 - accuracy: 0.5003 - val_loss: 0.9162 - val_accuracy: 0.4996\n",
      "Epoch 43/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9161 - accuracy: 0.5003 - val_loss: 0.9167 - val_accuracy: 0.4996\n",
      "Epoch 44/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9164 - accuracy: 0.5003 - val_loss: 0.9168 - val_accuracy: 0.4996\n",
      "Epoch 45/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9162 - accuracy: 0.5003 - val_loss: 0.9169 - val_accuracy: 0.4996\n",
      "Epoch 46/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9160 - accuracy: 0.5003 - val_loss: 0.9169 - val_accuracy: 0.4996\n",
      "Epoch 47/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9159 - accuracy: 0.5003 - val_loss: 0.9165 - val_accuracy: 0.4996\n",
      "Epoch 48/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9160 - accuracy: 0.5003 - val_loss: 0.9171 - val_accuracy: 0.4996\n",
      "Epoch 49/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9160 - accuracy: 0.5003 - val_loss: 0.9208 - val_accuracy: 0.4996\n",
      "Epoch 50/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9160 - accuracy: 0.5003 - val_loss: 0.9157 - val_accuracy: 0.4996\n",
      "Epoch 51/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9160 - accuracy: 0.5003 - val_loss: 0.9167 - val_accuracy: 0.4996\n",
      "Epoch 52/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9157 - accuracy: 0.5003 - val_loss: 0.9171 - val_accuracy: 0.4996\n",
      "Epoch 53/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9156 - accuracy: 0.5003 - val_loss: 0.9174 - val_accuracy: 0.4996\n",
      "Epoch 54/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9156 - accuracy: 0.5003 - val_loss: 0.9156 - val_accuracy: 0.4996\n",
      "Epoch 55/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9157 - accuracy: 0.5003 - val_loss: 0.9183 - val_accuracy: 0.4996\n",
      "Epoch 56/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9158 - accuracy: 0.5003 - val_loss: 0.9162 - val_accuracy: 0.4996\n",
      "Epoch 57/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9157 - accuracy: 0.5003 - val_loss: 0.9157 - val_accuracy: 0.4996\n",
      "Epoch 58/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9158 - accuracy: 0.5003 - val_loss: 0.9146 - val_accuracy: 0.4996\n",
      "Epoch 59/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9149 - val_accuracy: 0.4996\n",
      "Epoch 60/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9160 - accuracy: 0.5003 - val_loss: 0.9164 - val_accuracy: 0.4996\n",
      "Epoch 61/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9154 - accuracy: 0.5003 - val_loss: 0.9147 - val_accuracy: 0.4996\n",
      "Epoch 62/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9162 - val_accuracy: 0.4996\n",
      "Epoch 63/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9140 - val_accuracy: 0.4996\n",
      "Epoch 64/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9151 - accuracy: 0.5003 - val_loss: 0.9144 - val_accuracy: 0.4996\n",
      "Epoch 65/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9152 - val_accuracy: 0.4996\n",
      "Epoch 66/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9157 - accuracy: 0.5003 - val_loss: 0.9156 - val_accuracy: 0.4996\n",
      "Epoch 67/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9156 - accuracy: 0.5003 - val_loss: 0.9161 - val_accuracy: 0.4996\n",
      "Epoch 68/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9140 - val_accuracy: 0.4996\n",
      "Epoch 69/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9179 - val_accuracy: 0.4996\n",
      "Epoch 70/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9164 - val_accuracy: 0.4996\n",
      "Epoch 71/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9148 - accuracy: 0.5003 - val_loss: 0.9182 - val_accuracy: 0.4996\n",
      "Epoch 72/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9138 - val_accuracy: 0.4996\n",
      "Epoch 73/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9149 - val_accuracy: 0.4996\n",
      "Epoch 74/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9150 - accuracy: 0.5003 - val_loss: 0.9181 - val_accuracy: 0.4996\n",
      "Epoch 75/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9154 - accuracy: 0.5003 - val_loss: 0.9212 - val_accuracy: 0.4996\n",
      "Epoch 76/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9158 - val_accuracy: 0.4996\n",
      "Epoch 77/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9145 - val_accuracy: 0.4996\n",
      "Epoch 78/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9149 - val_accuracy: 0.4996\n",
      "Epoch 79/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9151 - accuracy: 0.5003 - val_loss: 0.9171 - val_accuracy: 0.4996\n",
      "Epoch 80/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9210 - val_accuracy: 0.4996\n",
      "Epoch 81/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9154 - accuracy: 0.5003 - val_loss: 0.9180 - val_accuracy: 0.4996\n",
      "Epoch 82/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9157 - accuracy: 0.5003 - val_loss: 0.9144 - val_accuracy: 0.4996\n",
      "Epoch 83/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9145 - val_accuracy: 0.4996\n",
      "Epoch 84/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9139 - val_accuracy: 0.4996\n",
      "Epoch 85/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9150 - accuracy: 0.5003 - val_loss: 0.9136 - val_accuracy: 0.4996\n",
      "Epoch 86/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9150 - accuracy: 0.5003 - val_loss: 0.9158 - val_accuracy: 0.4996\n",
      "Epoch 87/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9141 - val_accuracy: 0.4996\n",
      "Epoch 88/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9145 - val_accuracy: 0.4996\n",
      "Epoch 89/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9154 - accuracy: 0.5003 - val_loss: 0.9168 - val_accuracy: 0.4996\n",
      "Epoch 90/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9151 - accuracy: 0.5003 - val_loss: 0.9147 - val_accuracy: 0.4996\n",
      "Epoch 91/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9147 - val_accuracy: 0.4996\n",
      "Epoch 92/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9149 - accuracy: 0.5003 - val_loss: 0.9162 - val_accuracy: 0.4996\n",
      "Epoch 93/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9173 - val_accuracy: 0.4996\n",
      "Epoch 94/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9155 - accuracy: 0.5003 - val_loss: 0.9162 - val_accuracy: 0.4996\n",
      "Epoch 95/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9151 - accuracy: 0.5003 - val_loss: 0.9150 - val_accuracy: 0.4996\n",
      "Epoch 96/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9149 - val_accuracy: 0.4996\n",
      "Epoch 97/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9158 - accuracy: 0.5003 - val_loss: 0.9204 - val_accuracy: 0.4996\n",
      "Epoch 98/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9152 - accuracy: 0.5003 - val_loss: 0.9150 - val_accuracy: 0.4996\n",
      "Epoch 99/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.9153 - accuracy: 0.5003 - val_loss: 0.9193 - val_accuracy: 0.4996\n",
      "Epoch 100/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.9156 - accuracy: 0.5003 - val_loss: 0.9149 - val_accuracy: 0.4996\n",
      "1714/1714 [==============================] - 5s 3ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 35}\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9519 - accuracy: 0.5410 - val_loss: 0.9004 - val_accuracy: 0.5756\n",
      "Epoch 2/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8661 - accuracy: 0.6040 - val_loss: 0.8556 - val_accuracy: 0.6094\n",
      "Epoch 3/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8457 - accuracy: 0.6147 - val_loss: 0.8425 - val_accuracy: 0.6189\n",
      "Epoch 4/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8367 - accuracy: 0.6210 - val_loss: 0.8398 - val_accuracy: 0.6193\n",
      "Epoch 5/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8303 - accuracy: 0.6257 - val_loss: 0.8279 - val_accuracy: 0.6292\n",
      "Epoch 6/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8253 - accuracy: 0.6293 - val_loss: 0.8273 - val_accuracy: 0.6273\n",
      "Epoch 7/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8231 - accuracy: 0.6303 - val_loss: 0.8284 - val_accuracy: 0.6277\n",
      "Epoch 8/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8200 - accuracy: 0.6338 - val_loss: 0.8180 - val_accuracy: 0.6333\n",
      "Epoch 9/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8175 - accuracy: 0.6348 - val_loss: 0.8156 - val_accuracy: 0.6339\n",
      "Epoch 10/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8149 - accuracy: 0.6369 - val_loss: 0.8125 - val_accuracy: 0.6382\n",
      "Epoch 11/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8137 - accuracy: 0.6369 - val_loss: 0.8198 - val_accuracy: 0.6306\n",
      "Epoch 12/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8120 - accuracy: 0.6383 - val_loss: 0.8142 - val_accuracy: 0.6399\n",
      "Epoch 13/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8111 - accuracy: 0.6395 - val_loss: 0.8103 - val_accuracy: 0.6371\n",
      "Epoch 14/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8106 - accuracy: 0.6389 - val_loss: 0.8127 - val_accuracy: 0.6344\n",
      "Epoch 15/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8086 - accuracy: 0.6406 - val_loss: 0.8090 - val_accuracy: 0.6422\n",
      "Epoch 16/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8081 - accuracy: 0.6396 - val_loss: 0.8125 - val_accuracy: 0.6360\n",
      "Epoch 17/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8067 - accuracy: 0.6413 - val_loss: 0.8046 - val_accuracy: 0.6406\n",
      "Epoch 18/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8060 - accuracy: 0.6415 - val_loss: 0.8055 - val_accuracy: 0.6434\n",
      "Epoch 19/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8044 - accuracy: 0.6421 - val_loss: 0.8114 - val_accuracy: 0.6381\n",
      "Epoch 20/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8022 - accuracy: 0.6443 - val_loss: 0.8038 - val_accuracy: 0.6445\n",
      "Epoch 21/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8019 - accuracy: 0.6444 - val_loss: 0.7995 - val_accuracy: 0.6480\n",
      "Epoch 22/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8003 - accuracy: 0.6453 - val_loss: 0.8024 - val_accuracy: 0.6449\n",
      "Epoch 23/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7983 - accuracy: 0.6454 - val_loss: 0.8003 - val_accuracy: 0.6428\n",
      "Epoch 24/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7978 - accuracy: 0.6454 - val_loss: 0.7965 - val_accuracy: 0.6474\n",
      "Epoch 25/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7954 - accuracy: 0.6462 - val_loss: 0.7990 - val_accuracy: 0.6471\n",
      "Epoch 26/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7932 - accuracy: 0.6474 - val_loss: 0.7920 - val_accuracy: 0.6508\n",
      "Epoch 27/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7920 - accuracy: 0.6463 - val_loss: 0.8026 - val_accuracy: 0.6398\n",
      "Epoch 28/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7898 - accuracy: 0.6466 - val_loss: 0.7877 - val_accuracy: 0.6480\n",
      "Epoch 29/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7891 - accuracy: 0.6467 - val_loss: 0.7920 - val_accuracy: 0.6429\n",
      "Epoch 30/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7884 - accuracy: 0.6471 - val_loss: 0.7982 - val_accuracy: 0.6400\n",
      "Epoch 31/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7879 - accuracy: 0.6470 - val_loss: 0.7859 - val_accuracy: 0.6479\n",
      "Epoch 32/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7866 - accuracy: 0.6472 - val_loss: 0.7938 - val_accuracy: 0.6413\n",
      "Epoch 33/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7853 - accuracy: 0.6485 - val_loss: 0.7828 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7849 - accuracy: 0.6482 - val_loss: 0.7880 - val_accuracy: 0.6472\n",
      "Epoch 35/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7839 - accuracy: 0.6492 - val_loss: 0.7882 - val_accuracy: 0.6474\n",
      "Epoch 36/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7836 - accuracy: 0.6494 - val_loss: 0.7859 - val_accuracy: 0.6469\n",
      "Epoch 37/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7816 - accuracy: 0.6505 - val_loss: 0.7983 - val_accuracy: 0.6387\n",
      "Epoch 38/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7812 - accuracy: 0.6505 - val_loss: 0.7858 - val_accuracy: 0.6489\n",
      "Epoch 39/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7816 - accuracy: 0.6499 - val_loss: 0.7867 - val_accuracy: 0.6459\n",
      "Epoch 40/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7794 - accuracy: 0.6515 - val_loss: 0.7928 - val_accuracy: 0.6474\n",
      "Epoch 41/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7804 - accuracy: 0.6509 - val_loss: 0.7832 - val_accuracy: 0.6482\n",
      "Epoch 42/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7790 - accuracy: 0.6518 - val_loss: 0.7832 - val_accuracy: 0.6499\n",
      "Epoch 43/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7792 - accuracy: 0.6522 - val_loss: 0.7853 - val_accuracy: 0.6492\n",
      "Epoch 44/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7789 - accuracy: 0.6526 - val_loss: 0.8083 - val_accuracy: 0.6314\n",
      "Epoch 45/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7782 - accuracy: 0.6535 - val_loss: 0.7856 - val_accuracy: 0.6474\n",
      "Epoch 46/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7780 - accuracy: 0.6541 - val_loss: 0.7940 - val_accuracy: 0.6402\n",
      "Epoch 47/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7776 - accuracy: 0.6540 - val_loss: 0.7760 - val_accuracy: 0.6568\n",
      "Epoch 48/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7769 - accuracy: 0.6539 - val_loss: 0.7826 - val_accuracy: 0.6472\n",
      "Epoch 49/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7775 - accuracy: 0.6544 - val_loss: 0.7824 - val_accuracy: 0.6516\n",
      "Epoch 50/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7780 - accuracy: 0.6533 - val_loss: 0.7808 - val_accuracy: 0.6531\n",
      "Epoch 51/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7772 - accuracy: 0.6538 - val_loss: 0.7795 - val_accuracy: 0.6520\n",
      "Epoch 52/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7768 - accuracy: 0.6543 - val_loss: 0.7803 - val_accuracy: 0.6513\n",
      "Epoch 53/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7761 - accuracy: 0.6545 - val_loss: 0.7809 - val_accuracy: 0.6515\n",
      "Epoch 54/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7768 - accuracy: 0.6539 - val_loss: 0.7860 - val_accuracy: 0.6482\n",
      "Epoch 55/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7766 - accuracy: 0.6544 - val_loss: 0.7754 - val_accuracy: 0.6567\n",
      "Epoch 56/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7760 - accuracy: 0.6544 - val_loss: 0.7810 - val_accuracy: 0.6528\n",
      "Epoch 57/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7761 - accuracy: 0.6549 - val_loss: 0.7816 - val_accuracy: 0.6475\n",
      "Epoch 58/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7760 - accuracy: 0.6544 - val_loss: 0.7864 - val_accuracy: 0.6525\n",
      "Epoch 59/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7752 - accuracy: 0.6554 - val_loss: 0.7818 - val_accuracy: 0.6500\n",
      "Epoch 60/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7762 - accuracy: 0.6549 - val_loss: 0.7774 - val_accuracy: 0.6542\n",
      "Epoch 61/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7757 - accuracy: 0.6545 - val_loss: 0.7780 - val_accuracy: 0.6512\n",
      "Epoch 62/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7753 - accuracy: 0.6556 - val_loss: 0.7856 - val_accuracy: 0.6454\n",
      "Epoch 63/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7762 - accuracy: 0.6540 - val_loss: 0.7802 - val_accuracy: 0.6553\n",
      "Epoch 64/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7758 - accuracy: 0.6552 - val_loss: 0.7752 - val_accuracy: 0.6542\n",
      "Epoch 65/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7746 - accuracy: 0.6549 - val_loss: 0.7754 - val_accuracy: 0.6546\n",
      "Epoch 66/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7752 - accuracy: 0.6547 - val_loss: 0.7814 - val_accuracy: 0.6510\n",
      "Epoch 67/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7750 - accuracy: 0.6559 - val_loss: 0.7776 - val_accuracy: 0.6547\n",
      "Epoch 68/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7747 - accuracy: 0.6555 - val_loss: 0.7774 - val_accuracy: 0.6540\n",
      "Epoch 69/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7746 - accuracy: 0.6559 - val_loss: 0.7828 - val_accuracy: 0.6498\n",
      "Epoch 70/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7746 - accuracy: 0.6563 - val_loss: 0.7790 - val_accuracy: 0.6500\n",
      "Epoch 71/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7751 - accuracy: 0.6552 - val_loss: 0.7787 - val_accuracy: 0.6530\n",
      "Epoch 72/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7743 - accuracy: 0.6552 - val_loss: 0.7789 - val_accuracy: 0.6504\n",
      "Epoch 73/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7743 - accuracy: 0.6555 - val_loss: 0.7754 - val_accuracy: 0.6560\n",
      "Epoch 74/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7741 - accuracy: 0.6554 - val_loss: 0.7754 - val_accuracy: 0.6574\n",
      "Epoch 75/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7735 - accuracy: 0.6552 - val_loss: 0.7811 - val_accuracy: 0.6500\n",
      "Epoch 76/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7746 - accuracy: 0.6543 - val_loss: 0.7746 - val_accuracy: 0.6552\n",
      "Epoch 77/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7747 - accuracy: 0.6543 - val_loss: 0.7785 - val_accuracy: 0.6529\n",
      "Epoch 78/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7737 - accuracy: 0.6560 - val_loss: 0.7765 - val_accuracy: 0.6538\n",
      "Epoch 79/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7744 - accuracy: 0.6550 - val_loss: 0.7814 - val_accuracy: 0.6513\n",
      "Epoch 80/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7752 - accuracy: 0.6546 - val_loss: 0.7787 - val_accuracy: 0.6554\n",
      "Epoch 81/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7745 - accuracy: 0.6555 - val_loss: 0.7729 - val_accuracy: 0.6552\n",
      "Epoch 82/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7743 - accuracy: 0.6555 - val_loss: 0.7781 - val_accuracy: 0.6529\n",
      "Epoch 83/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7736 - accuracy: 0.6555 - val_loss: 0.7825 - val_accuracy: 0.6482\n",
      "Epoch 84/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7746 - accuracy: 0.6551 - val_loss: 0.7782 - val_accuracy: 0.6534\n",
      "Epoch 85/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7737 - accuracy: 0.6549 - val_loss: 0.7760 - val_accuracy: 0.6561\n",
      "Epoch 86/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7731 - accuracy: 0.6563 - val_loss: 0.7723 - val_accuracy: 0.6570\n",
      "Epoch 87/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7733 - accuracy: 0.6560 - val_loss: 0.7775 - val_accuracy: 0.6515\n",
      "Epoch 88/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7744 - accuracy: 0.6559 - val_loss: 0.7792 - val_accuracy: 0.6536\n",
      "Epoch 89/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7744 - accuracy: 0.6552 - val_loss: 0.7732 - val_accuracy: 0.6581\n",
      "Epoch 90/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7736 - accuracy: 0.6554 - val_loss: 0.7758 - val_accuracy: 0.6544\n",
      "Epoch 91/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7735 - accuracy: 0.6557 - val_loss: 0.7750 - val_accuracy: 0.6519\n",
      "Epoch 92/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7732 - accuracy: 0.6555 - val_loss: 0.7746 - val_accuracy: 0.6537\n",
      "Epoch 93/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7735 - accuracy: 0.6553 - val_loss: 0.7826 - val_accuracy: 0.6536\n",
      "Epoch 94/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7740 - accuracy: 0.6555 - val_loss: 0.7731 - val_accuracy: 0.6553\n",
      "Epoch 95/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7734 - accuracy: 0.6559 - val_loss: 0.7759 - val_accuracy: 0.6566\n",
      "Epoch 96/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7735 - accuracy: 0.6562 - val_loss: 0.7790 - val_accuracy: 0.6532\n",
      "Epoch 97/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7734 - accuracy: 0.6556 - val_loss: 0.7789 - val_accuracy: 0.6538\n",
      "Epoch 98/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7732 - accuracy: 0.6557 - val_loss: 0.7748 - val_accuracy: 0.6571\n",
      "Epoch 99/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7738 - accuracy: 0.6556 - val_loss: 0.7750 - val_accuracy: 0.6563\n",
      "Epoch 100/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7730 - accuracy: 0.6551 - val_loss: 0.7784 - val_accuracy: 0.6578\n",
      "1714/1714 [==============================] - 5s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "857/857 [==============================] - 13s 14ms/step - loss: 0.9280 - accuracy: 0.5555 - val_loss: 0.8662 - val_accuracy: 0.6045\n",
      "Epoch 2/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8480 - accuracy: 0.6156 - val_loss: 0.8380 - val_accuracy: 0.6200\n",
      "Epoch 3/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8320 - accuracy: 0.6260 - val_loss: 0.8234 - val_accuracy: 0.6321\n",
      "Epoch 4/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8236 - accuracy: 0.6311 - val_loss: 0.8200 - val_accuracy: 0.6324\n",
      "Epoch 5/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8176 - accuracy: 0.6350 - val_loss: 0.8184 - val_accuracy: 0.6326\n",
      "Epoch 6/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8144 - accuracy: 0.6374 - val_loss: 0.8183 - val_accuracy: 0.6337\n",
      "Epoch 7/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8123 - accuracy: 0.6390 - val_loss: 0.8122 - val_accuracy: 0.6383\n",
      "Epoch 8/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8111 - accuracy: 0.6388 - val_loss: 0.8051 - val_accuracy: 0.6434\n",
      "Epoch 9/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8092 - accuracy: 0.6405 - val_loss: 0.8096 - val_accuracy: 0.6398\n",
      "Epoch 10/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8090 - accuracy: 0.6404 - val_loss: 0.8072 - val_accuracy: 0.6423\n",
      "Epoch 11/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8067 - accuracy: 0.6424 - val_loss: 0.8025 - val_accuracy: 0.6474\n",
      "Epoch 12/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8052 - accuracy: 0.6430 - val_loss: 0.8043 - val_accuracy: 0.6449\n",
      "Epoch 13/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8046 - accuracy: 0.6437 - val_loss: 0.8005 - val_accuracy: 0.6460\n",
      "Epoch 14/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8044 - accuracy: 0.6432 - val_loss: 0.8076 - val_accuracy: 0.6381\n",
      "Epoch 15/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8046 - accuracy: 0.6432 - val_loss: 0.8015 - val_accuracy: 0.6463\n",
      "Epoch 16/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8048 - accuracy: 0.6427 - val_loss: 0.8058 - val_accuracy: 0.6441\n",
      "Epoch 17/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8034 - accuracy: 0.6434 - val_loss: 0.8114 - val_accuracy: 0.6340\n",
      "Epoch 18/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8026 - accuracy: 0.6439 - val_loss: 0.8064 - val_accuracy: 0.6363\n",
      "Epoch 19/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8030 - accuracy: 0.6434 - val_loss: 0.7978 - val_accuracy: 0.6504\n",
      "Epoch 20/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8025 - accuracy: 0.6436 - val_loss: 0.7981 - val_accuracy: 0.6428\n",
      "Epoch 21/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8012 - accuracy: 0.6430 - val_loss: 0.8144 - val_accuracy: 0.6329\n",
      "Epoch 22/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8020 - accuracy: 0.6421 - val_loss: 0.8061 - val_accuracy: 0.6390\n",
      "Epoch 23/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8020 - accuracy: 0.6415 - val_loss: 0.8062 - val_accuracy: 0.6427\n",
      "Epoch 24/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8016 - accuracy: 0.6423 - val_loss: 0.7986 - val_accuracy: 0.6440\n",
      "Epoch 25/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8008 - accuracy: 0.6422 - val_loss: 0.8030 - val_accuracy: 0.6392\n",
      "Epoch 26/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8011 - accuracy: 0.6415 - val_loss: 0.7987 - val_accuracy: 0.6422\n",
      "Epoch 27/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.8000 - accuracy: 0.6429 - val_loss: 0.7986 - val_accuracy: 0.6425\n",
      "Epoch 28/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7993 - accuracy: 0.6423 - val_loss: 0.7975 - val_accuracy: 0.6433\n",
      "Epoch 29/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7993 - accuracy: 0.6419 - val_loss: 0.7974 - val_accuracy: 0.6416\n",
      "Epoch 30/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7987 - accuracy: 0.6423 - val_loss: 0.8028 - val_accuracy: 0.6398\n",
      "Epoch 31/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7996 - accuracy: 0.6415 - val_loss: 0.7989 - val_accuracy: 0.6429\n",
      "Epoch 32/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7988 - accuracy: 0.6413 - val_loss: 0.7957 - val_accuracy: 0.6446\n",
      "Epoch 33/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7981 - accuracy: 0.6420 - val_loss: 0.7955 - val_accuracy: 0.6466\n",
      "Epoch 34/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7989 - accuracy: 0.6414 - val_loss: 0.7978 - val_accuracy: 0.6453\n",
      "Epoch 35/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7973 - accuracy: 0.6431 - val_loss: 0.7970 - val_accuracy: 0.6412\n",
      "Epoch 36/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7976 - accuracy: 0.6425 - val_loss: 0.7957 - val_accuracy: 0.6464\n",
      "Epoch 37/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7963 - accuracy: 0.6435 - val_loss: 0.7983 - val_accuracy: 0.6398\n",
      "Epoch 38/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7957 - accuracy: 0.6438 - val_loss: 0.7972 - val_accuracy: 0.6440\n",
      "Epoch 39/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7972 - accuracy: 0.6423 - val_loss: 0.8003 - val_accuracy: 0.6405\n",
      "Epoch 40/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7964 - accuracy: 0.6431 - val_loss: 0.7938 - val_accuracy: 0.6413\n",
      "Epoch 41/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7958 - accuracy: 0.6431 - val_loss: 0.7965 - val_accuracy: 0.6422\n",
      "Epoch 42/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7951 - accuracy: 0.6441 - val_loss: 0.7951 - val_accuracy: 0.6439\n",
      "Epoch 43/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7969 - accuracy: 0.6423 - val_loss: 0.7941 - val_accuracy: 0.6416\n",
      "Epoch 44/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7948 - accuracy: 0.6438 - val_loss: 0.7966 - val_accuracy: 0.6429\n",
      "Epoch 45/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7943 - accuracy: 0.6444 - val_loss: 0.8004 - val_accuracy: 0.6456\n",
      "Epoch 46/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7952 - accuracy: 0.6442 - val_loss: 0.7923 - val_accuracy: 0.6459\n",
      "Epoch 47/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7934 - accuracy: 0.6446 - val_loss: 0.7937 - val_accuracy: 0.6412\n",
      "Epoch 48/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7945 - accuracy: 0.6441 - val_loss: 0.7951 - val_accuracy: 0.6449\n",
      "Epoch 49/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7949 - accuracy: 0.6434 - val_loss: 0.7946 - val_accuracy: 0.6412\n",
      "Epoch 50/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7937 - accuracy: 0.6448 - val_loss: 0.8110 - val_accuracy: 0.6258\n",
      "Epoch 51/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7940 - accuracy: 0.6445 - val_loss: 0.7903 - val_accuracy: 0.6460\n",
      "Epoch 52/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7932 - accuracy: 0.6446 - val_loss: 0.7967 - val_accuracy: 0.6355\n",
      "Epoch 53/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7934 - accuracy: 0.6442 - val_loss: 0.7876 - val_accuracy: 0.6477\n",
      "Epoch 54/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7930 - accuracy: 0.6451 - val_loss: 0.7889 - val_accuracy: 0.6465\n",
      "Epoch 55/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7920 - accuracy: 0.6453 - val_loss: 0.7894 - val_accuracy: 0.6481\n",
      "Epoch 56/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7930 - accuracy: 0.6443 - val_loss: 0.7962 - val_accuracy: 0.6391\n",
      "Epoch 57/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7927 - accuracy: 0.6445 - val_loss: 0.8030 - val_accuracy: 0.6298\n",
      "Epoch 58/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7933 - accuracy: 0.6440 - val_loss: 0.8026 - val_accuracy: 0.6315\n",
      "Epoch 59/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7920 - accuracy: 0.6448 - val_loss: 0.7941 - val_accuracy: 0.6424\n",
      "Epoch 60/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7922 - accuracy: 0.6444 - val_loss: 0.7983 - val_accuracy: 0.6418\n",
      "Epoch 61/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7917 - accuracy: 0.6454 - val_loss: 0.8012 - val_accuracy: 0.6386\n",
      "Epoch 62/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7908 - accuracy: 0.6459 - val_loss: 0.8001 - val_accuracy: 0.6372\n",
      "Epoch 63/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7903 - accuracy: 0.6463 - val_loss: 0.8097 - val_accuracy: 0.6302\n",
      "Epoch 64/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7903 - accuracy: 0.6464 - val_loss: 0.7894 - val_accuracy: 0.6453\n",
      "Epoch 65/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7893 - accuracy: 0.6465 - val_loss: 0.8108 - val_accuracy: 0.6318\n",
      "Epoch 66/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7900 - accuracy: 0.6466 - val_loss: 0.7865 - val_accuracy: 0.6495\n",
      "Epoch 67/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7901 - accuracy: 0.6459 - val_loss: 0.7868 - val_accuracy: 0.6479\n",
      "Epoch 68/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7908 - accuracy: 0.6454 - val_loss: 0.7949 - val_accuracy: 0.6386\n",
      "Epoch 69/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7906 - accuracy: 0.6462 - val_loss: 0.7946 - val_accuracy: 0.6447\n",
      "Epoch 70/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7914 - accuracy: 0.6457 - val_loss: 0.7933 - val_accuracy: 0.6435\n",
      "Epoch 71/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7902 - accuracy: 0.6464 - val_loss: 0.7914 - val_accuracy: 0.6459\n",
      "Epoch 72/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7906 - accuracy: 0.6466 - val_loss: 0.7869 - val_accuracy: 0.6495\n",
      "Epoch 73/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7908 - accuracy: 0.6458 - val_loss: 0.7914 - val_accuracy: 0.6448\n",
      "Epoch 74/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7890 - accuracy: 0.6471 - val_loss: 0.7872 - val_accuracy: 0.6496\n",
      "Epoch 75/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7898 - accuracy: 0.6469 - val_loss: 0.7910 - val_accuracy: 0.6455\n",
      "Epoch 76/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7895 - accuracy: 0.6465 - val_loss: 0.7865 - val_accuracy: 0.6483\n",
      "Epoch 77/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7903 - accuracy: 0.6460 - val_loss: 0.7925 - val_accuracy: 0.6416\n",
      "Epoch 78/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7888 - accuracy: 0.6468 - val_loss: 0.7919 - val_accuracy: 0.6423\n",
      "Epoch 79/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7886 - accuracy: 0.6472 - val_loss: 0.8051 - val_accuracy: 0.6371\n",
      "Epoch 80/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7896 - accuracy: 0.6466 - val_loss: 0.7998 - val_accuracy: 0.6373\n",
      "Epoch 81/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7892 - accuracy: 0.6467 - val_loss: 0.7853 - val_accuracy: 0.6523\n",
      "Epoch 82/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7887 - accuracy: 0.6473 - val_loss: 0.7865 - val_accuracy: 0.6471\n",
      "Epoch 83/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7880 - accuracy: 0.6481 - val_loss: 0.7849 - val_accuracy: 0.6514\n",
      "Epoch 84/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7880 - accuracy: 0.6477 - val_loss: 0.7861 - val_accuracy: 0.6499\n",
      "Epoch 85/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7876 - accuracy: 0.6479 - val_loss: 0.7903 - val_accuracy: 0.6472\n",
      "Epoch 86/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7873 - accuracy: 0.6481 - val_loss: 0.7864 - val_accuracy: 0.6495\n",
      "Epoch 87/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7874 - accuracy: 0.6484 - val_loss: 0.7852 - val_accuracy: 0.6500\n",
      "Epoch 88/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7864 - accuracy: 0.6493 - val_loss: 0.7827 - val_accuracy: 0.6518\n",
      "Epoch 89/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7882 - accuracy: 0.6471 - val_loss: 0.7871 - val_accuracy: 0.6472\n",
      "Epoch 90/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7874 - accuracy: 0.6477 - val_loss: 0.7847 - val_accuracy: 0.6486\n",
      "Epoch 91/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7880 - accuracy: 0.6473 - val_loss: 0.7993 - val_accuracy: 0.6387\n",
      "Epoch 92/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7862 - accuracy: 0.6486 - val_loss: 0.7870 - val_accuracy: 0.6497\n",
      "Epoch 93/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7869 - accuracy: 0.6485 - val_loss: 0.7836 - val_accuracy: 0.6508\n",
      "Epoch 94/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7871 - accuracy: 0.6479 - val_loss: 0.7832 - val_accuracy: 0.6505\n",
      "Epoch 95/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7882 - accuracy: 0.6475 - val_loss: 0.7928 - val_accuracy: 0.6430\n",
      "Epoch 96/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7875 - accuracy: 0.6475 - val_loss: 0.7840 - val_accuracy: 0.6483\n",
      "Epoch 97/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7872 - accuracy: 0.6481 - val_loss: 0.7856 - val_accuracy: 0.6498\n",
      "Epoch 98/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7868 - accuracy: 0.6479 - val_loss: 0.7917 - val_accuracy: 0.6446\n",
      "Epoch 99/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7859 - accuracy: 0.6484 - val_loss: 0.7861 - val_accuracy: 0.6487\n",
      "Epoch 100/100\n",
      "857/857 [==============================] - 10s 12ms/step - loss: 0.7857 - accuracy: 0.6488 - val_loss: 0.7878 - val_accuracy: 0.6478\n",
      "1714/1714 [==============================] - 5s 3ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 100}\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "857/857 [==============================] - 14s 13ms/step - loss: 0.9283 - accuracy: 0.5554 - val_loss: 0.8827 - val_accuracy: 0.5909\n",
      "Epoch 2/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8607 - accuracy: 0.6089 - val_loss: 0.8503 - val_accuracy: 0.6154\n",
      "Epoch 3/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8432 - accuracy: 0.6189 - val_loss: 0.8372 - val_accuracy: 0.6230\n",
      "Epoch 4/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8356 - accuracy: 0.6240 - val_loss: 0.8309 - val_accuracy: 0.6249\n",
      "Epoch 5/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8305 - accuracy: 0.6263 - val_loss: 0.8332 - val_accuracy: 0.6246\n",
      "Epoch 6/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8267 - accuracy: 0.6276 - val_loss: 0.8442 - val_accuracy: 0.6077\n",
      "Epoch 7/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8237 - accuracy: 0.6288 - val_loss: 0.8246 - val_accuracy: 0.6256\n",
      "Epoch 8/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8214 - accuracy: 0.6297 - val_loss: 0.8260 - val_accuracy: 0.6284\n",
      "Epoch 9/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8169 - accuracy: 0.6310 - val_loss: 0.8199 - val_accuracy: 0.6296\n",
      "Epoch 10/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8126 - accuracy: 0.6326 - val_loss: 0.8147 - val_accuracy: 0.6350\n",
      "Epoch 11/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8101 - accuracy: 0.6337 - val_loss: 0.8078 - val_accuracy: 0.6342\n",
      "Epoch 12/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8095 - accuracy: 0.6340 - val_loss: 0.8109 - val_accuracy: 0.6319\n",
      "Epoch 13/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8074 - accuracy: 0.6357 - val_loss: 0.8115 - val_accuracy: 0.6280\n",
      "Epoch 14/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8077 - accuracy: 0.6346 - val_loss: 0.8099 - val_accuracy: 0.6330\n",
      "Epoch 15/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8074 - accuracy: 0.6353 - val_loss: 0.8057 - val_accuracy: 0.6376\n",
      "Epoch 16/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8073 - accuracy: 0.6349 - val_loss: 0.8123 - val_accuracy: 0.6300\n",
      "Epoch 17/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8066 - accuracy: 0.6348 - val_loss: 0.8088 - val_accuracy: 0.6359\n",
      "Epoch 18/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8045 - accuracy: 0.6371 - val_loss: 0.8124 - val_accuracy: 0.6335\n",
      "Epoch 19/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8059 - accuracy: 0.6358 - val_loss: 0.8081 - val_accuracy: 0.6352\n",
      "Epoch 20/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8044 - accuracy: 0.6367 - val_loss: 0.8102 - val_accuracy: 0.6350\n",
      "Epoch 21/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8038 - accuracy: 0.6378 - val_loss: 0.8051 - val_accuracy: 0.6331\n",
      "Epoch 22/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8048 - accuracy: 0.6368 - val_loss: 0.8147 - val_accuracy: 0.6246\n",
      "Epoch 23/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8027 - accuracy: 0.6375 - val_loss: 0.8043 - val_accuracy: 0.6361\n",
      "Epoch 24/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8026 - accuracy: 0.6379 - val_loss: 0.8010 - val_accuracy: 0.6390\n",
      "Epoch 25/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8031 - accuracy: 0.6380 - val_loss: 0.8188 - val_accuracy: 0.6257\n",
      "Epoch 26/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8020 - accuracy: 0.6382 - val_loss: 0.8034 - val_accuracy: 0.6366\n",
      "Epoch 27/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8016 - accuracy: 0.6388 - val_loss: 0.8265 - val_accuracy: 0.6201\n",
      "Epoch 28/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8032 - accuracy: 0.6381 - val_loss: 0.8131 - val_accuracy: 0.6296\n",
      "Epoch 29/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8025 - accuracy: 0.6374 - val_loss: 0.8022 - val_accuracy: 0.6385\n",
      "Epoch 30/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8013 - accuracy: 0.6393 - val_loss: 0.8005 - val_accuracy: 0.6377\n",
      "Epoch 31/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8014 - accuracy: 0.6389 - val_loss: 0.8011 - val_accuracy: 0.6401\n",
      "Epoch 32/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8022 - accuracy: 0.6376 - val_loss: 0.7988 - val_accuracy: 0.6391\n",
      "Epoch 33/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8017 - accuracy: 0.6380 - val_loss: 0.8082 - val_accuracy: 0.6303\n",
      "Epoch 34/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7999 - accuracy: 0.6399 - val_loss: 0.7984 - val_accuracy: 0.6379\n",
      "Epoch 35/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8000 - accuracy: 0.6392 - val_loss: 0.8076 - val_accuracy: 0.6321\n",
      "Epoch 36/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7998 - accuracy: 0.6397 - val_loss: 0.8059 - val_accuracy: 0.6329\n",
      "Epoch 37/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8011 - accuracy: 0.6391 - val_loss: 0.8159 - val_accuracy: 0.6255\n",
      "Epoch 38/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7992 - accuracy: 0.6401 - val_loss: 0.8039 - val_accuracy: 0.6336\n",
      "Epoch 39/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7993 - accuracy: 0.6402 - val_loss: 0.7960 - val_accuracy: 0.6406\n",
      "Epoch 40/100\n",
      "857/857 [==============================] - 12s 13ms/step - loss: 0.8001 - accuracy: 0.6394 - val_loss: 0.7995 - val_accuracy: 0.6396\n",
      "Epoch 41/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7999 - accuracy: 0.6391 - val_loss: 0.7963 - val_accuracy: 0.6419\n",
      "Epoch 42/100\n",
      "857/857 [==============================] - 12s 13ms/step - loss: 0.7974 - accuracy: 0.6416 - val_loss: 0.7960 - val_accuracy: 0.6379\n",
      "Epoch 43/100\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.7976 - accuracy: 0.6405 - val_loss: 0.8002 - val_accuracy: 0.6412\n",
      "Epoch 44/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7973 - accuracy: 0.6412 - val_loss: 0.7998 - val_accuracy: 0.6392\n",
      "Epoch 45/100\n",
      "857/857 [==============================] - 12s 13ms/step - loss: 0.7980 - accuracy: 0.6397 - val_loss: 0.7969 - val_accuracy: 0.6383\n",
      "Epoch 46/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7976 - accuracy: 0.6400 - val_loss: 0.7973 - val_accuracy: 0.6402\n",
      "Epoch 47/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7978 - accuracy: 0.6405 - val_loss: 0.8298 - val_accuracy: 0.6183\n",
      "Epoch 48/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7968 - accuracy: 0.6406 - val_loss: 0.8014 - val_accuracy: 0.6359\n",
      "Epoch 49/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7981 - accuracy: 0.6406 - val_loss: 0.7951 - val_accuracy: 0.6394\n",
      "Epoch 50/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7967 - accuracy: 0.6408 - val_loss: 0.7977 - val_accuracy: 0.6418\n",
      "Epoch 51/100\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.7967 - accuracy: 0.6406 - val_loss: 0.7988 - val_accuracy: 0.6377\n",
      "Epoch 52/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7958 - accuracy: 0.6414 - val_loss: 0.8032 - val_accuracy: 0.6368\n",
      "Epoch 53/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7963 - accuracy: 0.6411 - val_loss: 0.8035 - val_accuracy: 0.6347\n",
      "Epoch 54/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7964 - accuracy: 0.6410 - val_loss: 0.8033 - val_accuracy: 0.6356\n",
      "Epoch 55/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7964 - accuracy: 0.6414 - val_loss: 0.7971 - val_accuracy: 0.6393\n",
      "Epoch 56/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7964 - accuracy: 0.6407 - val_loss: 0.8190 - val_accuracy: 0.6254\n",
      "Epoch 57/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7953 - accuracy: 0.6415 - val_loss: 0.8029 - val_accuracy: 0.6348\n",
      "Epoch 58/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7945 - accuracy: 0.6421 - val_loss: 0.7963 - val_accuracy: 0.6414\n",
      "Epoch 59/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7957 - accuracy: 0.6414 - val_loss: 0.7919 - val_accuracy: 0.6425\n",
      "Epoch 60/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7969 - accuracy: 0.6409 - val_loss: 0.7942 - val_accuracy: 0.6429\n",
      "Epoch 61/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7946 - accuracy: 0.6418 - val_loss: 0.7916 - val_accuracy: 0.6407\n",
      "Epoch 62/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7956 - accuracy: 0.6414 - val_loss: 0.7980 - val_accuracy: 0.6398\n",
      "Epoch 63/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7963 - accuracy: 0.6410 - val_loss: 0.8104 - val_accuracy: 0.6284\n",
      "Epoch 64/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7961 - accuracy: 0.6415 - val_loss: 0.7932 - val_accuracy: 0.6394\n",
      "Epoch 65/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7957 - accuracy: 0.6408 - val_loss: 0.7964 - val_accuracy: 0.6370\n",
      "Epoch 66/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7947 - accuracy: 0.6416 - val_loss: 0.7990 - val_accuracy: 0.6365\n",
      "Epoch 67/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7953 - accuracy: 0.6410 - val_loss: 0.7949 - val_accuracy: 0.6360\n",
      "Epoch 68/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7942 - accuracy: 0.6421 - val_loss: 0.7982 - val_accuracy: 0.6381\n",
      "Epoch 69/100\n",
      "857/857 [==============================] - 12s 13ms/step - loss: 0.7956 - accuracy: 0.6412 - val_loss: 0.8002 - val_accuracy: 0.6351\n",
      "Epoch 70/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7952 - accuracy: 0.6413 - val_loss: 0.7955 - val_accuracy: 0.6388\n",
      "Epoch 71/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7943 - accuracy: 0.6421 - val_loss: 0.7946 - val_accuracy: 0.6373\n",
      "Epoch 72/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7960 - accuracy: 0.6405 - val_loss: 0.8015 - val_accuracy: 0.6375\n",
      "Epoch 73/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7951 - accuracy: 0.6416 - val_loss: 0.7903 - val_accuracy: 0.6452\n",
      "Epoch 74/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7939 - accuracy: 0.6423 - val_loss: 0.7938 - val_accuracy: 0.6428\n",
      "Epoch 75/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7942 - accuracy: 0.6423 - val_loss: 0.7991 - val_accuracy: 0.6419\n",
      "Epoch 76/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7936 - accuracy: 0.6424 - val_loss: 0.7945 - val_accuracy: 0.6403\n",
      "Epoch 77/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7941 - accuracy: 0.6419 - val_loss: 0.7940 - val_accuracy: 0.6409\n",
      "Epoch 78/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7948 - accuracy: 0.6415 - val_loss: 0.8053 - val_accuracy: 0.6339\n",
      "Epoch 79/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7937 - accuracy: 0.6425 - val_loss: 0.7972 - val_accuracy: 0.6349\n",
      "Epoch 80/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7938 - accuracy: 0.6422 - val_loss: 0.7919 - val_accuracy: 0.6422\n",
      "Epoch 81/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7940 - accuracy: 0.6416 - val_loss: 0.7942 - val_accuracy: 0.6446\n",
      "Epoch 82/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7942 - accuracy: 0.6422 - val_loss: 0.7880 - val_accuracy: 0.6440\n",
      "Epoch 83/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7942 - accuracy: 0.6422 - val_loss: 0.7977 - val_accuracy: 0.6349\n",
      "Epoch 84/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7935 - accuracy: 0.6418 - val_loss: 0.7885 - val_accuracy: 0.6421\n",
      "Epoch 85/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7941 - accuracy: 0.6421 - val_loss: 0.7940 - val_accuracy: 0.6411\n",
      "Epoch 86/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7926 - accuracy: 0.6425 - val_loss: 0.7892 - val_accuracy: 0.6450\n",
      "Epoch 87/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7911 - accuracy: 0.6438 - val_loss: 0.7860 - val_accuracy: 0.6445\n",
      "Epoch 88/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7908 - accuracy: 0.6439 - val_loss: 0.8259 - val_accuracy: 0.6208\n",
      "Epoch 89/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7920 - accuracy: 0.6435 - val_loss: 0.7869 - val_accuracy: 0.6418\n",
      "Epoch 90/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7914 - accuracy: 0.6433 - val_loss: 0.8018 - val_accuracy: 0.6332\n",
      "Epoch 91/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7912 - accuracy: 0.6437 - val_loss: 0.7969 - val_accuracy: 0.6411\n",
      "Epoch 92/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7917 - accuracy: 0.6429 - val_loss: 0.7893 - val_accuracy: 0.6408\n",
      "Epoch 93/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7913 - accuracy: 0.6434 - val_loss: 0.8043 - val_accuracy: 0.6328\n",
      "Epoch 94/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7920 - accuracy: 0.6430 - val_loss: 0.7869 - val_accuracy: 0.6444\n",
      "Epoch 95/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7906 - accuracy: 0.6438 - val_loss: 0.7863 - val_accuracy: 0.6474\n",
      "Epoch 96/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7910 - accuracy: 0.6440 - val_loss: 0.7840 - val_accuracy: 0.6437\n",
      "Epoch 97/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7897 - accuracy: 0.6447 - val_loss: 0.7889 - val_accuracy: 0.6434\n",
      "Epoch 98/100\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.7898 - accuracy: 0.6439 - val_loss: 0.7873 - val_accuracy: 0.6467\n",
      "Epoch 99/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7898 - accuracy: 0.6435 - val_loss: 0.7945 - val_accuracy: 0.6382\n",
      "Epoch 100/100\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.7895 - accuracy: 0.6441 - val_loss: 0.7897 - val_accuracy: 0.6423\n",
      "1714/1714 [==============================] - 5s 3ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 200, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 4}\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/200\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9762 - accuracy: 0.4980 - val_loss: 0.9512 - val_accuracy: 0.4996\n",
      "Epoch 2/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9384 - accuracy: 0.5002 - val_loss: 0.9353 - val_accuracy: 0.4996\n",
      "Epoch 3/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9338 - accuracy: 0.5003 - val_loss: 0.9324 - val_accuracy: 0.4996\n",
      "Epoch 4/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.9315 - accuracy: 0.5002 - val_loss: 0.9288 - val_accuracy: 0.4995\n",
      "Epoch 5/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9291 - accuracy: 0.5002 - val_loss: 0.9280 - val_accuracy: 0.4995\n",
      "Epoch 6/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9283 - accuracy: 0.5004 - val_loss: 0.9240 - val_accuracy: 0.5022\n",
      "Epoch 7/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.9063 - accuracy: 0.5442 - val_loss: 0.8809 - val_accuracy: 0.5970\n",
      "Epoch 8/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8641 - accuracy: 0.6154 - val_loss: 0.8660 - val_accuracy: 0.6076\n",
      "Epoch 9/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8553 - accuracy: 0.6216 - val_loss: 0.8487 - val_accuracy: 0.6280\n",
      "Epoch 10/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8536 - accuracy: 0.6235 - val_loss: 0.8471 - val_accuracy: 0.6287\n",
      "Epoch 11/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8517 - accuracy: 0.6254 - val_loss: 0.8636 - val_accuracy: 0.6102\n",
      "Epoch 12/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8502 - accuracy: 0.6258 - val_loss: 0.8504 - val_accuracy: 0.6242\n",
      "Epoch 13/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8486 - accuracy: 0.6273 - val_loss: 0.8529 - val_accuracy: 0.6241\n",
      "Epoch 14/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8483 - accuracy: 0.6277 - val_loss: 0.8463 - val_accuracy: 0.6301\n",
      "Epoch 15/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8471 - accuracy: 0.6289 - val_loss: 0.8496 - val_accuracy: 0.6234\n",
      "Epoch 16/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8461 - accuracy: 0.6297 - val_loss: 0.8578 - val_accuracy: 0.6176\n",
      "Epoch 17/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8458 - accuracy: 0.6298 - val_loss: 0.8486 - val_accuracy: 0.6260\n",
      "Epoch 18/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8458 - accuracy: 0.6303 - val_loss: 0.8446 - val_accuracy: 0.6329\n",
      "Epoch 19/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8448 - accuracy: 0.6310 - val_loss: 0.8558 - val_accuracy: 0.6178\n",
      "Epoch 20/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8432 - accuracy: 0.6322 - val_loss: 0.8465 - val_accuracy: 0.6258\n",
      "Epoch 21/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.8436 - accuracy: 0.6326 - val_loss: 0.8479 - val_accuracy: 0.6299\n",
      "Epoch 22/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8433 - accuracy: 0.6329 - val_loss: 0.8479 - val_accuracy: 0.6262\n",
      "Epoch 23/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8425 - accuracy: 0.6333 - val_loss: 0.8469 - val_accuracy: 0.6311\n",
      "Epoch 24/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8421 - accuracy: 0.6330 - val_loss: 0.8409 - val_accuracy: 0.6342\n",
      "Epoch 25/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.8425 - accuracy: 0.6335 - val_loss: 0.8407 - val_accuracy: 0.6351\n",
      "Epoch 26/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8411 - accuracy: 0.6345 - val_loss: 0.8441 - val_accuracy: 0.6315\n",
      "Epoch 27/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8418 - accuracy: 0.6340 - val_loss: 0.8446 - val_accuracy: 0.6347\n",
      "Epoch 28/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8413 - accuracy: 0.6344 - val_loss: 0.8401 - val_accuracy: 0.6342\n",
      "Epoch 29/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8403 - accuracy: 0.6351 - val_loss: 0.8439 - val_accuracy: 0.6317\n",
      "Epoch 30/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8401 - accuracy: 0.6352 - val_loss: 0.8403 - val_accuracy: 0.6340\n",
      "Epoch 31/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8399 - accuracy: 0.6361 - val_loss: 0.8392 - val_accuracy: 0.6392\n",
      "Epoch 32/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8387 - accuracy: 0.6367 - val_loss: 0.8352 - val_accuracy: 0.6420\n",
      "Epoch 33/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8377 - accuracy: 0.6383 - val_loss: 0.8353 - val_accuracy: 0.6397\n",
      "Epoch 34/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8395 - accuracy: 0.6355 - val_loss: 0.8377 - val_accuracy: 0.6403\n",
      "Epoch 35/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8377 - accuracy: 0.6380 - val_loss: 0.8385 - val_accuracy: 0.6385\n",
      "Epoch 36/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8381 - accuracy: 0.6370 - val_loss: 0.8369 - val_accuracy: 0.6393\n",
      "Epoch 37/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8369 - accuracy: 0.6384 - val_loss: 0.8353 - val_accuracy: 0.6433\n",
      "Epoch 38/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8368 - accuracy: 0.6384 - val_loss: 0.8401 - val_accuracy: 0.6349\n",
      "Epoch 39/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8373 - accuracy: 0.6377 - val_loss: 0.8407 - val_accuracy: 0.6376\n",
      "Epoch 40/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8374 - accuracy: 0.6376 - val_loss: 0.8385 - val_accuracy: 0.6387\n",
      "Epoch 41/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8364 - accuracy: 0.6381 - val_loss: 0.8435 - val_accuracy: 0.6317\n",
      "Epoch 42/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8377 - accuracy: 0.6373 - val_loss: 0.8345 - val_accuracy: 0.6407\n",
      "Epoch 43/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8365 - accuracy: 0.6379 - val_loss: 0.8441 - val_accuracy: 0.6348\n",
      "Epoch 44/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.8361 - accuracy: 0.6392 - val_loss: 0.8363 - val_accuracy: 0.6369\n",
      "Epoch 45/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.8356 - accuracy: 0.6389 - val_loss: 0.8349 - val_accuracy: 0.6382\n",
      "Epoch 46/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8346 - accuracy: 0.6397 - val_loss: 0.8363 - val_accuracy: 0.6377\n",
      "Epoch 47/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8361 - accuracy: 0.6388 - val_loss: 0.8374 - val_accuracy: 0.6354\n",
      "Epoch 48/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8357 - accuracy: 0.6388 - val_loss: 0.8545 - val_accuracy: 0.6202\n",
      "Epoch 49/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8353 - accuracy: 0.6391 - val_loss: 0.8364 - val_accuracy: 0.6381\n",
      "Epoch 50/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8355 - accuracy: 0.6397 - val_loss: 0.8415 - val_accuracy: 0.6335\n",
      "Epoch 51/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8347 - accuracy: 0.6399 - val_loss: 0.8406 - val_accuracy: 0.6354\n",
      "Epoch 52/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8344 - accuracy: 0.6403 - val_loss: 0.8386 - val_accuracy: 0.6370\n",
      "Epoch 53/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8345 - accuracy: 0.6407 - val_loss: 0.8436 - val_accuracy: 0.6328\n",
      "Epoch 54/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8352 - accuracy: 0.6393 - val_loss: 0.8425 - val_accuracy: 0.6338\n",
      "Epoch 55/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8345 - accuracy: 0.6402 - val_loss: 0.8372 - val_accuracy: 0.6361\n",
      "Epoch 56/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8341 - accuracy: 0.6404 - val_loss: 0.8363 - val_accuracy: 0.6387\n",
      "Epoch 57/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8348 - accuracy: 0.6397 - val_loss: 0.8333 - val_accuracy: 0.6445\n",
      "Epoch 58/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8339 - accuracy: 0.6400 - val_loss: 0.8417 - val_accuracy: 0.6319\n",
      "Epoch 59/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8342 - accuracy: 0.6403 - val_loss: 0.8415 - val_accuracy: 0.6350\n",
      "Epoch 60/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8336 - accuracy: 0.6412 - val_loss: 0.8362 - val_accuracy: 0.6397\n",
      "Epoch 61/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8338 - accuracy: 0.6410 - val_loss: 0.8327 - val_accuracy: 0.6411\n",
      "Epoch 62/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8340 - accuracy: 0.6403 - val_loss: 0.8362 - val_accuracy: 0.6383\n",
      "Epoch 63/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8336 - accuracy: 0.6403 - val_loss: 0.8427 - val_accuracy: 0.6322\n",
      "Epoch 64/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8328 - accuracy: 0.6419 - val_loss: 0.8372 - val_accuracy: 0.6360\n",
      "Epoch 65/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8337 - accuracy: 0.6407 - val_loss: 0.8459 - val_accuracy: 0.6260\n",
      "Epoch 66/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8331 - accuracy: 0.6410 - val_loss: 0.8377 - val_accuracy: 0.6373\n",
      "Epoch 67/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8335 - accuracy: 0.6409 - val_loss: 0.8357 - val_accuracy: 0.6359\n",
      "Epoch 68/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8336 - accuracy: 0.6409 - val_loss: 0.8376 - val_accuracy: 0.6382\n",
      "Epoch 69/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8353 - accuracy: 0.6391 - val_loss: 0.8507 - val_accuracy: 0.6183\n",
      "Epoch 70/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8344 - accuracy: 0.6402 - val_loss: 0.8320 - val_accuracy: 0.6429\n",
      "Epoch 71/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8342 - accuracy: 0.6401 - val_loss: 0.8332 - val_accuracy: 0.6439\n",
      "Epoch 72/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8346 - accuracy: 0.6397 - val_loss: 0.8331 - val_accuracy: 0.6391\n",
      "Epoch 73/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8332 - accuracy: 0.6399 - val_loss: 0.8331 - val_accuracy: 0.6402\n",
      "Epoch 74/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8331 - accuracy: 0.6412 - val_loss: 0.8318 - val_accuracy: 0.6415\n",
      "Epoch 75/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8341 - accuracy: 0.6398 - val_loss: 0.8367 - val_accuracy: 0.6380\n",
      "Epoch 76/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8336 - accuracy: 0.6401 - val_loss: 0.8326 - val_accuracy: 0.6439\n",
      "Epoch 77/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8337 - accuracy: 0.6402 - val_loss: 0.8340 - val_accuracy: 0.6393\n",
      "Epoch 78/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8336 - accuracy: 0.6407 - val_loss: 0.8421 - val_accuracy: 0.6325\n",
      "Epoch 79/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8331 - accuracy: 0.6408 - val_loss: 0.8360 - val_accuracy: 0.6384\n",
      "Epoch 80/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8334 - accuracy: 0.6407 - val_loss: 0.8341 - val_accuracy: 0.6424\n",
      "Epoch 81/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8335 - accuracy: 0.6407 - val_loss: 0.8412 - val_accuracy: 0.6348\n",
      "Epoch 82/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8345 - accuracy: 0.6401 - val_loss: 0.8336 - val_accuracy: 0.6423\n",
      "Epoch 83/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8339 - accuracy: 0.6400 - val_loss: 0.8389 - val_accuracy: 0.6380\n",
      "Epoch 84/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8346 - accuracy: 0.6391 - val_loss: 0.8331 - val_accuracy: 0.6417\n",
      "Epoch 85/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8329 - accuracy: 0.6416 - val_loss: 0.8431 - val_accuracy: 0.6276\n",
      "Epoch 86/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8341 - accuracy: 0.6398 - val_loss: 0.8349 - val_accuracy: 0.6416\n",
      "Epoch 87/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8329 - accuracy: 0.6410 - val_loss: 0.8346 - val_accuracy: 0.6383\n",
      "Epoch 88/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8339 - accuracy: 0.6399 - val_loss: 0.8340 - val_accuracy: 0.6396\n",
      "Epoch 89/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8342 - accuracy: 0.6399 - val_loss: 0.8343 - val_accuracy: 0.6389\n",
      "Epoch 90/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8329 - accuracy: 0.6413 - val_loss: 0.8353 - val_accuracy: 0.6392\n",
      "Epoch 91/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8328 - accuracy: 0.6414 - val_loss: 0.8324 - val_accuracy: 0.6422\n",
      "Epoch 92/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8335 - accuracy: 0.6400 - val_loss: 0.8344 - val_accuracy: 0.6395\n",
      "Epoch 93/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8338 - accuracy: 0.6407 - val_loss: 0.8327 - val_accuracy: 0.6412\n",
      "Epoch 94/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8344 - accuracy: 0.6397 - val_loss: 0.8359 - val_accuracy: 0.6366\n",
      "1714/1714 [==============================] - 5s 3ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 512, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [128, 64, 32, 16], \"EPOCHS\": 50, \"FINAL_SPARSITY\": 0.35, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"CNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 35, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.7755006551742554\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'CNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [128, 64, 32, 16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 512, 'EPOCHS': 200, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 35}\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_profile_input (InputLaye  [(None, 8, 13, 1)]           0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " conv1 (QConv2D)             (None, 8, 13, 4)             40        ['y_profile_input[0][0]']     \n",
      "                                                                                                  \n",
      " relu1 (QActivation)         (None, 8, 13, 4)             0         ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool1 (MaxPooling2D)     (None, 4, 6, 4)              0         ['relu1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2 (QConv2D)             (None, 4, 6, 7)              259       ['maxpool1[0][0]']            \n",
      "                                                                                                  \n",
      " relu2 (QActivation)         (None, 4, 6, 7)              0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2 (MaxPooling2D)     (None, 2, 3, 7)              0         ['relu2[0][0]']               \n",
      "                                                                                                  \n",
      " flattened (Flatten)         (None, 42)                   0         ['maxpool2[0][0]']            \n",
      "                                                                                                  \n",
      " y0_input (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concat (Concatenate)        (None, 43)                   0         ['flattened[0][0]',           \n",
      "                                                                     'y0_input[0][0]']            \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 7)                    308       ['concat[0][0]']              \n",
      "                                                                                                  \n",
      " relu3 (QActivation)         (None, 7)                    0         ['dense1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    24        ['relu3[0][0]']               \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 631 (2.46 KB)\n",
      "Trainable params: 631 (2.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "conv1                f=4 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu1                quantized_relu(15,0)\n",
      "conv2                f=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu2                quantized_relu(15,0)\n",
      "dense1               u=7 quantized_bits(10,2,0,alpha=1.0) quantized_bits(10,2,0,alpha=1.0) \n",
      "relu3                quantized_relu(10,0)\n",
      "dense_output         u=3 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/200\n",
      "857/857 [==============================] - 13s 13ms/step - loss: 0.9415 - accuracy: 0.5499 - val_loss: 0.8861 - val_accuracy: 0.5911\n",
      "Epoch 2/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8693 - accuracy: 0.6008 - val_loss: 0.8594 - val_accuracy: 0.6076\n",
      "Epoch 3/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8545 - accuracy: 0.6089 - val_loss: 0.8525 - val_accuracy: 0.6104\n",
      "Epoch 4/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8468 - accuracy: 0.6138 - val_loss: 0.8486 - val_accuracy: 0.6122\n",
      "Epoch 5/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8406 - accuracy: 0.6191 - val_loss: 0.8402 - val_accuracy: 0.6200\n",
      "Epoch 6/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8371 - accuracy: 0.6211 - val_loss: 0.8388 - val_accuracy: 0.6174\n",
      "Epoch 7/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8338 - accuracy: 0.6230 - val_loss: 0.8619 - val_accuracy: 0.6040\n",
      "Epoch 8/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8327 - accuracy: 0.6238 - val_loss: 0.8297 - val_accuracy: 0.6294\n",
      "Epoch 9/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8304 - accuracy: 0.6261 - val_loss: 0.8293 - val_accuracy: 0.6280\n",
      "Epoch 10/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8284 - accuracy: 0.6264 - val_loss: 0.8286 - val_accuracy: 0.6288\n",
      "Epoch 11/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8255 - accuracy: 0.6288 - val_loss: 0.8304 - val_accuracy: 0.6282\n",
      "Epoch 12/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8247 - accuracy: 0.6290 - val_loss: 0.8241 - val_accuracy: 0.6300\n",
      "Epoch 13/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8233 - accuracy: 0.6303 - val_loss: 0.8345 - val_accuracy: 0.6189\n",
      "Epoch 14/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8229 - accuracy: 0.6302 - val_loss: 0.8161 - val_accuracy: 0.6358\n",
      "Epoch 15/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8224 - accuracy: 0.6306 - val_loss: 0.8581 - val_accuracy: 0.6086\n",
      "Epoch 16/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8217 - accuracy: 0.6307 - val_loss: 0.8292 - val_accuracy: 0.6239\n",
      "Epoch 17/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8213 - accuracy: 0.6311 - val_loss: 0.8187 - val_accuracy: 0.6342\n",
      "Epoch 18/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8219 - accuracy: 0.6303 - val_loss: 0.8252 - val_accuracy: 0.6261\n",
      "Epoch 19/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8207 - accuracy: 0.6311 - val_loss: 0.8216 - val_accuracy: 0.6292\n",
      "Epoch 20/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8186 - accuracy: 0.6330 - val_loss: 0.8175 - val_accuracy: 0.6354\n",
      "Epoch 21/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8200 - accuracy: 0.6313 - val_loss: 0.8252 - val_accuracy: 0.6297\n",
      "Epoch 22/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8196 - accuracy: 0.6319 - val_loss: 0.8172 - val_accuracy: 0.6336\n",
      "Epoch 23/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8185 - accuracy: 0.6329 - val_loss: 0.8411 - val_accuracy: 0.6221\n",
      "Epoch 24/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8182 - accuracy: 0.6329 - val_loss: 0.8197 - val_accuracy: 0.6305\n",
      "Epoch 25/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8176 - accuracy: 0.6326 - val_loss: 0.8169 - val_accuracy: 0.6346\n",
      "Epoch 26/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8183 - accuracy: 0.6327 - val_loss: 0.8128 - val_accuracy: 0.6373\n",
      "Epoch 27/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8181 - accuracy: 0.6332 - val_loss: 0.8148 - val_accuracy: 0.6339\n",
      "Epoch 28/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8173 - accuracy: 0.6336 - val_loss: 0.8295 - val_accuracy: 0.6243\n",
      "Epoch 29/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8156 - accuracy: 0.6346 - val_loss: 0.8290 - val_accuracy: 0.6234\n",
      "Epoch 30/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8169 - accuracy: 0.6330 - val_loss: 0.8189 - val_accuracy: 0.6315\n",
      "Epoch 31/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8163 - accuracy: 0.6346 - val_loss: 0.8437 - val_accuracy: 0.6083\n",
      "Epoch 32/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8155 - accuracy: 0.6348 - val_loss: 0.8171 - val_accuracy: 0.6350\n",
      "Epoch 33/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8150 - accuracy: 0.6346 - val_loss: 0.8185 - val_accuracy: 0.6314\n",
      "Epoch 34/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8145 - accuracy: 0.6351 - val_loss: 0.8214 - val_accuracy: 0.6295\n",
      "Epoch 35/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8159 - accuracy: 0.6340 - val_loss: 0.8143 - val_accuracy: 0.6362\n",
      "Epoch 36/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8147 - accuracy: 0.6354 - val_loss: 0.8166 - val_accuracy: 0.6334\n",
      "Epoch 37/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8145 - accuracy: 0.6348 - val_loss: 0.8203 - val_accuracy: 0.6312\n",
      "Epoch 38/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8141 - accuracy: 0.6352 - val_loss: 0.8101 - val_accuracy: 0.6401\n",
      "Epoch 39/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8134 - accuracy: 0.6359 - val_loss: 0.8186 - val_accuracy: 0.6278\n",
      "Epoch 40/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8126 - accuracy: 0.6361 - val_loss: 0.8165 - val_accuracy: 0.6372\n",
      "Epoch 41/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8123 - accuracy: 0.6366 - val_loss: 0.8094 - val_accuracy: 0.6395\n",
      "Epoch 42/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8122 - accuracy: 0.6368 - val_loss: 0.8219 - val_accuracy: 0.6272\n",
      "Epoch 43/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8129 - accuracy: 0.6364 - val_loss: 0.8276 - val_accuracy: 0.6284\n",
      "Epoch 44/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8112 - accuracy: 0.6373 - val_loss: 0.8172 - val_accuracy: 0.6289\n",
      "Epoch 45/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8134 - accuracy: 0.6357 - val_loss: 0.8138 - val_accuracy: 0.6314\n",
      "Epoch 46/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8113 - accuracy: 0.6364 - val_loss: 0.8267 - val_accuracy: 0.6267\n",
      "Epoch 47/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8115 - accuracy: 0.6359 - val_loss: 0.8175 - val_accuracy: 0.6309\n",
      "Epoch 48/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8110 - accuracy: 0.6365 - val_loss: 0.8111 - val_accuracy: 0.6392\n",
      "Epoch 49/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8103 - accuracy: 0.6381 - val_loss: 0.8129 - val_accuracy: 0.6392\n",
      "Epoch 50/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8111 - accuracy: 0.6372 - val_loss: 0.8119 - val_accuracy: 0.6370\n",
      "Epoch 51/200\n",
      "857/857 [==============================] - 11s 12ms/step - loss: 0.8105 - accuracy: 0.6374 - val_loss: 0.8217 - val_accuracy: 0.6333\n",
      "Epoch 52/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8115 - accuracy: 0.6362 - val_loss: 0.8108 - val_accuracy: 0.6409\n",
      "Epoch 53/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8097 - accuracy: 0.6377 - val_loss: 0.8104 - val_accuracy: 0.6337\n",
      "Epoch 54/200\n",
      "857/857 [==============================] - 11s 13ms/step - loss: 0.8110 - accuracy: 0.6367 - val_loss: 0.8209 - val_accuracy: 0.6295\n",
      "Epoch 55/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.8101 - accuracy: 0.6375 - val_loss: 0.8215 - val_accuracy: 0.6271\n",
      "Epoch 56/200\n",
      "857/857 [==============================] - 12s 14ms/step - loss: 0.8091 - accuracy: 0.6385 - val_loss: 0.8031 - val_accuracy: 0.6433\n",
      "Epoch 57/200\n",
      "857/857 [==============================] - 13s 15ms/step - loss: 0.8099 - accuracy: 0.6380 - val_loss: 0.8330 - val_accuracy: 0.6264\n",
      "Epoch 58/200\n",
      "857/857 [==============================] - 13s 15ms/step - loss: 0.8100 - accuracy: 0.6368 - val_loss: 0.8054 - val_accuracy: 0.6430\n",
      "Epoch 59/200\n",
      "185/857 [=====>........................] - ETA: 9s - loss: 0.8062 - accuracy: 0.6395"
     ]
    }
   ],
   "source": [
    "results = hyperparameter_search(data, HYPERPARAMETERS, param_grid, result_file=SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [72], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 72)                7632      \n",
      "                                                                 \n",
      " q_activation_9 (QActivatio  (None, 72)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 72)                288       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 219       \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8139 (31.79 KB)\n",
      "Trainable params: 7995 (31.23 KB)\n",
      "Non-trainable params: 144 (576.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=72 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_9       quantized_relu(6,0)\n",
      "batch_normalization_9 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 8ms/step - loss: 1.0659 - precision_7: 0.4859 - val_loss: 1.0226 - val_precision_7: 0.5021\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 1.0134 - precision_7: 0.5101 - val_loss: 1.0053 - val_precision_7: 0.5199\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 1.0037 - precision_7: 0.5163 - val_loss: 1.0045 - val_precision_7: 0.5129\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9943 - precision_7: 0.5223 - val_loss: 0.9891 - val_precision_7: 0.5256\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9889 - precision_7: 0.5278 - val_loss: 0.9948 - val_precision_7: 0.5188\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9866 - precision_7: 0.5308 - val_loss: 0.9901 - val_precision_7: 0.5234\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9832 - precision_7: 0.5338 - val_loss: 0.9956 - val_precision_7: 0.5305\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9802 - precision_7: 0.5358 - val_loss: 0.9874 - val_precision_7: 0.5310\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9735 - precision_7: 0.5416 - val_loss: 0.9833 - val_precision_7: 0.5392\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9714 - precision_7: 0.5442 - val_loss: 0.9780 - val_precision_7: 0.5415\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9677 - precision_7: 0.5457 - val_loss: 0.9723 - val_precision_7: 0.5453\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9675 - precision_7: 0.5469 - val_loss: 0.9691 - val_precision_7: 0.5649\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9658 - precision_7: 0.5486 - val_loss: 0.9912 - val_precision_7: 0.5320\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9638 - precision_7: 0.5497 - val_loss: 0.9737 - val_precision_7: 0.5468\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9601 - precision_7: 0.5524 - val_loss: 0.9778 - val_precision_7: 0.5364\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9611 - precision_7: 0.5510 - val_loss: 0.9611 - val_precision_7: 0.5485\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9588 - precision_7: 0.5544 - val_loss: 0.9673 - val_precision_7: 0.5414\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9634 - precision_7: 0.5505 - val_loss: 0.9780 - val_precision_7: 0.5319\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9617 - precision_7: 0.5517 - val_loss: 0.9638 - val_precision_7: 0.5561\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9601 - precision_7: 0.5524 - val_loss: 0.9736 - val_precision_7: 0.5394\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9570 - precision_7: 0.5555 - val_loss: 0.9648 - val_precision_7: 0.5442\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9605 - precision_7: 0.5524 - val_loss: 0.9690 - val_precision_7: 0.5506\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9608 - precision_7: 0.5505 - val_loss: 0.9717 - val_precision_7: 0.5441\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9598 - precision_7: 0.5536 - val_loss: 0.9757 - val_precision_7: 0.5473\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9574 - precision_7: 0.5557 - val_loss: 0.9741 - val_precision_7: 0.5406\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9533 - precision_7: 0.5590 - val_loss: 0.9579 - val_precision_7: 0.5633\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9541 - precision_7: 0.5575 - val_loss: 0.9543 - val_precision_7: 0.5565\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9555 - precision_7: 0.5572 - val_loss: 0.9623 - val_precision_7: 0.5494\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9564 - precision_7: 0.5542 - val_loss: 0.9796 - val_precision_7: 0.5403\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9517 - precision_7: 0.5586 - val_loss: 0.9591 - val_precision_7: 0.5556\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9529 - precision_7: 0.5595 - val_loss: 0.9669 - val_precision_7: 0.5504\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9539 - precision_7: 0.5581 - val_loss: 0.9549 - val_precision_7: 0.5597\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9499 - precision_7: 0.5599 - val_loss: 0.9591 - val_precision_7: 0.5654\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9517 - precision_7: 0.5589 - val_loss: 0.9607 - val_precision_7: 0.5547\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9540 - precision_7: 0.5566 - val_loss: 0.9553 - val_precision_7: 0.5538\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9507 - precision_7: 0.5605 - val_loss: 0.9592 - val_precision_7: 0.5588\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9534 - precision_7: 0.5584 - val_loss: 0.9512 - val_precision_7: 0.5610\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9525 - precision_7: 0.5583 - val_loss: 0.9654 - val_precision_7: 0.5579\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9508 - precision_7: 0.5591 - val_loss: 0.9600 - val_precision_7: 0.5477\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9503 - precision_7: 0.5604 - val_loss: 0.9799 - val_precision_7: 0.5405\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9510 - precision_7: 0.5596 - val_loss: 0.9502 - val_precision_7: 0.5549\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9503 - precision_7: 0.5596 - val_loss: 0.9584 - val_precision_7: 0.5533\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9480 - precision_7: 0.5617 - val_loss: 0.9627 - val_precision_7: 0.5489\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9504 - precision_7: 0.5599 - val_loss: 0.9631 - val_precision_7: 0.5536\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9510 - precision_7: 0.5589 - val_loss: 1.0425 - val_precision_7: 0.4881\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9484 - precision_7: 0.5617 - val_loss: 0.9662 - val_precision_7: 0.5487\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9525 - precision_7: 0.5589 - val_loss: 0.9511 - val_precision_7: 0.5650\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9495 - precision_7: 0.5612 - val_loss: 0.9744 - val_precision_7: 0.5399\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9507 - precision_7: 0.5594 - val_loss: 0.9690 - val_precision_7: 0.5446\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9484 - precision_7: 0.5614 - val_loss: 0.9529 - val_precision_7: 0.5599\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9484 - precision_7: 0.5605 - val_loss: 0.9611 - val_precision_7: 0.5525\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9510 - precision_7: 0.5603 - val_loss: 0.9869 - val_precision_7: 0.5412\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9514 - precision_7: 0.5588 - val_loss: 0.9524 - val_precision_7: 0.5531\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9466 - precision_7: 0.5635 - val_loss: 0.9712 - val_precision_7: 0.5416\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9471 - precision_7: 0.5622 - val_loss: 0.9457 - val_precision_7: 0.5723\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9509 - precision_7: 0.5598 - val_loss: 0.9738 - val_precision_7: 0.5384\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9516 - precision_7: 0.5582 - val_loss: 0.9483 - val_precision_7: 0.5643\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9494 - precision_7: 0.5605 - val_loss: 0.9905 - val_precision_7: 0.5262\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9511 - precision_7: 0.5594 - val_loss: 0.9603 - val_precision_7: 0.5582\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9500 - precision_7: 0.5601 - val_loss: 0.9644 - val_precision_7: 0.5475\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9477 - precision_7: 0.5610 - val_loss: 1.0062 - val_precision_7: 0.5193\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9447 - precision_7: 0.5639 - val_loss: 0.9910 - val_precision_7: 0.5265\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9453 - precision_7: 0.5646 - val_loss: 0.9530 - val_precision_7: 0.5620\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9435 - precision_7: 0.5646 - val_loss: 0.9684 - val_precision_7: 0.5527\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9429 - precision_7: 0.5655 - val_loss: 0.9434 - val_precision_7: 0.5615\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.9433 - precision_7: 0.5643 - val_loss: 0.9463 - val_precision_7: 0.5603\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9430 - precision_7: 0.5656 - val_loss: 0.9636 - val_precision_7: 0.5497\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9461 - precision_7: 0.5627 - val_loss: 0.9653 - val_precision_7: 0.5424\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9455 - precision_7: 0.5629 - val_loss: 0.9585 - val_precision_7: 0.5491\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9471 - precision_7: 0.5622 - val_loss: 0.9662 - val_precision_7: 0.5436\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9455 - precision_7: 0.5639 - val_loss: 0.9627 - val_precision_7: 0.5581\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9444 - precision_7: 0.5650 - val_loss: 0.9463 - val_precision_7: 0.5589\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9403 - precision_7: 0.5674 - val_loss: 0.9653 - val_precision_7: 0.5467\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9421 - precision_7: 0.5657 - val_loss: 0.9488 - val_precision_7: 0.5619\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9446 - precision_7: 0.5644 - val_loss: 0.9473 - val_precision_7: 0.5709\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9431 - precision_7: 0.5652 - val_loss: 0.9749 - val_precision_7: 0.5363\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9466 - precision_7: 0.5625 - val_loss: 0.9736 - val_precision_7: 0.5352\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9449 - precision_7: 0.5628 - val_loss: 0.9631 - val_precision_7: 0.5535\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9446 - precision_7: 0.5642 - val_loss: 0.9413 - val_precision_7: 0.5741\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9424 - precision_7: 0.5647 - val_loss: 0.9804 - val_precision_7: 0.5390\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9409 - precision_7: 0.5662 - val_loss: 0.9408 - val_precision_7: 0.5605\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9456 - precision_7: 0.5628 - val_loss: 0.9687 - val_precision_7: 0.5388\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9485 - precision_7: 0.5609 - val_loss: 0.9620 - val_precision_7: 0.5554\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9526 - precision_7: 0.5587 - val_loss: 0.9615 - val_precision_7: 0.5502\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9525 - precision_7: 0.5584 - val_loss: 0.9717 - val_precision_7: 0.5439\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9511 - precision_7: 0.5598 - val_loss: 0.9534 - val_precision_7: 0.5557\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9475 - precision_7: 0.5625 - val_loss: 0.9581 - val_precision_7: 0.5522\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9480 - precision_7: 0.5616 - val_loss: 0.9623 - val_precision_7: 0.5453\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9502 - precision_7: 0.5592 - val_loss: 0.9610 - val_precision_7: 0.5549\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9506 - precision_7: 0.5598 - val_loss: 0.9579 - val_precision_7: 0.5467\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9496 - precision_7: 0.5599 - val_loss: 0.9557 - val_precision_7: 0.5674\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9509 - precision_7: 0.5598 - val_loss: 0.9548 - val_precision_7: 0.5511\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9489 - precision_7: 0.5615 - val_loss: 0.9562 - val_precision_7: 0.5702\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9502 - precision_7: 0.5609 - val_loss: 0.9747 - val_precision_7: 0.5491\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9487 - precision_7: 0.5611 - val_loss: 0.9567 - val_precision_7: 0.5568\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9473 - precision_7: 0.5613 - val_loss: 0.9512 - val_precision_7: 0.5540\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9496 - precision_7: 0.5595 - val_loss: 0.9511 - val_precision_7: 0.5624\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9471 - precision_7: 0.5624 - val_loss: 0.9684 - val_precision_7: 0.5471\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9471 - precision_7: 0.5623 - val_loss: 0.9600 - val_precision_7: 0.5474\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9507 - precision_7: 0.5588 - val_loss: 0.9797 - val_precision_7: 0.5329\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_9\n",
      "cannot prune layer batch_normalization_9\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 9ms/step - loss: 0.5739 - accuracy: 0.5398 - val_loss: 0.6037 - val_accuracy: 0.4855\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5674 - accuracy: 0.5402 - val_loss: 0.5707 - val_accuracy: 0.5300\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5654 - accuracy: 0.5428 - val_loss: 0.5654 - val_accuracy: 0.5443\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5618 - accuracy: 0.5505 - val_loss: 0.5748 - val_accuracy: 0.5316\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5569 - accuracy: 0.5583 - val_loss: 0.5628 - val_accuracy: 0.5542\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5543 - accuracy: 0.5623 - val_loss: 0.5684 - val_accuracy: 0.5450\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5517 - accuracy: 0.5674 - val_loss: 0.5684 - val_accuracy: 0.5399\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5505 - accuracy: 0.5700 - val_loss: 0.5585 - val_accuracy: 0.5614\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5518 - accuracy: 0.5685 - val_loss: 0.5580 - val_accuracy: 0.5608\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5521 - accuracy: 0.5676 - val_loss: 0.5577 - val_accuracy: 0.5608\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5516 - accuracy: 0.5674 - val_loss: 0.5597 - val_accuracy: 0.5536\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5518 - accuracy: 0.5669 - val_loss: 0.5560 - val_accuracy: 0.5589\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5520 - accuracy: 0.5672 - val_loss: 0.5622 - val_accuracy: 0.5552\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5507 - accuracy: 0.5693 - val_loss: 0.5574 - val_accuracy: 0.5640\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5489 - accuracy: 0.5729 - val_loss: 0.5525 - val_accuracy: 0.5674\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5498 - accuracy: 0.5709 - val_loss: 0.5515 - val_accuracy: 0.5653\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5485 - accuracy: 0.5728 - val_loss: 0.5693 - val_accuracy: 0.5413\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5502 - accuracy: 0.5699 - val_loss: 0.5575 - val_accuracy: 0.5613\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5501 - accuracy: 0.5690 - val_loss: 0.5507 - val_accuracy: 0.5688\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5507 - accuracy: 0.5691 - val_loss: 0.5588 - val_accuracy: 0.5572\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5507 - accuracy: 0.5682 - val_loss: 0.5621 - val_accuracy: 0.5530\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5515 - accuracy: 0.5672 - val_loss: 0.5525 - val_accuracy: 0.5653\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5510 - accuracy: 0.5690 - val_loss: 0.5523 - val_accuracy: 0.5675\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5513 - accuracy: 0.5685 - val_loss: 0.5596 - val_accuracy: 0.5582\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5523 - accuracy: 0.5659 - val_loss: 0.5562 - val_accuracy: 0.5597\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5513 - accuracy: 0.5687 - val_loss: 0.5523 - val_accuracy: 0.5639\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5506 - accuracy: 0.5699 - val_loss: 0.5558 - val_accuracy: 0.5663\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5509 - accuracy: 0.5695 - val_loss: 0.5553 - val_accuracy: 0.5646\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5513 - accuracy: 0.5690 - val_loss: 0.5513 - val_accuracy: 0.5691\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5505 - accuracy: 0.5696 - val_loss: 0.5520 - val_accuracy: 0.5676\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5494 - accuracy: 0.5707 - val_loss: 0.5601 - val_accuracy: 0.5584\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5518 - accuracy: 0.5677 - val_loss: 0.5680 - val_accuracy: 0.5520\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5517 - accuracy: 0.5684 - val_loss: 0.5577 - val_accuracy: 0.5598\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5514 - accuracy: 0.5691 - val_loss: 0.5603 - val_accuracy: 0.5573\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5517 - accuracy: 0.5674 - val_loss: 0.5570 - val_accuracy: 0.5621\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5511 - accuracy: 0.5692 - val_loss: 0.5707 - val_accuracy: 0.5410\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5511 - accuracy: 0.5687 - val_loss: 0.5527 - val_accuracy: 0.5653\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5510 - accuracy: 0.5690 - val_loss: 0.5584 - val_accuracy: 0.5582\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5519 - accuracy: 0.5679 - val_loss: 0.5712 - val_accuracy: 0.5461\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5523 - accuracy: 0.5659 - val_loss: 0.5558 - val_accuracy: 0.5625\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5522 - accuracy: 0.5670 - val_loss: 0.5572 - val_accuracy: 0.5614\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5532 - accuracy: 0.5671 - val_loss: 0.5588 - val_accuracy: 0.5559\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5523 - accuracy: 0.5677 - val_loss: 0.5551 - val_accuracy: 0.5634\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5530 - accuracy: 0.5659 - val_loss: 0.5614 - val_accuracy: 0.5577\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5530 - accuracy: 0.5654 - val_loss: 0.5606 - val_accuracy: 0.5560\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5528 - accuracy: 0.5652 - val_loss: 0.5621 - val_accuracy: 0.5510\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5527 - accuracy: 0.5663 - val_loss: 0.5536 - val_accuracy: 0.5644\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5507 - accuracy: 0.5691 - val_loss: 0.5643 - val_accuracy: 0.5526\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5512 - accuracy: 0.5689 - val_loss: 0.5679 - val_accuracy: 0.5453\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.5517 - accuracy: 0.5679 - val_loss: 0.5509 - val_accuracy: 0.5701\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [64], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 64)                6784      \n",
      "                                                                 \n",
      " q_activation_10 (QActivati  (None, 64)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 195       \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7235 (28.26 KB)\n",
      "Trainable params: 7107 (27.76 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=64 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_10      quantized_relu(6,0)\n",
      "batch_normalization_10 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 9ms/step - loss: 1.0738 - precision_8: 0.4837 - val_loss: 1.0357 - val_precision_8: 0.5050\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 1.0182 - precision_8: 0.5084 - val_loss: 1.0255 - val_precision_8: 0.5020\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 1.0034 - precision_8: 0.5159 - val_loss: 1.0023 - val_precision_8: 0.5150\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9909 - precision_8: 0.5248 - val_loss: 1.0412 - val_precision_8: 0.4933\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9814 - precision_8: 0.5345 - val_loss: 0.9984 - val_precision_8: 0.5231\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9778 - precision_8: 0.5391 - val_loss: 0.9808 - val_precision_8: 0.5405\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9782 - precision_8: 0.5379 - val_loss: 0.9845 - val_precision_8: 0.5345\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9763 - precision_8: 0.5397 - val_loss: 1.0099 - val_precision_8: 0.5161\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9740 - precision_8: 0.5409 - val_loss: 0.9943 - val_precision_8: 0.5235\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9713 - precision_8: 0.5432 - val_loss: 0.9740 - val_precision_8: 0.5381\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9677 - precision_8: 0.5459 - val_loss: 0.9944 - val_precision_8: 0.5247\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9681 - precision_8: 0.5473 - val_loss: 0.9741 - val_precision_8: 0.5383\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9618 - precision_8: 0.5505 - val_loss: 0.9718 - val_precision_8: 0.5467\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9604 - precision_8: 0.5525 - val_loss: 0.9864 - val_precision_8: 0.5309\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9644 - precision_8: 0.5503 - val_loss: 0.9578 - val_precision_8: 0.5523\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9626 - precision_8: 0.5495 - val_loss: 0.9652 - val_precision_8: 0.5453\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9629 - precision_8: 0.5499 - val_loss: 0.9868 - val_precision_8: 0.5402\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9604 - precision_8: 0.5526 - val_loss: 0.9570 - val_precision_8: 0.5490\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9582 - precision_8: 0.5543 - val_loss: 0.9590 - val_precision_8: 0.5577\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9544 - precision_8: 0.5567 - val_loss: 0.9676 - val_precision_8: 0.5494\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9527 - precision_8: 0.5585 - val_loss: 0.9603 - val_precision_8: 0.5532\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9550 - precision_8: 0.5558 - val_loss: 0.9776 - val_precision_8: 0.5386\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9495 - precision_8: 0.5607 - val_loss: 0.9680 - val_precision_8: 0.5502\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9498 - precision_8: 0.5608 - val_loss: 0.9434 - val_precision_8: 0.5678\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9508 - precision_8: 0.5597 - val_loss: 0.9684 - val_precision_8: 0.5460\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9518 - precision_8: 0.5582 - val_loss: 0.9501 - val_precision_8: 0.5663\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9513 - precision_8: 0.5586 - val_loss: 0.9735 - val_precision_8: 0.5457\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9535 - precision_8: 0.5571 - val_loss: 0.9519 - val_precision_8: 0.5545\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9512 - precision_8: 0.5590 - val_loss: 0.9651 - val_precision_8: 0.5443\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9516 - precision_8: 0.5592 - val_loss: 0.9554 - val_precision_8: 0.5598\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9499 - precision_8: 0.5604 - val_loss: 0.9770 - val_precision_8: 0.5457\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9535 - precision_8: 0.5590 - val_loss: 0.9542 - val_precision_8: 0.5545\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9515 - precision_8: 0.5595 - val_loss: 0.9816 - val_precision_8: 0.5375\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9532 - precision_8: 0.5578 - val_loss: 0.9483 - val_precision_8: 0.5593\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9502 - precision_8: 0.5599 - val_loss: 1.0023 - val_precision_8: 0.5200\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9492 - precision_8: 0.5599 - val_loss: 0.9442 - val_precision_8: 0.5673\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9510 - precision_8: 0.5586 - val_loss: 0.9813 - val_precision_8: 0.5230\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9481 - precision_8: 0.5604 - val_loss: 0.9708 - val_precision_8: 0.5456\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9480 - precision_8: 0.5610 - val_loss: 0.9503 - val_precision_8: 0.5598\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9521 - precision_8: 0.5589 - val_loss: 0.9557 - val_precision_8: 0.5688\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9466 - precision_8: 0.5631 - val_loss: 0.9454 - val_precision_8: 0.5591\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9484 - precision_8: 0.5606 - val_loss: 0.9842 - val_precision_8: 0.5376\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.9465 - precision_8: 0.5632 - val_loss: 0.9685 - val_precision_8: 0.5490\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9481 - precision_8: 0.5615 - val_loss: 0.9819 - val_precision_8: 0.5412\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_10\n",
      "cannot prune layer batch_normalization_10\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 9ms/step - loss: 0.5760 - accuracy: 0.5401 - val_loss: 0.5712 - val_accuracy: 0.5368\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5672 - accuracy: 0.5417 - val_loss: 0.5712 - val_accuracy: 0.5368\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5695 - accuracy: 0.5382 - val_loss: 0.5711 - val_accuracy: 0.5376\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5671 - accuracy: 0.5417 - val_loss: 0.5685 - val_accuracy: 0.5351\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5647 - accuracy: 0.5455 - val_loss: 0.5667 - val_accuracy: 0.5450\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5586 - accuracy: 0.5550 - val_loss: 0.5609 - val_accuracy: 0.5503\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5563 - accuracy: 0.5599 - val_loss: 0.5584 - val_accuracy: 0.5584\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5566 - accuracy: 0.5603 - val_loss: 0.5594 - val_accuracy: 0.5553\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5567 - accuracy: 0.5599 - val_loss: 0.5662 - val_accuracy: 0.5462\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5568 - accuracy: 0.5598 - val_loss: 0.5800 - val_accuracy: 0.5316\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5550 - accuracy: 0.5629 - val_loss: 0.5561 - val_accuracy: 0.5605\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5552 - accuracy: 0.5626 - val_loss: 0.5570 - val_accuracy: 0.5594\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5547 - accuracy: 0.5633 - val_loss: 0.5617 - val_accuracy: 0.5542\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5563 - accuracy: 0.5605 - val_loss: 0.5602 - val_accuracy: 0.5530\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5562 - accuracy: 0.5605 - val_loss: 0.5584 - val_accuracy: 0.5589\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5565 - accuracy: 0.5602 - val_loss: 0.5591 - val_accuracy: 0.5580\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5567 - accuracy: 0.5603 - val_loss: 0.5668 - val_accuracy: 0.5453\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5566 - accuracy: 0.5604 - val_loss: 0.5619 - val_accuracy: 0.5497\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5556 - accuracy: 0.5616 - val_loss: 0.5755 - val_accuracy: 0.5382\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5568 - accuracy: 0.5608 - val_loss: 0.5633 - val_accuracy: 0.5567\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5552 - accuracy: 0.5621 - val_loss: 0.5571 - val_accuracy: 0.5635\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5576 - accuracy: 0.5595 - val_loss: 0.5654 - val_accuracy: 0.5503\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5559 - accuracy: 0.5610 - val_loss: 0.5667 - val_accuracy: 0.5466\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5570 - accuracy: 0.5596 - val_loss: 0.5617 - val_accuracy: 0.5546\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5561 - accuracy: 0.5614 - val_loss: 0.5617 - val_accuracy: 0.5511\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5581 - accuracy: 0.5579 - val_loss: 0.5704 - val_accuracy: 0.5435\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5572 - accuracy: 0.5588 - val_loss: 0.5642 - val_accuracy: 0.5505\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5575 - accuracy: 0.5577 - val_loss: 0.5698 - val_accuracy: 0.5447\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5566 - accuracy: 0.5597 - val_loss: 0.5619 - val_accuracy: 0.5497\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5579 - accuracy: 0.5579 - val_loss: 0.5650 - val_accuracy: 0.5497\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5575 - accuracy: 0.5587 - val_loss: 0.5560 - val_accuracy: 0.5589\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5582 - accuracy: 0.5574 - val_loss: 0.5663 - val_accuracy: 0.5516\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5586 - accuracy: 0.5577 - val_loss: 0.5584 - val_accuracy: 0.5629\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5581 - accuracy: 0.5571 - val_loss: 0.5638 - val_accuracy: 0.5484\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5584 - accuracy: 0.5571 - val_loss: 0.5631 - val_accuracy: 0.5521\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5585 - accuracy: 0.5567 - val_loss: 0.5692 - val_accuracy: 0.5440\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5578 - accuracy: 0.5577 - val_loss: 0.5577 - val_accuracy: 0.5585\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5570 - accuracy: 0.5591 - val_loss: 0.5565 - val_accuracy: 0.5576\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5576 - accuracy: 0.5579 - val_loss: 0.5644 - val_accuracy: 0.5510\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5582 - accuracy: 0.5572 - val_loss: 0.5657 - val_accuracy: 0.5470\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5574 - accuracy: 0.5585 - val_loss: 0.5829 - val_accuracy: 0.5278\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5578 - accuracy: 0.5576 - val_loss: 0.5599 - val_accuracy: 0.5510\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5572 - accuracy: 0.5592 - val_loss: 0.5627 - val_accuracy: 0.5484\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5568 - accuracy: 0.5594 - val_loss: 0.5591 - val_accuracy: 0.5552\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5556 - accuracy: 0.5617 - val_loss: 0.5642 - val_accuracy: 0.5512\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5558 - accuracy: 0.5617 - val_loss: 0.5664 - val_accuracy: 0.5434\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5581 - accuracy: 0.5574 - val_loss: 0.5620 - val_accuracy: 0.5527\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5572 - accuracy: 0.5591 - val_loss: 0.5641 - val_accuracy: 0.5488\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5568 - accuracy: 0.5596 - val_loss: 0.5599 - val_accuracy: 0.5627\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5575 - accuracy: 0.5585 - val_loss: 0.5655 - val_accuracy: 0.5517\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [48], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 48)                5088      \n",
      "                                                                 \n",
      " q_activation_11 (QActivati  (None, 48)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 48)                192       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 147       \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5427 (21.20 KB)\n",
      "Trainable params: 5331 (20.82 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=48 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_11      quantized_relu(6,0)\n",
      "batch_normalization_11 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 1.0614 - precision_9: 0.4832 - val_loss: 1.0240 - val_precision_9: 0.5048\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0183 - precision_9: 0.5079 - val_loss: 1.0141 - val_precision_9: 0.5108\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0064 - precision_9: 0.5131 - val_loss: 1.0208 - val_precision_9: 0.5068\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0023 - precision_9: 0.5183 - val_loss: 1.0076 - val_precision_9: 0.5116\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9985 - precision_9: 0.5176 - val_loss: 1.0021 - val_precision_9: 0.5135\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9941 - precision_9: 0.5228 - val_loss: 0.9959 - val_precision_9: 0.5215\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9917 - precision_9: 0.5243 - val_loss: 0.9850 - val_precision_9: 0.5331\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9879 - precision_9: 0.5276 - val_loss: 0.9879 - val_precision_9: 0.5246\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9798 - precision_9: 0.5354 - val_loss: 0.9929 - val_precision_9: 0.5267\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9799 - precision_9: 0.5355 - val_loss: 0.9777 - val_precision_9: 0.5350\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9725 - precision_9: 0.5417 - val_loss: 0.9922 - val_precision_9: 0.5295\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9759 - precision_9: 0.5406 - val_loss: 0.9804 - val_precision_9: 0.5505\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9754 - precision_9: 0.5409 - val_loss: 0.9941 - val_precision_9: 0.5231\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9741 - precision_9: 0.5418 - val_loss: 1.0171 - val_precision_9: 0.5156\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9701 - precision_9: 0.5444 - val_loss: 0.9740 - val_precision_9: 0.5467\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9720 - precision_9: 0.5433 - val_loss: 0.9714 - val_precision_9: 0.5446\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9680 - precision_9: 0.5462 - val_loss: 0.9791 - val_precision_9: 0.5439\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9657 - precision_9: 0.5484 - val_loss: 0.9696 - val_precision_9: 0.5436\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9643 - precision_9: 0.5494 - val_loss: 0.9827 - val_precision_9: 0.5358\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9688 - precision_9: 0.5458 - val_loss: 0.9958 - val_precision_9: 0.5260\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9659 - precision_9: 0.5482 - val_loss: 0.9654 - val_precision_9: 0.5392\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9626 - precision_9: 0.5502 - val_loss: 1.0087 - val_precision_9: 0.5254\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9675 - precision_9: 0.5465 - val_loss: 0.9681 - val_precision_9: 0.5490\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9652 - precision_9: 0.5487 - val_loss: 0.9696 - val_precision_9: 0.5391\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9639 - precision_9: 0.5479 - val_loss: 0.9732 - val_precision_9: 0.5407\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9621 - precision_9: 0.5503 - val_loss: 0.9692 - val_precision_9: 0.5415\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9600 - precision_9: 0.5534 - val_loss: 0.9628 - val_precision_9: 0.5723\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9638 - precision_9: 0.5495 - val_loss: 0.9951 - val_precision_9: 0.5310\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9630 - precision_9: 0.5487 - val_loss: 0.9673 - val_precision_9: 0.5547\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9595 - precision_9: 0.5537 - val_loss: 0.9736 - val_precision_9: 0.5528\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9547 - precision_9: 0.5569 - val_loss: 0.9764 - val_precision_9: 0.5476\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9580 - precision_9: 0.5546 - val_loss: 0.9747 - val_precision_9: 0.5400\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9606 - precision_9: 0.5521 - val_loss: 0.9766 - val_precision_9: 0.5442\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9571 - precision_9: 0.5551 - val_loss: 0.9531 - val_precision_9: 0.5594\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9587 - precision_9: 0.5540 - val_loss: 0.9969 - val_precision_9: 0.5283\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9550 - precision_9: 0.5570 - val_loss: 0.9823 - val_precision_9: 0.5323\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9597 - precision_9: 0.5522 - val_loss: 0.9621 - val_precision_9: 0.5435\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9603 - precision_9: 0.5526 - val_loss: 1.0000 - val_precision_9: 0.5326\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9621 - precision_9: 0.5497 - val_loss: 0.9719 - val_precision_9: 0.5523\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9624 - precision_9: 0.5518 - val_loss: 0.9700 - val_precision_9: 0.5461\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9591 - precision_9: 0.5539 - val_loss: 0.9724 - val_precision_9: 0.5454\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9610 - precision_9: 0.5514 - val_loss: 0.9668 - val_precision_9: 0.5529\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9588 - precision_9: 0.5523 - val_loss: 0.9700 - val_precision_9: 0.5533\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9594 - precision_9: 0.5542 - val_loss: 1.0005 - val_precision_9: 0.5159\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9615 - precision_9: 0.5514 - val_loss: 0.9609 - val_precision_9: 0.5460\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9624 - precision_9: 0.5518 - val_loss: 0.9759 - val_precision_9: 0.5359\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9599 - precision_9: 0.5533 - val_loss: 0.9573 - val_precision_9: 0.5446\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9590 - precision_9: 0.5539 - val_loss: 1.0013 - val_precision_9: 0.5333\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9622 - precision_9: 0.5515 - val_loss: 0.9966 - val_precision_9: 0.5269\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9588 - precision_9: 0.5535 - val_loss: 0.9767 - val_precision_9: 0.5453\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9561 - precision_9: 0.5558 - val_loss: 0.9728 - val_precision_9: 0.5480\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9609 - precision_9: 0.5522 - val_loss: 0.9805 - val_precision_9: 0.5405\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9602 - precision_9: 0.5521 - val_loss: 0.9746 - val_precision_9: 0.5443\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.9582 - precision_9: 0.5541 - val_loss: 0.9761 - val_precision_9: 0.5358\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_11\n",
      "cannot prune layer batch_normalization_11\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 8ms/step - loss: 0.5797 - accuracy: 0.5335 - val_loss: 0.5802 - val_accuracy: 0.5238\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5731 - accuracy: 0.5315 - val_loss: 0.5801 - val_accuracy: 0.5231\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5714 - accuracy: 0.5344 - val_loss: 0.5746 - val_accuracy: 0.5298\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5681 - accuracy: 0.5396 - val_loss: 0.5807 - val_accuracy: 0.5265\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5647 - accuracy: 0.5448 - val_loss: 0.5679 - val_accuracy: 0.5391\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5609 - accuracy: 0.5523 - val_loss: 0.5595 - val_accuracy: 0.5527\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5589 - accuracy: 0.5555 - val_loss: 0.5695 - val_accuracy: 0.5410\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5582 - accuracy: 0.5561 - val_loss: 0.5653 - val_accuracy: 0.5485\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5573 - accuracy: 0.5582 - val_loss: 0.5638 - val_accuracy: 0.5527\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5577 - accuracy: 0.5574 - val_loss: 0.5599 - val_accuracy: 0.5563\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5582 - accuracy: 0.5571 - val_loss: 0.5829 - val_accuracy: 0.5329\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5570 - accuracy: 0.5591 - val_loss: 0.5713 - val_accuracy: 0.5394\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5579 - accuracy: 0.5574 - val_loss: 0.5602 - val_accuracy: 0.5533\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5584 - accuracy: 0.5565 - val_loss: 0.5702 - val_accuracy: 0.5450\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5602 - accuracy: 0.5549 - val_loss: 0.5594 - val_accuracy: 0.5577\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5595 - accuracy: 0.5547 - val_loss: 0.5672 - val_accuracy: 0.5417\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5594 - accuracy: 0.5554 - val_loss: 0.5668 - val_accuracy: 0.5469\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5585 - accuracy: 0.5568 - val_loss: 0.5582 - val_accuracy: 0.5578\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5584 - accuracy: 0.5570 - val_loss: 0.5704 - val_accuracy: 0.5400\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5590 - accuracy: 0.5560 - val_loss: 0.5664 - val_accuracy: 0.5469\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5591 - accuracy: 0.5554 - val_loss: 0.5726 - val_accuracy: 0.5376\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5589 - accuracy: 0.5558 - val_loss: 0.5761 - val_accuracy: 0.5429\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5580 - accuracy: 0.5576 - val_loss: 0.5725 - val_accuracy: 0.5445\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5582 - accuracy: 0.5571 - val_loss: 0.5544 - val_accuracy: 0.5593\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5573 - accuracy: 0.5594 - val_loss: 0.5677 - val_accuracy: 0.5424\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5576 - accuracy: 0.5586 - val_loss: 0.5648 - val_accuracy: 0.5466\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5576 - accuracy: 0.5583 - val_loss: 0.5652 - val_accuracy: 0.5497\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5576 - accuracy: 0.5585 - val_loss: 0.5562 - val_accuracy: 0.5599\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5575 - accuracy: 0.5596 - val_loss: 0.5690 - val_accuracy: 0.5468\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5580 - accuracy: 0.5586 - val_loss: 0.5781 - val_accuracy: 0.5389\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5573 - accuracy: 0.5586 - val_loss: 0.5647 - val_accuracy: 0.5524\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5594 - accuracy: 0.5555 - val_loss: 0.5576 - val_accuracy: 0.5538\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5577 - accuracy: 0.5581 - val_loss: 0.5669 - val_accuracy: 0.5482\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5575 - accuracy: 0.5585 - val_loss: 0.5647 - val_accuracy: 0.5512\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5581 - accuracy: 0.5577 - val_loss: 0.5632 - val_accuracy: 0.5511\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5591 - accuracy: 0.5558 - val_loss: 0.5736 - val_accuracy: 0.5437\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5583 - accuracy: 0.5570 - val_loss: 0.5618 - val_accuracy: 0.5549\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5583 - accuracy: 0.5580 - val_loss: 0.5703 - val_accuracy: 0.5407\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5581 - accuracy: 0.5578 - val_loss: 0.5925 - val_accuracy: 0.5203\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5564 - accuracy: 0.5600 - val_loss: 0.5634 - val_accuracy: 0.5550\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5575 - accuracy: 0.5582 - val_loss: 0.5600 - val_accuracy: 0.5567\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5579 - accuracy: 0.5582 - val_loss: 0.5599 - val_accuracy: 0.5519\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5569 - accuracy: 0.5592 - val_loss: 0.5847 - val_accuracy: 0.5284\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5567 - accuracy: 0.5594 - val_loss: 0.5631 - val_accuracy: 0.5541\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5564 - accuracy: 0.5597 - val_loss: 0.5654 - val_accuracy: 0.5507\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5561 - accuracy: 0.5600 - val_loss: 0.5602 - val_accuracy: 0.5568\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5560 - accuracy: 0.5607 - val_loss: 0.5548 - val_accuracy: 0.5658\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5570 - accuracy: 0.5589 - val_loss: 0.5696 - val_accuracy: 0.5439\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5574 - accuracy: 0.5580 - val_loss: 0.5575 - val_accuracy: 0.5572\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.5566 - accuracy: 0.5596 - val_loss: 0.5738 - val_accuracy: 0.5379\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " q_activation_12 (QActivati  (None, 32)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 99        \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3619 (14.14 KB)\n",
      "Trainable params: 3555 (13.89 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_12      quantized_relu(6,0)\n",
      "batch_normalization_12 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 1.0725 - precision_10: 0.4827 - val_loss: 1.0325 - val_precision_10: 0.5016\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0240 - precision_10: 0.5050 - val_loss: 1.0207 - val_precision_10: 0.5102\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0130 - precision_10: 0.5092 - val_loss: 1.0106 - val_precision_10: 0.5103\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0070 - precision_10: 0.5133 - val_loss: 1.0133 - val_precision_10: 0.5088\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0016 - precision_10: 0.5159 - val_loss: 0.9968 - val_precision_10: 0.5140\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0000 - precision_10: 0.5182 - val_loss: 1.0062 - val_precision_10: 0.5125\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9991 - precision_10: 0.5198 - val_loss: 1.0133 - val_precision_10: 0.5199\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9957 - precision_10: 0.5230 - val_loss: 0.9937 - val_precision_10: 0.5239\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9921 - precision_10: 0.5252 - val_loss: 1.0209 - val_precision_10: 0.5081\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9915 - precision_10: 0.5258 - val_loss: 1.0038 - val_precision_10: 0.5217\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9895 - precision_10: 0.5287 - val_loss: 1.0397 - val_precision_10: 0.4909\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9850 - precision_10: 0.5318 - val_loss: 0.9961 - val_precision_10: 0.5224\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9839 - precision_10: 0.5327 - val_loss: 0.9861 - val_precision_10: 0.5347\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9842 - precision_10: 0.5337 - val_loss: 0.9969 - val_precision_10: 0.5254\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9810 - precision_10: 0.5354 - val_loss: 1.0003 - val_precision_10: 0.5190\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9788 - precision_10: 0.5372 - val_loss: 1.0033 - val_precision_10: 0.5209\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9801 - precision_10: 0.5368 - val_loss: 0.9898 - val_precision_10: 0.5275\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9790 - precision_10: 0.5368 - val_loss: 1.0010 - val_precision_10: 0.5243\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9759 - precision_10: 0.5388 - val_loss: 0.9900 - val_precision_10: 0.5272\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9765 - precision_10: 0.5393 - val_loss: 0.9820 - val_precision_10: 0.5393\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9752 - precision_10: 0.5395 - val_loss: 1.0079 - val_precision_10: 0.5088\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9734 - precision_10: 0.5432 - val_loss: 0.9797 - val_precision_10: 0.5381\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9721 - precision_10: 0.5421 - val_loss: 0.9676 - val_precision_10: 0.5473\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9728 - precision_10: 0.5418 - val_loss: 0.9773 - val_precision_10: 0.5369\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9743 - precision_10: 0.5410 - val_loss: 0.9773 - val_precision_10: 0.5376\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9726 - precision_10: 0.5420 - val_loss: 0.9866 - val_precision_10: 0.5283\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9755 - precision_10: 0.5396 - val_loss: 0.9916 - val_precision_10: 0.5304\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9778 - precision_10: 0.5378 - val_loss: 0.9874 - val_precision_10: 0.5255\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9753 - precision_10: 0.5396 - val_loss: 0.9813 - val_precision_10: 0.5341\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9748 - precision_10: 0.5396 - val_loss: 1.0005 - val_precision_10: 0.5210\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9765 - precision_10: 0.5395 - val_loss: 0.9895 - val_precision_10: 0.5307\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9711 - precision_10: 0.5430 - val_loss: 0.9978 - val_precision_10: 0.5299\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9719 - precision_10: 0.5428 - val_loss: 0.9706 - val_precision_10: 0.5459\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9762 - precision_10: 0.5403 - val_loss: 0.9919 - val_precision_10: 0.5234\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9747 - precision_10: 0.5408 - val_loss: 0.9608 - val_precision_10: 0.5494\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9723 - precision_10: 0.5421 - val_loss: 0.9846 - val_precision_10: 0.5337\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9692 - precision_10: 0.5448 - val_loss: 0.9802 - val_precision_10: 0.5373\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9707 - precision_10: 0.5434 - val_loss: 0.9845 - val_precision_10: 0.5409\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9708 - precision_10: 0.5434 - val_loss: 0.9658 - val_precision_10: 0.5462\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9730 - precision_10: 0.5415 - val_loss: 0.9716 - val_precision_10: 0.5429\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9739 - precision_10: 0.5414 - val_loss: 0.9968 - val_precision_10: 0.5333\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9688 - precision_10: 0.5451 - val_loss: 0.9874 - val_precision_10: 0.5279\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9695 - precision_10: 0.5442 - val_loss: 0.9799 - val_precision_10: 0.5397\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9716 - precision_10: 0.5432 - val_loss: 0.9800 - val_precision_10: 0.5355\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9740 - precision_10: 0.5413 - val_loss: 0.9682 - val_precision_10: 0.5514\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9728 - precision_10: 0.5423 - val_loss: 0.9810 - val_precision_10: 0.5312\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9740 - precision_10: 0.5415 - val_loss: 0.9717 - val_precision_10: 0.5402\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9743 - precision_10: 0.5406 - val_loss: 1.0011 - val_precision_10: 0.5185\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9757 - precision_10: 0.5401 - val_loss: 0.9832 - val_precision_10: 0.5288\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9768 - precision_10: 0.5392 - val_loss: 1.0222 - val_precision_10: 0.5131\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9741 - precision_10: 0.5425 - val_loss: 0.9856 - val_precision_10: 0.5418\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9711 - precision_10: 0.5423 - val_loss: 0.9750 - val_precision_10: 0.5344\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9708 - precision_10: 0.5442 - val_loss: 0.9771 - val_precision_10: 0.5415\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9687 - precision_10: 0.5452 - val_loss: 0.9768 - val_precision_10: 0.5302\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9711 - precision_10: 0.5434 - val_loss: 1.0286 - val_precision_10: 0.5080\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_12\n",
      "cannot prune layer batch_normalization_12\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.5857 - accuracy: 0.5189 - val_loss: 0.5870 - val_accuracy: 0.5206\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5785 - accuracy: 0.5219 - val_loss: 0.5847 - val_accuracy: 0.5199\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5791 - accuracy: 0.5203 - val_loss: 0.5827 - val_accuracy: 0.5176\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5762 - accuracy: 0.5249 - val_loss: 0.5831 - val_accuracy: 0.5257\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5742 - accuracy: 0.5276 - val_loss: 0.5766 - val_accuracy: 0.5225\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5705 - accuracy: 0.5339 - val_loss: 0.5805 - val_accuracy: 0.5251\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5690 - accuracy: 0.5372 - val_loss: 0.5668 - val_accuracy: 0.5385\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5695 - accuracy: 0.5353 - val_loss: 0.5812 - val_accuracy: 0.5231\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5698 - accuracy: 0.5362 - val_loss: 0.5686 - val_accuracy: 0.5348\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5697 - accuracy: 0.5359 - val_loss: 0.5692 - val_accuracy: 0.5343\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5700 - accuracy: 0.5368 - val_loss: 0.5722 - val_accuracy: 0.5264\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5717 - accuracy: 0.5330 - val_loss: 0.5802 - val_accuracy: 0.5308\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5716 - accuracy: 0.5337 - val_loss: 0.5700 - val_accuracy: 0.5350\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5725 - accuracy: 0.5322 - val_loss: 0.5745 - val_accuracy: 0.5293\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5717 - accuracy: 0.5332 - val_loss: 0.5782 - val_accuracy: 0.5239\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5718 - accuracy: 0.5334 - val_loss: 0.5735 - val_accuracy: 0.5300\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5717 - accuracy: 0.5336 - val_loss: 0.5756 - val_accuracy: 0.5254\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5721 - accuracy: 0.5315 - val_loss: 0.5732 - val_accuracy: 0.5229\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5716 - accuracy: 0.5330 - val_loss: 0.5690 - val_accuracy: 0.5378\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5716 - accuracy: 0.5325 - val_loss: 0.5726 - val_accuracy: 0.5331\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5720 - accuracy: 0.5334 - val_loss: 0.5821 - val_accuracy: 0.5253\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5710 - accuracy: 0.5338 - val_loss: 0.5851 - val_accuracy: 0.5179\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5721 - accuracy: 0.5325 - val_loss: 0.5783 - val_accuracy: 0.5298\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5712 - accuracy: 0.5339 - val_loss: 0.5725 - val_accuracy: 0.5287\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5724 - accuracy: 0.5325 - val_loss: 0.5746 - val_accuracy: 0.5267\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5721 - accuracy: 0.5320 - val_loss: 0.5834 - val_accuracy: 0.5220\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5708 - accuracy: 0.5339 - val_loss: 0.5709 - val_accuracy: 0.5258\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5721 - accuracy: 0.5330 - val_loss: 0.5724 - val_accuracy: 0.5266\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5698 - accuracy: 0.5355 - val_loss: 0.5771 - val_accuracy: 0.5287\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5705 - accuracy: 0.5348 - val_loss: 0.5788 - val_accuracy: 0.5316\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5709 - accuracy: 0.5344 - val_loss: 0.5754 - val_accuracy: 0.5235\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5705 - accuracy: 0.5355 - val_loss: 0.5780 - val_accuracy: 0.5220\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5711 - accuracy: 0.5353 - val_loss: 0.5767 - val_accuracy: 0.5283\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5706 - accuracy: 0.5349 - val_loss: 0.5743 - val_accuracy: 0.5333\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5700 - accuracy: 0.5357 - val_loss: 0.5742 - val_accuracy: 0.5294\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5704 - accuracy: 0.5350 - val_loss: 0.5746 - val_accuracy: 0.5224\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5706 - accuracy: 0.5339 - val_loss: 0.5757 - val_accuracy: 0.5292\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5707 - accuracy: 0.5345 - val_loss: 0.5811 - val_accuracy: 0.5269\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5701 - accuracy: 0.5358 - val_loss: 0.5719 - val_accuracy: 0.5337\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5693 - accuracy: 0.5378 - val_loss: 0.5847 - val_accuracy: 0.5166\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5703 - accuracy: 0.5352 - val_loss: 0.5827 - val_accuracy: 0.5238\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5719 - accuracy: 0.5338 - val_loss: 0.5750 - val_accuracy: 0.5338\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5705 - accuracy: 0.5349 - val_loss: 0.5685 - val_accuracy: 0.5404\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5698 - accuracy: 0.5372 - val_loss: 0.5707 - val_accuracy: 0.5340\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5702 - accuracy: 0.5368 - val_loss: 0.5773 - val_accuracy: 0.5323\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5697 - accuracy: 0.5376 - val_loss: 0.5716 - val_accuracy: 0.5317\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5683 - accuracy: 0.5395 - val_loss: 0.5737 - val_accuracy: 0.5362\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5688 - accuracy: 0.5382 - val_loss: 0.5719 - val_accuracy: 0.5338\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5689 - accuracy: 0.5384 - val_loss: 0.5730 - val_accuracy: 0.5317\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5681 - accuracy: 0.5389 - val_loss: 0.5733 - val_accuracy: 0.5312\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [24], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 24)                2544      \n",
      "                                                                 \n",
      " q_activation_13 (QActivati  (None, 24)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 24)                96        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 75        \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2715 (10.61 KB)\n",
      "Trainable params: 2667 (10.42 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=24 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_13      quantized_relu(6,0)\n",
      "batch_normalization_13 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 1.0807 - precision_11: 0.4784 - val_loss: 1.0330 - val_precision_11: 0.5032\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0270 - precision_11: 0.5036 - val_loss: 1.0348 - val_precision_11: 0.5024\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0199 - precision_11: 0.5046 - val_loss: 1.0373 - val_precision_11: 0.5020\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0176 - precision_11: 0.5069 - val_loss: 1.0246 - val_precision_11: 0.5025\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0124 - precision_11: 0.5108 - val_loss: 1.0097 - val_precision_11: 0.5080\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0116 - precision_11: 0.5121 - val_loss: 1.0180 - val_precision_11: 0.5081\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0094 - precision_11: 0.5124 - val_loss: 1.0130 - val_precision_11: 0.5099\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0088 - precision_11: 0.5120 - val_loss: 1.0130 - val_precision_11: 0.5149\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0059 - precision_11: 0.5144 - val_loss: 1.0197 - val_precision_11: 0.5125\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0034 - precision_11: 0.5149 - val_loss: 1.0168 - val_precision_11: 0.5032\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0038 - precision_11: 0.5161 - val_loss: 1.0121 - val_precision_11: 0.5175\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0004 - precision_11: 0.5180 - val_loss: 1.0064 - val_precision_11: 0.5108\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0016 - precision_11: 0.5165 - val_loss: 1.0032 - val_precision_11: 0.5191\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9988 - precision_11: 0.5183 - val_loss: 1.0091 - val_precision_11: 0.5129\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9977 - precision_11: 0.5196 - val_loss: 1.0006 - val_precision_11: 0.5142\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9989 - precision_11: 0.5179 - val_loss: 1.0099 - val_precision_11: 0.5130\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9952 - precision_11: 0.5224 - val_loss: 1.0050 - val_precision_11: 0.5251\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9958 - precision_11: 0.5216 - val_loss: 1.0017 - val_precision_11: 0.5257\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9963 - precision_11: 0.5213 - val_loss: 1.0324 - val_precision_11: 0.5000\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9922 - precision_11: 0.5247 - val_loss: 1.0053 - val_precision_11: 0.5144\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9908 - precision_11: 0.5257 - val_loss: 0.9920 - val_precision_11: 0.5265\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9895 - precision_11: 0.5276 - val_loss: 0.9948 - val_precision_11: 0.5240\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9908 - precision_11: 0.5260 - val_loss: 0.9952 - val_precision_11: 0.5279\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9928 - precision_11: 0.5259 - val_loss: 1.0063 - val_precision_11: 0.5190\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9908 - precision_11: 0.5265 - val_loss: 1.0076 - val_precision_11: 0.5128\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9893 - precision_11: 0.5284 - val_loss: 1.0695 - val_precision_11: 0.4650\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9897 - precision_11: 0.5292 - val_loss: 1.0008 - val_precision_11: 0.5137\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9873 - precision_11: 0.5298 - val_loss: 0.9886 - val_precision_11: 0.5323\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9862 - precision_11: 0.5305 - val_loss: 0.9984 - val_precision_11: 0.5345\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9875 - precision_11: 0.5301 - val_loss: 0.9967 - val_precision_11: 0.5246\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9893 - precision_11: 0.5295 - val_loss: 1.0285 - val_precision_11: 0.4973\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9884 - precision_11: 0.5293 - val_loss: 1.0147 - val_precision_11: 0.5126\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9910 - precision_11: 0.5273 - val_loss: 0.9918 - val_precision_11: 0.5279\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9876 - precision_11: 0.5308 - val_loss: 0.9944 - val_precision_11: 0.5248\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9882 - precision_11: 0.5286 - val_loss: 0.9866 - val_precision_11: 0.5238\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9870 - precision_11: 0.5309 - val_loss: 1.0085 - val_precision_11: 0.5224\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9873 - precision_11: 0.5289 - val_loss: 0.9860 - val_precision_11: 0.5387\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9826 - precision_11: 0.5331 - val_loss: 0.9823 - val_precision_11: 0.5295\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9858 - precision_11: 0.5305 - val_loss: 1.0292 - val_precision_11: 0.5046\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9827 - precision_11: 0.5327 - val_loss: 1.0010 - val_precision_11: 0.5242\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9864 - precision_11: 0.5300 - val_loss: 0.9865 - val_precision_11: 0.5354\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9872 - precision_11: 0.5302 - val_loss: 0.9998 - val_precision_11: 0.5163\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9809 - precision_11: 0.5351 - val_loss: 0.9879 - val_precision_11: 0.5351\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9894 - precision_11: 0.5277 - val_loss: 0.9878 - val_precision_11: 0.5278\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9867 - precision_11: 0.5305 - val_loss: 0.9924 - val_precision_11: 0.5268\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9854 - precision_11: 0.5309 - val_loss: 0.9867 - val_precision_11: 0.5234\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9854 - precision_11: 0.5316 - val_loss: 0.9948 - val_precision_11: 0.5281\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9827 - precision_11: 0.5329 - val_loss: 1.0191 - val_precision_11: 0.5041\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9840 - precision_11: 0.5314 - val_loss: 1.0094 - val_precision_11: 0.5236\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9849 - precision_11: 0.5323 - val_loss: 1.0089 - val_precision_11: 0.5121\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9821 - precision_11: 0.5344 - val_loss: 0.9865 - val_precision_11: 0.5237\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9866 - precision_11: 0.5297 - val_loss: 0.9967 - val_precision_11: 0.5223\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9885 - precision_11: 0.5295 - val_loss: 0.9989 - val_precision_11: 0.5178\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9885 - precision_11: 0.5290 - val_loss: 0.9850 - val_precision_11: 0.5350\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9850 - precision_11: 0.5320 - val_loss: 0.9962 - val_precision_11: 0.5251\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9859 - precision_11: 0.5309 - val_loss: 1.0117 - val_precision_11: 0.5194\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9874 - precision_11: 0.5298 - val_loss: 0.9906 - val_precision_11: 0.5268\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9854 - precision_11: 0.5317 - val_loss: 0.9916 - val_precision_11: 0.5301\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_13\n",
      "cannot prune layer batch_normalization_13\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 0.5931 - accuracy: 0.5157 - val_loss: 0.5829 - val_accuracy: 0.5092\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5835 - accuracy: 0.5143 - val_loss: 0.5872 - val_accuracy: 0.5058\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5825 - accuracy: 0.5147 - val_loss: 0.5858 - val_accuracy: 0.5094\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5795 - accuracy: 0.5191 - val_loss: 0.5837 - val_accuracy: 0.5135\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5737 - accuracy: 0.5275 - val_loss: 0.5779 - val_accuracy: 0.5214\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5730 - accuracy: 0.5300 - val_loss: 0.5778 - val_accuracy: 0.5269\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5706 - accuracy: 0.5352 - val_loss: 0.5697 - val_accuracy: 0.5376\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5711 - accuracy: 0.5340 - val_loss: 0.5802 - val_accuracy: 0.5235\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5742 - accuracy: 0.5299 - val_loss: 0.5815 - val_accuracy: 0.5201\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5736 - accuracy: 0.5295 - val_loss: 0.5746 - val_accuracy: 0.5280\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5736 - accuracy: 0.5291 - val_loss: 0.5899 - val_accuracy: 0.5178\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5739 - accuracy: 0.5298 - val_loss: 0.5746 - val_accuracy: 0.5228\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5743 - accuracy: 0.5284 - val_loss: 0.5748 - val_accuracy: 0.5255\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5749 - accuracy: 0.5271 - val_loss: 0.5748 - val_accuracy: 0.5302\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5740 - accuracy: 0.5290 - val_loss: 0.5742 - val_accuracy: 0.5338\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5738 - accuracy: 0.5288 - val_loss: 0.5784 - val_accuracy: 0.5219\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5748 - accuracy: 0.5275 - val_loss: 0.5805 - val_accuracy: 0.5209\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5746 - accuracy: 0.5282 - val_loss: 0.5880 - val_accuracy: 0.5143\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5749 - accuracy: 0.5273 - val_loss: 0.5808 - val_accuracy: 0.5173\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5748 - accuracy: 0.5282 - val_loss: 0.5939 - val_accuracy: 0.5076\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5743 - accuracy: 0.5278 - val_loss: 0.5799 - val_accuracy: 0.5235\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5746 - accuracy: 0.5267 - val_loss: 0.5744 - val_accuracy: 0.5270\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5747 - accuracy: 0.5276 - val_loss: 0.5784 - val_accuracy: 0.5267\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5749 - accuracy: 0.5279 - val_loss: 0.5784 - val_accuracy: 0.5228\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5747 - accuracy: 0.5273 - val_loss: 0.5787 - val_accuracy: 0.5259\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5762 - accuracy: 0.5263 - val_loss: 0.5789 - val_accuracy: 0.5234\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5761 - accuracy: 0.5260 - val_loss: 0.5899 - val_accuracy: 0.5090\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5762 - accuracy: 0.5248 - val_loss: 0.5797 - val_accuracy: 0.5192\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5759 - accuracy: 0.5263 - val_loss: 0.5740 - val_accuracy: 0.5297\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5745 - accuracy: 0.5284 - val_loss: 0.5737 - val_accuracy: 0.5239\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5750 - accuracy: 0.5275 - val_loss: 0.5803 - val_accuracy: 0.5224\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5751 - accuracy: 0.5270 - val_loss: 0.5807 - val_accuracy: 0.5247\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5758 - accuracy: 0.5265 - val_loss: 0.5792 - val_accuracy: 0.5247\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5751 - accuracy: 0.5274 - val_loss: 0.5854 - val_accuracy: 0.5112\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5755 - accuracy: 0.5269 - val_loss: 0.6055 - val_accuracy: 0.4953\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5750 - accuracy: 0.5274 - val_loss: 0.5834 - val_accuracy: 0.5213\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5756 - accuracy: 0.5265 - val_loss: 0.5924 - val_accuracy: 0.5058\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5754 - accuracy: 0.5272 - val_loss: 0.5777 - val_accuracy: 0.5303\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5746 - accuracy: 0.5286 - val_loss: 0.5793 - val_accuracy: 0.5241\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5749 - accuracy: 0.5286 - val_loss: 0.5843 - val_accuracy: 0.5210\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5742 - accuracy: 0.5289 - val_loss: 0.5876 - val_accuracy: 0.5130\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5756 - accuracy: 0.5265 - val_loss: 0.5852 - val_accuracy: 0.5142\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5746 - accuracy: 0.5280 - val_loss: 0.5743 - val_accuracy: 0.5239\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5759 - accuracy: 0.5267 - val_loss: 0.5820 - val_accuracy: 0.5202\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5744 - accuracy: 0.5292 - val_loss: 0.5834 - val_accuracy: 0.5201\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5743 - accuracy: 0.5290 - val_loss: 0.5756 - val_accuracy: 0.5234\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5749 - accuracy: 0.5286 - val_loss: 0.5811 - val_accuracy: 0.5162\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5757 - accuracy: 0.5267 - val_loss: 0.5856 - val_accuracy: 0.5108\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5767 - accuracy: 0.5235 - val_loss: 0.5782 - val_accuracy: 0.5214\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5763 - accuracy: 0.5248 - val_loss: 0.5861 - val_accuracy: 0.5185\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " q_activation_14 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 51        \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1811 (7.07 KB)\n",
      "Trainable params: 1779 (6.95 KB)\n",
      "Non-trainable params: 32 (128.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_14      quantized_relu(6,0)\n",
      "batch_normalization_14 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 1.0814 - precision_12: 0.4746 - val_loss: 1.0362 - val_precision_12: 0.5038\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0276 - precision_12: 0.5055 - val_loss: 1.0294 - val_precision_12: 0.5026\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0194 - precision_12: 0.5070 - val_loss: 1.0209 - val_precision_12: 0.5066\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0150 - precision_12: 0.5082 - val_loss: 1.0129 - val_precision_12: 0.5057\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0129 - precision_12: 0.5107 - val_loss: 1.0217 - val_precision_12: 0.5054\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0107 - precision_12: 0.5105 - val_loss: 1.0295 - val_precision_12: 0.5016\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0082 - precision_12: 0.5110 - val_loss: 1.0232 - val_precision_12: 0.4785\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0088 - precision_12: 0.5115 - val_loss: 1.0245 - val_precision_12: 0.5035\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0036 - precision_12: 0.5145 - val_loss: 1.0151 - val_precision_12: 0.5000\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0048 - precision_12: 0.5136 - val_loss: 1.0060 - val_precision_12: 0.5109\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0068 - precision_12: 0.5127 - val_loss: 1.0157 - val_precision_12: 0.5087\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0077 - precision_12: 0.5122 - val_loss: 1.0068 - val_precision_12: 0.5079\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0073 - precision_12: 0.5137 - val_loss: 1.0076 - val_precision_12: 0.5162\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0078 - precision_12: 0.5125 - val_loss: 1.0109 - val_precision_12: 0.5096\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0074 - precision_12: 0.5129 - val_loss: 1.0106 - val_precision_12: 0.5109\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0054 - precision_12: 0.5137 - val_loss: 1.0111 - val_precision_12: 0.5146\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0033 - precision_12: 0.5157 - val_loss: 1.0162 - val_precision_12: 0.5084\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0025 - precision_12: 0.5176 - val_loss: 1.0135 - val_precision_12: 0.5101\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0048 - precision_12: 0.5156 - val_loss: 1.0320 - val_precision_12: 0.5075\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0055 - precision_12: 0.5140 - val_loss: 1.0520 - val_precision_12: 0.5012\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0041 - precision_12: 0.5154 - val_loss: 1.0156 - val_precision_12: 0.5102\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0003 - precision_12: 0.5181 - val_loss: 1.0089 - val_precision_12: 0.5100\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0004 - precision_12: 0.5184 - val_loss: 1.0075 - val_precision_12: 0.5136\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9984 - precision_12: 0.5202 - val_loss: 1.0058 - val_precision_12: 0.5145\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9983 - precision_12: 0.5201 - val_loss: 1.0020 - val_precision_12: 0.5196\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9993 - precision_12: 0.5204 - val_loss: 0.9928 - val_precision_12: 0.5258\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0008 - precision_12: 0.5188 - val_loss: 1.0316 - val_precision_12: 0.5087\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9973 - precision_12: 0.5214 - val_loss: 1.0161 - val_precision_12: 0.5111\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9976 - precision_12: 0.5220 - val_loss: 1.0044 - val_precision_12: 0.5157\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9960 - precision_12: 0.5225 - val_loss: 1.0187 - val_precision_12: 0.4987\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9998 - precision_12: 0.5213 - val_loss: 1.0065 - val_precision_12: 0.5247\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0020 - precision_12: 0.5184 - val_loss: 1.0189 - val_precision_12: 0.5095\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9973 - precision_12: 0.5215 - val_loss: 1.0116 - val_precision_12: 0.5120\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9966 - precision_12: 0.5224 - val_loss: 1.0043 - val_precision_12: 0.5153\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9960 - precision_12: 0.5230 - val_loss: 0.9967 - val_precision_12: 0.5210\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9910 - precision_12: 0.5266 - val_loss: 1.0094 - val_precision_12: 0.5216\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9932 - precision_12: 0.5241 - val_loss: 1.0085 - val_precision_12: 0.5136\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9980 - precision_12: 0.5217 - val_loss: 0.9975 - val_precision_12: 0.5179\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9954 - precision_12: 0.5234 - val_loss: 1.0156 - val_precision_12: 0.5127\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9931 - precision_12: 0.5252 - val_loss: 0.9873 - val_precision_12: 0.5278\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9923 - precision_12: 0.5265 - val_loss: 0.9961 - val_precision_12: 0.5178\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9914 - precision_12: 0.5272 - val_loss: 1.0153 - val_precision_12: 0.5102\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9910 - precision_12: 0.5266 - val_loss: 1.0122 - val_precision_12: 0.5221\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9930 - precision_12: 0.5248 - val_loss: 0.9937 - val_precision_12: 0.5217\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9903 - precision_12: 0.5271 - val_loss: 0.9956 - val_precision_12: 0.5257\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9951 - precision_12: 0.5241 - val_loss: 0.9986 - val_precision_12: 0.5199\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9933 - precision_12: 0.5250 - val_loss: 1.0144 - val_precision_12: 0.5201\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9948 - precision_12: 0.5248 - val_loss: 0.9997 - val_precision_12: 0.5198\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9918 - precision_12: 0.5253 - val_loss: 0.9921 - val_precision_12: 0.5293\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9955 - precision_12: 0.5233 - val_loss: 1.0012 - val_precision_12: 0.5174\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9965 - precision_12: 0.5229 - val_loss: 1.0121 - val_precision_12: 0.5086\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9986 - precision_12: 0.5217 - val_loss: 1.0164 - val_precision_12: 0.5041\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9993 - precision_12: 0.5195 - val_loss: 1.0493 - val_precision_12: 0.4829\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9987 - precision_12: 0.5204 - val_loss: 1.0055 - val_precision_12: 0.5198\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9988 - precision_12: 0.5216 - val_loss: 1.0115 - val_precision_12: 0.5094\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0008 - precision_12: 0.5189 - val_loss: 1.0166 - val_precision_12: 0.5073\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0004 - precision_12: 0.5202 - val_loss: 1.0235 - val_precision_12: 0.5043\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9997 - precision_12: 0.5203 - val_loss: 1.0000 - val_precision_12: 0.5174\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0007 - precision_12: 0.5203 - val_loss: 1.0220 - val_precision_12: 0.5086\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.9999 - precision_12: 0.5197 - val_loss: 1.0024 - val_precision_12: 0.5171\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_14\n",
      "cannot prune layer batch_normalization_14\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 8ms/step - loss: 0.5973 - accuracy: 0.5109 - val_loss: 0.6109 - val_accuracy: 0.5057\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5876 - accuracy: 0.5078 - val_loss: 0.5893 - val_accuracy: 0.5045\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5865 - accuracy: 0.5085 - val_loss: 0.5915 - val_accuracy: 0.5063\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5843 - accuracy: 0.5120 - val_loss: 0.5913 - val_accuracy: 0.5085\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5826 - accuracy: 0.5145 - val_loss: 0.5863 - val_accuracy: 0.5089\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5807 - accuracy: 0.5177 - val_loss: 0.5907 - val_accuracy: 0.5107\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5785 - accuracy: 0.5209 - val_loss: 0.5811 - val_accuracy: 0.5129\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5799 - accuracy: 0.5191 - val_loss: 0.5806 - val_accuracy: 0.5134\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5789 - accuracy: 0.5210 - val_loss: 0.5814 - val_accuracy: 0.5188\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5790 - accuracy: 0.5212 - val_loss: 0.5831 - val_accuracy: 0.5099\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5803 - accuracy: 0.5189 - val_loss: 0.5913 - val_accuracy: 0.5120\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5795 - accuracy: 0.5194 - val_loss: 0.5831 - val_accuracy: 0.5170\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5809 - accuracy: 0.5178 - val_loss: 0.5927 - val_accuracy: 0.5105\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5806 - accuracy: 0.5183 - val_loss: 0.5835 - val_accuracy: 0.5147\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5810 - accuracy: 0.5172 - val_loss: 0.5814 - val_accuracy: 0.5167\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5805 - accuracy: 0.5187 - val_loss: 0.5836 - val_accuracy: 0.5179\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5802 - accuracy: 0.5197 - val_loss: 0.5915 - val_accuracy: 0.5139\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5807 - accuracy: 0.5193 - val_loss: 0.5914 - val_accuracy: 0.5093\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5814 - accuracy: 0.5178 - val_loss: 0.5857 - val_accuracy: 0.5157\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5810 - accuracy: 0.5192 - val_loss: 0.5827 - val_accuracy: 0.5123\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5819 - accuracy: 0.5172 - val_loss: 0.5929 - val_accuracy: 0.5073\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5825 - accuracy: 0.5162 - val_loss: 0.5884 - val_accuracy: 0.5093\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5814 - accuracy: 0.5176 - val_loss: 0.6007 - val_accuracy: 0.5075\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5833 - accuracy: 0.5147 - val_loss: 0.5900 - val_accuracy: 0.5097\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5827 - accuracy: 0.5159 - val_loss: 0.5855 - val_accuracy: 0.5086\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5822 - accuracy: 0.5164 - val_loss: 0.5865 - val_accuracy: 0.5133\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5816 - accuracy: 0.5171 - val_loss: 0.5802 - val_accuracy: 0.5138\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5830 - accuracy: 0.5153 - val_loss: 0.5835 - val_accuracy: 0.5194\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5840 - accuracy: 0.5134 - val_loss: 0.5881 - val_accuracy: 0.5141\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5827 - accuracy: 0.5150 - val_loss: 0.5813 - val_accuracy: 0.5131\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5817 - accuracy: 0.5164 - val_loss: 0.5803 - val_accuracy: 0.5207\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5823 - accuracy: 0.5171 - val_loss: 0.5851 - val_accuracy: 0.5139\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5813 - accuracy: 0.5176 - val_loss: 0.5866 - val_accuracy: 0.5116\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5835 - accuracy: 0.5144 - val_loss: 0.5852 - val_accuracy: 0.5114\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5832 - accuracy: 0.5156 - val_loss: 0.5955 - val_accuracy: 0.5047\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5826 - accuracy: 0.5158 - val_loss: 0.5868 - val_accuracy: 0.5145\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5810 - accuracy: 0.5179 - val_loss: 0.5850 - val_accuracy: 0.5146\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5825 - accuracy: 0.5169 - val_loss: 0.5906 - val_accuracy: 0.5136\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5828 - accuracy: 0.5153 - val_loss: 0.5848 - val_accuracy: 0.5221\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5823 - accuracy: 0.5168 - val_loss: 0.5886 - val_accuracy: 0.5094\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5831 - accuracy: 0.5149 - val_loss: 0.5860 - val_accuracy: 0.5173\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5816 - accuracy: 0.5170 - val_loss: 0.5885 - val_accuracy: 0.5102\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5819 - accuracy: 0.5171 - val_loss: 0.6109 - val_accuracy: 0.4859\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5825 - accuracy: 0.5162 - val_loss: 0.5870 - val_accuracy: 0.5144\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5811 - accuracy: 0.5186 - val_loss: 0.5944 - val_accuracy: 0.5096\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5821 - accuracy: 0.5175 - val_loss: 0.5864 - val_accuracy: 0.5139\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5811 - accuracy: 0.5190 - val_loss: 0.6043 - val_accuracy: 0.4909\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5826 - accuracy: 0.5160 - val_loss: 0.5820 - val_accuracy: 0.5180\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5828 - accuracy: 0.5144 - val_loss: 0.5810 - val_accuracy: 0.5190\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5829 - accuracy: 0.5144 - val_loss: 0.5829 - val_accuracy: 0.5156\n",
      "1714/1714 [==============================] - 4s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'WEIGHTS_BITS': 4, 'BIAS_BITS': 4, 'ACTIVATION_BITS': 6, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.35, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " q_activation_15 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 27        \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 907 (3.54 KB)\n",
      "Trainable params: 891 (3.48 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "q_activation_15      quantized_relu(6,0)\n",
      "batch_normalization_15 is normal keras bn layer\n",
      "dense_output         u=3 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 8ms/step - loss: 1.1406 - precision_13: 0.4527 - val_loss: 1.0501 - val_precision_13: 0.4978\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0417 - precision_13: 0.5012 - val_loss: 1.0486 - val_precision_13: 0.4994\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0309 - precision_13: 0.5044 - val_loss: 1.0286 - val_precision_13: 0.5032\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0260 - precision_13: 0.5058 - val_loss: 1.0266 - val_precision_13: 0.5061\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0243 - precision_13: 0.5045 - val_loss: 1.0255 - val_precision_13: 0.5055\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0233 - precision_13: 0.5056 - val_loss: 1.0236 - val_precision_13: 0.5042\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0231 - precision_13: 0.5057 - val_loss: 1.0254 - val_precision_13: 0.5064\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0228 - precision_13: 0.5063 - val_loss: 1.0331 - val_precision_13: 0.5057\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0222 - precision_13: 0.5085 - val_loss: 1.0313 - val_precision_13: 0.5031\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0196 - precision_13: 0.5081 - val_loss: 1.0190 - val_precision_13: 0.5091\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0223 - precision_13: 0.5087 - val_loss: 1.0304 - val_precision_13: 0.5027\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0206 - precision_13: 0.5092 - val_loss: 1.0390 - val_precision_13: 0.4952\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0163 - precision_13: 0.5123 - val_loss: 1.0164 - val_precision_13: 0.5007\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0158 - precision_13: 0.5088 - val_loss: 1.0198 - val_precision_13: 0.5101\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0173 - precision_13: 0.5088 - val_loss: 1.0258 - val_precision_13: 0.4992\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0172 - precision_13: 0.5072 - val_loss: 1.0461 - val_precision_13: 0.4952\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0148 - precision_13: 0.5113 - val_loss: 1.0164 - val_precision_13: 0.5099\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0168 - precision_13: 0.5097 - val_loss: 1.0183 - val_precision_13: 0.5077\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0182 - precision_13: 0.5084 - val_loss: 1.0136 - val_precision_13: 0.5157\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0139 - precision_13: 0.5110 - val_loss: 1.0248 - val_precision_13: 0.5014\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0136 - precision_13: 0.5121 - val_loss: 1.0163 - val_precision_13: 0.5132\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0168 - precision_13: 0.5069 - val_loss: 1.0313 - val_precision_13: 0.4578\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0185 - precision_13: 0.5064 - val_loss: 1.0330 - val_precision_13: 0.5100\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0199 - precision_13: 0.5061 - val_loss: 1.0210 - val_precision_13: 0.5091\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0181 - precision_13: 0.5072 - val_loss: 1.0202 - val_precision_13: 0.5048\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0182 - precision_13: 0.5083 - val_loss: 1.0151 - val_precision_13: 0.5059\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0185 - precision_13: 0.5080 - val_loss: 1.0227 - val_precision_13: 0.5069\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0150 - precision_13: 0.5094 - val_loss: 1.0225 - val_precision_13: 0.5062\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0175 - precision_13: 0.5087 - val_loss: 1.0236 - val_precision_13: 0.5032\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0181 - precision_13: 0.5079 - val_loss: 1.0305 - val_precision_13: 0.5002\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0210 - precision_13: 0.5073 - val_loss: 1.0242 - val_precision_13: 0.4476\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0196 - precision_13: 0.5057 - val_loss: 1.0269 - val_precision_13: 0.5044\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0191 - precision_13: 0.5074 - val_loss: 1.0189 - val_precision_13: 0.5008\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0188 - precision_13: 0.5076 - val_loss: 1.0175 - val_precision_13: 0.5833\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0190 - precision_13: 0.5071 - val_loss: 1.0125 - val_precision_13: 0.5084\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0165 - precision_13: 0.5088 - val_loss: 1.0234 - val_precision_13: 0.5035\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0195 - precision_13: 0.5076 - val_loss: 1.0377 - val_precision_13: 0.5047\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0183 - precision_13: 0.5067 - val_loss: 1.0213 - val_precision_13: 0.5051\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0197 - precision_13: 0.5065 - val_loss: 1.0376 - val_precision_13: 0.4905\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0192 - precision_13: 0.5090 - val_loss: 1.0177 - val_precision_13: 0.5127\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0178 - precision_13: 0.5080 - val_loss: 1.0307 - val_precision_13: 0.5024\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0186 - precision_13: 0.5088 - val_loss: 1.0194 - val_precision_13: 0.5075\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0175 - precision_13: 0.5088 - val_loss: 1.0244 - val_precision_13: 0.5360\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0163 - precision_13: 0.5095 - val_loss: 1.0241 - val_precision_13: 0.5058\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0143 - precision_13: 0.5110 - val_loss: 1.0289 - val_precision_13: 0.5073\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0145 - precision_13: 0.5113 - val_loss: 1.0345 - val_precision_13: 0.5079\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0156 - precision_13: 0.5098 - val_loss: 1.0233 - val_precision_13: 0.5022\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0143 - precision_13: 0.5108 - val_loss: 1.0215 - val_precision_13: 0.5098\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0150 - precision_13: 0.5108 - val_loss: 1.0172 - val_precision_13: 0.5200\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0142 - precision_13: 0.5107 - val_loss: 1.0115 - val_precision_13: 0.5062\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0123 - precision_13: 0.5114 - val_loss: 1.0198 - val_precision_13: 0.5072\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0120 - precision_13: 0.5136 - val_loss: 1.0163 - val_precision_13: 0.5065\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0129 - precision_13: 0.5116 - val_loss: 1.0118 - val_precision_13: 0.5097\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0138 - precision_13: 0.5104 - val_loss: 1.0279 - val_precision_13: 0.5051\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0152 - precision_13: 0.5123 - val_loss: 1.0191 - val_precision_13: 0.5081\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0132 - precision_13: 0.5121 - val_loss: 1.0106 - val_precision_13: 0.5118\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0142 - precision_13: 0.5107 - val_loss: 1.0176 - val_precision_13: 0.5063\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0155 - precision_13: 0.5099 - val_loss: 1.0136 - val_precision_13: 0.5236\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0140 - precision_13: 0.5105 - val_loss: 1.0178 - val_precision_13: 0.5078\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0123 - precision_13: 0.5105 - val_loss: 1.0117 - val_precision_13: 0.5162\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0132 - precision_13: 0.5109 - val_loss: 1.0206 - val_precision_13: 0.5064\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0148 - precision_13: 0.5100 - val_loss: 1.0203 - val_precision_13: 0.5055\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0155 - precision_13: 0.5102 - val_loss: 1.0475 - val_precision_13: 0.4947\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0153 - precision_13: 0.5097 - val_loss: 1.0401 - val_precision_13: 0.4958\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0152 - precision_13: 0.5098 - val_loss: 1.0101 - val_precision_13: 0.5099\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0148 - precision_13: 0.5097 - val_loss: 1.0237 - val_precision_13: 0.5128\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0139 - precision_13: 0.5114 - val_loss: 1.0229 - val_precision_13: 0.5152\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0150 - precision_13: 0.5104 - val_loss: 1.0227 - val_precision_13: 0.5176\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0159 - precision_13: 0.5093 - val_loss: 1.0189 - val_precision_13: 0.5076\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0148 - precision_13: 0.5112 - val_loss: 1.0202 - val_precision_13: 0.5015\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0137 - precision_13: 0.5107 - val_loss: 1.0183 - val_precision_13: 0.5081\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0117 - precision_13: 0.5110 - val_loss: 1.0179 - val_precision_13: 0.5170\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0142 - precision_13: 0.5104 - val_loss: 1.0324 - val_precision_13: 0.5037\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0142 - precision_13: 0.5103 - val_loss: 1.0228 - val_precision_13: 0.5202\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0166 - precision_13: 0.5092 - val_loss: 1.0151 - val_precision_13: 0.5083\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0158 - precision_13: 0.5080 - val_loss: 1.0137 - val_precision_13: 0.5124\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0145 - precision_13: 0.5102 - val_loss: 1.0249 - val_precision_13: 0.5014\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0148 - precision_13: 0.5100 - val_loss: 1.0143 - val_precision_13: 0.5079\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0144 - precision_13: 0.5089 - val_loss: 1.0249 - val_precision_13: 0.5044\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0135 - precision_13: 0.5109 - val_loss: 1.0289 - val_precision_13: 0.5059\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0150 - precision_13: 0.5096 - val_loss: 1.0257 - val_precision_13: 0.5071\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0142 - precision_13: 0.5097 - val_loss: 1.0258 - val_precision_13: 0.5039\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0148 - precision_13: 0.5090 - val_loss: 1.0289 - val_precision_13: 0.4981\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0153 - precision_13: 0.5092 - val_loss: 1.0204 - val_precision_13: 0.4977\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 1.0137 - precision_13: 0.5103 - val_loss: 1.0238 - val_precision_13: 0.5040\n",
      "pruning layer dense1\n",
      "cannot prune layer q_activation_15\n",
      "cannot prune layer batch_normalization_15\n",
      "pruning layer dense_output\n",
      "cannot prune layer output_softmax\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 9s 8ms/step - loss: 0.6067 - accuracy: 0.5008 - val_loss: 0.6005 - val_accuracy: 0.4979\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5965 - accuracy: 0.5008 - val_loss: 0.6005 - val_accuracy: 0.4978\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5955 - accuracy: 0.5005 - val_loss: 0.5968 - val_accuracy: 0.4974\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5958 - accuracy: 0.5008 - val_loss: 0.5948 - val_accuracy: 0.4978\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5916 - accuracy: 0.5025 - val_loss: 0.5926 - val_accuracy: 0.5008\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5913 - accuracy: 0.5024 - val_loss: 0.6102 - val_accuracy: 0.4999\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5925 - accuracy: 0.5031 - val_loss: 0.5901 - val_accuracy: 0.4980\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5913 - accuracy: 0.5027 - val_loss: 0.5931 - val_accuracy: 0.5051\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5912 - accuracy: 0.5028 - val_loss: 0.6007 - val_accuracy: 0.4980\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5913 - accuracy: 0.5023 - val_loss: 0.6040 - val_accuracy: 0.4980\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5915 - accuracy: 0.5016 - val_loss: 0.5964 - val_accuracy: 0.4988\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.5921 - accuracy: 0.5016 - val_loss: 0.5897 - val_accuracy: 0.4981\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5918 - accuracy: 0.5024 - val_loss: 0.5900 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5927 - accuracy: 0.5020 - val_loss: 0.5972 - val_accuracy: 0.4974\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5927 - accuracy: 0.5020 - val_loss: 0.5977 - val_accuracy: 0.4979\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5921 - accuracy: 0.5015 - val_loss: 0.5948 - val_accuracy: 0.4994\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5916 - accuracy: 0.5022 - val_loss: 0.5991 - val_accuracy: 0.4939\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5919 - accuracy: 0.5022 - val_loss: 0.5944 - val_accuracy: 0.4978\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5920 - accuracy: 0.5025 - val_loss: 0.5935 - val_accuracy: 0.5004\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5916 - accuracy: 0.5028 - val_loss: 0.6006 - val_accuracy: 0.4974\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5923 - accuracy: 0.5022 - val_loss: 0.6053 - val_accuracy: 0.4755\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5924 - accuracy: 0.5020 - val_loss: 0.6007 - val_accuracy: 0.4983\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5928 - accuracy: 0.5015 - val_loss: 0.6041 - val_accuracy: 0.4986\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5934 - accuracy: 0.5020 - val_loss: 0.5925 - val_accuracy: 0.4980\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5932 - accuracy: 0.5021 - val_loss: 0.5959 - val_accuracy: 0.4995\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5931 - accuracy: 0.5017 - val_loss: 0.5974 - val_accuracy: 0.4949\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5927 - accuracy: 0.5020 - val_loss: 0.5992 - val_accuracy: 0.4977\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5927 - accuracy: 0.5026 - val_loss: 0.5924 - val_accuracy: 0.4985\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5932 - accuracy: 0.5018 - val_loss: 0.5931 - val_accuracy: 0.4993\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5928 - accuracy: 0.5030 - val_loss: 0.5921 - val_accuracy: 0.4979\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5924 - accuracy: 0.5022 - val_loss: 0.6024 - val_accuracy: 0.4951\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5928 - accuracy: 0.5020 - val_loss: 0.5958 - val_accuracy: 0.4980\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5928 - accuracy: 0.5016 - val_loss: 0.5934 - val_accuracy: 0.4996\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5927 - accuracy: 0.5022 - val_loss: 0.5998 - val_accuracy: 0.4978\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5933 - accuracy: 0.5018 - val_loss: 0.5987 - val_accuracy: 0.4979\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5934 - accuracy: 0.5013 - val_loss: 0.5939 - val_accuracy: 0.4979\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5931 - accuracy: 0.5014 - val_loss: 0.6002 - val_accuracy: 0.4974\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5933 - accuracy: 0.5017 - val_loss: 0.5967 - val_accuracy: 0.4978\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5932 - accuracy: 0.5018 - val_loss: 0.5992 - val_accuracy: 0.4994\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5939 - accuracy: 0.5011 - val_loss: 0.6051 - val_accuracy: 0.4976\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.5947 - accuracy: 0.5012 - val_loss: 0.5995 - val_accuracy: 0.4980\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5939 - accuracy: 0.5009 - val_loss: 0.5935 - val_accuracy: 0.4979\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5933 - accuracy: 0.5016 - val_loss: 0.5947 - val_accuracy: 0.4979\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5933 - accuracy: 0.5014 - val_loss: 0.5961 - val_accuracy: 0.4981\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5932 - accuracy: 0.5016 - val_loss: 0.5954 - val_accuracy: 0.4978\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5929 - accuracy: 0.5025 - val_loss: 0.5961 - val_accuracy: 0.4991\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5934 - accuracy: 0.5020 - val_loss: 0.5960 - val_accuracy: 0.4981\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.5935 - accuracy: 0.5016 - val_loss: 0.5944 - val_accuracy: 0.4978\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5938 - accuracy: 0.5010 - val_loss: 0.5970 - val_accuracy: 0.4969\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.5938 - accuracy: 0.5011 - val_loss: 0.6038 - val_accuracy: 0.4951\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [72], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.487087607383728\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = hyperparameter_search(data, HYPERPARAMETERS, param_grid, result_file=SAVE_FILE)\n",
    "    send_email_notification(\"All done with hyperparameter search\", 'Done!')\n",
    "except Exception as e:\n",
    "    print(\"Error encountered:\", e)\n",
    "    send_email_notification(\"Hyperparameter search ran into an error\", 'Go fix it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE REFORMATTING\n",
    "# (RUN LATER WHEN THEY ARENT BEING WRITTEN TO)\n",
    "\n",
    "def reformat_hyperparameter_results(input_file, output_file):\n",
    "    # Read the original JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process and format the metrics\n",
    "    for key, value in data.items():\n",
    "        if \"metrics\" in value:\n",
    "            value[\"metrics\"] = format_metrics(value[\"metrics\"])\n",
    "\n",
    "    # Write the updated data to the new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Define input and output file names\n",
    "input_file = 'one_layerDNN_results.json'\n",
    "output_file = 'one_layerDNN_results.json'\n",
    "\n",
    "# Call the function to reformat the JSON data\n",
    "reformat_hyperparameter_results(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Read / Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 72)                7632      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 72)                288       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 72)                0         \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 73        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7993 (31.22 KB)\n",
      "Trainable params: 7849 (30.66 KB)\n",
      "Non-trainable params: 144 (576.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=72 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "batch_normalization  is normal keras bn layer\n",
      "q_activation         quantized_relu(6,0)\n",
      "dense_output         u=1 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) act=<function sigmoid at 0x7f2a5eac70d0>\n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 1 0]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6953 - accuracy: 0.5028 - val_loss: 0.6932 - val_accuracy: 0.5062\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5091 - val_loss: 0.6932 - val_accuracy: 0.5048\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6929 - val_accuracy: 0.5106\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5130 - val_loss: 0.6928 - val_accuracy: 0.5118\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5131 - val_loss: 0.6946 - val_accuracy: 0.5076\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5151 - val_loss: 0.6924 - val_accuracy: 0.5115\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5163 - val_loss: 0.6924 - val_accuracy: 0.5125\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5159 - val_loss: 0.6926 - val_accuracy: 0.5132\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6919 - val_accuracy: 0.5173\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5185 - val_loss: 0.6928 - val_accuracy: 0.5127\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5189 - val_loss: 0.6926 - val_accuracy: 0.5112\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5195 - val_loss: 0.6924 - val_accuracy: 0.5132\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5185 - val_loss: 0.6926 - val_accuracy: 0.5138\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5196 - val_loss: 0.6920 - val_accuracy: 0.5198\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5213 - val_loss: 0.6919 - val_accuracy: 0.5125\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5211 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5214 - val_loss: 0.6932 - val_accuracy: 0.5102\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5216 - val_loss: 0.6922 - val_accuracy: 0.5154\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5214 - val_loss: 0.6916 - val_accuracy: 0.5219\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5222 - val_loss: 0.6912 - val_accuracy: 0.5219\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5214 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5215 - val_loss: 0.6920 - val_accuracy: 0.5171\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5212 - val_loss: 0.6917 - val_accuracy: 0.5195\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5221 - val_loss: 0.6919 - val_accuracy: 0.5197\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5218 - val_loss: 0.6915 - val_accuracy: 0.5211\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5223\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5225 - val_loss: 0.6935 - val_accuracy: 0.5122\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5221 - val_loss: 0.6931 - val_accuracy: 0.5135\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5221 - val_loss: 0.6916 - val_accuracy: 0.5219\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5224 - val_loss: 0.6930 - val_accuracy: 0.5120\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5212 - val_loss: 0.6931 - val_accuracy: 0.5172\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5230 - val_loss: 0.6922 - val_accuracy: 0.5139\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5220 - val_loss: 0.6938 - val_accuracy: 0.5136\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5211 - val_loss: 0.6933 - val_accuracy: 0.5095\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5214 - val_loss: 0.6929 - val_accuracy: 0.5185\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5236 - val_loss: 0.6927 - val_accuracy: 0.5180\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5211 - val_loss: 0.6928 - val_accuracy: 0.5131\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5196 - val_loss: 0.6942 - val_accuracy: 0.5139\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5209 - val_loss: 0.6918 - val_accuracy: 0.5196\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6911 - accuracy: 0.5213 - val_loss: 0.6922 - val_accuracy: 0.5203\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization\n",
      "cannot prune layer q_activation\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6908 - accuracy: 0.5216 - val_loss: 0.6920 - val_accuracy: 0.5174\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5217 - val_loss: 0.6920 - val_accuracy: 0.5191\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5229 - val_loss: 0.6929 - val_accuracy: 0.5157\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6906 - accuracy: 0.5226 - val_loss: 0.6919 - val_accuracy: 0.5193\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5200\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5225 - val_loss: 0.6914 - val_accuracy: 0.5229\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5230 - val_loss: 0.6918 - val_accuracy: 0.5194\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5233 - val_loss: 0.6915 - val_accuracy: 0.5213\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5218 - val_loss: 0.6916 - val_accuracy: 0.5211\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6906 - accuracy: 0.5227 - val_loss: 0.6928 - val_accuracy: 0.5187\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6906 - accuracy: 0.5231 - val_loss: 0.6923 - val_accuracy: 0.5197\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5224 - val_loss: 0.6927 - val_accuracy: 0.5159\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5234 - val_loss: 0.6917 - val_accuracy: 0.5189\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5241 - val_loss: 0.6922 - val_accuracy: 0.5170\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5232 - val_loss: 0.6929 - val_accuracy: 0.5139\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5227 - val_loss: 0.6923 - val_accuracy: 0.5143\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5232 - val_loss: 0.6917 - val_accuracy: 0.5220\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6907 - accuracy: 0.5225 - val_loss: 0.6921 - val_accuracy: 0.5178\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5238 - val_loss: 0.6923 - val_accuracy: 0.5166\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5228 - val_loss: 0.6932 - val_accuracy: 0.5126\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5215 - val_loss: 0.6938 - val_accuracy: 0.5124\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5219 - val_loss: 0.6921 - val_accuracy: 0.5166\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6917 - accuracy: 0.5208 - val_loss: 0.6923 - val_accuracy: 0.5133\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6919 - accuracy: 0.5202 - val_loss: 0.6937 - val_accuracy: 0.5145\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 0.6918 - accuracy: 0.5209 - val_loss: 0.6930 - val_accuracy: 0.5167\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5213 - val_loss: 0.6920 - val_accuracy: 0.5196\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5218 - val_loss: 0.6938 - val_accuracy: 0.5199\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5211 - val_loss: 0.6945 - val_accuracy: 0.5155\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6911 - accuracy: 0.5225 - val_loss: 0.6917 - val_accuracy: 0.5217\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5237 - val_loss: 0.6925 - val_accuracy: 0.5144\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6911 - accuracy: 0.5222 - val_loss: 0.6919 - val_accuracy: 0.5214\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5232 - val_loss: 0.6965 - val_accuracy: 0.5094\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5231 - val_loss: 0.6933 - val_accuracy: 0.5145\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6923 - val_accuracy: 0.5160\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5237 - val_loss: 0.6921 - val_accuracy: 0.5164\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5224 - val_loss: 0.6918 - val_accuracy: 0.5203\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5236 - val_loss: 0.6937 - val_accuracy: 0.5221\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5238 - val_loss: 0.6938 - val_accuracy: 0.5191\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5244 - val_loss: 0.6924 - val_accuracy: 0.5148\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5245 - val_loss: 0.6935 - val_accuracy: 0.5197\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5236 - val_loss: 0.6919 - val_accuracy: 0.5227\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5243 - val_loss: 0.6916 - val_accuracy: 0.5170\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5245 - val_loss: 0.6915 - val_accuracy: 0.5217\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5245 - val_loss: 0.6921 - val_accuracy: 0.5174\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5248 - val_loss: 0.6923 - val_accuracy: 0.5213\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5245 - val_loss: 0.6920 - val_accuracy: 0.5216\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6902 - accuracy: 0.5249 - val_loss: 0.6935 - val_accuracy: 0.5189\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6902 - accuracy: 0.5254 - val_loss: 0.6930 - val_accuracy: 0.5170\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5247 - val_loss: 0.6938 - val_accuracy: 0.5184\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5241 - val_loss: 0.6916 - val_accuracy: 0.5214\n"
     ]
    }
   ],
   "source": [
    "model, train_metrics = train_model(data, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.54334795]\n",
      " [0.37937838]\n",
      " [0.5418938 ]\n",
      " [0.5472221 ]\n",
      " [0.52926344]\n",
      " [0.49853513]\n",
      " [0.5535053 ]\n",
      " [0.44408256]\n",
      " [0.47609246]\n",
      " [0.49804688]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m getTargetMetrics(test_results)\n\u001b[1;32m      3\u001b[0m displayPerformance(data, test_results, metrics)\n",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m (predictions_prob \u001b[38;5;241m>\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compute confusion matrix\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43mtarget_test_data_coded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, predicted_class)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate signal efficiency and background rejection\u001b[39;00m\n\u001b[1;32m     26\u001b[0m signal_efficiency \u001b[38;5;241m=\u001b[39m cm[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(cm[\u001b[38;5;241m1\u001b[39m, :])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_batch_norm(dense_layer, bn_layer):\n",
    "    W, b = dense_layer.get_weights()\n",
    "    gamma, beta, moving_mean, moving_var = bn_layer.get_weights()\n",
    "\n",
    "    epsilon = bn_layer.epsilon\n",
    "    std = np.sqrt(moving_var + epsilon)\n",
    "    new_W = gamma / std * W\n",
    "    new_b = gamma / std * (b - moving_mean) + beta\n",
    "\n",
    "    return new_W, new_b\n",
    "\n",
    "def create_folded_model(original_model): # Fold batch normalization layers into dense layers\n",
    "    inputs = original_model.input\n",
    "    x = inputs\n",
    "    new_layers = []\n",
    "\n",
    "    for layer in original_model.layers:\n",
    "        if isinstance(layer, QDense):\n",
    "            next_layer = new_layers[-1] if new_layers else inputs\n",
    "            if isinstance(next_layer, BatchNormalization):\n",
    "                # Fold the BatchNormalization into the previous Dense layer\n",
    "                new_W, new_b = fold_batch_norm(layer, next_layer)\n",
    "                x = QDense(layer.units, weights=[new_W, new_b], kernel_quantizer=layer.kernel_quantizer, bias_quantizer=layer.bias_quantizer)(x)\n",
    "                new_layers.pop()  # Remove the BatchNormalization layer\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        elif not isinstance(layer, BatchNormalization):\n",
    "            x = layer(x)\n",
    "        new_layers.append(x)\n",
    "\n",
    "    outputs = x\n",
    "\n",
    "    new_model = Model(inputs, outputs)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_9\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_timed_input (InputLayer)  multiple                     0         ['y_timed_input[0][0]']       \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 8)                    848       ['y_timed_input[1][0]']       \n",
      "                                                                                                  \n",
      " q_activation_11 (QActivati  (None, 8)                    0         ['dense1[2][0]']              \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 3)                    27        ['q_activation_11[3][0]']     \n",
      "                                                                                                  \n",
      " output_softmax (Activation  (None, 3)                    0         ['dense_output[2][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 875 (3.42 KB)\n",
      "Trainable params: 875 (3.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Create the folded model\n",
    "new_model = create_folded_model(model)\n",
    "\n",
    "# Verify the new model\n",
    "new_model.summary()\n",
    "\n",
    "new_model.save(f'./DNN_L1_S8_best_performance_quant_folded.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "new_model.save(f'./DNN_L3_S32_best_performance_folded.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the model when done\n",
    "model.save(f'./DNN_L3_S32_best_performance_quant.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 64)                6784      \n",
      "                                                                 \n",
      " q_activation_6 (QActivatio  (None, 64)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " q_activation_7 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense3 (QDense)             (None, 16)                528       \n",
      "                                                                 \n",
      " q_activation_8 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 16)                64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 3)                 51        \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 3)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9891 (38.64 KB)\n",
      "Trainable params: 9667 (37.76 KB)\n",
      "Non-trainable params: 224 (896.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load in a saved model from the h5 file\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "loaded_model = load_model('./DNN_L3_S64_best_performance.h5', custom_objects=co)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m(data, loaded_model)\n\u001b[1;32m      2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m getTargetMetrics(test_results)\n\u001b[1;32m      3\u001b[0m displayPerformance(data, test_results, metrics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, loaded_model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write input data to file\n",
    "with open('DNN_hp_input_features.dat', 'w') as file:\n",
    "    for row in input_train_data_combined:\n",
    "        line = ' '.join(map(str, row))  # Convert each number to string and join with space\n",
    "        file.write(line + '\\n')\n",
    "# Write target data to file\n",
    "with open('./DNN_hp_predictions_small.dat', 'w') as file:\n",
    "    for score in target_test_data_coded:\n",
    "        file.write(str(score[0]) + '\\n')  # Convert number to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set display by time slice\n",
    "def display_dataset(input_dataset, target_dataset, i, gif=False):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset[i]\n",
    "    target_datapoint = target_dataset[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if input_datapoint.shape != (20, 13, 21):\n",
    "        raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point\")\n",
    "    \n",
    "    for i in range(8):\n",
    "        for j in range(13):\n",
    "            for k in range(21):\n",
    "                print(input_datapoint[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"--------\", i)\n",
    "\n",
    "    # Extracting the transverse momentum (pt) from the target_data dataset\n",
    "    pt = target_datapoint[8]  # Assuming the 9th variable is at index 8\n",
    "\n",
    "    fig, ax_main = plt.subplots(figsize=(8, 6))\n",
    "    divider = make_axes_locatable(ax_main)\n",
    "\n",
    "    # Add row sum plot as an inset to the main plot\n",
    "    ax_row = divider.append_axes(\"right\", size=\"20%\", pad=0.4)\n",
    "\n",
    "    # Add column sum plot below the main plot\n",
    "    ax_column = divider.append_axes(\"bottom\", size=\"20%\", pad=0.5)\n",
    "\n",
    "    # Initial plot\n",
    "    im = ax_main.imshow(input_datapoint[0, :, :], cmap='plasma')\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[2]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "    # Function to update the animation\n",
    "    def update(t):\n",
    "        # Update main plot\n",
    "        data = input_datapoint[t, :, :]\n",
    "        im.set_data(data)\n",
    "\n",
    "        # Update row sum plot\n",
    "        ax_row.clear()\n",
    "        ax_row.barh(np.arange(data.shape[0]), np.sum(data, axis=1), color='red')\n",
    "        ax_row.set_ylim(0, data.shape[0]-1)\n",
    "        ax_row.set_yticks(np.arange(data.shape[0]))\n",
    "        ax_row.set_xlim(np.min(input_datapoint[:, :, :].sum(axis=2)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=2)) * 1.1)\n",
    "        ax_row.set_xlabel(\"Row Sum\")\n",
    "\n",
    "        # Update column sum plot\n",
    "        ax_column.clear()\n",
    "        ax_column.bar(np.arange(data.shape[1]), np.sum(data, axis=0), color='blue')\n",
    "        ax_column.set_xlim(0, data.shape[1]-1)\n",
    "        ax_column.set_xticks(np.arange(data.shape[1]))\n",
    "        ax_column.set_ylim(np.min(input_datapoint[:, :, :].sum(axis=1)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=1)) * 1.1)\n",
    "        ax_column.set_ylabel(\"Column Sum\")\n",
    "\n",
    "        # Update labels and grid\n",
    "        ax_main.set_xlabel(\"X Position\")\n",
    "        ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "        # Update title for the entire figure\n",
    "        fig.suptitle(f\"Timestep: {t+1} | Data Point: {i} | pt: {pt:.2f} GeV\")\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=20, repeat=True)\n",
    "\n",
    "    gif_path = f\"data_point.gif\"\n",
    "    if gif:\n",
    "        # Save the animation as a GIF\n",
    "        writer = PillowWriter(fps=1000 // FRAME_TIME)\n",
    "        ani.save(gif_path, writer=writer)\n",
    "\n",
    "    plt.close()\n",
    "    return display(HTML(ani.to_jshtml())), gif_path\n",
    "\n",
    "def display_model_IO(input_dataset_combined, target_dataset_coded, i):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset_combined[i]\n",
    "    target_datapoint = target_dataset_coded[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        if input_datapoint.shape != (NUM_TIME_SLICES * 13 + 1,):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 2\")\n",
    "        input_datapoint = input_datapoint[:-1].reshape(NUM_TIME_SLICES, 13)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        if input_datapoint[0].shape != (NUM_TIME_SLICES, 13):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 3\")\n",
    "        input_datapoint = input_datapoint[0]\n",
    "        \n",
    "    # Extracting the label from the target datapoint\n",
    "    if target_datapoint[0] == 1:\n",
    "        label = f\"High p_t (over {TEST_PT_THRESHOLD} GeV)\"\n",
    "    elif target_datapoint[1] == 1:\n",
    "        label = f\"low p_t and negative charge\"\n",
    "    elif target_datapoint[2] == 1:\n",
    "        label = f\"low p_t and positive charge\"\n",
    "    else: \n",
    "        raise ValueError(\"Invalid labelling for the target data point\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax_main = plt.subplots(figsize=(4,4))\n",
    "    print(input_datapoint.shape)\n",
    "    print(input_datapoint)\n",
    "    im = ax_main.imshow(input_datapoint.T, cmap='coolwarm_r', vmin=-1, vmax=1)\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[0]))\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Update labels and grid\n",
    "    ax_main.set_xlabel(\"Time Slice\")\n",
    "    ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "\n",
    "    # Update title for the entire figure\n",
    "    fig.suptitle(f\"Data Point: {i} | label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_model_IO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rand_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m FRAME_TIME \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m  \u001b[38;5;66;03m# milliseconds between frames\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdisplay_model_IO\u001b[49m(input_data_combined_example, target_data_coded_example, rand_idx)\n\u001b[1;32m      5\u001b[0m animation, gif \u001b[38;5;241m=\u001b[39m display_dataset(input_data_example, target_data_example, rand_idx, gif\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_model_IO' is not defined"
     ]
    }
   ],
   "source": [
    "# DATASET DISPLAY\n",
    "rand_idx = random.randint(0, 100)\n",
    "FRAME_TIME = 120  # milliseconds between frames\n",
    "display_model_IO(input_data_combined_example, target_data_coded_example, rand_idx)\n",
    "animation, gif = display_dataset(input_data_example, target_data_example, rand_idx, gif=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import os\n",
    "os.environ['XILINX_HLS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis_HLS/2023.1'\n",
    "os.environ['XILINX_VIVADO'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vivado/2023.1'\n",
    "os.environ['XILINX_VITIS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis/2023.1'\n",
    "os.environ['XILINX_AP_INCLUDE'] = '/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/HLS_arbitrary_Precision_Types/include'\n",
    "os.environ['PATH'] = os.environ['XILINX_HLS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VITIS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_AP_INCLUDE'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "strip_model = strip_pruning(qmodel_pruned)\n",
    "hls_config = hls4ml.utils.config_from_keras_model(strip_model , granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    print(Layer)\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "# If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "hls_config['LayerName']['output_sigmoid']['Strategy'] = 'Stable'\n",
    "hls_config['LayerName']['output_sigmoid']['Precision'] = 'ap_fixed<32,8,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vitis')\n",
    "\n",
    "cfg['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg['HLSConfig'] = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'cnn_debug/'\n",
    "cfg['XilinxPart'] = 'xcku040-ffva1156-2-e'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()\n",
    "#hls_model.profile()\n",
    "hls_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in qmodel_pruned.layers:\n",
    "    for i, w in enumerate(layer.weights):\n",
    "        try:\n",
    "            print(\"weight is\", w.numpy(), \"for layer number\", i)  # TF 2.x\n",
    "        except Exception:\n",
    "            print(\"weight is\", layer.get_weights()[i], \"for layer number\", i) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Smart_Pixel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
