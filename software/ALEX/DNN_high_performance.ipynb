{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 09:48:34.418901: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 09:48:34.506011: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 09:48:34.507009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 09:48:36.720569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.13.1\n",
      "keras version: 2.13.1\n",
      "qkeras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "# Machine Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import PReLU, Input, LSTM, Flatten, Concatenate, Dense, Conv2D, TimeDistributed, MaxPooling2D, ReLU, Dropout, BatchNormalization, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow_model_optimization\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "import keras\n",
    "print(\"keras version:\",keras.__version__)\n",
    "import qkeras\n",
    "from qkeras import QActivation, QDense, QConv2D, QBatchNormalization, QConv2DBatchnorm\n",
    "from qkeras import quantized_relu, quantized_bits\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "print(\"qkeras version:\",keras.__version__)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Display and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Data management\n",
    "import psutil\n",
    "import h5py\n",
    "# Memory management\n",
    "import gc\n",
    "# Notifications\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def send_email_notification(subject, content):\n",
    "    sender_email = os.getenv('EMAIL_USER')\n",
    "    receiver_email = \"alexander.j.yue@gmail.com\"\n",
    "    password = os.getenv('EMAIL_PASS')\n",
    "\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = receiver_email\n",
    "    message[\"Subject\"] = subject\n",
    "    body = content\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message.as_string())\n",
    "\n",
    "# Memory monitoring functions\n",
    "def print_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory usage percentage: {memory.percent}%\")\n",
    "\n",
    "def print_cpu_usage():\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pixel cluster to transverse momentum dataset into the input_data and target_data\n",
    "def load_combine_shuffle_data_optimized_hdf5():\n",
    "    # Load the dataset from Kenny's computer\n",
    "    with h5py.File('/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/fl32_data_v3.hdf5', 'r') as h5f:\n",
    "        combined_input = None\n",
    "        combined_target = None\n",
    "\n",
    "        for data_type in ['sig', 'bkg']:\n",
    "            # Construct dataset names\n",
    "            input_dataset_name = f'{data_type}_input'\n",
    "            target_dataset_name = f'{data_type}_target'\n",
    "\n",
    "            # Check if the dataset exists and load data sequentially\n",
    "            if input_dataset_name in h5f and target_dataset_name in h5f:\n",
    "                input_data = h5f[input_dataset_name][:].astype(np.float32)\n",
    "                target_data = h5f[target_dataset_name][:].astype(np.float32)\n",
    "\n",
    "                if combined_input is None:\n",
    "                    combined_input = input_data\n",
    "                    combined_target = target_data\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "                else:\n",
    "                    print_memory_usage()\n",
    "                    combined_input = np.vstack((combined_input, input_data))\n",
    "                    combined_target = np.vstack((combined_target, target_data))\n",
    "                    # Free memory of the loaded data\n",
    "                    del input_data, target_data\n",
    "                    gc.collect()\n",
    "\n",
    "            else:\n",
    "                print(f\"Dataset {input_dataset_name} or {target_dataset_name} not found.\")\n",
    "\n",
    "        # Shuffling\n",
    "        indices = np.arange(combined_input.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        combined_input = combined_input[indices]\n",
    "        combined_target = combined_target[indices]\n",
    "\n",
    "        return combined_input, combined_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load dataset into memory\n",
    "    input_data, target_data = load_combine_shuffle_data_optimized_hdf5()\n",
    "    # Format the dataset into a 20x13x21 tensor (time, y, x)\n",
    "    input_data = input_data.reshape(input_data.shape[0],20,13,21)\n",
    "    return input_data, target_data\n",
    "\n",
    "def process_dataset(input_data, target_data, hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    TRAIN_PT_THRESHOLD = hyperparams[\"TRAIN_PT_THRESHOLD\"]\n",
    "    TEST_PT_THRESHOLD = hyperparams[\"TEST_PT_THRESHOLD\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "\n",
    "    # Split 80% of data into training data, 10% for validation data and 10% for testing data\n",
    "    input_train_data, input_temp, target_train_data, target_temp = \\\n",
    "    train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "    del input_data\n",
    "    del target_data\n",
    "    gc.collect()\n",
    "    input_validate_data, input_test_data, target_validate_data, target_test_data = \\\n",
    "    train_test_split(input_temp, target_temp, test_size=0.5, random_state=42)\n",
    "    del input_temp\n",
    "    del target_temp\n",
    "    gc.collect()\n",
    "\n",
    "    # Save some data for displaying\n",
    "    input_data_example = input_test_data[0:100,:]\n",
    "    target_data_example = target_test_data[0:100,:]\n",
    "\n",
    "    # Fit the scalers on the training data to it all scales the exact same\n",
    "    input_scaler = StandardScaler()\n",
    "    input_scaler.fit(input_train_data[:, :NUM_TIME_SLICES, :, :].reshape(-1,8*13))\n",
    "    y0_scaler = StandardScaler()\n",
    "    y0_scaler.fit(target_train_data[:,7].reshape(-1, 1))\n",
    "\n",
    "    # Process the data into input shape and labels for training\n",
    "    def process_data(input_data, target_data, pt_threshold):\n",
    "        if input_data.shape[1:] == (20, 13, 21) and target_data.shape[1:] == (13, ):\n",
    "\n",
    "            # Truncate down to first time slices\n",
    "            input_data = input_data[:, :NUM_TIME_SLICES, :, :]\n",
    "\n",
    "            # sum over the x axis to turn the input data into a 2D NUM_TIME_SLICES x 13 tensor (time, y)\n",
    "            input_data = np.sum(input_data, axis=3)\n",
    "\n",
    "            if OUTPUT == \"SOFTMAX\" or OUTPUT == \"LINEAR\":\n",
    "                # Encode the target data into one_hot encoding\n",
    "                one_hot = np.zeros((target_data.shape[0], 3))\n",
    "                # Assign 1 for p_t > pt_threshold in GeV, for low p_t put 1 in slot 2 for negative and a 1 in slot 3 for positive\n",
    "                one_hot[np.abs(target_data[:, 8]) >= pt_threshold, 0] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] > 0), 1] = 1\n",
    "                one_hot[(np.abs(target_data[:, 8]) < pt_threshold) & (target_data[:, 8] < 0), 2] = 1\n",
    "                label_data = one_hot\n",
    "            # elif OUTPUT == \"ARGMAX\": # DOES NOT WORK \n",
    "            #     label_data = np.argmax(one_hot, axis=1).astype(np.int64)\n",
    "            #     print(\"one hot is \", one_hot)\n",
    "            elif OUTPUT == \"SINGLE\":\n",
    "            # Binary labels for 0th category\n",
    "                label_data = (np.abs(target_data[:, 8]) >= pt_threshold).astype(np.int64)\n",
    "\n",
    "            # Flatten the input data\n",
    "            input_data = input_data.reshape(-1,NUM_TIME_SLICES*13)\n",
    "\n",
    "            # Normalize the input data to have mean| 0 and std 1\n",
    "            \n",
    "            input_data = input_scaler.transform(input_data)\n",
    "            # # Replace all values < 1 with 1 so they log to 0\n",
    "            # input_data = np.where(np.abs(input_data) < 1.0, 1.0, input_data)\n",
    "            # # Apply logarithmic scaling\n",
    "            # input_data = np.log(np.abs(input_data)) * np.sign(input_data)\n",
    "            # # Min-max normalization (global)\n",
    "            # min_val = np.min(input_data)\n",
    "            # max_val = np.max(input_data)\n",
    "            # print(f\"max of log of data is {max_val} and min is {min_val}\")\n",
    "            # input_data = (input_data) / np.max([max_val,min_val])\n",
    "\n",
    "            # Get the y_0 data\n",
    "            y0_data = target_data[:,7].reshape(-1, 1)\n",
    "            y0_data = y0_scaler.transform(y0_data)\n",
    "            # Combine with input data\n",
    "            if (MODEL_TYPE == \"DNN\"):\n",
    "                # For DNN we concatenate in the y_0 data\n",
    "                input_data_combined = np.hstack((input_data, y0_data))\n",
    "\n",
    "                \n",
    "            elif (MODEL_TYPE == \"CNN\"):\n",
    "                # Reshape data into a matrix for the convolutions\n",
    "                input_data= input_data.reshape(-1, NUM_TIME_SLICES, 13)\n",
    "                # Package with the y_0 data to be added later\n",
    "                input_data_combined = [input_data, y0_data]\n",
    "\n",
    "            return input_data_combined, label_data\n",
    "        else:\n",
    "            raise ValueError(\"Wrong array shape!\")\n",
    "\n",
    "    # Apply data processing to our datasets\n",
    "    input_train_data_combined, target_train_data_coded = process_data(input_train_data, target_train_data, TRAIN_PT_THRESHOLD)\n",
    "    input_validate_data_combined, target_validate_data_coded = process_data(input_validate_data, target_validate_data, TRAIN_PT_THRESHOLD)\n",
    "    input_test_data_combined, target_test_data_coded = process_data(input_test_data, target_test_data, TEST_PT_THRESHOLD)\n",
    "\n",
    "    # Save some data for displaying\n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        input_data_combined_example = input_test_data_combined[0:100,:]\n",
    "        if OUTPUT == \"ARGMAX\" or OUTPUT == \"SINGLE\":\n",
    "            target_data_coded_example = target_test_data_coded[0:100]\n",
    "        else:\n",
    "            target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    elif MODEL_TYPE == \"CNN\":\n",
    "        input_data_combined_example = np.hstack((input_test_data_combined[0][0:100,:].reshape(100, -1), input_test_data_combined[1][0:100,:]))\n",
    "        target_data_coded_example = target_test_data_coded[0:100,:]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    processed_dataset = {\n",
    "        \"input_train_data_combined\": input_train_data_combined,\n",
    "        \"target_train_data_coded\": target_train_data_coded,\n",
    "        \"input_validate_data_combined\": input_validate_data_combined,\n",
    "        \"target_validate_data_coded\": target_validate_data_coded,\n",
    "        \"input_test_data_combined\": input_test_data_combined,\n",
    "        \"target_test_data_coded\": target_test_data_coded,\n",
    "\n",
    "        \"input_data_example\": input_data_example,\n",
    "        \"target_data_example\": target_data_example,\n",
    "        \"input_data_combined_example\": input_data_combined_example,\n",
    "        \"target_data_coded_example\": target_data_coded_example,\n",
    "    }\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class ArgmaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ArgmaxLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(tf.argmax(inputs, axis=-1), dtype=tf.int64)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ArgmaxLayer, self).get_config()\n",
    "        return config\n",
    "    \n",
    "def qDNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    DNN_LAYERS = hyperparams[\"DNN_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    y_timed_input = Input(shape=(NUM_TIME_SLICES*13 + 1,), name='y_timed_input')\n",
    "    layer = y_timed_input\n",
    "    \n",
    "    for i, size in enumerate(DNN_LAYERS):\n",
    "        layer = QDense(size, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name=f'dense{i+1}')(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS))(layer)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "        output = ArgmaxLayer(name='output_argmax')(output)\n",
    "        print(f\"Argmax output dtype: {output.dtype}\")\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        output = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS), \n",
    "                bias_quantizer=quantized_bits(BIAS_BITS), \n",
    "                # activation='sigmoid', \n",
    "                name='dense_output')(layer)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported output type\")\n",
    "   \n",
    "    model = Model(inputs=y_timed_input, outputs=output)\n",
    "\n",
    "    if OUTPUT == \"SOFTMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"ARGMAX\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=[Precision()])\n",
    "    elif OUTPUT == \"LINEAR\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "              loss='mse', \n",
    "              metrics=[Precision()])\n",
    "    elif OUTPUT == \"SINGLE\":\n",
    "        model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def qCNNmodel(hyperparams):\n",
    "    NUM_TIME_SLICES = hyperparams[\"NUM_TIME_SLICES\"]\n",
    "    CONV_LAYER_DEPTHS = hyperparams[\"CONV_LAYER_DEPTHS\"]\n",
    "    CONV_LAYER_KERNELS = hyperparams[\"CONV_LAYER_KERNELS\"]\n",
    "    CONV_LAYER_STRIDES = hyperparams[\"CONV_LAYER_STRIDES\"]\n",
    "    MAX_POOLING_SIZE = hyperparams[\"MAX_POOLING_SIZE\"]\n",
    "    FLATTENED_LAYERS = hyperparams[\"FLATTENED_LAYERS\"]\n",
    "    WEIGHTS_BITS = hyperparams[\"WEIGHTS_BITS\"]\n",
    "    BIAS_BITS = hyperparams[\"BIAS_BITS\"]\n",
    "    INTEGER_BITS = hyperparams[\"INTEGER_BITS\"]\n",
    "    ACTIVATION_BITS = hyperparams[\"ACTIVATION_BITS\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "\n",
    "    y_profile_input = Input(shape=(NUM_TIME_SLICES, 13, 1), name='y_profile_input')  # Adjust the shape based on your input\n",
    "    layer = y_profile_input\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(len(CONV_LAYER_DEPTHS)):\n",
    "        layer = QConv2D(\n",
    "        CONV_LAYER_DEPTHS[i],\n",
    "        kernel_size=CONV_LAYER_KERNELS[i],\n",
    "        strides=CONV_LAYER_STRIDES[i],\n",
    "        kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "        padding='same',\n",
    "        use_bias=True,\n",
    "        name=f'conv{i+1}'\n",
    "        )(layer)\n",
    "        layer = QActivation(quantized_relu(ACTIVATION_BITS), name=f'relu{i+1}')(layer)\n",
    "        layer = MaxPooling2D(pool_size=MAX_POOLING_SIZE, name=f'maxpool{i+1}')(layer)\n",
    "\n",
    "    # Flatten the output to feed into a dense layer\n",
    "    layer = Flatten(name='flattened')(layer)\n",
    "\n",
    "    # Flatten and concatenate with y0 input\n",
    "    y0_input = Input(shape=(1,), name='y0_input')\n",
    "    layer = Concatenate(name='concat')([layer, y0_input])\n",
    "\n",
    "    # Post-flattening dense layers\n",
    "    for i in range(len(FLATTENED_LAYERS)):\n",
    "        layer = QDense(FLATTENED_LAYERS[i], kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0),\n",
    "                    bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name=f'dense{i+1}')(layer)\n",
    "        layer = QActivation(quantized_relu(10), name=f'relu{len(CONV_LAYER_DEPTHS)+i+1}')(layer)\n",
    "\n",
    "    # Output layer (adjust based on your classification problem)\n",
    "    output = QDense(3, kernel_quantizer=quantized_bits(WEIGHTS_BITS), bias_quantizer=quantized_bits(BIAS_BITS), name='dense_output')(layer)\n",
    "    output = Activation(\"softmax\", name='output_softmax')(output)\n",
    "    # layer = QDense(1, kernel_quantizer=quantized_bits(WEIGHTS_BITS,INTEGER_BITS,alpha=1.0), \n",
    "    #                bias_quantizer=quantized_bits(BIAS_BITS,INTEGER_BITS,alpha=1.0), name='output_dense')(layer)\n",
    "    # output = Activation(\"sigmoid\", name='output_sigmoid')(layer)\n",
    "\n",
    "    model = Model(inputs=[y_profile_input, y0_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy']) # loss='binary_crossentropy'\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Pruning the model\n",
    "def pruneFunction(layer, train_data_size, hyperparams):\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    FINAL_SPARSITY = hyperparams[\"FINAL_SPARSITY\"]\n",
    "    PRUNE_START_EPOCH = hyperparams[\"PRUNE_START_EPOCH\"]\n",
    "    NUM_PRUNE_EPOCHS = hyperparams[\"NUM_PRUNE_EPOCHS\"]\n",
    "\n",
    "    steps_per_epoch = train_data_size // BATCH_SIZE #input_train_data_combined.shape[0]\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.0,\n",
    "            final_sparsity=FINAL_SPARSITY,\n",
    "            begin_step=steps_per_epoch * PRUNE_START_EPOCH,\n",
    "            end_step=steps_per_epoch * (PRUNE_START_EPOCH + NUM_PRUNE_EPOCHS),\n",
    "            frequency=steps_per_epoch # prune after every epoch\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "    if isinstance(layer, QDense):\n",
    "        if layer.name != 'output_softmax' and layer.name != 'dense2':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        elif layer.name != 'output_softmax' and layer.name != 'dense1':\n",
    "            print(f\"pruning layer {layer.name}\")\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "        else:\n",
    "            print(f\"cannot prune layer {layer.name}\")\n",
    "            return layer\n",
    "\n",
    "    else:\n",
    "        print(f\"cannot prune layer {layer.name}\")\n",
    "        return layer\n",
    "    \n",
    "def pruneFunctionWrapper(train_data_size, hyperparams):\n",
    "    def wrapper(layer):\n",
    "        return pruneFunction(layer, train_data_size, hyperparams)\n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "# Function to calculate sparsity\n",
    "def calculate_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights = layer.get_weights()[0]\n",
    "            total_params += weights.size\n",
    "            zero_params += np.sum(weights == 0)\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, hyperparams):\n",
    "    input_train_data_combined = data[\"input_train_data_combined\"]\n",
    "    target_train_data_coded = data[\"target_train_data_coded\"]\n",
    "    input_validate_data_combined = data[\"input_validate_data_combined\"]\n",
    "    target_validate_data_coded = data[\"target_validate_data_coded\"]\n",
    "\n",
    "    MODEL_TYPE = hyperparams[\"MODEL_TYPE\"]\n",
    "    PATIENCE = hyperparams[\"PATIENCE\"]\n",
    "    EPOCHS = hyperparams[\"EPOCHS\"]\n",
    "    BATCH_SIZE = hyperparams[\"BATCH_SIZE\"]\n",
    "    LEARNING_RATE = hyperparams[\"LEARNING_RATE\"]\n",
    "    POST_PRUNE_EPOCHS = hyperparams[\"POST_PRUNE_EPOCHS\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define the model\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        model = qDNNmodel(hyperparams)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        model = qCNNmodel(hyperparams)\n",
    "    else:\n",
    "        raise ValueError(\"Not a supported model type\")\n",
    "\n",
    "    model.summary()\n",
    "    print_qmodel_summary(model)\n",
    "    print(f\"Initial Sparsity: {calculate_sparsity(model) * 100:.2f}%\")\n",
    "\n",
    "    train_metrics = {}\n",
    "\n",
    "    # Train the model\n",
    "    earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=PATIENCE, restore_best_weights=True)\n",
    "    print(\"shape 12323 is \", target_train_data_coded.shape, \"data is like\", target_validate_data_coded[1:5])\n",
    "    history = model.fit(\n",
    "        input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "        validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[earlyStop_callback]\n",
    "    )\n",
    "    # Best at this step val_loss 0.7085\n",
    "    train_metrics[\"val_loss\"] = history.history['val_loss'][-1]\n",
    "\n",
    "    # Prune the DNN model \n",
    "    if MODEL_TYPE == \"DNN\":\n",
    "        model_pruned = keras.models.clone_model(model, clone_function=pruneFunctionWrapper(input_train_data_combined.shape[0], hyperparams))\n",
    "        model_pruned.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Re-train the pruned model\n",
    "        history = model_pruned.fit(\n",
    "            input_train_data_combined, target_train_data_coded,  # Training data and labels\n",
    "            validation_data=(input_validate_data_combined, target_validate_data_coded),  # Validation data\n",
    "            epochs=POST_PRUNE_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks = [pruning_callbacks.UpdatePruningStep()]\n",
    "        ) \n",
    "        # best at this step val_loss 0.4314\n",
    "\n",
    "        model = strip_pruning(model_pruned)\n",
    "        # train_metrics[\"pruned_sparsity\"] = calculate_sparsity(model)\n",
    "\n",
    "    try:\n",
    "        train_metrics[\"pruned_val_loss\"] = history.history['val_loss'][-1]\n",
    "    except:\n",
    "        print(\"Error: no post-pruning val_loss found\")\n",
    "        train_metrics[\"pruned_val_loss\"] = train_metrics[\"val_loss\"]\n",
    "\n",
    "    return model, train_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data, model, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    input_test_data_combined = data[\"input_test_data_combined\"]\n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "\n",
    "    \n",
    "    # Test the model at threshold 0.5\n",
    "    predictions = model.predict(input_test_data_combined)\n",
    "    print(predictions[:10, :])\n",
    "    predictions_prob = predictions[:,0]\n",
    "    predictions_labels = (predictions_prob >= 0.5).astype(int).flatten()\n",
    "\n",
    "    # Test the model at different thresholds\n",
    "    thresholds = np.linspace(0.0, 1.0, 1000)\n",
    "    signal_efficiencies = []\n",
    "    background_rejections = []\n",
    "    max_sum_se = 0\n",
    "    max_sum_br = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # predicted_class = ((predictions_prob[:, 0] + threshold > predictions_prob[:, 1]) & (predictions_prob[:, 0] + threshold > predictions_prob[:, 2])).astype(int)\n",
    "        predicted_class = (predictions_prob > threshold).astype(int)\n",
    "        # Compute confusion matrix\n",
    "        if OUTPUT == \"SINGLE\":\n",
    "            cm = confusion_matrix(target_test_data_coded[:], predicted_class)\n",
    "        else:\n",
    "            cm = confusion_matrix(target_test_data_coded[:, 0], predicted_class)\n",
    "\n",
    "        # Calculate signal efficiency and background rejection\n",
    "        signal_efficiency = cm[1, 1] / np.sum(cm[1, :])\n",
    "        background_rejection = cm[0, 0] / np.sum(cm[0, :])\n",
    "\n",
    "        # Store metrics\n",
    "        signal_efficiencies.append(signal_efficiency)\n",
    "        background_rejections.append(background_rejection)\n",
    "\n",
    "        # get maximum added score\n",
    "        if signal_efficiency + background_rejection > max_sum_se + max_sum_br:\n",
    "            max_sum_se = signal_efficiency\n",
    "            max_sum_br = background_rejection\n",
    "    \n",
    "    test_results = {\n",
    "        \"predictions_prob\": predictions_prob,\n",
    "        \"predictions_labels\": predictions_labels,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"signal_efficiencies\": signal_efficiencies,\n",
    "        \"background_rejections\": background_rejections,\n",
    "        \"max_sum_se\": max_sum_se,\n",
    "        \"max_sum_br\": max_sum_br,\n",
    "    }\n",
    "\n",
    "    return test_results\n",
    "\n",
    "def ShowConfusionMatrix(data, test_results, hyperparams):\n",
    "    OUTPUT = hyperparams[\"OUTPUT\"]\n",
    "    \n",
    "    target_test_data_coded = data[\"target_test_data_coded\"]\n",
    "    predictions_labels = test_results[\"predictions_labels\"]\n",
    "\n",
    "    if OUTPUT == \"SINGLE\":\n",
    "        cm = confusion_matrix(target_test_data_coded[:], predictions_labels)\n",
    "    else:\n",
    "        cm = confusion_matrix(target_test_data_coded[:, 0], predictions_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='YlGnBu')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def showMetricsByThreshold(test_results):\n",
    "    thresholds = test_results[\"thresholds\"]\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, signal_efficiencies, label='Signal Efficiency')\n",
    "    plt.plot(thresholds, background_rejections, label='Background Rejection')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Effect of Threshold on Signal Efficiency and Background Rejection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def showEfficiencyVSRejection(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(signal_efficiencies, background_rejections, marker='o')\n",
    "    plt.xlabel('Signal Efficiency')\n",
    "    plt.ylabel('Background Rejection')\n",
    "    plt.title('Background Rejection vs. Signal Efficiency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def find_closest(sorted_array, value):\n",
    "    # Ensure the array is a NumPy array\n",
    "    sorted_array = np.array(sorted_array)\n",
    "    # Compute the absolute difference\n",
    "    abs_diff = np.abs(sorted_array - value)\n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = np.argmin(abs_diff)\n",
    "    return closest_index\n",
    "\n",
    "def getTargetMetrics(test_results):\n",
    "    signal_efficiencies = test_results[\"signal_efficiencies\"]\n",
    "    background_rejections = test_results[\"background_rejections\"]\n",
    "\n",
    "    target_efficiencies = [0.873, 0.90, 0.93, 0.96, 0.98, 0.99, 0.995, 0.999]\n",
    "    metrics = []\n",
    "    for target in target_efficiencies:\n",
    "        index = find_closest(signal_efficiencies, target)\n",
    "        metrics.append((signal_efficiencies[index], background_rejections[index]))\n",
    "        # print(f\"Signal Efficiency: {signal_efficiencies[index]*100:.1f}%,\",f\"Background Rejections: {background_rejections[index]*100:.1f}%\")\n",
    "    return metrics\n",
    "\n",
    "def displayPerformance(data, test_results, metrics, hyperparams):\n",
    "    ShowConfusionMatrix(data, test_results, hyperparams)\n",
    "    showMetricsByThreshold(test_results)\n",
    "    showEfficiencyVSRejection(test_results)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(metrics):\n",
    "    # Convert metrics list of tuples to a formatted string\n",
    "    return \", \".join([f\"({m1:.2f}, {m2:.2f})\" for m1, m2 in metrics])\n",
    "\n",
    "def hyperparameter_search(data, base_hyperparams, param_grid, result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file if it exists\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        hyperparams = dict(zip(keys, v))\n",
    "        # Update base hyperparameters with the current set\n",
    "        current_hyperparams = base_hyperparams.copy()\n",
    "        current_hyperparams.update(hyperparams)\n",
    "\n",
    "        # Convert hyperparameters to a string for use as a dictionary key\n",
    "        hyperparams_str = json.dumps(current_hyperparams, sort_keys=True)\n",
    "\n",
    "        # Check if these hyperparameters have been tried before\n",
    "        if hyperparams_str in all_results:\n",
    "            print(f\"Skipping already tested hyperparameters: {current_hyperparams}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Testing hyperparameters: {current_hyperparams}\")\n",
    "\n",
    "        # Train the model\n",
    "        model, train_metrics = train_model(data, current_hyperparams)\n",
    "\n",
    "        # Test the model\n",
    "        test_results = test_model(data, model, current_hyperparams)\n",
    "        metrics = format_metrics(getTargetMetrics(test_results))\n",
    "        \n",
    "        test_scores = {\n",
    "            \"max_sum_se\": test_results[\"max_sum_se\"],\n",
    "            \"max_sum_br\": test_results[\"max_sum_br\"],\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        # Add all keys and values from train_metrics into test_scores\n",
    "        test_scores.update(train_metrics)\n",
    "\n",
    "        # Save the results to the file\n",
    "        all_results[hyperparams_str] = test_scores\n",
    "        with open(result_file, 'w') as file:\n",
    "            json.dump(all_results, file, indent=4)\n",
    "\n",
    "\n",
    "        # If new best found, email alex\n",
    "        if test_scores[\"pruned_val_loss\"] < find_min_pruned_val_loss(result_file):\n",
    "\n",
    "            # Format metrics for the email\n",
    "            metrics_str = \", \".join([f\"({m1:.3f}, {m2:.3f})\" for m1, m2 in test_scores[\"metrics\"]])\n",
    "\n",
    "            # email results\n",
    "            model_name = \"undefined\"\n",
    "            if (current_hyperparams[\"MODEL_TYPE\"] == \"DNN\"):\n",
    "                model_name = \"Dean\"\n",
    "            elif (current_hyperparams[\"MODEL_TYPE\"] == \"CNN\"):\n",
    "                model_name = \"Connor\"\n",
    "            send_email_notification(\"ML Training Report\", \n",
    "                f' \\\n",
    "                Your model {model_name} has finished training. \\\n",
    "                He got a grade of {test_scores[\"max_sum_se\"]*100:.1f}% in SE and {test_scores[\"max_sum_br\"]*100:.1f}% in BR. \\\n",
    "                New lowest validated loss: {test_scores[\"pruned_val_loss\"]} \\n \\\n",
    "                All metrics: {metrics_str} \\n \\\n",
    "                Hyperparams: {hyperparams_str} \\\n",
    "                ')\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def find_min_pruned_val_loss(result_file='hyperparameter_results.json'):\n",
    "    # Load existing results from file\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as file:\n",
    "            all_results = json.load(file)\n",
    "    else:\n",
    "        print(f\"No results found in {result_file}\")\n",
    "        return None\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    min_hyperparams = None\n",
    "\n",
    "    # Iterate through the results to find the minimum pruned_val_loss\n",
    "    for hyperparams_str, results in all_results.items():\n",
    "        if \"pruned_val_loss\" in results:\n",
    "            pruned_val_loss = results[\"pruned_val_loss\"]\n",
    "            if pruned_val_loss < min_loss:\n",
    "                min_loss = pruned_val_loss\n",
    "                min_hyperparams = hyperparams_str\n",
    "\n",
    "    # Print the hyperparameters with the minimum pruned_val_loss\n",
    "    if min_hyperparams is not None:\n",
    "        print(f\"Hyperparameters with minimum pruned_val_loss: {min_hyperparams}\")\n",
    "        print(f\"Minimum pruned_val_loss: {min_loss}\")\n",
    "    else:\n",
    "        print(\"No entry with pruned_val_loss found\")\n",
    "\n",
    "    return min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6512852311134338"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    # Model Type\n",
    "    \"MODEL_TYPE\": \"DNN\",  # DNN or CNN\n",
    "    # Input format\n",
    "    \"NUM_TIME_SLICES\": 8,\n",
    "    \"TRAIN_PT_THRESHOLD\": 2,  # in GeV\n",
    "    \"TEST_PT_THRESHOLD\": 2,  # in GeV\n",
    "    # DNN model mormat\n",
    "    \"DNN_LAYERS\": [24, 12],\n",
    "    # CNN model format\n",
    "    \"CONV_LAYER_DEPTHS\": [4, 7],\n",
    "    \"CONV_LAYER_KERNELS\": [(3, 3), (3, 3)],\n",
    "    \"CONV_LAYER_STRIDES\": [(1, 1), (1, 1)],\n",
    "    \"FLATTENED_LAYERS\": [7],\n",
    "    \"MAX_POOLING_SIZE\": (2, 2),\n",
    "    # Output function\n",
    "    \"OUTPUT\": \"SINGLE\", # SOFTMAX or ARGMAX (not working) or LINEAR or SINGLE\n",
    "    # Model quantization\n",
    "    \"WEIGHTS_BITS\": 4,\n",
    "    \"BIAS_BITS\": 4,\n",
    "    \"ACTIVATION_BITS\": 6,\n",
    "    \"INTEGER_BITS\": 2,\n",
    "    # Training\n",
    "    \"LEARNING_RATE\": 0.002,\n",
    "    \"BATCH_SIZE\": 1024,  # Number of samples per gradient update\n",
    "    \"EPOCHS\": 150,  # Number of epochs to train\n",
    "    \"PATIENCE\": 20,  # Stop after this number of epochs without improvement\n",
    "    # Pruning\n",
    "    \"PRUNE_START_EPOCH\": 0,  # Number of epochs before pruning\n",
    "    \"NUM_PRUNE_EPOCHS\": 10,\n",
    "    \"FINAL_SPARSITY\": 0.0,\n",
    "    \"POST_PRUNE_EPOCHS\": 50,\n",
    "}\n",
    "\n",
    "SAVE_FILE = \"tiny_single_output\"+HYPERPARAMETERS[\"MODEL_TYPE\"]+\"_results.json\"\n",
    "\n",
    "param_grid = {\n",
    "    \"DNN_LAYERS\": [[32], [8], [16, 8]],\n",
    "    \"FINAL_SPARSITY\": [0.0, 0.3, 0.5], \n",
    "    \"LEARNING_RATE\": [0.005, 0.001, 0.0005],\n",
    "}\n",
    "\n",
    "find_min_pruned_val_loss(result_file=(SAVE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 376.23 GB\n",
      "Available memory: 305.24 GB\n",
      "Used memory: 59.54 GB\n",
      "Memory usage percentage: 18.9%\n",
      "Total memory: 376.23 GB\n",
      "Available memory: 293.81 GB\n",
      "Used memory: 70.96 GB\n",
      "Memory usage percentage: 21.9%\n"
     ]
    }
   ],
   "source": [
    "input_data, target_data = load_dataset()\n",
    "data = process_dataset(input_data, target_data, HYPERPARAMETERS) # Depends only on: NUM_TIME_SLICES MODEL_TYPE TRAIN_PT_THRESHOLD TEST_PT_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32)                128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization  is normal keras bn layer\n",
      "q_activation         quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 1.0083 - accuracy: 0.5021 - val_loss: 0.7014 - val_accuracy: 0.5033\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.5056 - val_loss: 0.6964 - val_accuracy: 0.5037\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5069 - val_loss: 0.6943 - val_accuracy: 0.5083\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6937 - accuracy: 0.5093 - val_loss: 0.6949 - val_accuracy: 0.5075\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6951 - val_accuracy: 0.5069\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5099 - val_loss: 0.6937 - val_accuracy: 0.5087\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6933 - val_accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5097 - val_loss: 0.6932 - val_accuracy: 0.5146\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6945 - val_accuracy: 0.5119\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5131 - val_loss: 0.6941 - val_accuracy: 0.5118\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5121 - val_loss: 0.6959 - val_accuracy: 0.5010\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5126 - val_loss: 0.6931 - val_accuracy: 0.5139\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6925 - val_accuracy: 0.5127\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5131\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.5128\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6925 - val_accuracy: 0.5129\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6925 - val_accuracy: 0.5143\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5154 - val_loss: 0.6932 - val_accuracy: 0.5084\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6925 - val_accuracy: 0.5110\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6924 - val_accuracy: 0.5176\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5148 - val_loss: 0.6932 - val_accuracy: 0.5107\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5149 - val_loss: 0.6924 - val_accuracy: 0.5170\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5153 - val_loss: 0.6922 - val_accuracy: 0.5136\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5159 - val_loss: 0.6927 - val_accuracy: 0.5154\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5156 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5156 - val_loss: 0.6926 - val_accuracy: 0.5083\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5167 - val_loss: 0.6920 - val_accuracy: 0.5154\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5172 - val_loss: 0.6927 - val_accuracy: 0.5147\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6936 - val_accuracy: 0.5132\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5172 - val_loss: 0.6935 - val_accuracy: 0.5107\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5162 - val_loss: 0.6918 - val_accuracy: 0.5176\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5181 - val_loss: 0.6919 - val_accuracy: 0.5202\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5163 - val_loss: 0.6918 - val_accuracy: 0.5180\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5188 - val_loss: 0.6935 - val_accuracy: 0.5122\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5171 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5187 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5165 - val_loss: 0.6939 - val_accuracy: 0.5147\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5169 - val_loss: 0.6923 - val_accuracy: 0.5147\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5192 - val_loss: 0.6926 - val_accuracy: 0.5083\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6926 - val_accuracy: 0.5091\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5194 - val_loss: 0.6920 - val_accuracy: 0.5117\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5181 - val_loss: 0.6915 - val_accuracy: 0.5153\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5193 - val_loss: 0.6915 - val_accuracy: 0.5198\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5188 - val_loss: 0.6917 - val_accuracy: 0.5181\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5183 - val_loss: 0.6927 - val_accuracy: 0.5098\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5188 - val_loss: 0.6912 - val_accuracy: 0.5176\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5196 - val_loss: 0.6918 - val_accuracy: 0.5174\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5189 - val_loss: 0.6919 - val_accuracy: 0.5187\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5203 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5209 - val_loss: 0.6919 - val_accuracy: 0.5177\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5164\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5182 - val_loss: 0.7033 - val_accuracy: 0.5050\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5153 - val_loss: 0.6914 - val_accuracy: 0.5183\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5190 - val_loss: 0.6918 - val_accuracy: 0.5200\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5205 - val_loss: 0.6915 - val_accuracy: 0.5160\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5208 - val_loss: 0.6920 - val_accuracy: 0.5124\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5191 - val_loss: 0.6920 - val_accuracy: 0.5181\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5201 - val_loss: 0.6914 - val_accuracy: 0.5168\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5197 - val_loss: 0.6922 - val_accuracy: 0.5131\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5195 - val_loss: 0.6918 - val_accuracy: 0.5185\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5198 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5197 - val_loss: 0.6918 - val_accuracy: 0.5141\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5193 - val_loss: 0.6911 - val_accuracy: 0.5174\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5196 - val_loss: 0.6915 - val_accuracy: 0.5168\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5200\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5201 - val_loss: 0.6918 - val_accuracy: 0.5133\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5207 - val_loss: 0.6919 - val_accuracy: 0.5187\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5189 - val_loss: 0.6914 - val_accuracy: 0.5198\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5198 - val_loss: 0.6932 - val_accuracy: 0.5071\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5213 - val_loss: 0.6929 - val_accuracy: 0.5063\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5196 - val_loss: 0.6913 - val_accuracy: 0.5211\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5213 - val_loss: 0.6914 - val_accuracy: 0.5214\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5211 - val_loss: 0.6917 - val_accuracy: 0.5142\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5208 - val_loss: 0.6916 - val_accuracy: 0.5200\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5209 - val_loss: 0.6912 - val_accuracy: 0.5215\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5198 - val_loss: 0.6914 - val_accuracy: 0.5192\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5173\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5219 - val_loss: 0.6918 - val_accuracy: 0.5182\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5201 - val_loss: 0.6914 - val_accuracy: 0.5166\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5190 - val_loss: 0.6917 - val_accuracy: 0.5204\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5210 - val_loss: 0.6909 - val_accuracy: 0.5178\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5198 - val_loss: 0.6917 - val_accuracy: 0.5164\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5210 - val_loss: 0.6916 - val_accuracy: 0.5210\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5206 - val_loss: 0.6916 - val_accuracy: 0.5150\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5208 - val_loss: 0.6917 - val_accuracy: 0.5146\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5211 - val_loss: 0.6910 - val_accuracy: 0.5199\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5195 - val_loss: 0.6920 - val_accuracy: 0.5179\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5200 - val_loss: 0.6917 - val_accuracy: 0.5131\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5211 - val_loss: 0.6909 - val_accuracy: 0.5179\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5208 - val_loss: 0.6910 - val_accuracy: 0.5211\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5202 - val_loss: 0.6910 - val_accuracy: 0.5231\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5212 - val_loss: 0.6919 - val_accuracy: 0.5143\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5204 - val_loss: 0.6916 - val_accuracy: 0.5203\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5215 - val_loss: 0.6915 - val_accuracy: 0.5229\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5221 - val_loss: 0.6919 - val_accuracy: 0.5125\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5206 - val_loss: 0.6931 - val_accuracy: 0.5104\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization\n",
      "cannot prune layer q_activation\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.6901 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5206\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5211 - val_loss: 0.6913 - val_accuracy: 0.5194\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5213 - val_loss: 0.6916 - val_accuracy: 0.5160\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5222 - val_loss: 0.6917 - val_accuracy: 0.5197\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5219 - val_loss: 0.6919 - val_accuracy: 0.5204\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5225 - val_loss: 0.6910 - val_accuracy: 0.5164\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5209 - val_loss: 0.6909 - val_accuracy: 0.5208\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5224 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5194\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5207 - val_loss: 0.6912 - val_accuracy: 0.5186\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5215 - val_loss: 0.6912 - val_accuracy: 0.5160\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5218 - val_loss: 0.6922 - val_accuracy: 0.5200\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5214 - val_loss: 0.6908 - val_accuracy: 0.5205\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5209 - val_loss: 0.6927 - val_accuracy: 0.5136\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5216 - val_loss: 0.6906 - val_accuracy: 0.5212\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5215 - val_loss: 0.6912 - val_accuracy: 0.5210\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5210 - val_loss: 0.6913 - val_accuracy: 0.5225\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5220 - val_loss: 0.6910 - val_accuracy: 0.5202\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5217 - val_loss: 0.6929 - val_accuracy: 0.5143\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5214 - val_loss: 0.6928 - val_accuracy: 0.5145\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5222 - val_loss: 0.6910 - val_accuracy: 0.5212\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5214 - val_loss: 0.6914 - val_accuracy: 0.5158\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5216 - val_loss: 0.6905 - val_accuracy: 0.5205\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5215 - val_loss: 0.6919 - val_accuracy: 0.5218\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5214 - val_loss: 0.6918 - val_accuracy: 0.5168\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5226 - val_loss: 0.6927 - val_accuracy: 0.5200\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5231 - val_loss: 0.6913 - val_accuracy: 0.5225\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5217 - val_loss: 0.6923 - val_accuracy: 0.5171\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5217 - val_loss: 0.6909 - val_accuracy: 0.5224\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5221 - val_loss: 0.6911 - val_accuracy: 0.5229\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5222 - val_loss: 0.6920 - val_accuracy: 0.5155\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5222 - val_loss: 0.6919 - val_accuracy: 0.5175\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5228 - val_loss: 0.6921 - val_accuracy: 0.5205\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5219 - val_loss: 0.6914 - val_accuracy: 0.5235\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5221 - val_loss: 0.6917 - val_accuracy: 0.5146\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5215 - val_loss: 0.6921 - val_accuracy: 0.5214\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5214 - val_loss: 0.6910 - val_accuracy: 0.5187\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5225 - val_loss: 0.6909 - val_accuracy: 0.5241\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5228 - val_loss: 0.6910 - val_accuracy: 0.5189\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5228 - val_loss: 0.6914 - val_accuracy: 0.5186\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5242\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5228 - val_loss: 0.6922 - val_accuracy: 0.5192\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6909 - val_accuracy: 0.5192\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6914 - val_accuracy: 0.5221\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5236 - val_loss: 0.6918 - val_accuracy: 0.5134\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5224 - val_loss: 0.6916 - val_accuracy: 0.5223\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5223 - val_loss: 0.6908 - val_accuracy: 0.5208\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5230 - val_loss: 0.6911 - val_accuracy: 0.5192\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5221 - val_loss: 0.6924 - val_accuracy: 0.5202\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5219 - val_loss: 0.6909 - val_accuracy: 0.5172\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.49271134]\n",
      " [0.44613445]\n",
      " [0.36864293]\n",
      " [0.4831082 ]\n",
      " [0.4414056 ]\n",
      " [0.4911852 ]\n",
      " [0.47642758]\n",
      " [0.5030608 ]\n",
      " [0.5348677 ]\n",
      " [0.3936602 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.690896213054657\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_1 is normal keras bn layer\n",
      "q_activation_1       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.8280 - accuracy: 0.5023 - val_loss: 0.7278 - val_accuracy: 0.5013\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7196 - accuracy: 0.5034 - val_loss: 0.7140 - val_accuracy: 0.5055\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7095 - accuracy: 0.5047 - val_loss: 0.7078 - val_accuracy: 0.5032\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7043 - accuracy: 0.5054 - val_loss: 0.7031 - val_accuracy: 0.5047\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7013 - accuracy: 0.5064 - val_loss: 0.7018 - val_accuracy: 0.5054\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6993 - accuracy: 0.5066 - val_loss: 0.7001 - val_accuracy: 0.5075\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6979 - accuracy: 0.5076 - val_loss: 0.6991 - val_accuracy: 0.5066\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.5084 - val_loss: 0.6981 - val_accuracy: 0.5101\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5095 - val_loss: 0.6973 - val_accuracy: 0.5095\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5096 - val_loss: 0.6962 - val_accuracy: 0.5109\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5109 - val_loss: 0.6951 - val_accuracy: 0.5112\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5110 - val_loss: 0.6950 - val_accuracy: 0.5125\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5122 - val_loss: 0.6943 - val_accuracy: 0.5139\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5134 - val_loss: 0.6941 - val_accuracy: 0.5129\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5129 - val_loss: 0.6937 - val_accuracy: 0.5160\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5141 - val_loss: 0.6932 - val_accuracy: 0.5118\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5155 - val_loss: 0.6936 - val_accuracy: 0.5173\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5151 - val_loss: 0.6936 - val_accuracy: 0.5186\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5162 - val_loss: 0.6926 - val_accuracy: 0.5218\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5158 - val_loss: 0.6934 - val_accuracy: 0.5168\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5180 - val_loss: 0.6930 - val_accuracy: 0.5173\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5172 - val_loss: 0.6933 - val_accuracy: 0.5197\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5170 - val_loss: 0.6924 - val_accuracy: 0.5223\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5185 - val_loss: 0.6924 - val_accuracy: 0.5197\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5230\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5206 - val_loss: 0.6937 - val_accuracy: 0.5193\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5213 - val_loss: 0.6920 - val_accuracy: 0.5184\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5207 - val_loss: 0.6917 - val_accuracy: 0.5225\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5226 - val_loss: 0.6916 - val_accuracy: 0.5248\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5236 - val_loss: 0.6924 - val_accuracy: 0.5229\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5254 - val_loss: 0.6914 - val_accuracy: 0.5224\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5251 - val_loss: 0.6917 - val_accuracy: 0.5210\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5265 - val_loss: 0.6910 - val_accuracy: 0.5276\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5261 - val_loss: 0.6911 - val_accuracy: 0.5259\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5283 - val_loss: 0.6918 - val_accuracy: 0.5301\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5287 - val_loss: 0.6907 - val_accuracy: 0.5330\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5297 - val_loss: 0.6909 - val_accuracy: 0.5265\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5302 - val_loss: 0.6907 - val_accuracy: 0.5355\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5321 - val_loss: 0.6911 - val_accuracy: 0.5337\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5314 - val_loss: 0.6919 - val_accuracy: 0.5263\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5316 - val_loss: 0.6907 - val_accuracy: 0.5308\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5330 - val_loss: 0.6902 - val_accuracy: 0.5281\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5341 - val_loss: 0.6910 - val_accuracy: 0.5295\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5337 - val_loss: 0.6899 - val_accuracy: 0.5334\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5358 - val_loss: 0.6902 - val_accuracy: 0.5368\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5350 - val_loss: 0.6900 - val_accuracy: 0.5343\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6885 - accuracy: 0.5364 - val_loss: 0.6906 - val_accuracy: 0.5311\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5356 - val_loss: 0.6900 - val_accuracy: 0.5384\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5357 - val_loss: 0.6893 - val_accuracy: 0.5380\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5356 - val_loss: 0.6901 - val_accuracy: 0.5369\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5365 - val_loss: 0.6906 - val_accuracy: 0.5271\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5368 - val_loss: 0.6890 - val_accuracy: 0.5369\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5383 - val_loss: 0.6890 - val_accuracy: 0.5391\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5379 - val_loss: 0.6893 - val_accuracy: 0.5407\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5382 - val_loss: 0.6912 - val_accuracy: 0.5360\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5375 - val_loss: 0.6889 - val_accuracy: 0.5370\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5377 - val_loss: 0.6903 - val_accuracy: 0.5327\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5389 - val_loss: 0.6890 - val_accuracy: 0.5407\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5398 - val_loss: 0.6889 - val_accuracy: 0.5426\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5399 - val_loss: 0.6892 - val_accuracy: 0.5415\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5405 - val_loss: 0.6890 - val_accuracy: 0.5367\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5396 - val_loss: 0.6895 - val_accuracy: 0.5396\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5397 - val_loss: 0.6923 - val_accuracy: 0.5172\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5406 - val_loss: 0.6888 - val_accuracy: 0.5349\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5407 - val_loss: 0.6892 - val_accuracy: 0.5387\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5408 - val_loss: 0.6888 - val_accuracy: 0.5435\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5419 - val_loss: 0.6892 - val_accuracy: 0.5369\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5421 - val_loss: 0.6898 - val_accuracy: 0.5359\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5380\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5419 - val_loss: 0.6892 - val_accuracy: 0.5425\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5413 - val_loss: 0.6888 - val_accuracy: 0.5434\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5435 - val_loss: 0.6880 - val_accuracy: 0.5455\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5426 - val_loss: 0.6887 - val_accuracy: 0.5454\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5414 - val_loss: 0.6890 - val_accuracy: 0.5407\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5427 - val_loss: 0.6884 - val_accuracy: 0.5398\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5433 - val_loss: 0.6903 - val_accuracy: 0.5373\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5432 - val_loss: 0.6886 - val_accuracy: 0.5418\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5430 - val_loss: 0.6884 - val_accuracy: 0.5444\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5438 - val_loss: 0.6878 - val_accuracy: 0.5449\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5437 - val_loss: 0.6879 - val_accuracy: 0.5485\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5432 - val_loss: 0.6896 - val_accuracy: 0.5386\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5432 - val_loss: 0.6895 - val_accuracy: 0.5372\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5438 - val_loss: 0.6875 - val_accuracy: 0.5460\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5436 - val_loss: 0.6879 - val_accuracy: 0.5424\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5433 - val_loss: 0.6886 - val_accuracy: 0.5459\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5445 - val_loss: 0.6877 - val_accuracy: 0.5426\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5448 - val_loss: 0.6891 - val_accuracy: 0.5329\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5451 - val_loss: 0.6877 - val_accuracy: 0.5424\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5447 - val_loss: 0.6882 - val_accuracy: 0.5447\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5461 - val_loss: 0.6872 - val_accuracy: 0.5445\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5457 - val_loss: 0.6875 - val_accuracy: 0.5415\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5458 - val_loss: 0.6882 - val_accuracy: 0.5452\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5462 - val_loss: 0.6870 - val_accuracy: 0.5426\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5465 - val_loss: 0.6884 - val_accuracy: 0.5452\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5468 - val_loss: 0.6878 - val_accuracy: 0.5407\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5453 - val_loss: 0.6885 - val_accuracy: 0.5453\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5469 - val_loss: 0.6872 - val_accuracy: 0.5519\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.5465 - val_loss: 0.6875 - val_accuracy: 0.5400\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5465 - val_loss: 0.6889 - val_accuracy: 0.5401\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5467 - val_loss: 0.6871 - val_accuracy: 0.5469\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_1\n",
      "cannot prune layer q_activation_1\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.6849 - accuracy: 0.5472 - val_loss: 0.6879 - val_accuracy: 0.5457\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5475 - val_loss: 0.6866 - val_accuracy: 0.5507\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5473 - val_loss: 0.6858 - val_accuracy: 0.5503\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5467 - val_loss: 0.6859 - val_accuracy: 0.5530\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5472 - val_loss: 0.6869 - val_accuracy: 0.5476\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5473 - val_loss: 0.6877 - val_accuracy: 0.5444\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5482 - val_loss: 0.6877 - val_accuracy: 0.5504\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5485 - val_loss: 0.6868 - val_accuracy: 0.5429\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5484 - val_loss: 0.6868 - val_accuracy: 0.5509\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5483 - val_loss: 0.6878 - val_accuracy: 0.5470\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5480 - val_loss: 0.6864 - val_accuracy: 0.5451\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5494 - val_loss: 0.6880 - val_accuracy: 0.5433\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5479 - val_loss: 0.6864 - val_accuracy: 0.5465\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5495 - val_loss: 0.6882 - val_accuracy: 0.5389\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5497 - val_loss: 0.6873 - val_accuracy: 0.5490\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5498 - val_loss: 0.6875 - val_accuracy: 0.5421\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5509 - val_loss: 0.6872 - val_accuracy: 0.5510\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5491 - val_loss: 0.6868 - val_accuracy: 0.5531\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5485 - val_loss: 0.6870 - val_accuracy: 0.5483\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5490 - val_loss: 0.6878 - val_accuracy: 0.5375\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5489 - val_loss: 0.6868 - val_accuracy: 0.5459\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5505 - val_loss: 0.6873 - val_accuracy: 0.5431\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5498 - val_loss: 0.6895 - val_accuracy: 0.5467\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5495 - val_loss: 0.6870 - val_accuracy: 0.5522\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5501 - val_loss: 0.6869 - val_accuracy: 0.5487\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5512 - val_loss: 0.6882 - val_accuracy: 0.5339\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5492 - val_loss: 0.6864 - val_accuracy: 0.5554\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5502 - val_loss: 0.6870 - val_accuracy: 0.5479\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5501 - val_loss: 0.6857 - val_accuracy: 0.5484\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5499 - val_loss: 0.6854 - val_accuracy: 0.5491\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5516 - val_loss: 0.6864 - val_accuracy: 0.5512\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5505 - val_loss: 0.6879 - val_accuracy: 0.5506\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5500 - val_loss: 0.6866 - val_accuracy: 0.5519\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5513 - val_loss: 0.6864 - val_accuracy: 0.5477\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5501 - val_loss: 0.6869 - val_accuracy: 0.5494\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5507 - val_loss: 0.6873 - val_accuracy: 0.5472\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5521 - val_loss: 0.6868 - val_accuracy: 0.5526\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5503 - val_loss: 0.6863 - val_accuracy: 0.5438\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6832 - accuracy: 0.5513 - val_loss: 0.6853 - val_accuracy: 0.5527\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.6867 - val_accuracy: 0.5513\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5515 - val_loss: 0.6880 - val_accuracy: 0.5354\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5518 - val_loss: 0.6890 - val_accuracy: 0.5408\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5522 - val_loss: 0.6853 - val_accuracy: 0.5521\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6832 - accuracy: 0.5515 - val_loss: 0.6863 - val_accuracy: 0.5514\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5528 - val_loss: 0.6858 - val_accuracy: 0.5555\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5534 - val_loss: 0.6858 - val_accuracy: 0.5503\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5436 - val_loss: 0.6887 - val_accuracy: 0.5477\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5459 - val_loss: 0.6885 - val_accuracy: 0.5348\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5500 - val_loss: 0.6866 - val_accuracy: 0.5503\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.6872 - val_accuracy: 0.5484\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.594092  ]\n",
      " [0.5417926 ]\n",
      " [0.5669484 ]\n",
      " [0.5669919 ]\n",
      " [0.47461084]\n",
      " [0.5343758 ]\n",
      " [0.42844597]\n",
      " [0.48886654]\n",
      " [0.49869126]\n",
      " [0.51261735]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6871724724769592\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_2 is normal keras bn layer\n",
      "q_activation_2       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 1.0781 - accuracy: 0.5035 - val_loss: 0.7507 - val_accuracy: 0.5057\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7342 - accuracy: 0.5050 - val_loss: 0.7269 - val_accuracy: 0.5052\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7197 - accuracy: 0.5044 - val_loss: 0.7179 - val_accuracy: 0.5044\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7118 - accuracy: 0.5051 - val_loss: 0.7110 - val_accuracy: 0.5083\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7074 - accuracy: 0.5059 - val_loss: 0.7083 - val_accuracy: 0.5064\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7043 - accuracy: 0.5062 - val_loss: 0.7038 - val_accuracy: 0.5086\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7021 - accuracy: 0.5076 - val_loss: 0.7015 - val_accuracy: 0.5104\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7001 - accuracy: 0.5083 - val_loss: 0.6998 - val_accuracy: 0.5091\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6988 - accuracy: 0.5089 - val_loss: 0.6984 - val_accuracy: 0.5124\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5104 - val_loss: 0.6975 - val_accuracy: 0.5130\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5100 - val_loss: 0.6966 - val_accuracy: 0.5126\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5105 - val_loss: 0.6980 - val_accuracy: 0.5063\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5112 - val_loss: 0.6958 - val_accuracy: 0.5133\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5117 - val_loss: 0.6955 - val_accuracy: 0.5088\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5125 - val_loss: 0.6953 - val_accuracy: 0.5078\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5124 - val_loss: 0.6945 - val_accuracy: 0.5147\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5132 - val_loss: 0.6948 - val_accuracy: 0.5061\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5148 - val_loss: 0.6941 - val_accuracy: 0.5084\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5130 - val_loss: 0.6949 - val_accuracy: 0.5055\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6936 - val_accuracy: 0.5175\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5146 - val_loss: 0.6944 - val_accuracy: 0.5105\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5155 - val_loss: 0.6938 - val_accuracy: 0.5123\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5154 - val_loss: 0.6940 - val_accuracy: 0.5128\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5147 - val_loss: 0.6937 - val_accuracy: 0.5168\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5149 - val_loss: 0.6930 - val_accuracy: 0.5205\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5166 - val_loss: 0.6935 - val_accuracy: 0.5127\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5163 - val_loss: 0.6932 - val_accuracy: 0.5165\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.6929 - val_accuracy: 0.5190\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5167 - val_loss: 0.6931 - val_accuracy: 0.5223\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5203\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5173 - val_loss: 0.6938 - val_accuracy: 0.5158\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5162 - val_loss: 0.6927 - val_accuracy: 0.5206\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5178 - val_loss: 0.6930 - val_accuracy: 0.5169\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5175 - val_loss: 0.6928 - val_accuracy: 0.5172\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5180 - val_loss: 0.6928 - val_accuracy: 0.5196\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.5223\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5191 - val_loss: 0.6926 - val_accuracy: 0.5185\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5189 - val_loss: 0.6932 - val_accuracy: 0.5068\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5209 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5207 - val_loss: 0.6927 - val_accuracy: 0.5215\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5215 - val_loss: 0.6932 - val_accuracy: 0.5176\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5200 - val_loss: 0.6927 - val_accuracy: 0.5227\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5213 - val_loss: 0.6925 - val_accuracy: 0.5147\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5222 - val_loss: 0.6921 - val_accuracy: 0.5205\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5219 - val_loss: 0.6923 - val_accuracy: 0.5229\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5229 - val_loss: 0.6921 - val_accuracy: 0.5234\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5226 - val_loss: 0.6921 - val_accuracy: 0.5197\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5234 - val_loss: 0.6919 - val_accuracy: 0.5242\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5241 - val_loss: 0.6921 - val_accuracy: 0.5246\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5247 - val_loss: 0.6924 - val_accuracy: 0.5207\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5241 - val_loss: 0.6920 - val_accuracy: 0.5252\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5255 - val_loss: 0.6922 - val_accuracy: 0.5249\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5256 - val_loss: 0.6917 - val_accuracy: 0.5240\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5263 - val_loss: 0.6916 - val_accuracy: 0.5241\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5263 - val_loss: 0.6914 - val_accuracy: 0.5270\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5267 - val_loss: 0.6915 - val_accuracy: 0.5287\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5274 - val_loss: 0.6918 - val_accuracy: 0.5301\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5257 - val_loss: 0.6913 - val_accuracy: 0.5240\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5282 - val_loss: 0.6914 - val_accuracy: 0.5307\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5286 - val_loss: 0.6917 - val_accuracy: 0.5238\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5287 - val_loss: 0.6931 - val_accuracy: 0.5191\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5295 - val_loss: 0.6912 - val_accuracy: 0.5254\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5296 - val_loss: 0.6910 - val_accuracy: 0.5286\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5316 - val_loss: 0.6912 - val_accuracy: 0.5283\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5310 - val_loss: 0.6904 - val_accuracy: 0.5339\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5310 - val_loss: 0.6917 - val_accuracy: 0.5281\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5297 - val_loss: 0.6910 - val_accuracy: 0.5312\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5322 - val_loss: 0.6908 - val_accuracy: 0.5264\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5330 - val_loss: 0.6901 - val_accuracy: 0.5287\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5343 - val_loss: 0.6899 - val_accuracy: 0.5325\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5344 - val_loss: 0.6902 - val_accuracy: 0.5358\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5351 - val_loss: 0.6902 - val_accuracy: 0.5260\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5356 - val_loss: 0.6905 - val_accuracy: 0.5330\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5345 - val_loss: 0.6895 - val_accuracy: 0.5390\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5363 - val_loss: 0.6899 - val_accuracy: 0.5339\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5364 - val_loss: 0.6892 - val_accuracy: 0.5418\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5375 - val_loss: 0.6894 - val_accuracy: 0.5307\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5372 - val_loss: 0.6900 - val_accuracy: 0.5383\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5378 - val_loss: 0.6901 - val_accuracy: 0.5374\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5373 - val_loss: 0.6889 - val_accuracy: 0.5372\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5381 - val_loss: 0.6893 - val_accuracy: 0.5389\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5380 - val_loss: 0.6898 - val_accuracy: 0.5374\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5385 - val_loss: 0.6892 - val_accuracy: 0.5421\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5395 - val_loss: 0.6891 - val_accuracy: 0.5395\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5402 - val_loss: 0.6893 - val_accuracy: 0.5360\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5394 - val_loss: 0.6889 - val_accuracy: 0.5420\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5394 - val_loss: 0.6885 - val_accuracy: 0.5357\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5403 - val_loss: 0.6887 - val_accuracy: 0.5417\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5406 - val_loss: 0.6892 - val_accuracy: 0.5367\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5413 - val_loss: 0.6895 - val_accuracy: 0.5377\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5408 - val_loss: 0.6884 - val_accuracy: 0.5437\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5412 - val_loss: 0.6884 - val_accuracy: 0.5387\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5413 - val_loss: 0.6886 - val_accuracy: 0.5424\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5418 - val_loss: 0.6885 - val_accuracy: 0.5454\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5408 - val_loss: 0.6884 - val_accuracy: 0.5414\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5420 - val_loss: 0.6886 - val_accuracy: 0.5423\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5431 - val_loss: 0.6884 - val_accuracy: 0.5460\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5437 - val_loss: 0.6881 - val_accuracy: 0.5435\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5418 - val_loss: 0.6886 - val_accuracy: 0.5407\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5422 - val_loss: 0.6894 - val_accuracy: 0.5256\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_2\n",
      "cannot prune layer q_activation_2\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6863 - accuracy: 0.5426 - val_loss: 0.6883 - val_accuracy: 0.5430\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5416 - val_loss: 0.6884 - val_accuracy: 0.5412\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5442 - val_loss: 0.6884 - val_accuracy: 0.5438\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5434 - val_loss: 0.6882 - val_accuracy: 0.5408\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5430 - val_loss: 0.6878 - val_accuracy: 0.5438\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5444 - val_loss: 0.6873 - val_accuracy: 0.5453\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5445 - val_loss: 0.6875 - val_accuracy: 0.5479\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5449 - val_loss: 0.6883 - val_accuracy: 0.5467\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5443 - val_loss: 0.6899 - val_accuracy: 0.5302\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5450 - val_loss: 0.6883 - val_accuracy: 0.5461\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5450 - val_loss: 0.6880 - val_accuracy: 0.5433\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5457 - val_loss: 0.6881 - val_accuracy: 0.5440\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5457 - val_loss: 0.6885 - val_accuracy: 0.5393\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5462 - val_loss: 0.6889 - val_accuracy: 0.5401\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5458 - val_loss: 0.6877 - val_accuracy: 0.5408\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5475 - val_loss: 0.6877 - val_accuracy: 0.5455\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5468 - val_loss: 0.6878 - val_accuracy: 0.5440\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5467 - val_loss: 0.6879 - val_accuracy: 0.5483\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5468 - val_loss: 0.6880 - val_accuracy: 0.5436\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5480 - val_loss: 0.6879 - val_accuracy: 0.5388\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5476 - val_loss: 0.6871 - val_accuracy: 0.5487\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5482 - val_loss: 0.6872 - val_accuracy: 0.5410\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5484 - val_loss: 0.6870 - val_accuracy: 0.5498\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5479 - val_loss: 0.6879 - val_accuracy: 0.5434\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5477 - val_loss: 0.6870 - val_accuracy: 0.5464\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5493 - val_loss: 0.6866 - val_accuracy: 0.5524\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5488 - val_loss: 0.6874 - val_accuracy: 0.5482\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5482 - val_loss: 0.6876 - val_accuracy: 0.5392\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5486 - val_loss: 0.6868 - val_accuracy: 0.5449\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5491 - val_loss: 0.6866 - val_accuracy: 0.5473\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5490 - val_loss: 0.6866 - val_accuracy: 0.5469\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5495 - val_loss: 0.6866 - val_accuracy: 0.5502\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5499 - val_loss: 0.6886 - val_accuracy: 0.5499\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5497 - val_loss: 0.6871 - val_accuracy: 0.5494\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5487 - val_loss: 0.6865 - val_accuracy: 0.5476\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5493 - val_loss: 0.6862 - val_accuracy: 0.5500\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5497 - val_loss: 0.6866 - val_accuracy: 0.5487\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5490 - val_loss: 0.6874 - val_accuracy: 0.5446\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5493 - val_loss: 0.6875 - val_accuracy: 0.5431\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5494 - val_loss: 0.6877 - val_accuracy: 0.5374\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5499 - val_loss: 0.6864 - val_accuracy: 0.5470\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5498 - val_loss: 0.6865 - val_accuracy: 0.5475\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5502 - val_loss: 0.6869 - val_accuracy: 0.5529\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5504 - val_loss: 0.6858 - val_accuracy: 0.5526\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5499 - val_loss: 0.6871 - val_accuracy: 0.5485\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5500 - val_loss: 0.6871 - val_accuracy: 0.5495\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5519 - val_loss: 0.6865 - val_accuracy: 0.5476\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6836 - accuracy: 0.5499 - val_loss: 0.6858 - val_accuracy: 0.5502\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5512 - val_loss: 0.6876 - val_accuracy: 0.5421\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5502 - val_loss: 0.6859 - val_accuracy: 0.5524\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.54515415]\n",
      " [0.48840824]\n",
      " [0.48026648]\n",
      " [0.5145495 ]\n",
      " [0.49188948]\n",
      " [0.48059466]\n",
      " [0.55572367]\n",
      " [0.49326596]\n",
      " [0.53241146]\n",
      " [0.5027146 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_3 is normal keras bn layer\n",
      "q_activation_3       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.7218 - accuracy: 0.5041 - val_loss: 0.6975 - val_accuracy: 0.5053\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5077 - val_loss: 0.6957 - val_accuracy: 0.5083\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5100 - val_loss: 0.6940 - val_accuracy: 0.5073\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.5129\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5118 - val_loss: 0.6944 - val_accuracy: 0.5065\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5111 - val_loss: 0.6932 - val_accuracy: 0.5137\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5109 - val_loss: 0.6940 - val_accuracy: 0.5134\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5120 - val_loss: 0.6927 - val_accuracy: 0.5154\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5125 - val_loss: 0.6953 - val_accuracy: 0.5024\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5140 - val_loss: 0.6934 - val_accuracy: 0.5160\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5124 - val_loss: 0.6931 - val_accuracy: 0.5044\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6927 - val_accuracy: 0.5162\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5050\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5155 - val_loss: 0.6934 - val_accuracy: 0.5161\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5130 - val_loss: 0.6924 - val_accuracy: 0.5093\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5154 - val_loss: 0.6921 - val_accuracy: 0.5157\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6939 - val_accuracy: 0.5050\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5151 - val_loss: 0.6919 - val_accuracy: 0.5178\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6928 - val_accuracy: 0.5185\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5150 - val_loss: 0.6920 - val_accuracy: 0.5199\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5165 - val_loss: 0.6935 - val_accuracy: 0.5137\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5167 - val_loss: 0.6929 - val_accuracy: 0.5163\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5158 - val_loss: 0.6919 - val_accuracy: 0.5181\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6937 - val_accuracy: 0.5158\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6921 - val_accuracy: 0.5184\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5175 - val_loss: 0.6922 - val_accuracy: 0.5181\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5175 - val_loss: 0.6925 - val_accuracy: 0.5167\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5173 - val_loss: 0.6917 - val_accuracy: 0.5142\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5187 - val_loss: 0.6917 - val_accuracy: 0.5165\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5173 - val_loss: 0.6918 - val_accuracy: 0.5208\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5191 - val_loss: 0.6919 - val_accuracy: 0.5150\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5180 - val_loss: 0.6919 - val_accuracy: 0.5135\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6915 - val_accuracy: 0.5142\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5185 - val_loss: 0.6916 - val_accuracy: 0.5163\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5195 - val_loss: 0.6917 - val_accuracy: 0.5145\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6922 - val_accuracy: 0.5193\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5179 - val_loss: 0.6916 - val_accuracy: 0.5151\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5185 - val_loss: 0.6914 - val_accuracy: 0.5205\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5185 - val_loss: 0.6914 - val_accuracy: 0.5191\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5194 - val_loss: 0.6913 - val_accuracy: 0.5165\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5191 - val_loss: 0.6918 - val_accuracy: 0.5125\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5178\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5198 - val_loss: 0.6917 - val_accuracy: 0.5163\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5207 - val_loss: 0.6920 - val_accuracy: 0.5156\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5202 - val_loss: 0.6928 - val_accuracy: 0.5187\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5208 - val_loss: 0.6934 - val_accuracy: 0.5076\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5203 - val_loss: 0.6914 - val_accuracy: 0.5170\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5197 - val_loss: 0.6915 - val_accuracy: 0.5205\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5196 - val_loss: 0.6920 - val_accuracy: 0.5183\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5213 - val_loss: 0.6931 - val_accuracy: 0.5092\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5190 - val_loss: 0.6921 - val_accuracy: 0.5118\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5206 - val_loss: 0.6915 - val_accuracy: 0.5174\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5198 - val_loss: 0.6916 - val_accuracy: 0.5198\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5213 - val_loss: 0.6914 - val_accuracy: 0.5190\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5201 - val_loss: 0.6912 - val_accuracy: 0.5207\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5214 - val_loss: 0.6922 - val_accuracy: 0.5168\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5189 - val_loss: 0.6909 - val_accuracy: 0.5170\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5208 - val_loss: 0.6912 - val_accuracy: 0.5171\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5211 - val_loss: 0.6936 - val_accuracy: 0.5096\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5197 - val_loss: 0.6911 - val_accuracy: 0.5209\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5221 - val_loss: 0.6919 - val_accuracy: 0.5207\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5222 - val_loss: 0.6927 - val_accuracy: 0.5132\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5207 - val_loss: 0.6928 - val_accuracy: 0.5085\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5154\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5220 - val_loss: 0.6911 - val_accuracy: 0.5227\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5213 - val_loss: 0.6918 - val_accuracy: 0.5189\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5215 - val_loss: 0.6914 - val_accuracy: 0.5197\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5209 - val_loss: 0.6916 - val_accuracy: 0.5208\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5224 - val_loss: 0.6930 - val_accuracy: 0.5134\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5210 - val_loss: 0.6914 - val_accuracy: 0.5112\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5222 - val_loss: 0.6911 - val_accuracy: 0.5157\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5218 - val_loss: 0.6915 - val_accuracy: 0.5180\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5210 - val_loss: 0.6918 - val_accuracy: 0.5142\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5213 - val_loss: 0.6910 - val_accuracy: 0.5200\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5212 - val_loss: 0.6910 - val_accuracy: 0.5200\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5214 - val_loss: 0.6915 - val_accuracy: 0.5182\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5205 - val_loss: 0.6920 - val_accuracy: 0.5185\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_3\n",
      "cannot prune layer q_activation_3\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6905 - accuracy: 0.5201 - val_loss: 0.6917 - val_accuracy: 0.5196\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5220 - val_loss: 0.6912 - val_accuracy: 0.5180\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5216 - val_loss: 0.6920 - val_accuracy: 0.5189\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5207 - val_loss: 0.6917 - val_accuracy: 0.5186\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5225 - val_loss: 0.6919 - val_accuracy: 0.5174\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6928 - val_accuracy: 0.5135\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5201 - val_loss: 0.6924 - val_accuracy: 0.5148\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5215 - val_loss: 0.6915 - val_accuracy: 0.5179\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5230 - val_loss: 0.6911 - val_accuracy: 0.5166\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5220 - val_loss: 0.6909 - val_accuracy: 0.5202\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5238 - val_loss: 0.6917 - val_accuracy: 0.5189\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5227 - val_loss: 0.6909 - val_accuracy: 0.5217\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5226 - val_loss: 0.6917 - val_accuracy: 0.5201\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5228\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5227 - val_loss: 0.6916 - val_accuracy: 0.5182\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5231 - val_loss: 0.6916 - val_accuracy: 0.5198\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5234 - val_loss: 0.6914 - val_accuracy: 0.5208\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5222 - val_loss: 0.6916 - val_accuracy: 0.5195\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5245 - val_loss: 0.6912 - val_accuracy: 0.5166\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5229 - val_loss: 0.6916 - val_accuracy: 0.5237\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5234 - val_loss: 0.6930 - val_accuracy: 0.5233\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5235 - val_loss: 0.6910 - val_accuracy: 0.5237\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5225 - val_loss: 0.6912 - val_accuracy: 0.5227\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5233 - val_loss: 0.6910 - val_accuracy: 0.5182\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5240 - val_loss: 0.6926 - val_accuracy: 0.5185\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5230 - val_loss: 0.6905 - val_accuracy: 0.5209\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5235 - val_loss: 0.6920 - val_accuracy: 0.5228\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5235 - val_loss: 0.6905 - val_accuracy: 0.5206\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5241 - val_loss: 0.6910 - val_accuracy: 0.5176\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5196\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5233 - val_loss: 0.6910 - val_accuracy: 0.5203\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5229 - val_loss: 0.6913 - val_accuracy: 0.5207\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5229 - val_loss: 0.6909 - val_accuracy: 0.5220\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5234 - val_loss: 0.6908 - val_accuracy: 0.5221\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5236 - val_loss: 0.6922 - val_accuracy: 0.5188\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5236 - val_loss: 0.6909 - val_accuracy: 0.5236\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5229 - val_loss: 0.6904 - val_accuracy: 0.5229\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5242 - val_loss: 0.6923 - val_accuracy: 0.5141\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5230 - val_loss: 0.6907 - val_accuracy: 0.5234\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5242 - val_loss: 0.6909 - val_accuracy: 0.5203\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5238 - val_loss: 0.6910 - val_accuracy: 0.5231\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5245 - val_loss: 0.6910 - val_accuracy: 0.5233\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5252 - val_loss: 0.6902 - val_accuracy: 0.5245\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5248 - val_loss: 0.6906 - val_accuracy: 0.5195\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5242 - val_loss: 0.6905 - val_accuracy: 0.5218\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5254 - val_loss: 0.6911 - val_accuracy: 0.5186\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5254 - val_loss: 0.6901 - val_accuracy: 0.5182\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5243 - val_loss: 0.6905 - val_accuracy: 0.5212\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5251 - val_loss: 0.6914 - val_accuracy: 0.5166\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5236 - val_loss: 0.6903 - val_accuracy: 0.5284\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.53180575]\n",
      " [0.49617183]\n",
      " [0.38453466]\n",
      " [0.49902287]\n",
      " [0.49902287]\n",
      " [0.5029075 ]\n",
      " [0.546709  ]\n",
      " [0.5037157 ]\n",
      " [0.53655463]\n",
      " [0.5368955 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_4 is normal keras bn layer\n",
      "q_activation_4       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.9856 - accuracy: 0.5013 - val_loss: 0.7180 - val_accuracy: 0.4993\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7126 - accuracy: 0.5017 - val_loss: 0.7095 - val_accuracy: 0.4987\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7038 - accuracy: 0.5032 - val_loss: 0.7045 - val_accuracy: 0.4991\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6997 - accuracy: 0.5051 - val_loss: 0.7005 - val_accuracy: 0.5025\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5053 - val_loss: 0.6986 - val_accuracy: 0.5041\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5068 - val_loss: 0.6978 - val_accuracy: 0.5097\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5090 - val_loss: 0.6967 - val_accuracy: 0.5075\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5099 - val_loss: 0.6956 - val_accuracy: 0.5040\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5114 - val_loss: 0.6950 - val_accuracy: 0.5135\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5117 - val_loss: 0.6945 - val_accuracy: 0.5139\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5133 - val_loss: 0.6942 - val_accuracy: 0.5130\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5138 - val_loss: 0.6948 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6938 - val_accuracy: 0.5182\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5140 - val_loss: 0.6940 - val_accuracy: 0.5035\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5131 - val_loss: 0.6934 - val_accuracy: 0.5172\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5149 - val_loss: 0.6949 - val_accuracy: 0.5100\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5149 - val_loss: 0.6934 - val_accuracy: 0.5108\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5167 - val_loss: 0.6928 - val_accuracy: 0.5150\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5179 - val_loss: 0.6932 - val_accuracy: 0.5218\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5177 - val_loss: 0.6930 - val_accuracy: 0.5142\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5174 - val_loss: 0.6927 - val_accuracy: 0.5186\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6928 - val_accuracy: 0.5179\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5184 - val_loss: 0.6925 - val_accuracy: 0.5195\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5191 - val_loss: 0.6929 - val_accuracy: 0.5201\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6934 - val_accuracy: 0.5184\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5195 - val_loss: 0.6926 - val_accuracy: 0.5211\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5210 - val_loss: 0.6927 - val_accuracy: 0.5231\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5214 - val_loss: 0.6922 - val_accuracy: 0.5201\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6925 - val_accuracy: 0.5186\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5218 - val_loss: 0.6934 - val_accuracy: 0.5181\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5218\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5242 - val_loss: 0.6924 - val_accuracy: 0.5199\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5229 - val_loss: 0.6942 - val_accuracy: 0.5222\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5248 - val_loss: 0.6919 - val_accuracy: 0.5203\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5246 - val_loss: 0.6917 - val_accuracy: 0.5233\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5248 - val_loss: 0.6921 - val_accuracy: 0.5237\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5253 - val_loss: 0.6924 - val_accuracy: 0.5218\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5253 - val_loss: 0.6931 - val_accuracy: 0.5209\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5261 - val_loss: 0.6921 - val_accuracy: 0.5162\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5265 - val_loss: 0.6916 - val_accuracy: 0.5224\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5259 - val_loss: 0.6918 - val_accuracy: 0.5190\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5265 - val_loss: 0.6917 - val_accuracy: 0.5245\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5274 - val_loss: 0.6919 - val_accuracy: 0.5183\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5279 - val_loss: 0.6920 - val_accuracy: 0.5177\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5282 - val_loss: 0.6922 - val_accuracy: 0.5138\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5288 - val_loss: 0.6910 - val_accuracy: 0.5257\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5289 - val_loss: 0.6910 - val_accuracy: 0.5213\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5287 - val_loss: 0.6910 - val_accuracy: 0.5261\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5297 - val_loss: 0.6913 - val_accuracy: 0.5253\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5315 - val_loss: 0.6907 - val_accuracy: 0.5278\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5304 - val_loss: 0.6906 - val_accuracy: 0.5280\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5308 - val_loss: 0.6908 - val_accuracy: 0.5293\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5313 - val_loss: 0.6908 - val_accuracy: 0.5292\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5327 - val_loss: 0.6919 - val_accuracy: 0.5203\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5328 - val_loss: 0.6911 - val_accuracy: 0.5233\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5316 - val_loss: 0.6905 - val_accuracy: 0.5314\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5329 - val_loss: 0.6902 - val_accuracy: 0.5314\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5332 - val_loss: 0.6908 - val_accuracy: 0.5293\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5342 - val_loss: 0.6913 - val_accuracy: 0.5224\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5337 - val_loss: 0.6901 - val_accuracy: 0.5354\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5347 - val_loss: 0.6892 - val_accuracy: 0.5377\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5342 - val_loss: 0.6896 - val_accuracy: 0.5346\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5341 - val_loss: 0.6899 - val_accuracy: 0.5297\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5359 - val_loss: 0.6897 - val_accuracy: 0.5314\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5352 - val_loss: 0.6906 - val_accuracy: 0.5254\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5359 - val_loss: 0.6904 - val_accuracy: 0.5371\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5367 - val_loss: 0.6895 - val_accuracy: 0.5371\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5383 - val_loss: 0.6895 - val_accuracy: 0.5373\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5362 - val_loss: 0.6889 - val_accuracy: 0.5426\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5364 - val_loss: 0.6890 - val_accuracy: 0.5394\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5375 - val_loss: 0.6898 - val_accuracy: 0.5410\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5364 - val_loss: 0.6899 - val_accuracy: 0.5375\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5366 - val_loss: 0.6901 - val_accuracy: 0.5384\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5376 - val_loss: 0.6892 - val_accuracy: 0.5380\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5377 - val_loss: 0.6899 - val_accuracy: 0.5350\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5374 - val_loss: 0.6898 - val_accuracy: 0.5393\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5378 - val_loss: 0.6896 - val_accuracy: 0.5269\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5387 - val_loss: 0.6904 - val_accuracy: 0.5397\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5377 - val_loss: 0.6886 - val_accuracy: 0.5380\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5379 - val_loss: 0.6900 - val_accuracy: 0.5267\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5387 - val_loss: 0.6892 - val_accuracy: 0.5427\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5385 - val_loss: 0.6887 - val_accuracy: 0.5395\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5386 - val_loss: 0.6898 - val_accuracy: 0.5389\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5393 - val_loss: 0.6891 - val_accuracy: 0.5407\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5390 - val_loss: 0.6880 - val_accuracy: 0.5415\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5397 - val_loss: 0.6890 - val_accuracy: 0.5415\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5396 - val_loss: 0.6887 - val_accuracy: 0.5418\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5404 - val_loss: 0.6887 - val_accuracy: 0.5338\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5397 - val_loss: 0.6891 - val_accuracy: 0.5328\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5397 - val_loss: 0.6881 - val_accuracy: 0.5421\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5390 - val_loss: 0.6890 - val_accuracy: 0.5370\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5402 - val_loss: 0.6894 - val_accuracy: 0.5414\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5418 - val_loss: 0.6887 - val_accuracy: 0.5389\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5404 - val_loss: 0.6891 - val_accuracy: 0.5423\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5406 - val_loss: 0.6890 - val_accuracy: 0.5345\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5409 - val_loss: 0.6890 - val_accuracy: 0.5375\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5406 - val_loss: 0.6891 - val_accuracy: 0.5312\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5406 - val_loss: 0.6889 - val_accuracy: 0.5434\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5416 - val_loss: 0.6900 - val_accuracy: 0.5396\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5406 - val_loss: 0.6908 - val_accuracy: 0.5348\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_4\n",
      "cannot prune layer q_activation_4\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6863 - accuracy: 0.5417 - val_loss: 0.6946 - val_accuracy: 0.5193\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5407 - val_loss: 0.6965 - val_accuracy: 0.5264\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5397 - val_loss: 0.7025 - val_accuracy: 0.5106\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5401 - val_loss: 0.6997 - val_accuracy: 0.5237\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5388 - val_loss: 0.7134 - val_accuracy: 0.5162\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5351 - val_loss: 0.6927 - val_accuracy: 0.5329\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5363 - val_loss: 0.6920 - val_accuracy: 0.5230\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5379 - val_loss: 0.6905 - val_accuracy: 0.5344\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5366 - val_loss: 0.6891 - val_accuracy: 0.5403\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5376 - val_loss: 0.6887 - val_accuracy: 0.5416\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5380 - val_loss: 0.6892 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5379 - val_loss: 0.6886 - val_accuracy: 0.5412\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5385 - val_loss: 0.6893 - val_accuracy: 0.5408\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5384 - val_loss: 0.6897 - val_accuracy: 0.5418\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5389 - val_loss: 0.6890 - val_accuracy: 0.5408\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5393 - val_loss: 0.6897 - val_accuracy: 0.5424\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5406 - val_loss: 0.6886 - val_accuracy: 0.5427\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5407 - val_loss: 0.6889 - val_accuracy: 0.5421\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5401 - val_loss: 0.6886 - val_accuracy: 0.5370\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5397 - val_loss: 0.6889 - val_accuracy: 0.5329\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5398 - val_loss: 0.6888 - val_accuracy: 0.5416\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5409 - val_loss: 0.6891 - val_accuracy: 0.5371\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5403 - val_loss: 0.6885 - val_accuracy: 0.5411\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5396 - val_loss: 0.6890 - val_accuracy: 0.5390\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5390 - val_loss: 0.6896 - val_accuracy: 0.5377\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5409 - val_loss: 0.6889 - val_accuracy: 0.5360\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5394 - val_loss: 0.6889 - val_accuracy: 0.5358\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5404 - val_loss: 0.6892 - val_accuracy: 0.5426\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5414 - val_loss: 0.6884 - val_accuracy: 0.5394\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5408 - val_loss: 0.6890 - val_accuracy: 0.5420\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5410 - val_loss: 0.6895 - val_accuracy: 0.5444\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5422 - val_loss: 0.6887 - val_accuracy: 0.5379\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5411 - val_loss: 0.6894 - val_accuracy: 0.5413\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5417 - val_loss: 0.6889 - val_accuracy: 0.5369\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5413 - val_loss: 0.6896 - val_accuracy: 0.5413\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5420 - val_loss: 0.6888 - val_accuracy: 0.5435\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5410 - val_loss: 0.6884 - val_accuracy: 0.5412\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5416 - val_loss: 0.6886 - val_accuracy: 0.5413\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5417 - val_loss: 0.6887 - val_accuracy: 0.5436\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5424 - val_loss: 0.6883 - val_accuracy: 0.5441\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5423 - val_loss: 0.6887 - val_accuracy: 0.5420\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5432 - val_loss: 0.6887 - val_accuracy: 0.5343\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5435 - val_loss: 0.6881 - val_accuracy: 0.5445\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5428 - val_loss: 0.6879 - val_accuracy: 0.5454\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5431 - val_loss: 0.6898 - val_accuracy: 0.5340\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5433 - val_loss: 0.6880 - val_accuracy: 0.5456\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5428 - val_loss: 0.6885 - val_accuracy: 0.5367\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5424 - val_loss: 0.6885 - val_accuracy: 0.5438\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5419 - val_loss: 0.6886 - val_accuracy: 0.5382\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5436 - val_loss: 0.6893 - val_accuracy: 0.5402\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5527928 ]\n",
      " [0.43001884]\n",
      " [0.3409859 ]\n",
      " [0.5174154 ]\n",
      " [0.52414083]\n",
      " [0.5141346 ]\n",
      " [0.5237605 ]\n",
      " [0.4863425 ]\n",
      " [0.5131716 ]\n",
      " [0.5148257 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_5 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_5 is normal keras bn layer\n",
      "q_activation_5       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.8246 - accuracy: 0.5013 - val_loss: 0.7381 - val_accuracy: 0.5028\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7264 - accuracy: 0.5021 - val_loss: 0.7238 - val_accuracy: 0.5059\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7138 - accuracy: 0.5032 - val_loss: 0.7125 - val_accuracy: 0.5075\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7072 - accuracy: 0.5049 - val_loss: 0.7088 - val_accuracy: 0.5080\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7040 - accuracy: 0.5045 - val_loss: 0.7045 - val_accuracy: 0.5063\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7010 - accuracy: 0.5055 - val_loss: 0.7018 - val_accuracy: 0.5083\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6990 - accuracy: 0.5060 - val_loss: 0.7000 - val_accuracy: 0.5121\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6982 - accuracy: 0.5071 - val_loss: 0.6981 - val_accuracy: 0.5075\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.5086 - val_loss: 0.6970 - val_accuracy: 0.5096\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5086 - val_loss: 0.6963 - val_accuracy: 0.5125\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5105 - val_loss: 0.6966 - val_accuracy: 0.5085\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5107 - val_loss: 0.6956 - val_accuracy: 0.5118\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5114 - val_loss: 0.6950 - val_accuracy: 0.5082\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5123 - val_loss: 0.6947 - val_accuracy: 0.5096\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5136 - val_loss: 0.6955 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5127 - val_loss: 0.6943 - val_accuracy: 0.5128\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5154 - val_loss: 0.6945 - val_accuracy: 0.5181\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5150 - val_loss: 0.6938 - val_accuracy: 0.5161\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5163 - val_loss: 0.6940 - val_accuracy: 0.5131\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5167 - val_loss: 0.6936 - val_accuracy: 0.5130\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5160 - val_loss: 0.6965 - val_accuracy: 0.5159\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5167 - val_loss: 0.6932 - val_accuracy: 0.5139\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5167 - val_loss: 0.6934 - val_accuracy: 0.5097\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5174 - val_loss: 0.6927 - val_accuracy: 0.5165\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5175 - val_loss: 0.6928 - val_accuracy: 0.5185\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5180 - val_loss: 0.6926 - val_accuracy: 0.5170\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5175 - val_loss: 0.6927 - val_accuracy: 0.5150\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6928 - val_accuracy: 0.5168\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.5244\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5196 - val_loss: 0.6922 - val_accuracy: 0.5222\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5185 - val_loss: 0.6934 - val_accuracy: 0.5092\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5203 - val_loss: 0.6924 - val_accuracy: 0.5217\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5201 - val_loss: 0.6923 - val_accuracy: 0.5208\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5219 - val_loss: 0.6933 - val_accuracy: 0.5105\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5219 - val_loss: 0.6922 - val_accuracy: 0.5239\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5220 - val_loss: 0.6924 - val_accuracy: 0.5172\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5214 - val_loss: 0.6923 - val_accuracy: 0.5253\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5232 - val_loss: 0.6924 - val_accuracy: 0.5222\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5225 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5233 - val_loss: 0.6921 - val_accuracy: 0.5227\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5224 - val_loss: 0.6926 - val_accuracy: 0.5171\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5233 - val_loss: 0.6920 - val_accuracy: 0.5251\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5243 - val_loss: 0.6927 - val_accuracy: 0.5206\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5235 - val_loss: 0.6934 - val_accuracy: 0.5217\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5239 - val_loss: 0.6920 - val_accuracy: 0.5277\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5243 - val_loss: 0.6923 - val_accuracy: 0.5245\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5246 - val_loss: 0.6923 - val_accuracy: 0.5269\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5246 - val_loss: 0.6921 - val_accuracy: 0.5242\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5258 - val_loss: 0.6920 - val_accuracy: 0.5218\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5262 - val_loss: 0.6920 - val_accuracy: 0.5277\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5252 - val_loss: 0.6918 - val_accuracy: 0.5282\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5271 - val_loss: 0.6918 - val_accuracy: 0.5250\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5290 - val_loss: 0.6921 - val_accuracy: 0.5259\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5272 - val_loss: 0.6917 - val_accuracy: 0.5294\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5285 - val_loss: 0.6916 - val_accuracy: 0.5283\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5292 - val_loss: 0.6914 - val_accuracy: 0.5286\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5299 - val_loss: 0.6912 - val_accuracy: 0.5320\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5300 - val_loss: 0.6916 - val_accuracy: 0.5228\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5309 - val_loss: 0.6920 - val_accuracy: 0.5290\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5304 - val_loss: 0.6910 - val_accuracy: 0.5319\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5305 - val_loss: 0.6913 - val_accuracy: 0.5319\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5303 - val_loss: 0.6912 - val_accuracy: 0.5246\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5318 - val_loss: 0.6913 - val_accuracy: 0.5324\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5316 - val_loss: 0.6909 - val_accuracy: 0.5300\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5319 - val_loss: 0.6905 - val_accuracy: 0.5294\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5321 - val_loss: 0.6914 - val_accuracy: 0.5288\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5324 - val_loss: 0.6910 - val_accuracy: 0.5290\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5325 - val_loss: 0.6914 - val_accuracy: 0.5290\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5327 - val_loss: 0.6912 - val_accuracy: 0.5258\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5337 - val_loss: 0.6906 - val_accuracy: 0.5265\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5333 - val_loss: 0.6906 - val_accuracy: 0.5297\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5348 - val_loss: 0.6912 - val_accuracy: 0.5267\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5344 - val_loss: 0.6914 - val_accuracy: 0.5291\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5349 - val_loss: 0.6904 - val_accuracy: 0.5334\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5341 - val_loss: 0.6908 - val_accuracy: 0.5267\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5357 - val_loss: 0.6901 - val_accuracy: 0.5371\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5350 - val_loss: 0.6900 - val_accuracy: 0.5374\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5366 - val_loss: 0.6909 - val_accuracy: 0.5379\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5351 - val_loss: 0.6903 - val_accuracy: 0.5351\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5360 - val_loss: 0.6903 - val_accuracy: 0.5368\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5361 - val_loss: 0.6897 - val_accuracy: 0.5385\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5367 - val_loss: 0.6903 - val_accuracy: 0.5281\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5368 - val_loss: 0.6905 - val_accuracy: 0.5331\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5382 - val_loss: 0.6898 - val_accuracy: 0.5355\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5368 - val_loss: 0.6894 - val_accuracy: 0.5374\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5373 - val_loss: 0.6902 - val_accuracy: 0.5377\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5381 - val_loss: 0.6895 - val_accuracy: 0.5348\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5379 - val_loss: 0.6893 - val_accuracy: 0.5351\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5387 - val_loss: 0.6899 - val_accuracy: 0.5356\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5380 - val_loss: 0.6900 - val_accuracy: 0.5394\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5385 - val_loss: 0.6893 - val_accuracy: 0.5331\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5386 - val_loss: 0.6888 - val_accuracy: 0.5383\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5377 - val_loss: 0.6898 - val_accuracy: 0.5376\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5383 - val_loss: 0.6905 - val_accuracy: 0.5333\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5397 - val_loss: 0.6905 - val_accuracy: 0.5424\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5399 - val_loss: 0.6897 - val_accuracy: 0.5395\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5390 - val_loss: 0.6900 - val_accuracy: 0.5422\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5391 - val_loss: 0.6884 - val_accuracy: 0.5419\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5408 - val_loss: 0.6899 - val_accuracy: 0.5334\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5410 - val_loss: 0.6887 - val_accuracy: 0.5385\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_5\n",
      "cannot prune layer q_activation_5\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6869 - accuracy: 0.5402 - val_loss: 0.6958 - val_accuracy: 0.5266\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5394 - val_loss: 0.6990 - val_accuracy: 0.5160\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5369 - val_loss: 0.6933 - val_accuracy: 0.5269\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5376 - val_loss: 0.7023 - val_accuracy: 0.5152\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5360 - val_loss: 0.6919 - val_accuracy: 0.5328\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5367 - val_loss: 0.6942 - val_accuracy: 0.5248\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5355 - val_loss: 0.6941 - val_accuracy: 0.5195\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5365 - val_loss: 0.6955 - val_accuracy: 0.5086\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5383 - val_loss: 0.6895 - val_accuracy: 0.5345\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5394 - val_loss: 0.6899 - val_accuracy: 0.5366\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5397 - val_loss: 0.6890 - val_accuracy: 0.5379\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5402 - val_loss: 0.6892 - val_accuracy: 0.5373\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5402 - val_loss: 0.6895 - val_accuracy: 0.5333\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5404 - val_loss: 0.6896 - val_accuracy: 0.5333\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5406 - val_loss: 0.6892 - val_accuracy: 0.5392\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5405 - val_loss: 0.6890 - val_accuracy: 0.5406\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5404 - val_loss: 0.6894 - val_accuracy: 0.5415\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5409 - val_loss: 0.6888 - val_accuracy: 0.5393\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5409 - val_loss: 0.6891 - val_accuracy: 0.5362\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5421 - val_loss: 0.6890 - val_accuracy: 0.5341\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5378\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5422 - val_loss: 0.6885 - val_accuracy: 0.5377\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5422 - val_loss: 0.6887 - val_accuracy: 0.5420\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5420 - val_loss: 0.6885 - val_accuracy: 0.5395\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5429 - val_loss: 0.6883 - val_accuracy: 0.5455\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5430 - val_loss: 0.6882 - val_accuracy: 0.5409\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5437 - val_loss: 0.6874 - val_accuracy: 0.5442\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5441 - val_loss: 0.6878 - val_accuracy: 0.5392\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5447 - val_loss: 0.6887 - val_accuracy: 0.5463\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5451 - val_loss: 0.6874 - val_accuracy: 0.5475\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5451 - val_loss: 0.6877 - val_accuracy: 0.5478\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5455 - val_loss: 0.6880 - val_accuracy: 0.5443\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5442 - val_loss: 0.6873 - val_accuracy: 0.5480\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5448 - val_loss: 0.6879 - val_accuracy: 0.5445\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5453 - val_loss: 0.6874 - val_accuracy: 0.5432\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5459 - val_loss: 0.6870 - val_accuracy: 0.5464\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5452 - val_loss: 0.6871 - val_accuracy: 0.5485\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5467 - val_loss: 0.6869 - val_accuracy: 0.5468\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5461 - val_loss: 0.6872 - val_accuracy: 0.5483\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5455 - val_loss: 0.6872 - val_accuracy: 0.5493\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5460 - val_loss: 0.6873 - val_accuracy: 0.5496\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5459 - val_loss: 0.6868 - val_accuracy: 0.5503\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5467 - val_loss: 0.6869 - val_accuracy: 0.5494\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5459 - val_loss: 0.6873 - val_accuracy: 0.5466\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5470 - val_loss: 0.6870 - val_accuracy: 0.5487\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5468 - val_loss: 0.6871 - val_accuracy: 0.5478\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5468 - val_loss: 0.6866 - val_accuracy: 0.5470\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5473 - val_loss: 0.6865 - val_accuracy: 0.5517\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5466 - val_loss: 0.6869 - val_accuracy: 0.5491\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5481 - val_loss: 0.6865 - val_accuracy: 0.5501\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.54238933]\n",
      " [0.45857644]\n",
      " [0.4156205 ]\n",
      " [0.52702904]\n",
      " [0.4643085 ]\n",
      " [0.51389855]\n",
      " [0.44813442]\n",
      " [0.4800812 ]\n",
      " [0.5253803 ]\n",
      " [0.45461053]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_6 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_6 is normal keras bn layer\n",
      "q_activation_6       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 6ms/step - loss: 0.8848 - accuracy: 0.5005 - val_loss: 0.6976 - val_accuracy: 0.5040\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5059 - val_loss: 0.6950 - val_accuracy: 0.5078\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5060 - val_loss: 0.6946 - val_accuracy: 0.5078\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5089 - val_loss: 0.6938 - val_accuracy: 0.5106\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5079 - val_loss: 0.6943 - val_accuracy: 0.5042\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5093 - val_loss: 0.6936 - val_accuracy: 0.5028\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6935 - val_accuracy: 0.5050\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6938 - val_accuracy: 0.5079\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5124 - val_loss: 0.6928 - val_accuracy: 0.5112\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5127 - val_loss: 0.6932 - val_accuracy: 0.5100\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5129\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5130 - val_loss: 0.6928 - val_accuracy: 0.5140\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5065\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6934 - val_accuracy: 0.5126\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6932 - val_accuracy: 0.5135\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5137 - val_loss: 0.6943 - val_accuracy: 0.5092\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6936 - val_accuracy: 0.5115\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5129 - val_loss: 0.6927 - val_accuracy: 0.5140\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5151 - val_loss: 0.6931 - val_accuracy: 0.5120\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6945 - val_accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5128\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6925 - val_accuracy: 0.5148\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5147 - val_loss: 0.6924 - val_accuracy: 0.5141\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.5154\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6925 - val_accuracy: 0.5167\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5169 - val_loss: 0.6927 - val_accuracy: 0.5127\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5170 - val_loss: 0.6925 - val_accuracy: 0.5160\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.6929 - val_accuracy: 0.5161\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.6925 - val_accuracy: 0.5107\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5178 - val_loss: 0.6919 - val_accuracy: 0.5167\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5138\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6927 - val_accuracy: 0.5145\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5172 - val_loss: 0.6930 - val_accuracy: 0.5181\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5163 - val_loss: 0.6923 - val_accuracy: 0.5157\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5163 - val_loss: 0.6924 - val_accuracy: 0.5201\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5181 - val_loss: 0.6925 - val_accuracy: 0.5138\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5168 - val_loss: 0.6924 - val_accuracy: 0.5093\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5185 - val_loss: 0.6936 - val_accuracy: 0.5150\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5174 - val_loss: 0.6918 - val_accuracy: 0.5165\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5196 - val_loss: 0.6918 - val_accuracy: 0.5200\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5173 - val_loss: 0.6924 - val_accuracy: 0.5191\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5183 - val_loss: 0.6924 - val_accuracy: 0.5129\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6934 - val_accuracy: 0.5150\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5188 - val_loss: 0.6920 - val_accuracy: 0.5183\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5180 - val_loss: 0.6933 - val_accuracy: 0.5187\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5192 - val_loss: 0.6920 - val_accuracy: 0.5193\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5193 - val_loss: 0.6920 - val_accuracy: 0.5155\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5183 - val_loss: 0.6923 - val_accuracy: 0.5095\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5201 - val_loss: 0.6918 - val_accuracy: 0.5190\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5194 - val_loss: 0.6918 - val_accuracy: 0.5217\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5190 - val_loss: 0.6928 - val_accuracy: 0.5156\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5182 - val_loss: 0.6923 - val_accuracy: 0.5114\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.6917 - val_accuracy: 0.5199\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5193 - val_loss: 0.6922 - val_accuracy: 0.5107\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5190 - val_loss: 0.6917 - val_accuracy: 0.5152\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5190 - val_loss: 0.6923 - val_accuracy: 0.5192\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5198 - val_loss: 0.6921 - val_accuracy: 0.5160\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5196 - val_loss: 0.6923 - val_accuracy: 0.5179\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5189 - val_loss: 0.6919 - val_accuracy: 0.5176\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5205 - val_loss: 0.6920 - val_accuracy: 0.5184\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5186\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5209 - val_loss: 0.6920 - val_accuracy: 0.5206\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5205 - val_loss: 0.6932 - val_accuracy: 0.5148\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5199 - val_loss: 0.6921 - val_accuracy: 0.5181\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5211 - val_loss: 0.6922 - val_accuracy: 0.5182\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5195 - val_loss: 0.6925 - val_accuracy: 0.5178\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5210 - val_loss: 0.6929 - val_accuracy: 0.5170\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5203 - val_loss: 0.6918 - val_accuracy: 0.5174\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5202 - val_loss: 0.6913 - val_accuracy: 0.5195\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5191 - val_loss: 0.6918 - val_accuracy: 0.5170\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5207 - val_loss: 0.6917 - val_accuracy: 0.5157\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5204 - val_loss: 0.6927 - val_accuracy: 0.5090\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5203 - val_loss: 0.6920 - val_accuracy: 0.5148\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5198 - val_loss: 0.6925 - val_accuracy: 0.5130\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5198 - val_loss: 0.6918 - val_accuracy: 0.5166\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5203 - val_loss: 0.6923 - val_accuracy: 0.5129\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5211 - val_loss: 0.6923 - val_accuracy: 0.5172\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5212 - val_loss: 0.6934 - val_accuracy: 0.5084\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5216 - val_loss: 0.6912 - val_accuracy: 0.5196\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5213 - val_loss: 0.6917 - val_accuracy: 0.5164\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5214 - val_loss: 0.6913 - val_accuracy: 0.5183\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6919 - val_accuracy: 0.5183\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5203\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5190\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5212 - val_loss: 0.6917 - val_accuracy: 0.5203\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5211 - val_loss: 0.6914 - val_accuracy: 0.5166\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6929 - val_accuracy: 0.5158\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5213 - val_loss: 0.6907 - val_accuracy: 0.5195\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5210 - val_loss: 0.6910 - val_accuracy: 0.5182\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5222 - val_loss: 0.6909 - val_accuracy: 0.5198\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5223 - val_loss: 0.6908 - val_accuracy: 0.5154\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5220 - val_loss: 0.6910 - val_accuracy: 0.5215\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5221 - val_loss: 0.6912 - val_accuracy: 0.5218\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5219 - val_loss: 0.6919 - val_accuracy: 0.5193\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5205 - val_loss: 0.6909 - val_accuracy: 0.5205\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5204 - val_loss: 0.6906 - val_accuracy: 0.5210\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5201 - val_loss: 0.6918 - val_accuracy: 0.5133\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5233 - val_loss: 0.6912 - val_accuracy: 0.5191\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_6\n",
      "cannot prune layer q_activation_6\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6902 - accuracy: 0.5207 - val_loss: 0.6913 - val_accuracy: 0.5203\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5191\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5207 - val_loss: 0.6944 - val_accuracy: 0.5147\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5188 - val_loss: 0.6957 - val_accuracy: 0.5145\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5184 - val_loss: 0.6966 - val_accuracy: 0.5092\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5166 - val_loss: 0.6919 - val_accuracy: 0.5115\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5177 - val_loss: 0.6927 - val_accuracy: 0.5166\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5165 - val_loss: 0.6915 - val_accuracy: 0.5182\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5172 - val_loss: 0.6911 - val_accuracy: 0.5185\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5175 - val_loss: 0.6912 - val_accuracy: 0.5201\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5176 - val_loss: 0.6914 - val_accuracy: 0.5144\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5177 - val_loss: 0.6912 - val_accuracy: 0.5149\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5186 - val_loss: 0.6907 - val_accuracy: 0.5185\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5177 - val_loss: 0.6917 - val_accuracy: 0.5142\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5193 - val_loss: 0.6911 - val_accuracy: 0.5178\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5199 - val_loss: 0.6908 - val_accuracy: 0.5185\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5195 - val_loss: 0.6909 - val_accuracy: 0.5210\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5192 - val_loss: 0.6906 - val_accuracy: 0.5205\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5187 - val_loss: 0.6909 - val_accuracy: 0.5196\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5188 - val_loss: 0.6908 - val_accuracy: 0.5187\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5194 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5199 - val_loss: 0.6913 - val_accuracy: 0.5171\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5191 - val_loss: 0.6911 - val_accuracy: 0.5198\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5196 - val_loss: 0.6913 - val_accuracy: 0.5203\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5207 - val_loss: 0.6909 - val_accuracy: 0.5187\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5190 - val_loss: 0.6911 - val_accuracy: 0.5209\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5203 - val_loss: 0.6911 - val_accuracy: 0.5216\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5223 - val_loss: 0.6909 - val_accuracy: 0.5183\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5207 - val_loss: 0.6905 - val_accuracy: 0.5216\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5203 - val_loss: 0.6907 - val_accuracy: 0.5236\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5205 - val_loss: 0.6915 - val_accuracy: 0.5191\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5218 - val_loss: 0.6909 - val_accuracy: 0.5177\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5213 - val_loss: 0.6906 - val_accuracy: 0.5218\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5216 - val_loss: 0.6906 - val_accuracy: 0.5175\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5222 - val_loss: 0.6908 - val_accuracy: 0.5196\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5215 - val_loss: 0.6907 - val_accuracy: 0.5239\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5226 - val_loss: 0.6907 - val_accuracy: 0.5213\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5228 - val_loss: 0.6904 - val_accuracy: 0.5218\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5215 - val_loss: 0.6911 - val_accuracy: 0.5187\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5221 - val_loss: 0.6903 - val_accuracy: 0.5204\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5226 - val_loss: 0.6906 - val_accuracy: 0.5231\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5226 - val_loss: 0.6907 - val_accuracy: 0.5206\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6900 - val_accuracy: 0.5226\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5221 - val_loss: 0.6904 - val_accuracy: 0.5197\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5220 - val_loss: 0.6903 - val_accuracy: 0.5184\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5230 - val_loss: 0.6922 - val_accuracy: 0.5208\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5224 - val_loss: 0.6899 - val_accuracy: 0.5253\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5238 - val_loss: 0.6903 - val_accuracy: 0.5215\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5234 - val_loss: 0.6905 - val_accuracy: 0.5204\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5235 - val_loss: 0.6916 - val_accuracy: 0.5195\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5043121 ]\n",
      " [0.47999275]\n",
      " [0.35167792]\n",
      " [0.49804348]\n",
      " [0.49804348]\n",
      " [0.46505955]\n",
      " [0.45588347]\n",
      " [0.4970996 ]\n",
      " [0.50893104]\n",
      " [0.5061359 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_7 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_7 is normal keras bn layer\n",
      "q_activation_7       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.9097 - accuracy: 0.5009 - val_loss: 0.7285 - val_accuracy: 0.5016\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7184 - accuracy: 0.5026 - val_loss: 0.7155 - val_accuracy: 0.5034\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7135 - accuracy: 0.5026 - val_loss: 0.7156 - val_accuracy: 0.5020\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7077 - accuracy: 0.5025 - val_loss: 0.7112 - val_accuracy: 0.5067\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7037 - accuracy: 0.5044 - val_loss: 0.7018 - val_accuracy: 0.5076\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7005 - accuracy: 0.5047 - val_loss: 0.6993 - val_accuracy: 0.5056\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5060 - val_loss: 0.6980 - val_accuracy: 0.5073\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6973 - accuracy: 0.5070 - val_loss: 0.6972 - val_accuracy: 0.5103\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5081 - val_loss: 0.6966 - val_accuracy: 0.5052\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5087 - val_loss: 0.6955 - val_accuracy: 0.5078\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6972 - accuracy: 0.5071 - val_loss: 0.6952 - val_accuracy: 0.5104\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5091 - val_loss: 0.6946 - val_accuracy: 0.5090\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5096 - val_loss: 0.6947 - val_accuracy: 0.5082\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5110 - val_loss: 0.6950 - val_accuracy: 0.5121\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5112 - val_loss: 0.6938 - val_accuracy: 0.5150\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5118 - val_loss: 0.6935 - val_accuracy: 0.5127\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5129 - val_loss: 0.6933 - val_accuracy: 0.5156\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5136 - val_loss: 0.6933 - val_accuracy: 0.5145\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5152 - val_loss: 0.6985 - val_accuracy: 0.5066\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5133 - val_loss: 0.6928 - val_accuracy: 0.5190\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5195\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5150 - val_loss: 0.6930 - val_accuracy: 0.5155\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5158 - val_loss: 0.6937 - val_accuracy: 0.5105\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5166 - val_loss: 0.6929 - val_accuracy: 0.5202\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5169 - val_loss: 0.6928 - val_accuracy: 0.5192\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5163\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5186 - val_loss: 0.6937 - val_accuracy: 0.5066\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5176 - val_loss: 0.6928 - val_accuracy: 0.5188\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5182 - val_loss: 0.6963 - val_accuracy: 0.5053\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5186 - val_loss: 0.6938 - val_accuracy: 0.5059\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5198 - val_loss: 0.6925 - val_accuracy: 0.5216\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5207 - val_loss: 0.6925 - val_accuracy: 0.5134\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5195 - val_loss: 0.6932 - val_accuracy: 0.5166\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5215 - val_loss: 0.6918 - val_accuracy: 0.5181\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5216 - val_loss: 0.6921 - val_accuracy: 0.5192\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5214 - val_loss: 0.6927 - val_accuracy: 0.5201\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5218 - val_loss: 0.6936 - val_accuracy: 0.5145\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6932 - val_accuracy: 0.5201\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5257\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5229 - val_loss: 0.6915 - val_accuracy: 0.5257\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5233 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6918 - val_accuracy: 0.5213\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5257 - val_loss: 0.6918 - val_accuracy: 0.5206\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5244 - val_loss: 0.6910 - val_accuracy: 0.5243\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5247 - val_loss: 0.6913 - val_accuracy: 0.5253\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5182\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6914 - val_accuracy: 0.5289\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5270 - val_loss: 0.6910 - val_accuracy: 0.5292\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5282 - val_loss: 0.6912 - val_accuracy: 0.5273\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5294 - val_loss: 0.6911 - val_accuracy: 0.5261\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5303 - val_loss: 0.6910 - val_accuracy: 0.5307\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5299 - val_loss: 0.6912 - val_accuracy: 0.5287\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5305 - val_loss: 0.6906 - val_accuracy: 0.5310\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5316 - val_loss: 0.6910 - val_accuracy: 0.5308\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5317 - val_loss: 0.6900 - val_accuracy: 0.5310\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5327 - val_loss: 0.6900 - val_accuracy: 0.5311\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5327 - val_loss: 0.6896 - val_accuracy: 0.5350\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5338 - val_loss: 0.6894 - val_accuracy: 0.5344\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5347 - val_loss: 0.6899 - val_accuracy: 0.5341\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5346 - val_loss: 0.6892 - val_accuracy: 0.5361\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5355 - val_loss: 0.6898 - val_accuracy: 0.5287\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5351 - val_loss: 0.6896 - val_accuracy: 0.5310\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5363 - val_loss: 0.6889 - val_accuracy: 0.5372\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5368 - val_loss: 0.6887 - val_accuracy: 0.5390\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5378 - val_loss: 0.6896 - val_accuracy: 0.5347\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5374 - val_loss: 0.6905 - val_accuracy: 0.5334\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5385 - val_loss: 0.6882 - val_accuracy: 0.5406\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5381 - val_loss: 0.6888 - val_accuracy: 0.5377\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5387 - val_loss: 0.6885 - val_accuracy: 0.5361\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5396 - val_loss: 0.6887 - val_accuracy: 0.5420\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5384 - val_loss: 0.6895 - val_accuracy: 0.5348\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5384 - val_loss: 0.6895 - val_accuracy: 0.5352\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5390 - val_loss: 0.6890 - val_accuracy: 0.5369\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5389 - val_loss: 0.6897 - val_accuracy: 0.5358\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5399 - val_loss: 0.6909 - val_accuracy: 0.5353\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5394 - val_loss: 0.6895 - val_accuracy: 0.5288\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5414 - val_loss: 0.6886 - val_accuracy: 0.5381\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5378\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5412 - val_loss: 0.6890 - val_accuracy: 0.5389\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5406 - val_loss: 0.6882 - val_accuracy: 0.5429\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5291 - val_loss: 0.6922 - val_accuracy: 0.5248\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5401 - val_loss: 0.6920 - val_accuracy: 0.5255\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5378\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5422 - val_loss: 0.6881 - val_accuracy: 0.5366\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5415 - val_loss: 0.6904 - val_accuracy: 0.5330\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5419 - val_loss: 0.6891 - val_accuracy: 0.5372\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5417 - val_loss: 0.6879 - val_accuracy: 0.5411\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5415 - val_loss: 0.6885 - val_accuracy: 0.5370\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5419 - val_loss: 0.6882 - val_accuracy: 0.5369\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5432 - val_loss: 0.6872 - val_accuracy: 0.5405\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5430 - val_loss: 0.6889 - val_accuracy: 0.5398\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5437 - val_loss: 0.6888 - val_accuracy: 0.5348\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6859 - accuracy: 0.5435 - val_loss: 0.6875 - val_accuracy: 0.5434\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5435 - val_loss: 0.6875 - val_accuracy: 0.5426\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5447 - val_loss: 0.6870 - val_accuracy: 0.5456\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5439 - val_loss: 0.6879 - val_accuracy: 0.5394\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5452 - val_loss: 0.6880 - val_accuracy: 0.5454\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5459 - val_loss: 0.6893 - val_accuracy: 0.5375\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5456 - val_loss: 0.6873 - val_accuracy: 0.5490\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5458 - val_loss: 0.6873 - val_accuracy: 0.5455\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_7\n",
      "cannot prune layer q_activation_7\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 8ms/step - loss: 0.6854 - accuracy: 0.5449 - val_loss: 0.6934 - val_accuracy: 0.5342\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6863 - accuracy: 0.5439 - val_loss: 0.7580 - val_accuracy: 0.5105\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5314 - val_loss: 0.7083 - val_accuracy: 0.5159\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5299 - val_loss: 0.7686 - val_accuracy: 0.5050\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5178 - val_loss: 0.6981 - val_accuracy: 0.5043\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6990 - val_accuracy: 0.5031\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6957 - val_accuracy: 0.5122\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6913 - accuracy: 0.5189 - val_loss: 0.6939 - val_accuracy: 0.5133\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5181\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6917 - val_accuracy: 0.5204\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5238 - val_loss: 0.6920 - val_accuracy: 0.5183\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5225 - val_loss: 0.6918 - val_accuracy: 0.5230\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6905 - accuracy: 0.5241 - val_loss: 0.6915 - val_accuracy: 0.5228\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5249 - val_loss: 0.6916 - val_accuracy: 0.5248\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6904 - accuracy: 0.5247 - val_loss: 0.6916 - val_accuracy: 0.5238\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5254 - val_loss: 0.6918 - val_accuracy: 0.5231\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6902 - accuracy: 0.5262 - val_loss: 0.6913 - val_accuracy: 0.5243\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5271 - val_loss: 0.6912 - val_accuracy: 0.5278\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5273 - val_loss: 0.6914 - val_accuracy: 0.5248\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5280 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6899 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5234\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5281 - val_loss: 0.6917 - val_accuracy: 0.5247\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5281 - val_loss: 0.6912 - val_accuracy: 0.5255\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5290 - val_loss: 0.6911 - val_accuracy: 0.5280\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5300 - val_loss: 0.6912 - val_accuracy: 0.5275\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5298 - val_loss: 0.6907 - val_accuracy: 0.5290\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6894 - accuracy: 0.5301 - val_loss: 0.6911 - val_accuracy: 0.5256\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6893 - accuracy: 0.5292 - val_loss: 0.6909 - val_accuracy: 0.5277\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5300 - val_loss: 0.6910 - val_accuracy: 0.5273\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5305 - val_loss: 0.6916 - val_accuracy: 0.5225\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5296 - val_loss: 0.6914 - val_accuracy: 0.5280\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5315 - val_loss: 0.6917 - val_accuracy: 0.5275\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5300 - val_loss: 0.6909 - val_accuracy: 0.5278\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6889 - accuracy: 0.5313 - val_loss: 0.6910 - val_accuracy: 0.5315\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6890 - accuracy: 0.5310 - val_loss: 0.6913 - val_accuracy: 0.5287\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6888 - accuracy: 0.5312 - val_loss: 0.6913 - val_accuracy: 0.5296\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5319 - val_loss: 0.6909 - val_accuracy: 0.5307\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5313 - val_loss: 0.6909 - val_accuracy: 0.5273\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5325 - val_loss: 0.6908 - val_accuracy: 0.5306\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5323 - val_loss: 0.6907 - val_accuracy: 0.5300\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5320 - val_loss: 0.6913 - val_accuracy: 0.5299\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6886 - accuracy: 0.5318 - val_loss: 0.6907 - val_accuracy: 0.5308\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6885 - accuracy: 0.5324 - val_loss: 0.6907 - val_accuracy: 0.5337\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5327 - val_loss: 0.6907 - val_accuracy: 0.5274\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5311 - val_loss: 0.6908 - val_accuracy: 0.5293\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5336 - val_loss: 0.6909 - val_accuracy: 0.5299\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6885 - accuracy: 0.5315 - val_loss: 0.6905 - val_accuracy: 0.5330\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5312 - val_loss: 0.6909 - val_accuracy: 0.5296\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6885 - accuracy: 0.5320 - val_loss: 0.6902 - val_accuracy: 0.5304\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5321 - val_loss: 0.6916 - val_accuracy: 0.5295\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5516276 ]\n",
      " [0.47115833]\n",
      " [0.4584468 ]\n",
      " [0.5217108 ]\n",
      " [0.58586395]\n",
      " [0.47295308]\n",
      " [0.42901093]\n",
      " [0.52336806]\n",
      " [0.5070867 ]\n",
      " [0.602911  ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [32], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_8 (QActivatio  (None, 32)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3553 (13.88 KB)\n",
      "Trainable params: 3489 (13.63 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_8 is normal keras bn layer\n",
      "q_activation_8       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 6ms/step - loss: 1.3717 - accuracy: 0.5023 - val_loss: 0.8908 - val_accuracy: 0.4983\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.8420 - accuracy: 0.5000 - val_loss: 0.7914 - val_accuracy: 0.4959\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7614 - accuracy: 0.5023 - val_loss: 0.7378 - val_accuracy: 0.5006\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7352 - accuracy: 0.5037 - val_loss: 0.7320 - val_accuracy: 0.5015\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7287 - accuracy: 0.5039 - val_loss: 0.7213 - val_accuracy: 0.5007\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7183 - accuracy: 0.5044 - val_loss: 0.7146 - val_accuracy: 0.5007\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7153 - accuracy: 0.5038 - val_loss: 0.7107 - val_accuracy: 0.5035\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7102 - accuracy: 0.5039 - val_loss: 0.7079 - val_accuracy: 0.5048\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7075 - accuracy: 0.5052 - val_loss: 0.7061 - val_accuracy: 0.5042\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7053 - accuracy: 0.5050 - val_loss: 0.7043 - val_accuracy: 0.5032\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7039 - accuracy: 0.5055 - val_loss: 0.7030 - val_accuracy: 0.5065\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7018 - accuracy: 0.5062 - val_loss: 0.7016 - val_accuracy: 0.5060\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7004 - accuracy: 0.5070 - val_loss: 0.7007 - val_accuracy: 0.5052\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.5074 - val_loss: 0.6999 - val_accuracy: 0.5063\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6985 - accuracy: 0.5083 - val_loss: 0.6990 - val_accuracy: 0.5046\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5096 - val_loss: 0.6998 - val_accuracy: 0.5090\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5098 - val_loss: 0.6969 - val_accuracy: 0.5075\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5112 - val_loss: 0.6964 - val_accuracy: 0.5108\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5114 - val_loss: 0.6955 - val_accuracy: 0.5127\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5124 - val_loss: 0.6952 - val_accuracy: 0.5113\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5133 - val_loss: 0.6949 - val_accuracy: 0.5131\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5139 - val_loss: 0.6949 - val_accuracy: 0.5086\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5138 - val_loss: 0.6944 - val_accuracy: 0.5110\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5139 - val_loss: 0.6939 - val_accuracy: 0.5135\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5139 - val_loss: 0.6940 - val_accuracy: 0.5125\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5155 - val_loss: 0.6936 - val_accuracy: 0.5103\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5155 - val_loss: 0.6933 - val_accuracy: 0.5171\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5164 - val_loss: 0.6936 - val_accuracy: 0.5108\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5172 - val_loss: 0.6933 - val_accuracy: 0.5182\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5181 - val_loss: 0.6943 - val_accuracy: 0.5139\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.6934 - val_accuracy: 0.5163\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5179 - val_loss: 0.6929 - val_accuracy: 0.5146\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6934 - val_accuracy: 0.5170\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5201 - val_loss: 0.6924 - val_accuracy: 0.5214\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5194 - val_loss: 0.6926 - val_accuracy: 0.5211\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6936 - val_accuracy: 0.5181\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5210 - val_loss: 0.6926 - val_accuracy: 0.5183\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6928 - val_accuracy: 0.5146\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5231 - val_loss: 0.6925 - val_accuracy: 0.5210\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5242\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5238 - val_loss: 0.6924 - val_accuracy: 0.5214\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5236 - val_loss: 0.6921 - val_accuracy: 0.5173\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5238 - val_loss: 0.6929 - val_accuracy: 0.5226\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5249 - val_loss: 0.6920 - val_accuracy: 0.5253\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5248 - val_loss: 0.6920 - val_accuracy: 0.5246\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5257 - val_loss: 0.6932 - val_accuracy: 0.5235\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5260 - val_loss: 0.6921 - val_accuracy: 0.5255\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5268 - val_loss: 0.6920 - val_accuracy: 0.5201\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5265 - val_loss: 0.6932 - val_accuracy: 0.5236\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5292\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5279 - val_loss: 0.6921 - val_accuracy: 0.5256\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5288 - val_loss: 0.6915 - val_accuracy: 0.5253\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5299 - val_loss: 0.6916 - val_accuracy: 0.5302\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5297 - val_loss: 0.6911 - val_accuracy: 0.5309\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5289 - val_loss: 0.6914 - val_accuracy: 0.5274\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5303 - val_loss: 0.6913 - val_accuracy: 0.5294\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5290 - val_loss: 0.6914 - val_accuracy: 0.5272\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5308 - val_loss: 0.6917 - val_accuracy: 0.5209\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5304 - val_loss: 0.6914 - val_accuracy: 0.5224\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5319 - val_loss: 0.6917 - val_accuracy: 0.5295\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5324 - val_loss: 0.6920 - val_accuracy: 0.5288\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5320 - val_loss: 0.6918 - val_accuracy: 0.5247\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5325 - val_loss: 0.6914 - val_accuracy: 0.5222\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5335 - val_loss: 0.6916 - val_accuracy: 0.5312\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5344 - val_loss: 0.6910 - val_accuracy: 0.5318\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5347 - val_loss: 0.6909 - val_accuracy: 0.5255\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5351 - val_loss: 0.6915 - val_accuracy: 0.5322\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6887 - accuracy: 0.5352 - val_loss: 0.6911 - val_accuracy: 0.5343\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5359 - val_loss: 0.6914 - val_accuracy: 0.5267\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5363 - val_loss: 0.6902 - val_accuracy: 0.5403\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5358 - val_loss: 0.6902 - val_accuracy: 0.5352\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5371 - val_loss: 0.6903 - val_accuracy: 0.5324\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5375 - val_loss: 0.6903 - val_accuracy: 0.5368\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5384 - val_loss: 0.6907 - val_accuracy: 0.5330\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5387 - val_loss: 0.6908 - val_accuracy: 0.5395\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5381 - val_loss: 0.6906 - val_accuracy: 0.5397\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5383 - val_loss: 0.6904 - val_accuracy: 0.5309\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5396 - val_loss: 0.6914 - val_accuracy: 0.5330\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5391 - val_loss: 0.6897 - val_accuracy: 0.5394\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5393 - val_loss: 0.6895 - val_accuracy: 0.5384\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6876 - accuracy: 0.5392 - val_loss: 0.6893 - val_accuracy: 0.5409\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5401 - val_loss: 0.6899 - val_accuracy: 0.5423\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5407 - val_loss: 0.6900 - val_accuracy: 0.5391\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5412 - val_loss: 0.6895 - val_accuracy: 0.5359\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5414 - val_loss: 0.6890 - val_accuracy: 0.5422\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5414 - val_loss: 0.6897 - val_accuracy: 0.5418\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5419 - val_loss: 0.6894 - val_accuracy: 0.5410\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5428\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5427 - val_loss: 0.6893 - val_accuracy: 0.5404\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5425 - val_loss: 0.6895 - val_accuracy: 0.5410\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6867 - accuracy: 0.5431 - val_loss: 0.6893 - val_accuracy: 0.5429\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5431 - val_loss: 0.6894 - val_accuracy: 0.5388\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6869 - accuracy: 0.5425 - val_loss: 0.6893 - val_accuracy: 0.5418\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5425 - val_loss: 0.6898 - val_accuracy: 0.5326\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.5436 - val_loss: 0.6890 - val_accuracy: 0.5337\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5415 - val_loss: 0.6894 - val_accuracy: 0.5389\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5400 - val_loss: 0.6884 - val_accuracy: 0.5427\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5418 - val_loss: 0.6892 - val_accuracy: 0.5423\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5411 - val_loss: 0.6901 - val_accuracy: 0.5318\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5418 - val_loss: 0.6900 - val_accuracy: 0.5358\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_8\n",
      "cannot prune layer q_activation_8\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.6878 - accuracy: 0.5420 - val_loss: 0.6962 - val_accuracy: 0.5185\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5412 - val_loss: 0.7065 - val_accuracy: 0.5128\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5280 - val_loss: 0.7143 - val_accuracy: 0.5073\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5238 - val_loss: 0.7061 - val_accuracy: 0.5072\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5191 - val_loss: 0.7133 - val_accuracy: 0.5089\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5190 - val_loss: 0.7007 - val_accuracy: 0.5105\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5171 - val_loss: 0.6990 - val_accuracy: 0.5108\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.6939 - val_accuracy: 0.5153\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6923 - val_accuracy: 0.5189\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5223 - val_loss: 0.6917 - val_accuracy: 0.5205\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6915 - val_accuracy: 0.5238\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5246 - val_loss: 0.6916 - val_accuracy: 0.5216\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5247 - val_loss: 0.6916 - val_accuracy: 0.5246\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5248 - val_loss: 0.6915 - val_accuracy: 0.5268\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5259 - val_loss: 0.6914 - val_accuracy: 0.5234\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6903 - accuracy: 0.5270 - val_loss: 0.6914 - val_accuracy: 0.5255\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5268 - val_loss: 0.6914 - val_accuracy: 0.5230\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5269 - val_loss: 0.6914 - val_accuracy: 0.5262\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5271 - val_loss: 0.6914 - val_accuracy: 0.5262\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5275 - val_loss: 0.6916 - val_accuracy: 0.5261\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5267 - val_loss: 0.6917 - val_accuracy: 0.5207\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5276 - val_loss: 0.6913 - val_accuracy: 0.5251\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5281 - val_loss: 0.6912 - val_accuracy: 0.5273\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5278 - val_loss: 0.6912 - val_accuracy: 0.5297\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5282 - val_loss: 0.6910 - val_accuracy: 0.5249\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5283 - val_loss: 0.6910 - val_accuracy: 0.5284\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5276 - val_loss: 0.6913 - val_accuracy: 0.5269\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5278 - val_loss: 0.6909 - val_accuracy: 0.5268\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5276 - val_loss: 0.6908 - val_accuracy: 0.5283\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6899 - accuracy: 0.5286 - val_loss: 0.6910 - val_accuracy: 0.5280\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5281 - val_loss: 0.6915 - val_accuracy: 0.5262\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5291 - val_loss: 0.6908 - val_accuracy: 0.5256\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5291 - val_loss: 0.6908 - val_accuracy: 0.5298\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5292 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6901 - accuracy: 0.5285 - val_loss: 0.6909 - val_accuracy: 0.5280\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5280 - val_loss: 0.6910 - val_accuracy: 0.5278\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5278 - val_loss: 0.6923 - val_accuracy: 0.5128\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5296 - val_loss: 0.6907 - val_accuracy: 0.5289\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5292 - val_loss: 0.6909 - val_accuracy: 0.5236\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5288 - val_loss: 0.6905 - val_accuracy: 0.5288\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5297 - val_loss: 0.6909 - val_accuracy: 0.5277\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5297 - val_loss: 0.6907 - val_accuracy: 0.5310\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5297 - val_loss: 0.6910 - val_accuracy: 0.5311\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5296 - val_loss: 0.6907 - val_accuracy: 0.5290\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5304 - val_loss: 0.6909 - val_accuracy: 0.5281\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5294 - val_loss: 0.6905 - val_accuracy: 0.5307\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5302 - val_loss: 0.6909 - val_accuracy: 0.5313\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5309 - val_loss: 0.6905 - val_accuracy: 0.5306\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5307 - val_loss: 0.6908 - val_accuracy: 0.5313\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5309 - val_loss: 0.6915 - val_accuracy: 0.5307\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.52505076]\n",
      " [0.4882121 ]\n",
      " [0.50194377]\n",
      " [0.5527388 ]\n",
      " [0.4975322 ]\n",
      " [0.5000148 ]\n",
      " [0.52294016]\n",
      " [0.52423984]\n",
      " [0.50194377]\n",
      " [0.54687417]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 8)                 32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " q_activation_9 (QActivatio  (None, 8)                 0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_9 is normal keras bn layer\n",
      "q_activation_9       quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.1418 - accuracy: 0.4986 - val_loss: 0.7005 - val_accuracy: 0.5003\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5020 - val_loss: 0.6945 - val_accuracy: 0.5024\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5032 - val_loss: 0.6944 - val_accuracy: 0.5063\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5040 - val_loss: 0.6937 - val_accuracy: 0.5082\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5049 - val_loss: 0.6940 - val_accuracy: 0.5053\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5055 - val_loss: 0.6938 - val_accuracy: 0.5069\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6955 - accuracy: 0.5050 - val_loss: 0.7030 - val_accuracy: 0.5002\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5053 - val_loss: 0.6940 - val_accuracy: 0.5042\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5047 - val_loss: 0.6947 - val_accuracy: 0.5061\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5047 - val_loss: 0.6949 - val_accuracy: 0.5007\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5045 - val_loss: 0.6943 - val_accuracy: 0.5051\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5066 - val_loss: 0.6927 - val_accuracy: 0.5087\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5017\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5086 - val_loss: 0.6932 - val_accuracy: 0.5072\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5093 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5085 - val_loss: 0.6928 - val_accuracy: 0.5073\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5089 - val_loss: 0.6927 - val_accuracy: 0.5112\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5101 - val_loss: 0.6927 - val_accuracy: 0.5117\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5101 - val_loss: 0.6928 - val_accuracy: 0.5116\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6927 - val_accuracy: 0.5109\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5112 - val_loss: 0.6926 - val_accuracy: 0.5144\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5120 - val_loss: 0.6923 - val_accuracy: 0.5132\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6927 - val_accuracy: 0.5138\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6926 - val_accuracy: 0.5117\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5118 - val_loss: 0.6924 - val_accuracy: 0.5141\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5117 - val_loss: 0.6924 - val_accuracy: 0.5160\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5138 - val_loss: 0.6924 - val_accuracy: 0.5135\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6924 - val_accuracy: 0.5157\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5127 - val_loss: 0.6925 - val_accuracy: 0.5114\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6928 - val_accuracy: 0.5104\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5137 - val_loss: 0.6930 - val_accuracy: 0.5020\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6921 - val_accuracy: 0.5166\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5136 - val_loss: 0.6922 - val_accuracy: 0.5119\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5147 - val_loss: 0.6919 - val_accuracy: 0.5149\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.6922 - val_accuracy: 0.5145\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5152 - val_loss: 0.6920 - val_accuracy: 0.5174\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5145 - val_loss: 0.6935 - val_accuracy: 0.5121\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6921 - val_accuracy: 0.5155\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5155 - val_loss: 0.6921 - val_accuracy: 0.5165\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5157 - val_loss: 0.6923 - val_accuracy: 0.5153\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5149 - val_loss: 0.6921 - val_accuracy: 0.5165\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5140 - val_loss: 0.6921 - val_accuracy: 0.5159\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5141 - val_loss: 0.6925 - val_accuracy: 0.5128\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5153 - val_loss: 0.6916 - val_accuracy: 0.5190\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5163 - val_loss: 0.6921 - val_accuracy: 0.5144\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5151 - val_loss: 0.6920 - val_accuracy: 0.5117\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5147 - val_loss: 0.6919 - val_accuracy: 0.5144\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5145 - val_loss: 0.6917 - val_accuracy: 0.5151\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6921 - val_accuracy: 0.5168\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5155 - val_loss: 0.6926 - val_accuracy: 0.5107\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5153 - val_loss: 0.6917 - val_accuracy: 0.5164\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5147 - val_loss: 0.6919 - val_accuracy: 0.5164\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5153 - val_loss: 0.6917 - val_accuracy: 0.5167\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5156 - val_loss: 0.6927 - val_accuracy: 0.5140\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5149 - val_loss: 0.6919 - val_accuracy: 0.5183\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5162\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5147 - val_loss: 0.6918 - val_accuracy: 0.5177\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5154 - val_loss: 0.6926 - val_accuracy: 0.5125\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5153 - val_loss: 0.6917 - val_accuracy: 0.5147\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5157 - val_loss: 0.6937 - val_accuracy: 0.5120\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5149 - val_loss: 0.6919 - val_accuracy: 0.5146\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5164 - val_loss: 0.6919 - val_accuracy: 0.5172\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5137 - val_loss: 0.6914 - val_accuracy: 0.5190\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5166 - val_loss: 0.6914 - val_accuracy: 0.5175\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5154 - val_loss: 0.6918 - val_accuracy: 0.5081\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5148 - val_loss: 0.6917 - val_accuracy: 0.5159\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5157 - val_loss: 0.6916 - val_accuracy: 0.5169\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5170 - val_loss: 0.6917 - val_accuracy: 0.5169\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5162 - val_loss: 0.6917 - val_accuracy: 0.5175\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5155 - val_loss: 0.6913 - val_accuracy: 0.5192\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6918 - val_accuracy: 0.5163\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5164 - val_loss: 0.6916 - val_accuracy: 0.5178\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5154 - val_loss: 0.6918 - val_accuracy: 0.5141\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5160 - val_loss: 0.6913 - val_accuracy: 0.5157\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5152 - val_loss: 0.6918 - val_accuracy: 0.5170\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5153 - val_loss: 0.6916 - val_accuracy: 0.5168\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5164 - val_loss: 0.6911 - val_accuracy: 0.5169\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5157 - val_loss: 0.6926 - val_accuracy: 0.5116\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6916 - val_accuracy: 0.5173\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5163 - val_loss: 0.6922 - val_accuracy: 0.5122\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5169 - val_loss: 0.6930 - val_accuracy: 0.5122\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5166 - val_loss: 0.6911 - val_accuracy: 0.5218\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5153\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5172 - val_loss: 0.6921 - val_accuracy: 0.5149\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6920 - val_accuracy: 0.5161\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5183 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5169 - val_loss: 0.6917 - val_accuracy: 0.5173\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5177 - val_loss: 0.6920 - val_accuracy: 0.5151\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5171 - val_loss: 0.6916 - val_accuracy: 0.5162\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5172 - val_loss: 0.6915 - val_accuracy: 0.5180\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5184 - val_loss: 0.6923 - val_accuracy: 0.5078\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5183 - val_loss: 0.6918 - val_accuracy: 0.5166\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5162 - val_loss: 0.6944 - val_accuracy: 0.5073\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5185 - val_loss: 0.6917 - val_accuracy: 0.5144\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5180 - val_loss: 0.6933 - val_accuracy: 0.5118\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5183 - val_loss: 0.6917 - val_accuracy: 0.5119\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6916 - val_accuracy: 0.5157\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5165 - val_loss: 0.6920 - val_accuracy: 0.5142\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6915 - val_accuracy: 0.5168\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_9\n",
      "cannot prune layer q_activation_9\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.6912 - accuracy: 0.5175 - val_loss: 0.6915 - val_accuracy: 0.5176\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5172 - val_loss: 0.6916 - val_accuracy: 0.5139\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5173 - val_loss: 0.6915 - val_accuracy: 0.5152\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6917 - val_accuracy: 0.5178\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5165 - val_loss: 0.6917 - val_accuracy: 0.5173\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5118\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5164 - val_loss: 0.6922 - val_accuracy: 0.5142\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5154\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5190\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5170 - val_loss: 0.6922 - val_accuracy: 0.5106\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5185 - val_loss: 0.6915 - val_accuracy: 0.5174\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5167 - val_loss: 0.6921 - val_accuracy: 0.5149\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5166 - val_loss: 0.6923 - val_accuracy: 0.5078\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5185 - val_loss: 0.6919 - val_accuracy: 0.5113\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5165 - val_loss: 0.6917 - val_accuracy: 0.5186\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.6928 - val_accuracy: 0.5156\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5175 - val_loss: 0.6918 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5172 - val_loss: 0.6917 - val_accuracy: 0.5141\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5176 - val_loss: 0.6917 - val_accuracy: 0.5117\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5172 - val_loss: 0.6918 - val_accuracy: 0.5141\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5169 - val_loss: 0.6913 - val_accuracy: 0.5152\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5172 - val_loss: 0.6918 - val_accuracy: 0.5145\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5177 - val_loss: 0.6943 - val_accuracy: 0.5088\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5171 - val_loss: 0.6929 - val_accuracy: 0.5082\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5155 - val_loss: 0.6914 - val_accuracy: 0.5148\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5174 - val_loss: 0.6913 - val_accuracy: 0.5148\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5170 - val_loss: 0.6922 - val_accuracy: 0.5126\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5171 - val_loss: 0.6927 - val_accuracy: 0.5136\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5172 - val_loss: 0.6917 - val_accuracy: 0.5155\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5177 - val_loss: 0.6915 - val_accuracy: 0.5128\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5172 - val_loss: 0.6915 - val_accuracy: 0.5156\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5177 - val_loss: 0.6917 - val_accuracy: 0.5147\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5160 - val_loss: 0.6918 - val_accuracy: 0.5132\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5170 - val_loss: 0.6916 - val_accuracy: 0.5161\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5173 - val_loss: 0.6919 - val_accuracy: 0.5135\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5166 - val_loss: 0.6920 - val_accuracy: 0.5123\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5179 - val_loss: 0.6928 - val_accuracy: 0.5147\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5173 - val_loss: 0.6920 - val_accuracy: 0.5130\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5161 - val_loss: 0.6929 - val_accuracy: 0.5119\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5165 - val_loss: 0.6915 - val_accuracy: 0.5180\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5175 - val_loss: 0.6924 - val_accuracy: 0.5115\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5178 - val_loss: 0.6921 - val_accuracy: 0.5096\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.6922 - val_accuracy: 0.5148\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5127\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5167 - val_loss: 0.6920 - val_accuracy: 0.5142\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5174 - val_loss: 0.6914 - val_accuracy: 0.5141\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5169 - val_loss: 0.6917 - val_accuracy: 0.5152\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5168 - val_loss: 0.6918 - val_accuracy: 0.5099\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5177 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5155 - val_loss: 0.6914 - val_accuracy: 0.5168\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5137489 ]\n",
      " [0.50390416]\n",
      " [0.4400655 ]\n",
      " [0.5043532 ]\n",
      " [0.50390416]\n",
      " [0.50390416]\n",
      " [0.5013205 ]\n",
      " [0.507397  ]\n",
      " [0.5414566 ]\n",
      " [0.4025777 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_10 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_10 is normal keras bn layer\n",
      "q_activation_10      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.4153 - accuracy: 0.4999 - val_loss: 0.8835 - val_accuracy: 0.4955\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.8545 - accuracy: 0.5002 - val_loss: 0.9163 - val_accuracy: 0.4980\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7986 - accuracy: 0.5010 - val_loss: 0.7838 - val_accuracy: 0.4986\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7524 - accuracy: 0.5010 - val_loss: 0.7487 - val_accuracy: 0.4992\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7288 - accuracy: 0.5008 - val_loss: 0.7224 - val_accuracy: 0.5029\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7171 - accuracy: 0.5011 - val_loss: 0.7128 - val_accuracy: 0.5028\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7192 - accuracy: 0.5012 - val_loss: 0.7138 - val_accuracy: 0.5019\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7077 - accuracy: 0.5013 - val_loss: 0.7038 - val_accuracy: 0.5021\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5019 - val_loss: 0.7012 - val_accuracy: 0.5017\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7002 - accuracy: 0.5018 - val_loss: 0.7037 - val_accuracy: 0.5039\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6986 - accuracy: 0.5018 - val_loss: 0.6990 - val_accuracy: 0.5028\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6975 - accuracy: 0.5020 - val_loss: 0.6981 - val_accuracy: 0.5041\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5035 - val_loss: 0.6969 - val_accuracy: 0.5048\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5034 - val_loss: 0.6958 - val_accuracy: 0.5053\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5032 - val_loss: 0.6955 - val_accuracy: 0.5061\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5046 - val_loss: 0.6952 - val_accuracy: 0.5038\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5038 - val_loss: 0.6947 - val_accuracy: 0.5064\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5047 - val_loss: 0.6943 - val_accuracy: 0.5066\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5051 - val_loss: 0.6944 - val_accuracy: 0.5035\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5047 - val_loss: 0.6942 - val_accuracy: 0.5031\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5061 - val_loss: 0.6938 - val_accuracy: 0.5061\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5063 - val_loss: 0.6942 - val_accuracy: 0.5054\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5061 - val_loss: 0.6937 - val_accuracy: 0.5066\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5073 - val_loss: 0.6940 - val_accuracy: 0.5063\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5064 - val_loss: 0.6935 - val_accuracy: 0.5078\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5070 - val_loss: 0.6937 - val_accuracy: 0.5074\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6931 - accuracy: 0.5074 - val_loss: 0.6939 - val_accuracy: 0.5038\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5079 - val_loss: 0.6937 - val_accuracy: 0.5091\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5083 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5084 - val_loss: 0.6928 - val_accuracy: 0.5123\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5092 - val_loss: 0.6928 - val_accuracy: 0.5114\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5091 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5095\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5110 - val_loss: 0.6941 - val_accuracy: 0.5048\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5119 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5116 - val_loss: 0.6929 - val_accuracy: 0.5139\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6929 - val_accuracy: 0.5138\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5131 - val_loss: 0.6927 - val_accuracy: 0.5146\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5118 - val_loss: 0.6926 - val_accuracy: 0.5194\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5134\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5122\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5172\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5150\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5168\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5133 - val_loss: 0.6931 - val_accuracy: 0.5158\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6925 - val_accuracy: 0.5157\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5128 - val_loss: 0.6926 - val_accuracy: 0.5155\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5139 - val_loss: 0.6928 - val_accuracy: 0.5175\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5130 - val_loss: 0.6926 - val_accuracy: 0.5139\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5147 - val_loss: 0.6935 - val_accuracy: 0.5153\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5137 - val_loss: 0.6929 - val_accuracy: 0.5150\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5142 - val_loss: 0.6926 - val_accuracy: 0.5148\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5155 - val_loss: 0.6928 - val_accuracy: 0.5117\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.5170\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5144 - val_loss: 0.6926 - val_accuracy: 0.5163\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5157 - val_loss: 0.6927 - val_accuracy: 0.5156\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5148 - val_loss: 0.6926 - val_accuracy: 0.5160\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5159 - val_loss: 0.6925 - val_accuracy: 0.5173\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5148 - val_loss: 0.6932 - val_accuracy: 0.5100\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5150 - val_loss: 0.6927 - val_accuracy: 0.5155\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5152 - val_loss: 0.6933 - val_accuracy: 0.5118\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6924 - val_accuracy: 0.5173\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5164 - val_loss: 0.6926 - val_accuracy: 0.5190\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6923 - val_accuracy: 0.5202\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6923 - val_accuracy: 0.5194\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5162 - val_loss: 0.6924 - val_accuracy: 0.5172\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5075\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5159 - val_loss: 0.6927 - val_accuracy: 0.5155\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5167 - val_loss: 0.6923 - val_accuracy: 0.5181\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6926 - val_accuracy: 0.5172\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5105\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5138 - val_loss: 0.6925 - val_accuracy: 0.5170\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6926 - val_accuracy: 0.5147\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5161\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5165 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5176 - val_loss: 0.6930 - val_accuracy: 0.5095\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5168 - val_loss: 0.6926 - val_accuracy: 0.5183\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6941 - val_accuracy: 0.5040\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5161 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5166 - val_loss: 0.6925 - val_accuracy: 0.5171\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5156 - val_loss: 0.6926 - val_accuracy: 0.5129\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5161 - val_loss: 0.6927 - val_accuracy: 0.5151\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5157 - val_loss: 0.6934 - val_accuracy: 0.5138\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5170 - val_loss: 0.6927 - val_accuracy: 0.5161\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5174 - val_loss: 0.6925 - val_accuracy: 0.5153\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_10\n",
      "cannot prune layer q_activation_10\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6923 - val_accuracy: 0.5184\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5169 - val_loss: 0.6933 - val_accuracy: 0.5029\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6924 - val_accuracy: 0.5180\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5164 - val_loss: 0.6923 - val_accuracy: 0.5182\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5159 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5153\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6929 - val_accuracy: 0.5130\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5170 - val_loss: 0.6927 - val_accuracy: 0.5164\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5165 - val_loss: 0.6929 - val_accuracy: 0.5106\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5158 - val_loss: 0.6932 - val_accuracy: 0.5072\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5163 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5184\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5171 - val_loss: 0.6924 - val_accuracy: 0.5125\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 4s 8ms/step - loss: 0.6920 - accuracy: 0.5162 - val_loss: 0.6926 - val_accuracy: 0.5174\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 3s 8ms/step - loss: 0.6920 - accuracy: 0.5158 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6925 - val_accuracy: 0.5158\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 3s 8ms/step - loss: 0.6920 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5150\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 4s 9ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5175\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6927 - val_accuracy: 0.5172\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 4s 8ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6929 - val_accuracy: 0.5151\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 0.6921 - accuracy: 0.5163 - val_loss: 0.6929 - val_accuracy: 0.5129\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 3s 8ms/step - loss: 0.6921 - accuracy: 0.5168 - val_loss: 0.6930 - val_accuracy: 0.5131\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 4s 10ms/step - loss: 0.6921 - accuracy: 0.5174 - val_loss: 0.6938 - val_accuracy: 0.5150\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 4s 9ms/step - loss: 0.6921 - accuracy: 0.5177 - val_loss: 0.6932 - val_accuracy: 0.5144\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 5s 12ms/step - loss: 0.6922 - accuracy: 0.5164 - val_loss: 0.6928 - val_accuracy: 0.5157\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 4s 10ms/step - loss: 0.6922 - accuracy: 0.5167 - val_loss: 0.6934 - val_accuracy: 0.5177\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5175 - val_loss: 0.6931 - val_accuracy: 0.5173\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5166 - val_loss: 0.6930 - val_accuracy: 0.5154\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5181 - val_loss: 0.6930 - val_accuracy: 0.5139\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5188 - val_loss: 0.6932 - val_accuracy: 0.5148\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5175 - val_loss: 0.6928 - val_accuracy: 0.5104\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5179 - val_loss: 0.6925 - val_accuracy: 0.5181\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5182 - val_loss: 0.6943 - val_accuracy: 0.5168\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5172 - val_loss: 0.6926 - val_accuracy: 0.5133\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5180 - val_loss: 0.6922 - val_accuracy: 0.5176\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5176 - val_loss: 0.6943 - val_accuracy: 0.5149\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5177 - val_loss: 0.6941 - val_accuracy: 0.5171\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5181\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5178 - val_loss: 0.6922 - val_accuracy: 0.5152\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5187 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5180 - val_loss: 0.6944 - val_accuracy: 0.5136\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5183 - val_loss: 0.6950 - val_accuracy: 0.5137\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5120\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5183 - val_loss: 0.6947 - val_accuracy: 0.5167\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5197 - val_loss: 0.6920 - val_accuracy: 0.5182\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5189 - val_loss: 0.6927 - val_accuracy: 0.5120\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5187 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5191 - val_loss: 0.6943 - val_accuracy: 0.5114\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5199 - val_loss: 0.6922 - val_accuracy: 0.5162\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5195 - val_loss: 0.6921 - val_accuracy: 0.5158\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5193758 ]\n",
      " [0.47413164]\n",
      " [0.4713085 ]\n",
      " [0.4863208 ]\n",
      " [0.46533448]\n",
      " [0.48709333]\n",
      " [0.5074612 ]\n",
      " [0.48669544]\n",
      " [0.54234684]\n",
      " [0.47419506]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_11 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_11 is normal keras bn layer\n",
      "q_activation_11      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.9379 - accuracy: 0.4999 - val_loss: 0.8197 - val_accuracy: 0.5012\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7859 - accuracy: 0.5011 - val_loss: 0.7656 - val_accuracy: 0.5019\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7559 - accuracy: 0.5008 - val_loss: 0.7475 - val_accuracy: 0.5022\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7448 - accuracy: 0.5018 - val_loss: 0.7433 - val_accuracy: 0.5012\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7360 - accuracy: 0.5010 - val_loss: 0.7299 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7298 - accuracy: 0.5019 - val_loss: 0.7298 - val_accuracy: 0.4999\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7272 - accuracy: 0.5025 - val_loss: 0.7257 - val_accuracy: 0.4990\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7248 - accuracy: 0.5022 - val_loss: 0.7216 - val_accuracy: 0.4978\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7186 - accuracy: 0.5023 - val_loss: 0.7192 - val_accuracy: 0.5001\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7155 - accuracy: 0.5026 - val_loss: 0.7182 - val_accuracy: 0.5002\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7707 - accuracy: 0.5027 - val_loss: 0.9333 - val_accuracy: 0.5018\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7275 - accuracy: 0.4993 - val_loss: 0.7139 - val_accuracy: 0.5003\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7127 - accuracy: 0.5014 - val_loss: 0.7098 - val_accuracy: 0.5001\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7098 - accuracy: 0.5031 - val_loss: 0.7088 - val_accuracy: 0.4991\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7082 - accuracy: 0.5028 - val_loss: 0.7083 - val_accuracy: 0.5022\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7067 - accuracy: 0.5024 - val_loss: 0.7063 - val_accuracy: 0.4993\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7055 - accuracy: 0.5025 - val_loss: 0.7056 - val_accuracy: 0.5004\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7046 - accuracy: 0.5025 - val_loss: 0.7043 - val_accuracy: 0.5016\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7038 - accuracy: 0.5031 - val_loss: 0.7035 - val_accuracy: 0.5005\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7033 - accuracy: 0.5025 - val_loss: 0.7034 - val_accuracy: 0.5011\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5026 - val_loss: 0.7025 - val_accuracy: 0.4996\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7012 - accuracy: 0.5026 - val_loss: 0.7016 - val_accuracy: 0.5003\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.5033 - val_loss: 0.7185 - val_accuracy: 0.4997\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7016 - accuracy: 0.5023 - val_loss: 0.7006 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6993 - accuracy: 0.5037 - val_loss: 0.7000 - val_accuracy: 0.5015\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.5037 - val_loss: 0.6995 - val_accuracy: 0.4992\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6978 - accuracy: 0.5027 - val_loss: 0.6987 - val_accuracy: 0.5024\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5030 - val_loss: 0.6989 - val_accuracy: 0.5032\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5035 - val_loss: 0.6980 - val_accuracy: 0.5025\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5043 - val_loss: 0.6969 - val_accuracy: 0.5028\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5042 - val_loss: 0.6963 - val_accuracy: 0.5014\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5049 - val_loss: 0.6960 - val_accuracy: 0.5032\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5042 - val_loss: 0.6957 - val_accuracy: 0.5031\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5058 - val_loss: 0.6952 - val_accuracy: 0.5047\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5056 - val_loss: 0.6948 - val_accuracy: 0.5059\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5077 - val_loss: 0.6946 - val_accuracy: 0.5070\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5070 - val_loss: 0.6943 - val_accuracy: 0.5055\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5073 - val_loss: 0.6944 - val_accuracy: 0.5080\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5069 - val_loss: 0.6941 - val_accuracy: 0.5119\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5074 - val_loss: 0.6940 - val_accuracy: 0.5083\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6937 - val_accuracy: 0.5042\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5072 - val_loss: 0.6942 - val_accuracy: 0.5039\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5077 - val_loss: 0.6940 - val_accuracy: 0.5068\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5076 - val_loss: 0.6937 - val_accuracy: 0.5073\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5077 - val_loss: 0.6935 - val_accuracy: 0.5053\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5077 - val_loss: 0.6937 - val_accuracy: 0.5084\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5078 - val_loss: 0.6935 - val_accuracy: 0.5074\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5085 - val_loss: 0.6937 - val_accuracy: 0.5048\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5090 - val_loss: 0.6935 - val_accuracy: 0.5089\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5078 - val_loss: 0.6933 - val_accuracy: 0.5074\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5082 - val_loss: 0.6933 - val_accuracy: 0.5093\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6938 - val_accuracy: 0.5068\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5076 - val_loss: 0.6934 - val_accuracy: 0.5096\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5088 - val_loss: 0.6931 - val_accuracy: 0.5097\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5086 - val_loss: 0.6934 - val_accuracy: 0.5060\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.5077\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5093 - val_loss: 0.6937 - val_accuracy: 0.5049\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5093 - val_loss: 0.6935 - val_accuracy: 0.5072\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5094 - val_loss: 0.6933 - val_accuracy: 0.5086\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5084 - val_loss: 0.6932 - val_accuracy: 0.5092\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5108 - val_loss: 0.6933 - val_accuracy: 0.5079\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5093 - val_loss: 0.6933 - val_accuracy: 0.5109\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5095 - val_loss: 0.6933 - val_accuracy: 0.5092\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5099 - val_loss: 0.6934 - val_accuracy: 0.5084\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5098 - val_loss: 0.6935 - val_accuracy: 0.5058\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6936 - val_accuracy: 0.5084\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6934 - val_accuracy: 0.5105\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5109 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5114 - val_loss: 0.6934 - val_accuracy: 0.5124\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5101 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5117 - val_loss: 0.6934 - val_accuracy: 0.5117\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5110\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5106 - val_loss: 0.6934 - val_accuracy: 0.5069\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5113 - val_loss: 0.6933 - val_accuracy: 0.5094\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5113 - val_loss: 0.6935 - val_accuracy: 0.5097\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5111 - val_loss: 0.6938 - val_accuracy: 0.5107\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5111 - val_loss: 0.6934 - val_accuracy: 0.5099\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5113 - val_loss: 0.6932 - val_accuracy: 0.5101\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5113 - val_loss: 0.6934 - val_accuracy: 0.5122\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5114 - val_loss: 0.6933 - val_accuracy: 0.5107\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_11\n",
      "cannot prune layer q_activation_11\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 5ms/step - loss: 0.6926 - accuracy: 0.5099 - val_loss: 0.6936 - val_accuracy: 0.5101\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5098 - val_loss: 0.6932 - val_accuracy: 0.5108\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6933 - val_accuracy: 0.5104\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6934 - val_accuracy: 0.5071\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5099 - val_loss: 0.6935 - val_accuracy: 0.5075\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5112 - val_loss: 0.6934 - val_accuracy: 0.5074\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5106 - val_loss: 0.6934 - val_accuracy: 0.5106\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6936 - val_accuracy: 0.5088\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6935 - val_accuracy: 0.5109\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5108 - val_loss: 0.6934 - val_accuracy: 0.5114\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5106 - val_loss: 0.6936 - val_accuracy: 0.5081\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5111 - val_loss: 0.6933 - val_accuracy: 0.5112\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5110 - val_loss: 0.6937 - val_accuracy: 0.5095\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5114 - val_loss: 0.6934 - val_accuracy: 0.5110\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6935 - val_accuracy: 0.5101\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5116 - val_loss: 0.6933 - val_accuracy: 0.5082\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5120 - val_loss: 0.6935 - val_accuracy: 0.5072\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5115 - val_loss: 0.6940 - val_accuracy: 0.5114\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5121 - val_loss: 0.6933 - val_accuracy: 0.5116\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.5059\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5120 - val_loss: 0.6938 - val_accuracy: 0.5089\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5121 - val_loss: 0.6934 - val_accuracy: 0.5114\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6934 - val_accuracy: 0.5096\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5118 - val_loss: 0.6933 - val_accuracy: 0.5110\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5123 - val_loss: 0.6933 - val_accuracy: 0.5081\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5128 - val_loss: 0.6941 - val_accuracy: 0.5095\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.5125\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5135 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6922 - accuracy: 0.5129 - val_loss: 0.6935 - val_accuracy: 0.5130\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5121 - val_loss: 0.6934 - val_accuracy: 0.5066\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.5107\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5127 - val_loss: 0.6933 - val_accuracy: 0.5104\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5127 - val_loss: 0.6938 - val_accuracy: 0.5096\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6930 - val_accuracy: 0.5147\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5124 - val_loss: 0.6932 - val_accuracy: 0.5088\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5121 - val_loss: 0.6930 - val_accuracy: 0.5153\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5138 - val_loss: 0.6933 - val_accuracy: 0.5133\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6932 - val_accuracy: 0.5146\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5123\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5127\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5124 - val_loss: 0.6930 - val_accuracy: 0.5160\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5140 - val_loss: 0.6930 - val_accuracy: 0.5152\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5130 - val_loss: 0.6932 - val_accuracy: 0.5139\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5126 - val_loss: 0.6929 - val_accuracy: 0.5160\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5129 - val_loss: 0.6929 - val_accuracy: 0.5130\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6932 - val_accuracy: 0.5123\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5130 - val_loss: 0.6930 - val_accuracy: 0.5112\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5131 - val_loss: 0.6930 - val_accuracy: 0.5120\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5167727 ]\n",
      " [0.47017086]\n",
      " [0.49378377]\n",
      " [0.49377072]\n",
      " [0.46918398]\n",
      " [0.47859538]\n",
      " [0.50255364]\n",
      " [0.50840104]\n",
      " [0.49999124]\n",
      " [0.5086561 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_12 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_12 is normal keras bn layer\n",
      "q_activation_12      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 4ms/step - loss: 0.7137 - accuracy: 0.5036 - val_loss: 0.6974 - val_accuracy: 0.5069\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5068 - val_loss: 0.6946 - val_accuracy: 0.5047\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5070 - val_loss: 0.6939 - val_accuracy: 0.5070\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5075 - val_loss: 0.6935 - val_accuracy: 0.5069\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5093 - val_loss: 0.6944 - val_accuracy: 0.5049\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.5042\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5077 - val_loss: 0.6940 - val_accuracy: 0.5074\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6931 - val_accuracy: 0.5055\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6931 - val_accuracy: 0.5083\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5089 - val_loss: 0.6930 - val_accuracy: 0.5027\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5087 - val_loss: 0.6931 - val_accuracy: 0.5069\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6927 - val_accuracy: 0.5076\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5092 - val_loss: 0.6929 - val_accuracy: 0.5098\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5112 - val_loss: 0.6927 - val_accuracy: 0.5126\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5111 - val_loss: 0.6926 - val_accuracy: 0.5115\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5104 - val_loss: 0.6931 - val_accuracy: 0.5090\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5110 - val_loss: 0.6928 - val_accuracy: 0.5098\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5112 - val_loss: 0.6928 - val_accuracy: 0.5065\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5102 - val_loss: 0.6952 - val_accuracy: 0.5119\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5110 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5056\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5108 - val_loss: 0.6927 - val_accuracy: 0.5068\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5116\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5119\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5132 - val_loss: 0.6923 - val_accuracy: 0.5107\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5120 - val_loss: 0.6928 - val_accuracy: 0.5105\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5119 - val_loss: 0.6927 - val_accuracy: 0.5067\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5132\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5137 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6922 - val_accuracy: 0.5137\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5132 - val_loss: 0.6925 - val_accuracy: 0.5142\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5145 - val_loss: 0.6923 - val_accuracy: 0.5127\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6931 - val_accuracy: 0.5087\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5139 - val_loss: 0.6926 - val_accuracy: 0.5149\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5135 - val_loss: 0.6936 - val_accuracy: 0.5041\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5112\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5126 - val_loss: 0.6927 - val_accuracy: 0.5075\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5031\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5139 - val_loss: 0.6932 - val_accuracy: 0.5032\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5145 - val_loss: 0.6924 - val_accuracy: 0.5140\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5131 - val_loss: 0.6930 - val_accuracy: 0.5123\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6924 - val_accuracy: 0.5106\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5131 - val_loss: 0.6923 - val_accuracy: 0.5139\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5135 - val_loss: 0.6922 - val_accuracy: 0.5163\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5151 - val_loss: 0.6929 - val_accuracy: 0.5046\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5143 - val_loss: 0.6923 - val_accuracy: 0.5159\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5138 - val_loss: 0.6924 - val_accuracy: 0.5125\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5146 - val_loss: 0.6925 - val_accuracy: 0.5113\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5130 - val_loss: 0.6922 - val_accuracy: 0.5165\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5146 - val_loss: 0.6922 - val_accuracy: 0.5086\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_12\n",
      "cannot prune layer q_activation_12\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6930 - val_accuracy: 0.5136\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5124 - val_loss: 0.6927 - val_accuracy: 0.5067\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5116 - val_loss: 0.6959 - val_accuracy: 0.5055\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5125\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5094 - val_loss: 0.6928 - val_accuracy: 0.5100\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5112 - val_loss: 0.6926 - val_accuracy: 0.5115\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5117 - val_loss: 0.6932 - val_accuracy: 0.5094\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5133 - val_loss: 0.6925 - val_accuracy: 0.5122\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5128 - val_loss: 0.6925 - val_accuracy: 0.5109\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5121 - val_loss: 0.6925 - val_accuracy: 0.5128\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5116 - val_loss: 0.6924 - val_accuracy: 0.5137\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5090\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5119 - val_loss: 0.6926 - val_accuracy: 0.5065\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5142 - val_loss: 0.6924 - val_accuracy: 0.5131\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5127 - val_loss: 0.6929 - val_accuracy: 0.5122\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5126 - val_loss: 0.6924 - val_accuracy: 0.5123\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5132\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5067\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5129 - val_loss: 0.6926 - val_accuracy: 0.5127\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5131 - val_loss: 0.6927 - val_accuracy: 0.5084\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5128 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5130 - val_loss: 0.6921 - val_accuracy: 0.5139\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6926 - val_accuracy: 0.5086\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6922 - val_accuracy: 0.5145\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5127 - val_loss: 0.6926 - val_accuracy: 0.5096\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5153 - val_loss: 0.6930 - val_accuracy: 0.5042\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6922 - val_accuracy: 0.5112\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5151\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6923 - val_accuracy: 0.5125\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6929 - val_accuracy: 0.5146\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5143 - val_loss: 0.6919 - val_accuracy: 0.5137\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5149 - val_loss: 0.6923 - val_accuracy: 0.5159\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6923 - val_accuracy: 0.5125\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5159 - val_loss: 0.6920 - val_accuracy: 0.5181\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5159 - val_loss: 0.6924 - val_accuracy: 0.5175\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5147 - val_loss: 0.6917 - val_accuracy: 0.5166\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5163 - val_loss: 0.6918 - val_accuracy: 0.5118\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6920 - val_accuracy: 0.5138\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5178 - val_loss: 0.6920 - val_accuracy: 0.5128\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5174 - val_loss: 0.6918 - val_accuracy: 0.5150\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5169 - val_loss: 0.6918 - val_accuracy: 0.5113\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5172 - val_loss: 0.6922 - val_accuracy: 0.5056\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5185 - val_loss: 0.6917 - val_accuracy: 0.5166\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5179 - val_loss: 0.6918 - val_accuracy: 0.5183\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5180 - val_loss: 0.6921 - val_accuracy: 0.5228\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5181 - val_loss: 0.6927 - val_accuracy: 0.5141\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5189 - val_loss: 0.6915 - val_accuracy: 0.5139\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5178 - val_loss: 0.6917 - val_accuracy: 0.5212\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5187 - val_loss: 0.6912 - val_accuracy: 0.5246\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5287694 ]\n",
      " [0.49826488]\n",
      " [0.4560505 ]\n",
      " [0.5176141 ]\n",
      " [0.5195251 ]\n",
      " [0.515619  ]\n",
      " [0.5085752 ]\n",
      " [0.49277112]\n",
      " [0.515619  ]\n",
      " [0.47289672]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_13 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_13 is normal keras bn layer\n",
      "q_activation_13      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 2.2125 - accuracy: 0.5030 - val_loss: 1.3910 - val_accuracy: 0.4978\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 1.0157 - accuracy: 0.4990 - val_loss: 0.8601 - val_accuracy: 0.4950\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.8357 - accuracy: 0.4977 - val_loss: 0.8026 - val_accuracy: 0.4950\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7983 - accuracy: 0.4972 - val_loss: 0.7764 - val_accuracy: 0.4972\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7718 - accuracy: 0.4958 - val_loss: 0.7528 - val_accuracy: 0.4965\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7395 - accuracy: 0.4963 - val_loss: 0.7255 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.4976 - val_loss: 0.7167 - val_accuracy: 0.4944\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7132 - accuracy: 0.4988 - val_loss: 0.7111 - val_accuracy: 0.4961\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7068 - accuracy: 0.4993 - val_loss: 0.7050 - val_accuracy: 0.4991\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.5000 - val_loss: 0.7016 - val_accuracy: 0.4982\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6999 - accuracy: 0.5016 - val_loss: 0.6997 - val_accuracy: 0.4986\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6982 - accuracy: 0.5014 - val_loss: 0.6981 - val_accuracy: 0.4989\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6970 - accuracy: 0.5037 - val_loss: 0.6972 - val_accuracy: 0.5014\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.5050 - val_loss: 0.6961 - val_accuracy: 0.5033\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5058 - val_loss: 0.6955 - val_accuracy: 0.5046\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5057 - val_loss: 0.6951 - val_accuracy: 0.5038\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5069 - val_loss: 0.6947 - val_accuracy: 0.5029\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5063 - val_loss: 0.6946 - val_accuracy: 0.5030\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5072 - val_loss: 0.6945 - val_accuracy: 0.5059\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5071 - val_loss: 0.6942 - val_accuracy: 0.5034\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5079 - val_loss: 0.6938 - val_accuracy: 0.5064\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5104 - val_loss: 0.6945 - val_accuracy: 0.5029\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5084 - val_loss: 0.6939 - val_accuracy: 0.5055\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5098 - val_loss: 0.6934 - val_accuracy: 0.5078\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5124\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6929 - val_accuracy: 0.5138\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6931 - val_accuracy: 0.5125\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5132 - val_loss: 0.6928 - val_accuracy: 0.5139\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5095\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5146\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5139 - val_loss: 0.6926 - val_accuracy: 0.5167\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.5138\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5134\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5182\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5161 - val_loss: 0.6955 - val_accuracy: 0.5078\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6924 - val_accuracy: 0.5209\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5159 - val_loss: 0.6924 - val_accuracy: 0.5169\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5157 - val_loss: 0.6928 - val_accuracy: 0.5124\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6924 - val_accuracy: 0.5208\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5163 - val_loss: 0.6921 - val_accuracy: 0.5202\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5176 - val_loss: 0.6919 - val_accuracy: 0.5219\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5184 - val_loss: 0.6921 - val_accuracy: 0.5197\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5177 - val_loss: 0.6922 - val_accuracy: 0.5205\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5185 - val_loss: 0.6930 - val_accuracy: 0.5178\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5245\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5199 - val_loss: 0.6924 - val_accuracy: 0.5177\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5197 - val_loss: 0.6923 - val_accuracy: 0.5214\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5199 - val_loss: 0.6920 - val_accuracy: 0.5192\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6919 - val_accuracy: 0.5207\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6918 - val_accuracy: 0.5249\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6920 - val_accuracy: 0.5196\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5210 - val_loss: 0.6920 - val_accuracy: 0.5228\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5221 - val_loss: 0.6918 - val_accuracy: 0.5235\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5216 - val_loss: 0.6921 - val_accuracy: 0.5178\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5207\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5228 - val_loss: 0.6921 - val_accuracy: 0.5216\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5232 - val_loss: 0.6913 - val_accuracy: 0.5269\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5221 - val_loss: 0.6918 - val_accuracy: 0.5223\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5234 - val_loss: 0.6918 - val_accuracy: 0.5232\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5231 - val_loss: 0.6923 - val_accuracy: 0.5174\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5232 - val_loss: 0.6913 - val_accuracy: 0.5280\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6926 - val_accuracy: 0.5199\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5245 - val_loss: 0.6913 - val_accuracy: 0.5241\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6912 - val_accuracy: 0.5244\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5234 - val_loss: 0.6919 - val_accuracy: 0.5199\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5229 - val_loss: 0.6915 - val_accuracy: 0.5216\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5225 - val_loss: 0.6927 - val_accuracy: 0.5209\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5231 - val_loss: 0.6915 - val_accuracy: 0.5239\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5241 - val_loss: 0.6912 - val_accuracy: 0.5251\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5247 - val_loss: 0.6914 - val_accuracy: 0.5292\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5238 - val_loss: 0.6915 - val_accuracy: 0.5266\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5244 - val_loss: 0.6911 - val_accuracy: 0.5262\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5256 - val_loss: 0.6909 - val_accuracy: 0.5297\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5240 - val_loss: 0.6922 - val_accuracy: 0.5162\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5249 - val_loss: 0.6907 - val_accuracy: 0.5278\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5254 - val_loss: 0.6905 - val_accuracy: 0.5308\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5253 - val_loss: 0.6912 - val_accuracy: 0.5247\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5258 - val_loss: 0.6915 - val_accuracy: 0.5197\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5255 - val_loss: 0.6907 - val_accuracy: 0.5290\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5262 - val_loss: 0.6904 - val_accuracy: 0.5276\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5243 - val_loss: 0.6914 - val_accuracy: 0.5236\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5235 - val_loss: 0.6911 - val_accuracy: 0.5236\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6916 - val_accuracy: 0.5279\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5254 - val_loss: 0.6964 - val_accuracy: 0.5124\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5197 - val_loss: 0.6919 - val_accuracy: 0.5254\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5233 - val_loss: 0.6915 - val_accuracy: 0.5259\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6912 - val_accuracy: 0.5264\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5233 - val_loss: 0.6913 - val_accuracy: 0.5240\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5237 - val_loss: 0.6910 - val_accuracy: 0.5284\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6910 - val_accuracy: 0.5272\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5234 - val_loss: 0.6911 - val_accuracy: 0.5262\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5242 - val_loss: 0.6915 - val_accuracy: 0.5219\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6910 - val_accuracy: 0.5265\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5241 - val_loss: 0.6911 - val_accuracy: 0.5230\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5242 - val_loss: 0.6910 - val_accuracy: 0.5230\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5241 - val_loss: 0.6908 - val_accuracy: 0.5284\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5250 - val_loss: 0.6917 - val_accuracy: 0.5160\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5225 - val_loss: 0.6916 - val_accuracy: 0.5249\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_13\n",
      "cannot prune layer q_activation_13\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 5ms/step - loss: 0.6908 - accuracy: 0.5245 - val_loss: 0.6949 - val_accuracy: 0.5133\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5165 - val_loss: 0.6921 - val_accuracy: 0.5166\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5184 - val_loss: 0.6940 - val_accuracy: 0.5112\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5148 - val_loss: 0.6924 - val_accuracy: 0.5111\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6918 - val_accuracy: 0.5177\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5172 - val_loss: 0.6930 - val_accuracy: 0.5144\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5173 - val_loss: 0.6949 - val_accuracy: 0.5132\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5147 - val_loss: 0.6921 - val_accuracy: 0.5181\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5152 - val_loss: 0.6919 - val_accuracy: 0.5179\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5159 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5108\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6920 - val_accuracy: 0.5144\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5181 - val_loss: 0.6920 - val_accuracy: 0.5179\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5184 - val_loss: 0.6920 - val_accuracy: 0.5234\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5191 - val_loss: 0.6918 - val_accuracy: 0.5204\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5186 - val_loss: 0.6917 - val_accuracy: 0.5206\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6920 - val_accuracy: 0.5227\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5179 - val_loss: 0.6923 - val_accuracy: 0.5147\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5198 - val_loss: 0.6916 - val_accuracy: 0.5222\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5199 - val_loss: 0.6918 - val_accuracy: 0.5230\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5199 - val_loss: 0.6915 - val_accuracy: 0.5240\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5195 - val_loss: 0.6916 - val_accuracy: 0.5226\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5204 - val_loss: 0.6915 - val_accuracy: 0.5242\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5205 - val_loss: 0.6914 - val_accuracy: 0.5215\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5203 - val_loss: 0.6919 - val_accuracy: 0.5235\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5197 - val_loss: 0.6917 - val_accuracy: 0.5186\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5207 - val_loss: 0.6915 - val_accuracy: 0.5222\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5202 - val_loss: 0.6915 - val_accuracy: 0.5238\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5210 - val_loss: 0.6916 - val_accuracy: 0.5220\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5213 - val_loss: 0.6915 - val_accuracy: 0.5238\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5215 - val_loss: 0.6914 - val_accuracy: 0.5210\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5206 - val_loss: 0.6917 - val_accuracy: 0.5202\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5205 - val_loss: 0.6916 - val_accuracy: 0.5219\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5215 - val_loss: 0.6917 - val_accuracy: 0.5251\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5223 - val_loss: 0.6918 - val_accuracy: 0.5214\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5218 - val_loss: 0.6914 - val_accuracy: 0.5247\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5209 - val_loss: 0.6917 - val_accuracy: 0.5224\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5220 - val_loss: 0.6913 - val_accuracy: 0.5227\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5218 - val_loss: 0.6913 - val_accuracy: 0.5283\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5207 - val_loss: 0.6922 - val_accuracy: 0.5203\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5216 - val_loss: 0.6912 - val_accuracy: 0.5227\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5220 - val_loss: 0.6912 - val_accuracy: 0.5259\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5227 - val_loss: 0.6915 - val_accuracy: 0.5245\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6913 - val_accuracy: 0.5236\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5197 - val_loss: 0.6920 - val_accuracy: 0.5199\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5208 - val_loss: 0.6923 - val_accuracy: 0.5198\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5208 - val_loss: 0.6919 - val_accuracy: 0.5180\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5206 - val_loss: 0.6919 - val_accuracy: 0.5201\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5212 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5184\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.51970047]\n",
      " [0.4964528 ]\n",
      " [0.4568876 ]\n",
      " [0.5000882 ]\n",
      " [0.4960817 ]\n",
      " [0.5071995 ]\n",
      " [0.39113945]\n",
      " [0.49285823]\n",
      " [0.48911703]\n",
      " [0.5014577 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_14 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_14 is normal keras bn layer\n",
      "q_activation_14      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.0163 - accuracy: 0.5016 - val_loss: 0.7790 - val_accuracy: 0.5019\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7573 - accuracy: 0.4998 - val_loss: 0.7355 - val_accuracy: 0.5040\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7295 - accuracy: 0.4991 - val_loss: 0.7198 - val_accuracy: 0.5027\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.4982 - val_loss: 0.7101 - val_accuracy: 0.5042\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7110 - accuracy: 0.4996 - val_loss: 0.7058 - val_accuracy: 0.5040\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7079 - accuracy: 0.5007 - val_loss: 0.7021 - val_accuracy: 0.5041\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7057 - accuracy: 0.5004 - val_loss: 0.7148 - val_accuracy: 0.5019\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7039 - accuracy: 0.5006 - val_loss: 0.6995 - val_accuracy: 0.5036\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7023 - accuracy: 0.5001 - val_loss: 0.7110 - val_accuracy: 0.5004\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7008 - accuracy: 0.5009 - val_loss: 0.6969 - val_accuracy: 0.5050\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.5017 - val_loss: 0.6957 - val_accuracy: 0.5032\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6987 - accuracy: 0.5019 - val_loss: 0.6952 - val_accuracy: 0.5052\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5046 - val_loss: 0.6949 - val_accuracy: 0.5062\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6973 - accuracy: 0.5034 - val_loss: 0.6944 - val_accuracy: 0.5082\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6969 - accuracy: 0.5050 - val_loss: 0.6940 - val_accuracy: 0.5104\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6966 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.5145\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5082 - val_loss: 0.6937 - val_accuracy: 0.5090\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6965 - accuracy: 0.5075 - val_loss: 0.7058 - val_accuracy: 0.5018\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5086 - val_loss: 0.6940 - val_accuracy: 0.5101\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6962 - accuracy: 0.5083 - val_loss: 0.6940 - val_accuracy: 0.5098\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6959 - accuracy: 0.5086 - val_loss: 0.6942 - val_accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6958 - accuracy: 0.5089 - val_loss: 0.6938 - val_accuracy: 0.5090\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5078 - val_loss: 0.6938 - val_accuracy: 0.5088\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5085 - val_loss: 0.6938 - val_accuracy: 0.5109\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5095 - val_loss: 0.6936 - val_accuracy: 0.5119\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5090 - val_loss: 0.6935 - val_accuracy: 0.5124\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5100 - val_loss: 0.6937 - val_accuracy: 0.5128\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.5082 - val_loss: 0.6931 - val_accuracy: 0.5140\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5106 - val_loss: 0.6930 - val_accuracy: 0.5154\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5100 - val_loss: 0.6939 - val_accuracy: 0.5106\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.5150\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5106 - val_loss: 0.6929 - val_accuracy: 0.5156\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5109 - val_loss: 0.7013 - val_accuracy: 0.5055\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5106 - val_loss: 0.6930 - val_accuracy: 0.5150\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5109 - val_loss: 0.6932 - val_accuracy: 0.5144\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5116 - val_loss: 0.6986 - val_accuracy: 0.5032\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5102 - val_loss: 0.6932 - val_accuracy: 0.5156\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5116 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5104 - val_loss: 0.6931 - val_accuracy: 0.5168\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5119 - val_loss: 0.6938 - val_accuracy: 0.5116\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5110 - val_loss: 0.6975 - val_accuracy: 0.5032\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5108 - val_loss: 0.6937 - val_accuracy: 0.5122\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5112 - val_loss: 0.6967 - val_accuracy: 0.5088\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5111 - val_loss: 0.6956 - val_accuracy: 0.5049\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5098 - val_loss: 0.6952 - val_accuracy: 0.5099\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5111 - val_loss: 0.6949 - val_accuracy: 0.5038\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5111 - val_loss: 0.6939 - val_accuracy: 0.5120\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6939 - accuracy: 0.5103 - val_loss: 0.6948 - val_accuracy: 0.5106\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5113 - val_loss: 0.6939 - val_accuracy: 0.5074\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5103 - val_loss: 0.6935 - val_accuracy: 0.5170\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5122 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5132 - val_loss: 0.6935 - val_accuracy: 0.5119\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_14\n",
      "cannot prune layer q_activation_14\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6944 - accuracy: 0.5100 - val_loss: 0.6944 - val_accuracy: 0.5128\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5098 - val_loss: 0.6946 - val_accuracy: 0.5106\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5080 - val_loss: 0.6941 - val_accuracy: 0.5102\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5076 - val_loss: 0.6939 - val_accuracy: 0.5092\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5075 - val_loss: 0.6960 - val_accuracy: 0.5084\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5075 - val_loss: 0.6959 - val_accuracy: 0.5045\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5073 - val_loss: 0.6960 - val_accuracy: 0.5032\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5073 - val_loss: 0.6950 - val_accuracy: 0.5026\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5079 - val_loss: 0.6970 - val_accuracy: 0.5017\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5072 - val_loss: 0.6940 - val_accuracy: 0.5144\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5091 - val_loss: 0.6941 - val_accuracy: 0.5132\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5098 - val_loss: 0.6950 - val_accuracy: 0.5101\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5105 - val_loss: 0.6933 - val_accuracy: 0.5059\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5102 - val_loss: 0.6932 - val_accuracy: 0.5164\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5108 - val_loss: 0.6931 - val_accuracy: 0.5061\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5109 - val_loss: 0.6930 - val_accuracy: 0.5133\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5106 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5122 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5056\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6929 - val_accuracy: 0.5040\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5133 - val_loss: 0.6935 - val_accuracy: 0.5099\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6934 - val_accuracy: 0.5084\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6929 - val_accuracy: 0.5054\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5133 - val_loss: 0.6929 - val_accuracy: 0.5163\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5080\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5139\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5128 - val_loss: 0.6930 - val_accuracy: 0.5088\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5137 - val_loss: 0.6929 - val_accuracy: 0.5133\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5132 - val_loss: 0.6928 - val_accuracy: 0.5141\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5148\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5129 - val_loss: 0.6929 - val_accuracy: 0.5136\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5166\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6929 - val_accuracy: 0.5115\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6929 - val_accuracy: 0.5149\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5160\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6929 - val_accuracy: 0.5145\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5143 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5143 - val_loss: 0.6930 - val_accuracy: 0.5122\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5125\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6928 - val_accuracy: 0.5142\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5151 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6930 - val_accuracy: 0.5015\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5140 - val_loss: 0.6931 - val_accuracy: 0.5160\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5145 - val_loss: 0.6927 - val_accuracy: 0.5169\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5138 - val_loss: 0.6927 - val_accuracy: 0.5160\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.50682414]\n",
      " [0.50682414]\n",
      " [0.47186506]\n",
      " [0.50682414]\n",
      " [0.46808618]\n",
      " [0.49051717]\n",
      " [0.51527476]\n",
      " [0.5034162 ]\n",
      " [0.49135882]\n",
      " [0.49400228]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_15 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_15 is normal keras bn layer\n",
      "q_activation_15      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 0.9444 - accuracy: 0.4988 - val_loss: 0.7013 - val_accuracy: 0.4999\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6965 - accuracy: 0.5031 - val_loss: 0.6949 - val_accuracy: 0.5062\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5054 - val_loss: 0.6940 - val_accuracy: 0.5085\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.5062 - val_loss: 0.6936 - val_accuracy: 0.5081\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5070 - val_loss: 0.6935 - val_accuracy: 0.5090\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.5073 - val_loss: 0.6932 - val_accuracy: 0.5066\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5081 - val_loss: 0.6943 - val_accuracy: 0.5072\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5074 - val_loss: 0.6939 - val_accuracy: 0.5077\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6963 - val_accuracy: 0.5039\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5074 - val_loss: 0.6939 - val_accuracy: 0.5087\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5083 - val_loss: 0.6933 - val_accuracy: 0.5049\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5074 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6951 - val_accuracy: 0.5066\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5088 - val_loss: 0.6938 - val_accuracy: 0.5044\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5083 - val_loss: 0.6935 - val_accuracy: 0.5076\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6934 - val_accuracy: 0.5071\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5084 - val_loss: 0.6928 - val_accuracy: 0.5098\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5074\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5103 - val_loss: 0.6938 - val_accuracy: 0.5079\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5097 - val_loss: 0.6935 - val_accuracy: 0.5022\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5092 - val_loss: 0.6929 - val_accuracy: 0.5086\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5105 - val_loss: 0.6934 - val_accuracy: 0.5087\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5072\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5125\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5103 - val_loss: 0.6941 - val_accuracy: 0.5078\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5116 - val_loss: 0.6933 - val_accuracy: 0.5092\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6937 - val_accuracy: 0.5119\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5120 - val_loss: 0.6937 - val_accuracy: 0.5026\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5122 - val_loss: 0.6929 - val_accuracy: 0.5051\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5116 - val_loss: 0.6929 - val_accuracy: 0.5144\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5119 - val_loss: 0.6928 - val_accuracy: 0.5116\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5132 - val_loss: 0.6925 - val_accuracy: 0.5138\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5130 - val_loss: 0.6932 - val_accuracy: 0.5055\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5118 - val_loss: 0.6931 - val_accuracy: 0.5093\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5129 - val_loss: 0.6925 - val_accuracy: 0.5149\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5125 - val_loss: 0.6926 - val_accuracy: 0.5106\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5122 - val_loss: 0.6925 - val_accuracy: 0.5118\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5144 - val_loss: 0.6929 - val_accuracy: 0.5119\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5131 - val_loss: 0.6925 - val_accuracy: 0.5092\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5131 - val_loss: 0.6925 - val_accuracy: 0.5099\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5143 - val_loss: 0.6926 - val_accuracy: 0.5110\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5138 - val_loss: 0.6926 - val_accuracy: 0.5131\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5139 - val_loss: 0.6923 - val_accuracy: 0.5135\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5131 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5138 - val_loss: 0.6923 - val_accuracy: 0.5127\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5157\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5107\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5125 - val_loss: 0.6926 - val_accuracy: 0.5106\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5134 - val_loss: 0.6923 - val_accuracy: 0.5137\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5142 - val_loss: 0.6922 - val_accuracy: 0.5128\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5144 - val_loss: 0.6924 - val_accuracy: 0.5163\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5133 - val_loss: 0.6924 - val_accuracy: 0.5110\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5153 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5143 - val_loss: 0.6922 - val_accuracy: 0.5127\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5134 - val_loss: 0.6934 - val_accuracy: 0.5137\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5147 - val_loss: 0.6922 - val_accuracy: 0.5136\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5141 - val_loss: 0.6920 - val_accuracy: 0.5156\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5146 - val_loss: 0.6922 - val_accuracy: 0.5123\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5145 - val_loss: 0.6922 - val_accuracy: 0.5160\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5155 - val_loss: 0.6920 - val_accuracy: 0.5096\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6928 - val_accuracy: 0.5088\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5141 - val_loss: 0.6922 - val_accuracy: 0.5102\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5148 - val_loss: 0.6923 - val_accuracy: 0.5073\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.5131\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5148 - val_loss: 0.6924 - val_accuracy: 0.5154\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5158 - val_loss: 0.6922 - val_accuracy: 0.5096\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5155 - val_loss: 0.6923 - val_accuracy: 0.5142\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5155 - val_loss: 0.6941 - val_accuracy: 0.5087\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5148 - val_loss: 0.6923 - val_accuracy: 0.5108\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5122\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5149 - val_loss: 0.6919 - val_accuracy: 0.5143\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5154 - val_loss: 0.6917 - val_accuracy: 0.5133\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5154 - val_loss: 0.6925 - val_accuracy: 0.5161\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6923 - val_accuracy: 0.5117\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5076\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5149 - val_loss: 0.6931 - val_accuracy: 0.5045\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5150 - val_loss: 0.6920 - val_accuracy: 0.5111\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6917 - val_accuracy: 0.5154\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5149 - val_loss: 0.6924 - val_accuracy: 0.5122\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5155 - val_loss: 0.6924 - val_accuracy: 0.5154\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5167 - val_loss: 0.6919 - val_accuracy: 0.5140\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5160 - val_loss: 0.6922 - val_accuracy: 0.5146\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5157 - val_loss: 0.6921 - val_accuracy: 0.5165\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5154 - val_loss: 0.6925 - val_accuracy: 0.5075\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6927 - val_accuracy: 0.5078\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5157 - val_loss: 0.6922 - val_accuracy: 0.5116\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6916 - val_accuracy: 0.5138\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5148 - val_loss: 0.6920 - val_accuracy: 0.5131\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6919 - val_accuracy: 0.5143\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5168 - val_loss: 0.6919 - val_accuracy: 0.5144\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5151 - val_loss: 0.6921 - val_accuracy: 0.5118\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5147 - val_loss: 0.6921 - val_accuracy: 0.5151\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5161 - val_loss: 0.6918 - val_accuracy: 0.5166\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5138 - val_loss: 0.6918 - val_accuracy: 0.5139\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5155 - val_loss: 0.6922 - val_accuracy: 0.5107\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5156 - val_loss: 0.6920 - val_accuracy: 0.5117\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5159 - val_loss: 0.6919 - val_accuracy: 0.5130\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5152 - val_loss: 0.6919 - val_accuracy: 0.5178\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5166\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5156 - val_loss: 0.6916 - val_accuracy: 0.5129\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_15\n",
      "cannot prune layer q_activation_15\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6915 - accuracy: 0.5159 - val_loss: 0.6920 - val_accuracy: 0.5151\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6968 - val_accuracy: 0.5088\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5095\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5120 - val_loss: 0.6939 - val_accuracy: 0.5057\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5113 - val_loss: 0.6931 - val_accuracy: 0.5064\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5093 - val_loss: 0.6936 - val_accuracy: 0.5059\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5104 - val_loss: 0.6928 - val_accuracy: 0.5035\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5099 - val_loss: 0.6928 - val_accuracy: 0.5046\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5097 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5115 - val_loss: 0.6926 - val_accuracy: 0.5065\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5107 - val_loss: 0.6924 - val_accuracy: 0.5092\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5117 - val_loss: 0.6924 - val_accuracy: 0.5091\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5107 - val_loss: 0.6925 - val_accuracy: 0.5082\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5127 - val_loss: 0.6924 - val_accuracy: 0.5092\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5105 - val_loss: 0.6925 - val_accuracy: 0.5102\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5120 - val_loss: 0.6923 - val_accuracy: 0.5076\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5108 - val_loss: 0.6925 - val_accuracy: 0.5100\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5117 - val_loss: 0.6924 - val_accuracy: 0.5111\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5115 - val_loss: 0.6924 - val_accuracy: 0.5106\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5117 - val_loss: 0.6922 - val_accuracy: 0.5098\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5109 - val_loss: 0.6922 - val_accuracy: 0.5127\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5112 - val_loss: 0.6925 - val_accuracy: 0.5096\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5122 - val_loss: 0.6926 - val_accuracy: 0.5070\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5123 - val_loss: 0.6923 - val_accuracy: 0.5086\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5114 - val_loss: 0.6927 - val_accuracy: 0.5107\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5120 - val_loss: 0.6925 - val_accuracy: 0.5104\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5115 - val_loss: 0.6927 - val_accuracy: 0.5119\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5128 - val_loss: 0.6922 - val_accuracy: 0.5110\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5117 - val_loss: 0.6923 - val_accuracy: 0.5077\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5118 - val_loss: 0.6923 - val_accuracy: 0.5125\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5110 - val_loss: 0.6926 - val_accuracy: 0.5066\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5120 - val_loss: 0.6922 - val_accuracy: 0.5089\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5110 - val_loss: 0.6926 - val_accuracy: 0.5113\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5118 - val_loss: 0.6924 - val_accuracy: 0.5098\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5117 - val_loss: 0.6923 - val_accuracy: 0.5109\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5118\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5114 - val_loss: 0.6922 - val_accuracy: 0.5113\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5124 - val_loss: 0.6921 - val_accuracy: 0.5110\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5119 - val_loss: 0.6924 - val_accuracy: 0.5098\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5115 - val_loss: 0.6924 - val_accuracy: 0.5141\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5123 - val_loss: 0.6923 - val_accuracy: 0.5141\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5119 - val_loss: 0.6923 - val_accuracy: 0.5124\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5121 - val_loss: 0.6926 - val_accuracy: 0.5144\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5127 - val_loss: 0.6923 - val_accuracy: 0.5101\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5118 - val_loss: 0.6919 - val_accuracy: 0.5147\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5147 - val_loss: 0.6920 - val_accuracy: 0.5076\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5126 - val_loss: 0.6919 - val_accuracy: 0.5127\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5128 - val_loss: 0.6921 - val_accuracy: 0.5104\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5138 - val_loss: 0.6920 - val_accuracy: 0.5142\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5125\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.50390625]\n",
      " [0.50390625]\n",
      " [0.53320223]\n",
      " [0.50390625]\n",
      " [0.50390625]\n",
      " [0.53320223]\n",
      " [0.47216904]\n",
      " [0.53320223]\n",
      " [0.56602585]\n",
      " [0.50390625]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_16 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_16 is normal keras bn layer\n",
      "q_activation_16      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 1.1746 - accuracy: 0.4992 - val_loss: 0.7708 - val_accuracy: 0.4999\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7373 - accuracy: 0.5000 - val_loss: 0.7285 - val_accuracy: 0.4961\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7196 - accuracy: 0.4999 - val_loss: 0.7161 - val_accuracy: 0.4947\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7108 - accuracy: 0.5012 - val_loss: 0.7073 - val_accuracy: 0.4963\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7060 - accuracy: 0.5018 - val_loss: 0.7037 - val_accuracy: 0.4972\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7033 - accuracy: 0.5020 - val_loss: 0.7022 - val_accuracy: 0.4975\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7006 - accuracy: 0.5032 - val_loss: 0.6996 - val_accuracy: 0.4986\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6985 - accuracy: 0.5031 - val_loss: 0.6980 - val_accuracy: 0.4987\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5037 - val_loss: 0.6968 - val_accuracy: 0.5010\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6967 - accuracy: 0.5028 - val_loss: 0.6962 - val_accuracy: 0.5021\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.5040 - val_loss: 0.6956 - val_accuracy: 0.5034\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6954 - accuracy: 0.5048 - val_loss: 0.6953 - val_accuracy: 0.5041\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5050 - val_loss: 0.6945 - val_accuracy: 0.5034\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5060 - val_loss: 0.6943 - val_accuracy: 0.5070\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6957 - accuracy: 0.5061 - val_loss: 0.6966 - val_accuracy: 0.5050\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5070 - val_loss: 0.6937 - val_accuracy: 0.5087\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5076 - val_loss: 0.6935 - val_accuracy: 0.5052\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5079 - val_loss: 0.6937 - val_accuracy: 0.5071\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5087 - val_loss: 0.6935 - val_accuracy: 0.5088\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5086 - val_loss: 0.6939 - val_accuracy: 0.5081\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5095 - val_loss: 0.6937 - val_accuracy: 0.5082\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5098 - val_loss: 0.6932 - val_accuracy: 0.5093\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5101 - val_loss: 0.6930 - val_accuracy: 0.5116\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5112\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5102 - val_loss: 0.6930 - val_accuracy: 0.5037\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6932 - val_accuracy: 0.5131\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5120 - val_loss: 0.6930 - val_accuracy: 0.5137\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5121 - val_loss: 0.6937 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5126 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5124 - val_loss: 0.6930 - val_accuracy: 0.5127\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5123 - val_loss: 0.6929 - val_accuracy: 0.5117\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5129\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6930 - val_accuracy: 0.5103\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5119 - val_loss: 0.6931 - val_accuracy: 0.5101\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5127\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5119\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6933 - val_accuracy: 0.5073\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5131 - val_loss: 0.6928 - val_accuracy: 0.5162\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5143\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5130\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6937 - val_accuracy: 0.5078\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5150 - val_loss: 0.6927 - val_accuracy: 0.5085\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5146 - val_loss: 0.6942 - val_accuracy: 0.4990\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6928 - val_accuracy: 0.5140\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5141 - val_loss: 0.6931 - val_accuracy: 0.5063\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5149 - val_loss: 0.6932 - val_accuracy: 0.5108\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6935 - val_accuracy: 0.5113\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5148 - val_loss: 0.6927 - val_accuracy: 0.5148\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5154 - val_loss: 0.6932 - val_accuracy: 0.5127\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5154 - val_loss: 0.6927 - val_accuracy: 0.5155\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5157 - val_loss: 0.6930 - val_accuracy: 0.5151\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5160 - val_loss: 0.6926 - val_accuracy: 0.5158\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5160 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5171 - val_loss: 0.6926 - val_accuracy: 0.5147\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5167 - val_loss: 0.6932 - val_accuracy: 0.5096\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5165 - val_loss: 0.6925 - val_accuracy: 0.5158\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6936 - val_accuracy: 0.5129\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6928 - val_accuracy: 0.5151\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5163\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5183 - val_loss: 0.6924 - val_accuracy: 0.5181\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5184 - val_loss: 0.6929 - val_accuracy: 0.5183\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5188 - val_loss: 0.6924 - val_accuracy: 0.5173\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5183 - val_loss: 0.6928 - val_accuracy: 0.5160\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5192 - val_loss: 0.6928 - val_accuracy: 0.5145\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5189 - val_loss: 0.6926 - val_accuracy: 0.5150\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5179 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5180 - val_loss: 0.6926 - val_accuracy: 0.5196\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5189 - val_loss: 0.6925 - val_accuracy: 0.5179\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5192 - val_loss: 0.6922 - val_accuracy: 0.5202\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5194 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5194 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5201 - val_loss: 0.6926 - val_accuracy: 0.5193\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5186\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5195 - val_loss: 0.6924 - val_accuracy: 0.5185\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5199 - val_loss: 0.6925 - val_accuracy: 0.5183\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5196 - val_loss: 0.6925 - val_accuracy: 0.5167\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5194 - val_loss: 0.6922 - val_accuracy: 0.5166\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5202 - val_loss: 0.6925 - val_accuracy: 0.5178\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5207 - val_loss: 0.6923 - val_accuracy: 0.5187\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5204 - val_loss: 0.6927 - val_accuracy: 0.5123\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5204 - val_loss: 0.6923 - val_accuracy: 0.5208\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5217 - val_loss: 0.6926 - val_accuracy: 0.5181\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5219 - val_loss: 0.6922 - val_accuracy: 0.5208\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5215 - val_loss: 0.6924 - val_accuracy: 0.5175\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5219 - val_loss: 0.6931 - val_accuracy: 0.5185\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5223 - val_loss: 0.6921 - val_accuracy: 0.5233\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6927 - val_accuracy: 0.5111\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5230 - val_loss: 0.6920 - val_accuracy: 0.5220\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5225 - val_loss: 0.6923 - val_accuracy: 0.5188\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5232 - val_loss: 0.6918 - val_accuracy: 0.5227\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5238 - val_loss: 0.6924 - val_accuracy: 0.5210\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5250 - val_loss: 0.6923 - val_accuracy: 0.5183\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5254 - val_loss: 0.6921 - val_accuracy: 0.5217\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5246 - val_loss: 0.6918 - val_accuracy: 0.5257\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5251 - val_loss: 0.6918 - val_accuracy: 0.5215\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_16\n",
      "cannot prune layer q_activation_16\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6910 - accuracy: 0.5249 - val_loss: 0.6957 - val_accuracy: 0.5032\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5225 - val_loss: 0.7022 - val_accuracy: 0.5092\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5158 - val_loss: 0.6982 - val_accuracy: 0.5029\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5147 - val_loss: 0.6943 - val_accuracy: 0.5184\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5149 - val_loss: 0.6985 - val_accuracy: 0.5113\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6934 - val_accuracy: 0.5092\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6930 - val_accuracy: 0.5146\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5059\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5165\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6922 - val_accuracy: 0.5170\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5161 - val_loss: 0.6922 - val_accuracy: 0.5171\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5160 - val_loss: 0.6922 - val_accuracy: 0.5160\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6921 - accuracy: 0.5162 - val_loss: 0.6920 - val_accuracy: 0.5176\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6921 - val_accuracy: 0.5137\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5173 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5171 - val_loss: 0.6920 - val_accuracy: 0.5181\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5160\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5158\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5171 - val_loss: 0.6922 - val_accuracy: 0.5145\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5166 - val_loss: 0.6919 - val_accuracy: 0.5195\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5172 - val_loss: 0.6920 - val_accuracy: 0.5198\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6919 - val_accuracy: 0.5188\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5181 - val_loss: 0.6920 - val_accuracy: 0.5152\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5160 - val_loss: 0.6923 - val_accuracy: 0.5146\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6922 - accuracy: 0.5134 - val_loss: 0.6923 - val_accuracy: 0.5150\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5141 - val_loss: 0.6921 - val_accuracy: 0.5167\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5170 - val_loss: 0.6920 - val_accuracy: 0.5168\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5170 - val_loss: 0.6919 - val_accuracy: 0.5206\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5173 - val_loss: 0.6919 - val_accuracy: 0.5179\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5190 - val_loss: 0.6920 - val_accuracy: 0.5202\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5174 - val_loss: 0.6919 - val_accuracy: 0.5158\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5180 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.5166\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5177 - val_loss: 0.6918 - val_accuracy: 0.5199\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5184 - val_loss: 0.6918 - val_accuracy: 0.5185\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5179 - val_loss: 0.6919 - val_accuracy: 0.5160\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5186 - val_loss: 0.6919 - val_accuracy: 0.5172\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5181 - val_loss: 0.6919 - val_accuracy: 0.5184\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5194 - val_loss: 0.6919 - val_accuracy: 0.5194\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6919 - accuracy: 0.5173 - val_loss: 0.6920 - val_accuracy: 0.5132\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5181 - val_loss: 0.6918 - val_accuracy: 0.5154\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5186 - val_loss: 0.6918 - val_accuracy: 0.5178\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5175 - val_loss: 0.6921 - val_accuracy: 0.5200\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5183 - val_loss: 0.6920 - val_accuracy: 0.5177\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5179 - val_loss: 0.6918 - val_accuracy: 0.5206\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5183 - val_loss: 0.6919 - val_accuracy: 0.5192\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5187 - val_loss: 0.6918 - val_accuracy: 0.5196\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5187 - val_loss: 0.6919 - val_accuracy: 0.5174\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5186 - val_loss: 0.6917 - val_accuracy: 0.5202\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5191 - val_loss: 0.6919 - val_accuracy: 0.5212\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.4917773 ]\n",
      " [0.52100885]\n",
      " [0.45864356]\n",
      " [0.5146806 ]\n",
      " [0.4999932 ]\n",
      " [0.53594697]\n",
      " [0.459651  ]\n",
      " [0.52412677]\n",
      " [0.51268566]\n",
      " [0.5242003 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_17 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889 (3.47 KB)\n",
      "Trainable params: 873 (3.41 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_17 is normal keras bn layer\n",
      "q_activation_17      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 4s 5ms/step - loss: 2.4865 - accuracy: 0.4992 - val_loss: 1.9730 - val_accuracy: 0.5026\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 1.7705 - accuracy: 0.4990 - val_loss: 1.4339 - val_accuracy: 0.5010\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 1.2635 - accuracy: 0.4995 - val_loss: 1.0687 - val_accuracy: 0.4991\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.9246 - accuracy: 0.4984 - val_loss: 0.7760 - val_accuracy: 0.4984\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7572 - accuracy: 0.4976 - val_loss: 0.7412 - val_accuracy: 0.4998\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7351 - accuracy: 0.4986 - val_loss: 0.7292 - val_accuracy: 0.5007\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7251 - accuracy: 0.4989 - val_loss: 0.7190 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7194 - accuracy: 0.4984 - val_loss: 0.7149 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7147 - accuracy: 0.5001 - val_loss: 0.7122 - val_accuracy: 0.5040\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7110 - accuracy: 0.5006 - val_loss: 0.7076 - val_accuracy: 0.5049\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7075 - accuracy: 0.5005 - val_loss: 0.7052 - val_accuracy: 0.5057\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7050 - accuracy: 0.5018 - val_loss: 0.7028 - val_accuracy: 0.5069\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7031 - accuracy: 0.5015 - val_loss: 0.7010 - val_accuracy: 0.5068\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.7014 - accuracy: 0.5022 - val_loss: 0.6996 - val_accuracy: 0.5069\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6999 - accuracy: 0.5022 - val_loss: 0.6984 - val_accuracy: 0.5075\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6984 - accuracy: 0.5035 - val_loss: 0.6974 - val_accuracy: 0.5065\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5043 - val_loss: 0.6965 - val_accuracy: 0.5046\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6963 - accuracy: 0.5046 - val_loss: 0.6957 - val_accuracy: 0.5050\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5045 - val_loss: 0.6951 - val_accuracy: 0.5057\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6952 - accuracy: 0.5050 - val_loss: 0.6947 - val_accuracy: 0.5061\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6948 - accuracy: 0.5056 - val_loss: 0.6945 - val_accuracy: 0.5081\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.5061 - val_loss: 0.6941 - val_accuracy: 0.5101\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5065 - val_loss: 0.6940 - val_accuracy: 0.5096\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6941 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5072\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6940 - accuracy: 0.5073 - val_loss: 0.6939 - val_accuracy: 0.5075\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5078 - val_loss: 0.6938 - val_accuracy: 0.5089\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5074 - val_loss: 0.6936 - val_accuracy: 0.5091\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5080 - val_loss: 0.6932 - val_accuracy: 0.5103\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5077 - val_loss: 0.6963 - val_accuracy: 0.5049\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5069 - val_loss: 0.6932 - val_accuracy: 0.5106\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5078 - val_loss: 0.6933 - val_accuracy: 0.5077\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5078\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6933 - accuracy: 0.5090 - val_loss: 0.6932 - val_accuracy: 0.5089\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5093 - val_loss: 0.6932 - val_accuracy: 0.5087\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6932 - val_accuracy: 0.5085\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5081 - val_loss: 0.6933 - val_accuracy: 0.5051\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5087\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5100 - val_loss: 0.6930 - val_accuracy: 0.5097\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5097 - val_loss: 0.6930 - val_accuracy: 0.5089\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5076\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5104 - val_loss: 0.6930 - val_accuracy: 0.5097\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6929 - val_accuracy: 0.5113\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5094\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.5054\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5099 - val_loss: 0.6929 - val_accuracy: 0.5108\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5104\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5112 - val_loss: 0.6930 - val_accuracy: 0.5077\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5138\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.5107\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5116\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6929 - val_accuracy: 0.5117\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5115 - val_loss: 0.6928 - val_accuracy: 0.5107\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6932 - val_accuracy: 0.5105\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5096\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5114 - val_loss: 0.6929 - val_accuracy: 0.5103\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6928 - val_accuracy: 0.5130\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5115 - val_loss: 0.6930 - val_accuracy: 0.5112\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5115\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5116 - val_loss: 0.6930 - val_accuracy: 0.5048\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5129 - val_loss: 0.6935 - val_accuracy: 0.5100\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.5032\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5121 - val_loss: 0.6952 - val_accuracy: 0.5118\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5122 - val_loss: 0.6926 - val_accuracy: 0.5134\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5115 - val_loss: 0.6956 - val_accuracy: 0.5058\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5134 - val_loss: 0.6928 - val_accuracy: 0.5136\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6929 - val_accuracy: 0.5134\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6931 - val_accuracy: 0.5108\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5135 - val_loss: 0.6929 - val_accuracy: 0.5123\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5131\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6929 - val_accuracy: 0.5154\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6928 - val_accuracy: 0.5168\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6930 - val_accuracy: 0.5103\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5092\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5133 - val_loss: 0.6928 - val_accuracy: 0.5144\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5150\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5139 - val_loss: 0.6927 - val_accuracy: 0.5151\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6927 - val_accuracy: 0.5164\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6929 - val_accuracy: 0.5137\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5138 - val_loss: 0.6928 - val_accuracy: 0.5148\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6927 - val_accuracy: 0.5165\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6923 - accuracy: 0.5147 - val_loss: 0.6927 - val_accuracy: 0.5132\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_17\n",
      "cannot prune layer q_activation_17\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 5s 5ms/step - loss: 0.6926 - accuracy: 0.5128 - val_loss: 0.6952 - val_accuracy: 0.5089\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5108 - val_loss: 0.6955 - val_accuracy: 0.4998\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6994 - val_accuracy: 0.5049\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6934 - accuracy: 0.5041 - val_loss: 0.6958 - val_accuracy: 0.5033\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6932 - accuracy: 0.5065 - val_loss: 0.7036 - val_accuracy: 0.5002\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6937 - accuracy: 0.5062 - val_loss: 0.6946 - val_accuracy: 0.5038\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5072 - val_loss: 0.6935 - val_accuracy: 0.5083\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.5081 - val_loss: 0.7023 - val_accuracy: 0.5069\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6935 - accuracy: 0.5070 - val_loss: 0.6946 - val_accuracy: 0.5097\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.5089\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5093 - val_loss: 0.6939 - val_accuracy: 0.5092\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5085 - val_loss: 0.6985 - val_accuracy: 0.5038\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5056\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6931 - val_accuracy: 0.5122\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5085\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5092 - val_loss: 0.6931 - val_accuracy: 0.5103\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6932 - val_accuracy: 0.5038\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5087 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5093\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5095 - val_loss: 0.6932 - val_accuracy: 0.5065\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6931 - val_accuracy: 0.5096\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5093 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5090 - val_loss: 0.6931 - val_accuracy: 0.5086\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6930 - val_accuracy: 0.5088\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6931 - val_accuracy: 0.5088\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6930 - val_accuracy: 0.5071\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5098 - val_loss: 0.6930 - val_accuracy: 0.5151\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5105 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5109 - val_loss: 0.6929 - val_accuracy: 0.5112\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5109 - val_loss: 0.6930 - val_accuracy: 0.5092\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5112\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5087\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5104 - val_loss: 0.6931 - val_accuracy: 0.5066\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5122\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5105 - val_loss: 0.6930 - val_accuracy: 0.5092\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.5061\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5105\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.5093\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5118\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5087\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6931 - val_accuracy: 0.5118\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5122\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5101 - val_loss: 0.6931 - val_accuracy: 0.5117\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5132\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5103 - val_loss: 0.6931 - val_accuracy: 0.5105\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5095 - val_loss: 0.6933 - val_accuracy: 0.5069\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 1s 3ms/step - loss: 0.6927 - accuracy: 0.5098 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6930 - val_accuracy: 0.5103\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5006437 ]\n",
      " [0.49495178]\n",
      " [0.52547264]\n",
      " [0.5082039 ]\n",
      " [0.50066847]\n",
      " [0.50189793]\n",
      " [0.51101667]\n",
      " [0.50817853]\n",
      " [0.50585014]\n",
      " [0.50777453]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_18 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_19 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_18 is normal keras bn layer\n",
      "q_activation_18      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_19 is normal keras bn layer\n",
      "q_activation_19      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.8867 - accuracy: 0.5018 - val_loss: 0.6983 - val_accuracy: 0.5028\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5043 - val_loss: 0.6950 - val_accuracy: 0.5038\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5049 - val_loss: 0.6942 - val_accuracy: 0.5041\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5063 - val_loss: 0.6938 - val_accuracy: 0.5056\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5074 - val_loss: 0.6936 - val_accuracy: 0.5060\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5081 - val_loss: 0.6964 - val_accuracy: 0.4993\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5061 - val_loss: 0.6933 - val_accuracy: 0.5061\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5082 - val_loss: 0.6932 - val_accuracy: 0.5086\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6939 - val_accuracy: 0.5065\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5074 - val_loss: 0.6936 - val_accuracy: 0.5090\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6928 - val_accuracy: 0.5061\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5132\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5052\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5123 - val_loss: 0.6937 - val_accuracy: 0.5082\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5109 - val_loss: 0.6929 - val_accuracy: 0.5114\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5134 - val_loss: 0.6921 - val_accuracy: 0.5150\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5139 - val_loss: 0.6922 - val_accuracy: 0.5066\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6926 - val_accuracy: 0.5074\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5152 - val_loss: 0.6924 - val_accuracy: 0.5117\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5154 - val_loss: 0.6919 - val_accuracy: 0.5108\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5149 - val_loss: 0.6933 - val_accuracy: 0.5113\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5103\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5162 - val_loss: 0.6919 - val_accuracy: 0.5152\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5160 - val_loss: 0.6928 - val_accuracy: 0.5154\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5144 - val_loss: 0.7145 - val_accuracy: 0.5076\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5123 - val_loss: 0.6937 - val_accuracy: 0.5092\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6949 - accuracy: 0.5111 - val_loss: 0.6941 - val_accuracy: 0.5100\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5122 - val_loss: 0.6918 - val_accuracy: 0.5163\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5152 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5150 - val_loss: 0.6926 - val_accuracy: 0.5134\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5195 - val_loss: 0.6907 - val_accuracy: 0.5188\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5212 - val_loss: 0.6937 - val_accuracy: 0.5103\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5268 - val_loss: 0.6924 - val_accuracy: 0.5152\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5291 - val_loss: 0.6897 - val_accuracy: 0.5276\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5338 - val_loss: 0.6916 - val_accuracy: 0.5275\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5362 - val_loss: 0.6886 - val_accuracy: 0.5244\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5414 - val_loss: 0.6896 - val_accuracy: 0.5278\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5438 - val_loss: 0.6859 - val_accuracy: 0.5319\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5462 - val_loss: 0.6822 - val_accuracy: 0.5367\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5499 - val_loss: 0.6794 - val_accuracy: 0.5458\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5522 - val_loss: 0.6773 - val_accuracy: 0.5552\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5518 - val_loss: 0.6793 - val_accuracy: 0.5492\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6772 - accuracy: 0.5551 - val_loss: 0.6824 - val_accuracy: 0.5428\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5567 - val_loss: 0.6745 - val_accuracy: 0.5620\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6756 - accuracy: 0.5559 - val_loss: 0.6717 - val_accuracy: 0.5579\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5596 - val_loss: 0.7153 - val_accuracy: 0.5163\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6746 - accuracy: 0.5585 - val_loss: 0.6913 - val_accuracy: 0.5354\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5599 - val_loss: 0.6786 - val_accuracy: 0.5449\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5628 - val_loss: 0.6664 - val_accuracy: 0.5719\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5634 - val_loss: 0.6812 - val_accuracy: 0.5441\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5153 - val_loss: 0.6950 - val_accuracy: 0.5044\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5079 - val_loss: 0.6931 - val_accuracy: 0.5044\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5087 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6923 - val_accuracy: 0.5124\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5140 - val_loss: 0.6921 - val_accuracy: 0.5106\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5187 - val_loss: 0.6911 - val_accuracy: 0.5223\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6896 - accuracy: 0.5261 - val_loss: 0.7003 - val_accuracy: 0.5019\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6842 - accuracy: 0.5397 - val_loss: 0.6828 - val_accuracy: 0.5429\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.5468 - val_loss: 0.6772 - val_accuracy: 0.5531\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5521 - val_loss: 0.6827 - val_accuracy: 0.5443\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5572 - val_loss: 0.6956 - val_accuracy: 0.5234\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5601 - val_loss: 0.6728 - val_accuracy: 0.5605\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5632 - val_loss: 0.6708 - val_accuracy: 0.5600\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5647 - val_loss: 0.6849 - val_accuracy: 0.5571\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.5657 - val_loss: 0.6751 - val_accuracy: 0.5526\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6701 - accuracy: 0.5679 - val_loss: 0.6703 - val_accuracy: 0.5617\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5692 - val_loss: 0.6846 - val_accuracy: 0.5471\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5702 - val_loss: 0.6729 - val_accuracy: 0.5579\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5690 - val_loss: 0.6632 - val_accuracy: 0.5739\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5669 - val_loss: 0.6697 - val_accuracy: 0.5614\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6754 - accuracy: 0.5652 - val_loss: 0.6714 - val_accuracy: 0.5617\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5678 - val_loss: 0.6614 - val_accuracy: 0.5786\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5632 - val_loss: 0.6954 - val_accuracy: 0.5217\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6784 - accuracy: 0.5610 - val_loss: 0.6668 - val_accuracy: 0.5635\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5645 - val_loss: 0.6656 - val_accuracy: 0.5664\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5653 - val_loss: 0.6735 - val_accuracy: 0.5559\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5626 - val_loss: 0.7422 - val_accuracy: 0.5288\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5633 - val_loss: 0.6835 - val_accuracy: 0.5482\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5631 - val_loss: 0.7150 - val_accuracy: 0.5587\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6820 - accuracy: 0.5601 - val_loss: 0.6773 - val_accuracy: 0.5482\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6817 - accuracy: 0.5593 - val_loss: 0.6831 - val_accuracy: 0.5508\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5581 - val_loss: 0.7100 - val_accuracy: 0.5596\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5588 - val_loss: 0.6777 - val_accuracy: 0.5431\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5572 - val_loss: 0.6897 - val_accuracy: 0.5724\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5571 - val_loss: 0.6785 - val_accuracy: 0.5499\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5567 - val_loss: 0.6832 - val_accuracy: 0.5249\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5536 - val_loss: 0.6828 - val_accuracy: 0.5745\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5524 - val_loss: 0.6728 - val_accuracy: 0.5829\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5518 - val_loss: 0.6999 - val_accuracy: 0.5230\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.5545 - val_loss: 0.6849 - val_accuracy: 0.5283\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5538 - val_loss: 0.7461 - val_accuracy: 0.5179\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5519 - val_loss: 0.6938 - val_accuracy: 0.5089\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_18\n",
      "cannot prune layer q_activation_18\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_19\n",
      "cannot prune layer q_activation_19\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6738 - accuracy: 0.5697 - val_loss: 0.6872 - val_accuracy: 0.5482\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6759 - accuracy: 0.5691 - val_loss: 0.7100 - val_accuracy: 0.5277\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5636 - val_loss: 0.6686 - val_accuracy: 0.5690\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6783 - accuracy: 0.5654 - val_loss: 0.7089 - val_accuracy: 0.5249\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5651 - val_loss: 0.7059 - val_accuracy: 0.5147\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6817 - accuracy: 0.5603 - val_loss: 0.7063 - val_accuracy: 0.5634\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6918 - accuracy: 0.5359 - val_loss: 0.6966 - val_accuracy: 0.4979\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7006 - accuracy: 0.5105 - val_loss: 0.6920 - val_accuracy: 0.5093\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5333 - val_loss: 0.6824 - val_accuracy: 0.5450\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6889 - accuracy: 0.5459 - val_loss: 0.6929 - val_accuracy: 0.5197\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5494 - val_loss: 0.6790 - val_accuracy: 0.5391\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6852 - accuracy: 0.5540 - val_loss: 0.6732 - val_accuracy: 0.5662\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6835 - accuracy: 0.5581 - val_loss: 0.6865 - val_accuracy: 0.5353\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6849 - accuracy: 0.5555 - val_loss: 0.6847 - val_accuracy: 0.5366\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5553 - val_loss: 0.6875 - val_accuracy: 0.5309\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5543 - val_loss: 0.7998 - val_accuracy: 0.4987\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6846 - accuracy: 0.5531 - val_loss: 0.6949 - val_accuracy: 0.5305\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5512 - val_loss: 0.6827 - val_accuracy: 0.5709\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6864 - accuracy: 0.5487 - val_loss: 0.6907 - val_accuracy: 0.5643\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6857 - accuracy: 0.5490 - val_loss: 0.6729 - val_accuracy: 0.5526\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5470 - val_loss: 0.6798 - val_accuracy: 0.5753\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6850 - accuracy: 0.5475 - val_loss: 0.6846 - val_accuracy: 0.5663\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5458 - val_loss: 0.6814 - val_accuracy: 0.5194\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5503 - val_loss: 0.6875 - val_accuracy: 0.5205\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6832 - accuracy: 0.5498 - val_loss: 0.6810 - val_accuracy: 0.5690\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6806 - accuracy: 0.5537 - val_loss: 0.6901 - val_accuracy: 0.5039\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5502 - val_loss: 0.7257 - val_accuracy: 0.5011\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5490 - val_loss: 0.6785 - val_accuracy: 0.5632\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.5507 - val_loss: 0.6777 - val_accuracy: 0.5682\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5463 - val_loss: 0.6700 - val_accuracy: 0.5804\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6797 - accuracy: 0.5546 - val_loss: 0.7329 - val_accuracy: 0.5200\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5554 - val_loss: 0.6896 - val_accuracy: 0.5250\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6808 - accuracy: 0.5540 - val_loss: 0.6761 - val_accuracy: 0.5691\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6785 - accuracy: 0.5577 - val_loss: 0.6753 - val_accuracy: 0.5673\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5598 - val_loss: 0.6699 - val_accuracy: 0.5765\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6770 - accuracy: 0.5595 - val_loss: 0.6659 - val_accuracy: 0.5786\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5586 - val_loss: 0.6676 - val_accuracy: 0.5777\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6765 - accuracy: 0.5614 - val_loss: 0.6726 - val_accuracy: 0.5734\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6802 - accuracy: 0.5576 - val_loss: 0.7201 - val_accuracy: 0.4987\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7114 - accuracy: 0.5000 - val_loss: 0.7145 - val_accuracy: 0.5013\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7087 - accuracy: 0.5002 - val_loss: 0.7092 - val_accuracy: 0.4997\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7081 - accuracy: 0.4996 - val_loss: 0.7097 - val_accuracy: 0.5001\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7082 - accuracy: 0.4995 - val_loss: 0.7057 - val_accuracy: 0.5001\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7077 - accuracy: 0.5016 - val_loss: 0.7044 - val_accuracy: 0.5001\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7073 - accuracy: 0.5017 - val_loss: 0.7179 - val_accuracy: 0.5039\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7059 - accuracy: 0.5037 - val_loss: 0.6989 - val_accuracy: 0.5003\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7031 - accuracy: 0.5076 - val_loss: 0.7567 - val_accuracy: 0.5022\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5299 - val_loss: 0.7437 - val_accuracy: 0.5118\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6871 - accuracy: 0.5440 - val_loss: 0.6995 - val_accuracy: 0.5036\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5542 - val_loss: 0.7425 - val_accuracy: 0.5431\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.63197905]\n",
      " [0.79255724]\n",
      " [0.5752537 ]\n",
      " [0.77751184]\n",
      " [0.67966664]\n",
      " [0.1668436 ]\n",
      " [0.54612315]\n",
      " [0.3950644 ]\n",
      " [0.7070758 ]\n",
      " [0.3002965 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [32], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6859056949615479\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_20 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_21 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_20 is normal keras bn layer\n",
      "q_activation_20      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_21 is normal keras bn layer\n",
      "q_activation_21      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 3.5777 - accuracy: 0.5000 - val_loss: 2.3057 - val_accuracy: 0.5028\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.9821 - accuracy: 0.5017 - val_loss: 1.5704 - val_accuracy: 0.5009\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.0179 - accuracy: 0.5017 - val_loss: 0.7444 - val_accuracy: 0.5022\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7350 - accuracy: 0.5019 - val_loss: 0.7309 - val_accuracy: 0.5050\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7296 - accuracy: 0.5021 - val_loss: 0.7539 - val_accuracy: 0.5038\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7284 - accuracy: 0.5012 - val_loss: 0.7139 - val_accuracy: 0.5042\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7118 - accuracy: 0.5025 - val_loss: 0.7083 - val_accuracy: 0.5060\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7076 - accuracy: 0.5025 - val_loss: 0.7056 - val_accuracy: 0.5058\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7049 - accuracy: 0.5035 - val_loss: 0.7032 - val_accuracy: 0.5056\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7030 - accuracy: 0.5033 - val_loss: 0.7014 - val_accuracy: 0.5069\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7015 - accuracy: 0.5034 - val_loss: 0.7001 - val_accuracy: 0.5065\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7001 - accuracy: 0.5043 - val_loss: 0.6989 - val_accuracy: 0.5069\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5046 - val_loss: 0.6982 - val_accuracy: 0.5063\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6981 - accuracy: 0.5047 - val_loss: 0.6976 - val_accuracy: 0.5049\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5038 - val_loss: 0.6970 - val_accuracy: 0.5052\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5039 - val_loss: 0.6965 - val_accuracy: 0.5044\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5042 - val_loss: 0.6961 - val_accuracy: 0.5048\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5038 - val_loss: 0.6957 - val_accuracy: 0.5039\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5038 - val_loss: 0.6953 - val_accuracy: 0.5042\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5047 - val_loss: 0.6951 - val_accuracy: 0.5046\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5056 - val_loss: 0.6948 - val_accuracy: 0.5056\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5048 - val_loss: 0.6943 - val_accuracy: 0.5066\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5068 - val_loss: 0.6946 - val_accuracy: 0.5066\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5062 - val_loss: 0.6941 - val_accuracy: 0.5051\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6942 - val_accuracy: 0.5034\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5068 - val_loss: 0.6938 - val_accuracy: 0.5063\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5075 - val_loss: 0.6939 - val_accuracy: 0.5038\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5079 - val_loss: 0.6938 - val_accuracy: 0.5053\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6935 - val_accuracy: 0.5101\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6934 - val_accuracy: 0.5089\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5092 - val_loss: 0.6930 - val_accuracy: 0.5117\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5107\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5103 - val_loss: 0.6933 - val_accuracy: 0.5077\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5104 - val_loss: 0.6930 - val_accuracy: 0.5111\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5110 - val_loss: 0.6929 - val_accuracy: 0.5136\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5124 - val_loss: 0.6926 - val_accuracy: 0.5114\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5135 - val_loss: 0.6923 - val_accuracy: 0.5148\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5159 - val_loss: 0.6922 - val_accuracy: 0.5141\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5173 - val_loss: 0.6921 - val_accuracy: 0.5141\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5197 - val_loss: 0.6917 - val_accuracy: 0.5218\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5216 - val_loss: 0.6919 - val_accuracy: 0.5121\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5254 - val_loss: 0.6915 - val_accuracy: 0.5177\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5281 - val_loss: 0.6900 - val_accuracy: 0.5263\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5311 - val_loss: 0.6900 - val_accuracy: 0.5315\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5334 - val_loss: 0.6885 - val_accuracy: 0.5314\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6873 - accuracy: 0.5366 - val_loss: 0.6879 - val_accuracy: 0.5350\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5392 - val_loss: 0.6864 - val_accuracy: 0.5337\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5430 - val_loss: 0.6849 - val_accuracy: 0.5421\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6831 - accuracy: 0.5468 - val_loss: 0.6867 - val_accuracy: 0.5348\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5488 - val_loss: 0.6815 - val_accuracy: 0.5536\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5517 - val_loss: 0.6801 - val_accuracy: 0.5546\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5560 - val_loss: 0.6813 - val_accuracy: 0.5440\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5477 - val_loss: 0.6810 - val_accuracy: 0.5520\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5583 - val_loss: 0.6941 - val_accuracy: 0.5418\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6766 - accuracy: 0.5594 - val_loss: 0.6783 - val_accuracy: 0.5617\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5568 - val_loss: 0.6793 - val_accuracy: 0.5504\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5647 - val_loss: 0.6721 - val_accuracy: 0.5683\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5689 - val_loss: 0.6734 - val_accuracy: 0.5677\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5673 - val_loss: 0.6808 - val_accuracy: 0.5550\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5680 - val_loss: 0.6719 - val_accuracy: 0.5664\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5639 - val_loss: 0.6705 - val_accuracy: 0.5747\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6866 - accuracy: 0.5363 - val_loss: 0.6873 - val_accuracy: 0.5334\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5555 - val_loss: 0.6893 - val_accuracy: 0.5307\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6762 - accuracy: 0.5576 - val_loss: 0.6782 - val_accuracy: 0.5559\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.5671 - val_loss: 0.6699 - val_accuracy: 0.5683\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5688 - val_loss: 0.6673 - val_accuracy: 0.5703\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6715 - accuracy: 0.5689 - val_loss: 0.6672 - val_accuracy: 0.5767\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5747 - val_loss: 0.6641 - val_accuracy: 0.5831\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6670 - accuracy: 0.5773 - val_loss: 0.6727 - val_accuracy: 0.5671\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5692 - val_loss: 0.6786 - val_accuracy: 0.5607\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6691 - accuracy: 0.5775 - val_loss: 0.6641 - val_accuracy: 0.5860\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5690 - val_loss: 0.6733 - val_accuracy: 0.5680\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.5723 - val_loss: 0.6677 - val_accuracy: 0.5744\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5818 - val_loss: 0.6688 - val_accuracy: 0.5663\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5738 - val_loss: 0.6678 - val_accuracy: 0.5750\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6658 - accuracy: 0.5810 - val_loss: 0.6641 - val_accuracy: 0.5817\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6805 - accuracy: 0.5591 - val_loss: 0.7090 - val_accuracy: 0.5084\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6989 - accuracy: 0.5091 - val_loss: 0.6969 - val_accuracy: 0.5125\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5138 - val_loss: 0.6947 - val_accuracy: 0.5129\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5166 - val_loss: 0.6931 - val_accuracy: 0.5206\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5195 - val_loss: 0.6920 - val_accuracy: 0.5216\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5242 - val_loss: 0.6905 - val_accuracy: 0.5276\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5317 - val_loss: 0.6883 - val_accuracy: 0.5348\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5399 - val_loss: 0.6864 - val_accuracy: 0.5382\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6824 - accuracy: 0.5516 - val_loss: 0.6863 - val_accuracy: 0.5433\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5617 - val_loss: 0.6746 - val_accuracy: 0.5687\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.5673 - val_loss: 0.6732 - val_accuracy: 0.5742\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6809 - accuracy: 0.5533 - val_loss: 0.6897 - val_accuracy: 0.5311\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_20\n",
      "cannot prune layer q_activation_20\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_21\n",
      "cannot prune layer q_activation_21\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6680 - accuracy: 0.5767 - val_loss: 0.6649 - val_accuracy: 0.5763\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6675 - accuracy: 0.5755 - val_loss: 0.6656 - val_accuracy: 0.5777\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6712 - accuracy: 0.5689 - val_loss: 0.6651 - val_accuracy: 0.5853\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6785 - accuracy: 0.5554 - val_loss: 0.6776 - val_accuracy: 0.5607\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5678 - val_loss: 0.6649 - val_accuracy: 0.5800\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6676 - accuracy: 0.5736 - val_loss: 0.6633 - val_accuracy: 0.5796\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5653 - val_loss: 0.6691 - val_accuracy: 0.5731\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6660 - accuracy: 0.5782 - val_loss: 0.6638 - val_accuracy: 0.5781\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6648 - accuracy: 0.5805 - val_loss: 0.6616 - val_accuracy: 0.5888\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6723 - accuracy: 0.5677 - val_loss: 0.6693 - val_accuracy: 0.5716\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5767 - val_loss: 0.6636 - val_accuracy: 0.5807\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6639 - accuracy: 0.5808 - val_loss: 0.6614 - val_accuracy: 0.5863\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7000 - accuracy: 0.5097 - val_loss: 0.6945 - val_accuracy: 0.5067\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.6923 - val_accuracy: 0.5156\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5163 - val_loss: 0.6903 - val_accuracy: 0.5205\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6857 - accuracy: 0.5383 - val_loss: 0.6817 - val_accuracy: 0.5495\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5615 - val_loss: 0.6725 - val_accuracy: 0.5699\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6681 - accuracy: 0.5750 - val_loss: 0.6655 - val_accuracy: 0.5781\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5729 - val_loss: 0.6664 - val_accuracy: 0.5732\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5563 - val_loss: 0.7067 - val_accuracy: 0.5104\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6961 - accuracy: 0.5133 - val_loss: 0.6935 - val_accuracy: 0.5168\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6915 - accuracy: 0.5236 - val_loss: 0.6901 - val_accuracy: 0.5280\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6887 - accuracy: 0.5323 - val_loss: 0.6867 - val_accuracy: 0.5382\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6861 - accuracy: 0.5398 - val_loss: 0.6838 - val_accuracy: 0.5446\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6837 - accuracy: 0.5444 - val_loss: 0.6816 - val_accuracy: 0.5493\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6811 - accuracy: 0.5500 - val_loss: 0.6783 - val_accuracy: 0.5574\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5556 - val_loss: 0.6771 - val_accuracy: 0.5553\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6750 - accuracy: 0.5617 - val_loss: 0.6752 - val_accuracy: 0.5623\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.5632 - val_loss: 0.6712 - val_accuracy: 0.5673\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6735 - accuracy: 0.5636 - val_loss: 0.6676 - val_accuracy: 0.5717\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5728 - val_loss: 0.6660 - val_accuracy: 0.5781\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5764 - val_loss: 0.6691 - val_accuracy: 0.5697\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.5658 - val_loss: 0.6764 - val_accuracy: 0.5622\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6655 - accuracy: 0.5780 - val_loss: 0.6621 - val_accuracy: 0.5806\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6658 - accuracy: 0.5775 - val_loss: 0.7363 - val_accuracy: 0.5333\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6651 - accuracy: 0.5766 - val_loss: 0.6971 - val_accuracy: 0.5531\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6636 - accuracy: 0.5798 - val_loss: 0.6736 - val_accuracy: 0.5597\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6623 - accuracy: 0.5802 - val_loss: 0.6615 - val_accuracy: 0.5839\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6659 - accuracy: 0.5744 - val_loss: 0.6608 - val_accuracy: 0.5811\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6640 - accuracy: 0.5783 - val_loss: 0.6560 - val_accuracy: 0.5896\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7014 - accuracy: 0.5065 - val_loss: 0.6955 - val_accuracy: 0.5027\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5120 - val_loss: 0.6921 - val_accuracy: 0.5163\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5190 - val_loss: 0.6906 - val_accuracy: 0.5241\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5246 - val_loss: 0.6886 - val_accuracy: 0.5318\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5347 - val_loss: 0.6856 - val_accuracy: 0.5399\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.5495 - val_loss: 0.6752 - val_accuracy: 0.5569\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6707 - accuracy: 0.5679 - val_loss: 0.6661 - val_accuracy: 0.5778\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6827 - accuracy: 0.5475 - val_loss: 0.6821 - val_accuracy: 0.5440\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5579 - val_loss: 0.6703 - val_accuracy: 0.5679\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.5749 - val_loss: 0.6639 - val_accuracy: 0.5773\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.48813772]\n",
      " [0.34591693]\n",
      " [0.5266944 ]\n",
      " [0.5017159 ]\n",
      " [0.60312015]\n",
      " [0.4395824 ]\n",
      " [0.24618518]\n",
      " [0.17970991]\n",
      " [0.53319013]\n",
      " [0.62421435]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.001, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6638756990432739\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.0, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_22 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_23 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_22 is normal keras bn layer\n",
      "q_activation_22      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_23 is normal keras bn layer\n",
      "q_activation_23      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 4.9261 - accuracy: 0.4993 - val_loss: 3.8541 - val_accuracy: 0.5021\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 2.5156 - accuracy: 0.5035 - val_loss: 1.9565 - val_accuracy: 0.5074\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.3146 - accuracy: 0.5025 - val_loss: 1.0048 - val_accuracy: 0.5055\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.9340 - accuracy: 0.5012 - val_loss: 0.8473 - val_accuracy: 0.5040\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7695 - accuracy: 0.5008 - val_loss: 0.7258 - val_accuracy: 0.5025\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7190 - accuracy: 0.5011 - val_loss: 0.7114 - val_accuracy: 0.5030\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7095 - accuracy: 0.5018 - val_loss: 0.7058 - val_accuracy: 0.5043\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7048 - accuracy: 0.5015 - val_loss: 0.7026 - val_accuracy: 0.5044\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7018 - accuracy: 0.5030 - val_loss: 0.7005 - val_accuracy: 0.5036\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7000 - accuracy: 0.5026 - val_loss: 0.6988 - val_accuracy: 0.5020\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6984 - accuracy: 0.5032 - val_loss: 0.6976 - val_accuracy: 0.5009\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6971 - accuracy: 0.5039 - val_loss: 0.6967 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5038 - val_loss: 0.6959 - val_accuracy: 0.5015\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5047 - val_loss: 0.6953 - val_accuracy: 0.5022\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5052 - val_loss: 0.6947 - val_accuracy: 0.5040\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5053 - val_loss: 0.6941 - val_accuracy: 0.5049\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5051 - val_loss: 0.6942 - val_accuracy: 0.5059\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5066 - val_loss: 0.6935 - val_accuracy: 0.5062\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5078 - val_loss: 0.6947 - val_accuracy: 0.5038\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5076 - val_loss: 0.6932 - val_accuracy: 0.5071\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6931 - val_accuracy: 0.5075\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5089 - val_loss: 0.6930 - val_accuracy: 0.5080\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5102 - val_loss: 0.6929 - val_accuracy: 0.5083\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5114 - val_loss: 0.6928 - val_accuracy: 0.5103\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5122 - val_loss: 0.6927 - val_accuracy: 0.5094\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5126 - val_loss: 0.6927 - val_accuracy: 0.5132\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6924 - val_accuracy: 0.5112\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5138 - val_loss: 0.6922 - val_accuracy: 0.5154\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5145 - val_loss: 0.6922 - val_accuracy: 0.5146\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5168 - val_loss: 0.6919 - val_accuracy: 0.5198\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5192 - val_loss: 0.6920 - val_accuracy: 0.5187\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5211 - val_loss: 0.6918 - val_accuracy: 0.5187\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5240 - val_loss: 0.6912 - val_accuracy: 0.5240\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5274 - val_loss: 0.6907 - val_accuracy: 0.5277\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5296 - val_loss: 0.6904 - val_accuracy: 0.5277\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6894 - accuracy: 0.5321 - val_loss: 0.6893 - val_accuracy: 0.5343\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5339 - val_loss: 0.6888 - val_accuracy: 0.5333\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6882 - accuracy: 0.5357 - val_loss: 0.6886 - val_accuracy: 0.5387\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6878 - accuracy: 0.5372 - val_loss: 0.6877 - val_accuracy: 0.5361\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5390 - val_loss: 0.6870 - val_accuracy: 0.5423\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5400 - val_loss: 0.6883 - val_accuracy: 0.5383\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6863 - accuracy: 0.5421 - val_loss: 0.6862 - val_accuracy: 0.5453\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5429 - val_loss: 0.6853 - val_accuracy: 0.5467\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5448 - val_loss: 0.6859 - val_accuracy: 0.5438\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5478 - val_loss: 0.6848 - val_accuracy: 0.5467\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5493 - val_loss: 0.6834 - val_accuracy: 0.5477\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5452 - val_loss: 0.6917 - val_accuracy: 0.5259\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5447 - val_loss: 0.6839 - val_accuracy: 0.5500\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6821 - accuracy: 0.5521 - val_loss: 0.6823 - val_accuracy: 0.5534\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6814 - accuracy: 0.5539 - val_loss: 0.6819 - val_accuracy: 0.5508\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.5545 - val_loss: 0.6827 - val_accuracy: 0.5515\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.5565 - val_loss: 0.6811 - val_accuracy: 0.5533\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6801 - accuracy: 0.5566 - val_loss: 0.6798 - val_accuracy: 0.5592\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5588 - val_loss: 0.6818 - val_accuracy: 0.5548\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5581 - val_loss: 0.6788 - val_accuracy: 0.5572\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6790 - accuracy: 0.5581 - val_loss: 0.6795 - val_accuracy: 0.5568\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6807 - accuracy: 0.5542 - val_loss: 0.6804 - val_accuracy: 0.5545\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6801 - accuracy: 0.5555 - val_loss: 0.6801 - val_accuracy: 0.5516\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6791 - accuracy: 0.5589 - val_loss: 0.6818 - val_accuracy: 0.5537\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5622 - val_loss: 0.6797 - val_accuracy: 0.5633\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5637 - val_loss: 0.6767 - val_accuracy: 0.5597\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6782 - accuracy: 0.5613 - val_loss: 0.6775 - val_accuracy: 0.5594\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6801 - accuracy: 0.5590 - val_loss: 0.6784 - val_accuracy: 0.5600\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5636 - val_loss: 0.6764 - val_accuracy: 0.5604\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6803 - accuracy: 0.5579 - val_loss: 0.7072 - val_accuracy: 0.5094\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5285 - val_loss: 0.6859 - val_accuracy: 0.5435\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6829 - accuracy: 0.5507 - val_loss: 0.6809 - val_accuracy: 0.5517\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6800 - accuracy: 0.5564 - val_loss: 0.6813 - val_accuracy: 0.5558\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6771 - accuracy: 0.5621 - val_loss: 0.6778 - val_accuracy: 0.5594\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6762 - accuracy: 0.5657 - val_loss: 0.6751 - val_accuracy: 0.5637\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6879 - accuracy: 0.5408 - val_loss: 0.6911 - val_accuracy: 0.5295\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5446 - val_loss: 0.6824 - val_accuracy: 0.5519\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.5548 - val_loss: 0.6794 - val_accuracy: 0.5602\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5614 - val_loss: 0.6757 - val_accuracy: 0.5696\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5648 - val_loss: 0.6743 - val_accuracy: 0.5698\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6744 - accuracy: 0.5690 - val_loss: 0.6760 - val_accuracy: 0.5652\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5700 - val_loss: 0.6761 - val_accuracy: 0.5702\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.5709 - val_loss: 0.6752 - val_accuracy: 0.5679\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5689 - val_loss: 0.6819 - val_accuracy: 0.5619\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6728 - accuracy: 0.5713 - val_loss: 0.6709 - val_accuracy: 0.5707\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5711 - val_loss: 0.6709 - val_accuracy: 0.5718\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.5611 - val_loss: 0.6966 - val_accuracy: 0.5143\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5406 - val_loss: 0.6807 - val_accuracy: 0.5539\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5580 - val_loss: 0.6846 - val_accuracy: 0.5515\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6756 - accuracy: 0.5644 - val_loss: 0.6742 - val_accuracy: 0.5708\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.5687 - val_loss: 0.6855 - val_accuracy: 0.5558\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5713 - val_loss: 0.6852 - val_accuracy: 0.5575\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5748 - val_loss: 0.6743 - val_accuracy: 0.5682\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6744 - accuracy: 0.5693 - val_loss: 0.6861 - val_accuracy: 0.5459\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.5704 - val_loss: 0.6747 - val_accuracy: 0.5672\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5784 - val_loss: 0.6681 - val_accuracy: 0.5745\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6681 - accuracy: 0.5791 - val_loss: 0.6695 - val_accuracy: 0.5676\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5806 - val_loss: 0.6690 - val_accuracy: 0.5794\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6672 - accuracy: 0.5813 - val_loss: 0.6677 - val_accuracy: 0.5783\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6667 - accuracy: 0.5821 - val_loss: 0.6658 - val_accuracy: 0.5826\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5785 - val_loss: 0.6636 - val_accuracy: 0.5836\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5798 - val_loss: 0.6668 - val_accuracy: 0.5827\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6665 - accuracy: 0.5822 - val_loss: 0.7046 - val_accuracy: 0.5499\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6695 - accuracy: 0.5784 - val_loss: 0.6655 - val_accuracy: 0.5779\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6655 - accuracy: 0.5840 - val_loss: 0.6654 - val_accuracy: 0.5751\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_22\n",
      "cannot prune layer q_activation_22\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_23\n",
      "cannot prune layer q_activation_23\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6647 - accuracy: 0.5852 - val_loss: 0.6709 - val_accuracy: 0.5783\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6657 - accuracy: 0.5829 - val_loss: 0.6674 - val_accuracy: 0.5813\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6665 - accuracy: 0.5812 - val_loss: 0.6644 - val_accuracy: 0.5840\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6646 - accuracy: 0.5846 - val_loss: 0.6652 - val_accuracy: 0.5739\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5846 - val_loss: 0.6636 - val_accuracy: 0.5856\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6661 - accuracy: 0.5825 - val_loss: 0.6671 - val_accuracy: 0.5834\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5783 - val_loss: 0.6606 - val_accuracy: 0.5924\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.5766 - val_loss: 0.6906 - val_accuracy: 0.5283\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6795 - accuracy: 0.5560 - val_loss: 0.6733 - val_accuracy: 0.5700\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6706 - accuracy: 0.5734 - val_loss: 0.6649 - val_accuracy: 0.5843\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6665 - accuracy: 0.5809 - val_loss: 0.6630 - val_accuracy: 0.5832\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.5832 - val_loss: 0.6621 - val_accuracy: 0.5817\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6640 - accuracy: 0.5851 - val_loss: 0.6598 - val_accuracy: 0.5907\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6614 - accuracy: 0.5893 - val_loss: 0.6575 - val_accuracy: 0.5938\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6619 - accuracy: 0.5893 - val_loss: 0.6582 - val_accuracy: 0.5945\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6623 - accuracy: 0.5880 - val_loss: 0.6603 - val_accuracy: 0.5853\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5824 - val_loss: 0.6833 - val_accuracy: 0.5545\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6647 - accuracy: 0.5839 - val_loss: 0.6756 - val_accuracy: 0.5776\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6612 - accuracy: 0.5901 - val_loss: 0.6584 - val_accuracy: 0.5932\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6796 - accuracy: 0.5723 - val_loss: 0.7221 - val_accuracy: 0.5205\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7010 - accuracy: 0.5302 - val_loss: 0.6931 - val_accuracy: 0.5389\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5410 - val_loss: 0.6860 - val_accuracy: 0.5430\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5457 - val_loss: 0.6828 - val_accuracy: 0.5500\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6820 - accuracy: 0.5519 - val_loss: 0.6803 - val_accuracy: 0.5545\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6785 - accuracy: 0.5580 - val_loss: 0.6825 - val_accuracy: 0.5467\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5641 - val_loss: 0.6714 - val_accuracy: 0.5690\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6719 - accuracy: 0.5680 - val_loss: 0.6686 - val_accuracy: 0.5754\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6871 - accuracy: 0.5433 - val_loss: 0.6825 - val_accuracy: 0.5525\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6751 - accuracy: 0.5635 - val_loss: 0.6709 - val_accuracy: 0.5724\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6693 - accuracy: 0.5736 - val_loss: 0.6652 - val_accuracy: 0.5807\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5806 - val_loss: 0.6615 - val_accuracy: 0.5860\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6771 - accuracy: 0.5616 - val_loss: 0.6706 - val_accuracy: 0.5704\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6634 - accuracy: 0.5823 - val_loss: 0.6617 - val_accuracy: 0.5848\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6629 - accuracy: 0.5831 - val_loss: 0.6681 - val_accuracy: 0.5663\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6637 - accuracy: 0.5829 - val_loss: 0.6707 - val_accuracy: 0.5526\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5827 - val_loss: 0.6687 - val_accuracy: 0.5747\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6617 - accuracy: 0.5856 - val_loss: 0.6686 - val_accuracy: 0.5797\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6610 - accuracy: 0.5879 - val_loss: 0.8326 - val_accuracy: 0.5579\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6688 - accuracy: 0.5761 - val_loss: 0.6758 - val_accuracy: 0.5756\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6597 - accuracy: 0.5890 - val_loss: 0.6609 - val_accuracy: 0.5819\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5894 - val_loss: 0.6568 - val_accuracy: 0.5887\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6599 - accuracy: 0.5900 - val_loss: 0.6594 - val_accuracy: 0.5865\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6583 - accuracy: 0.5910 - val_loss: 0.6616 - val_accuracy: 0.5838\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5849 - val_loss: 0.7171 - val_accuracy: 0.5312\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5455 - val_loss: 0.6749 - val_accuracy: 0.5596\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6679 - accuracy: 0.5731 - val_loss: 0.6629 - val_accuracy: 0.5825\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.5913 - val_loss: 0.6922 - val_accuracy: 0.5434\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6600 - accuracy: 0.5887 - val_loss: 0.6578 - val_accuracy: 0.5940\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6567 - accuracy: 0.5935 - val_loss: 0.6552 - val_accuracy: 0.5977\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6553 - accuracy: 0.5955 - val_loss: 0.6586 - val_accuracy: 0.5873\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.49830568]\n",
      " [0.42971623]\n",
      " [0.24524224]\n",
      " [0.42855072]\n",
      " [0.17766309]\n",
      " [0.62018496]\n",
      " [0.4458571 ]\n",
      " [0.3874336 ]\n",
      " [0.5136663 ]\n",
      " [0.63322836]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.0, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.0005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6586488485336304\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_24 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_25 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_24 is normal keras bn layer\n",
      "q_activation_24      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_25 is normal keras bn layer\n",
      "q_activation_25      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.8254 - accuracy: 0.5009 - val_loss: 0.6999 - val_accuracy: 0.5020\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5040 - val_loss: 0.6964 - val_accuracy: 0.5084\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5053 - val_loss: 0.6949 - val_accuracy: 0.5070\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5069 - val_loss: 0.6939 - val_accuracy: 0.5040\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5077 - val_loss: 0.6937 - val_accuracy: 0.5070\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5079 - val_loss: 0.6935 - val_accuracy: 0.5081\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6933 - val_accuracy: 0.5047\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5100 - val_loss: 0.6936 - val_accuracy: 0.5066\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5098 - val_loss: 0.6930 - val_accuracy: 0.5090\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5095 - val_loss: 0.6934 - val_accuracy: 0.5091\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5126 - val_loss: 0.6933 - val_accuracy: 0.5114\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5123 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5140 - val_loss: 0.6926 - val_accuracy: 0.5131\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5162 - val_loss: 0.6984 - val_accuracy: 0.4993\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5205 - val_loss: 0.6938 - val_accuracy: 0.5030\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5215 - val_loss: 0.6934 - val_accuracy: 0.5049\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5249 - val_loss: 0.6924 - val_accuracy: 0.5192\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6901 - accuracy: 0.5271 - val_loss: 0.6887 - val_accuracy: 0.5363\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6883 - accuracy: 0.5350 - val_loss: 0.6981 - val_accuracy: 0.5047\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5398 - val_loss: 0.6875 - val_accuracy: 0.5321\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6851 - accuracy: 0.5423 - val_loss: 0.6982 - val_accuracy: 0.5142\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6827 - accuracy: 0.5484 - val_loss: 0.6818 - val_accuracy: 0.5494\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6815 - accuracy: 0.5503 - val_loss: 0.6817 - val_accuracy: 0.5466\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.5512 - val_loss: 0.6787 - val_accuracy: 0.5500\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5564 - val_loss: 0.6771 - val_accuracy: 0.5545\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5602 - val_loss: 0.6850 - val_accuracy: 0.5478\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.5600 - val_loss: 0.6889 - val_accuracy: 0.5345\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6729 - accuracy: 0.5650 - val_loss: 0.6704 - val_accuracy: 0.5685\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5671 - val_loss: 0.6720 - val_accuracy: 0.5662\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6718 - accuracy: 0.5668 - val_loss: 0.6666 - val_accuracy: 0.5734\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6705 - accuracy: 0.5684 - val_loss: 0.6697 - val_accuracy: 0.5686\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6699 - accuracy: 0.5686 - val_loss: 0.6963 - val_accuracy: 0.5438\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5724 - val_loss: 0.6612 - val_accuracy: 0.5804\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6733 - accuracy: 0.5634 - val_loss: 0.6730 - val_accuracy: 0.5586\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6676 - accuracy: 0.5727 - val_loss: 0.6626 - val_accuracy: 0.5825\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6671 - accuracy: 0.5721 - val_loss: 0.6665 - val_accuracy: 0.5715\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5727 - val_loss: 0.6614 - val_accuracy: 0.5772\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6662 - accuracy: 0.5719 - val_loss: 0.6554 - val_accuracy: 0.5858\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5758 - val_loss: 0.6609 - val_accuracy: 0.5762\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5757 - val_loss: 0.6564 - val_accuracy: 0.5879\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5763 - val_loss: 0.6624 - val_accuracy: 0.5756\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6613 - accuracy: 0.5792 - val_loss: 0.6537 - val_accuracy: 0.5893\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5797 - val_loss: 0.6717 - val_accuracy: 0.5698\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5854 - val_loss: 0.6592 - val_accuracy: 0.5836\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6618 - accuracy: 0.5825 - val_loss: 0.6593 - val_accuracy: 0.5833\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.5849 - val_loss: 0.6558 - val_accuracy: 0.5873\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6598 - accuracy: 0.5841 - val_loss: 0.6672 - val_accuracy: 0.5709\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6577 - accuracy: 0.5870 - val_loss: 0.6618 - val_accuracy: 0.5808\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6581 - accuracy: 0.5844 - val_loss: 0.6608 - val_accuracy: 0.5774\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6576 - accuracy: 0.5858 - val_loss: 0.6603 - val_accuracy: 0.5801\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6586 - accuracy: 0.5831 - val_loss: 0.6506 - val_accuracy: 0.5935\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6574 - accuracy: 0.5856 - val_loss: 0.6527 - val_accuracy: 0.5927\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6694 - accuracy: 0.5632 - val_loss: 0.6958 - val_accuracy: 0.5087\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5130 - val_loss: 0.6919 - val_accuracy: 0.5064\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.5614 - val_loss: 0.6764 - val_accuracy: 0.5564\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5795 - val_loss: 0.6691 - val_accuracy: 0.5659\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6630 - accuracy: 0.5774 - val_loss: 0.6622 - val_accuracy: 0.5796\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6585 - accuracy: 0.5851 - val_loss: 0.6679 - val_accuracy: 0.5809\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6656 - accuracy: 0.5756 - val_loss: 0.6630 - val_accuracy: 0.5715\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.5841 - val_loss: 0.6613 - val_accuracy: 0.5832\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6620 - accuracy: 0.5794 - val_loss: 0.6834 - val_accuracy: 0.5485\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6602 - accuracy: 0.5827 - val_loss: 0.6629 - val_accuracy: 0.5759\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.5877 - val_loss: 0.6905 - val_accuracy: 0.5442\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5160 - val_loss: 0.6939 - val_accuracy: 0.5014\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5038 - val_loss: 0.6934 - val_accuracy: 0.5023\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5053 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5061 - val_loss: 0.6930 - val_accuracy: 0.5049\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5075 - val_loss: 0.6928 - val_accuracy: 0.5049\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5095 - val_loss: 0.6925 - val_accuracy: 0.5116\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5133 - val_loss: 0.6920 - val_accuracy: 0.5131\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5180 - val_loss: 0.6906 - val_accuracy: 0.5223\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_24\n",
      "cannot prune layer q_activation_24\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_25\n",
      "cannot prune layer q_activation_25\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 8s 7ms/step - loss: 0.6614 - accuracy: 0.5798 - val_loss: 0.6780 - val_accuracy: 0.5599\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6579 - accuracy: 0.5847 - val_loss: 0.6621 - val_accuracy: 0.5859\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6592 - accuracy: 0.5839 - val_loss: 0.7932 - val_accuracy: 0.5372\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.5917 - val_loss: 0.6586 - val_accuracy: 0.5851\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6544 - accuracy: 0.5908 - val_loss: 0.6594 - val_accuracy: 0.5834\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6652 - accuracy: 0.5727 - val_loss: 0.6951 - val_accuracy: 0.5016\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6931 - accuracy: 0.5065 - val_loss: 0.6925 - val_accuracy: 0.5126\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5127 - val_loss: 0.6919 - val_accuracy: 0.5149\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5210 - val_loss: 0.6911 - val_accuracy: 0.5137\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5377 - val_loss: 0.6832 - val_accuracy: 0.5404\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6805 - accuracy: 0.5497 - val_loss: 0.7125 - val_accuracy: 0.5134\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5554 - val_loss: 0.6736 - val_accuracy: 0.5588\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6697 - accuracy: 0.5669 - val_loss: 0.6665 - val_accuracy: 0.5640\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6627 - accuracy: 0.5772 - val_loss: 0.6666 - val_accuracy: 0.5608\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6630 - accuracy: 0.5763 - val_loss: 0.6630 - val_accuracy: 0.5708\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6594 - accuracy: 0.5821 - val_loss: 0.6563 - val_accuracy: 0.5828\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6576 - accuracy: 0.5872 - val_loss: 0.6640 - val_accuracy: 0.5755\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6565 - accuracy: 0.5875 - val_loss: 0.6557 - val_accuracy: 0.5899\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6571 - accuracy: 0.5859 - val_loss: 0.6510 - val_accuracy: 0.5928\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6649 - accuracy: 0.5860 - val_loss: 0.6752 - val_accuracy: 0.5804\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5849 - val_loss: 0.6621 - val_accuracy: 0.5839\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6551 - accuracy: 0.5925 - val_loss: 0.6535 - val_accuracy: 0.5942\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.5917 - val_loss: 0.6641 - val_accuracy: 0.5722\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.5933 - val_loss: 0.6557 - val_accuracy: 0.5885\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.5922 - val_loss: 0.6652 - val_accuracy: 0.5793\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6529 - accuracy: 0.5946 - val_loss: 0.6481 - val_accuracy: 0.6037\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6567 - accuracy: 0.5898 - val_loss: 0.6861 - val_accuracy: 0.5559\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6527 - accuracy: 0.5944 - val_loss: 0.6485 - val_accuracy: 0.6016\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.5964 - val_loss: 0.6542 - val_accuracy: 0.5891\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5715 - val_loss: 0.6628 - val_accuracy: 0.5818\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5841 - val_loss: 0.6525 - val_accuracy: 0.5967\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6553 - accuracy: 0.5903 - val_loss: 0.6564 - val_accuracy: 0.5832\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6535 - accuracy: 0.5934 - val_loss: 0.6585 - val_accuracy: 0.5876\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6518 - accuracy: 0.5965 - val_loss: 0.6533 - val_accuracy: 0.5958\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6520 - accuracy: 0.5949 - val_loss: 0.6491 - val_accuracy: 0.5941\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.5942 - val_loss: 0.6487 - val_accuracy: 0.5974\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6513 - accuracy: 0.5966 - val_loss: 0.6518 - val_accuracy: 0.5885\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6506 - accuracy: 0.5963 - val_loss: 0.6425 - val_accuracy: 0.6062\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6573 - accuracy: 0.5868 - val_loss: 0.7050 - val_accuracy: 0.5035\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5203 - val_loss: 0.6887 - val_accuracy: 0.5218\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6781 - accuracy: 0.5530 - val_loss: 0.7050 - val_accuracy: 0.5197\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5517 - val_loss: 0.6954 - val_accuracy: 0.5118\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6882 - accuracy: 0.5286 - val_loss: 0.6830 - val_accuracy: 0.5438\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6716 - accuracy: 0.5704 - val_loss: 0.6656 - val_accuracy: 0.5798\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6625 - accuracy: 0.5814 - val_loss: 0.6647 - val_accuracy: 0.5727\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6589 - accuracy: 0.5856 - val_loss: 0.6890 - val_accuracy: 0.5522\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6648 - accuracy: 0.5803 - val_loss: 0.6645 - val_accuracy: 0.5745\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6660 - accuracy: 0.5870 - val_loss: 0.6504 - val_accuracy: 0.6015\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.5945 - val_loss: 0.6518 - val_accuracy: 0.5921\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.5925 - val_loss: 0.6513 - val_accuracy: 0.6010\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.43460578]\n",
      " [0.5799792 ]\n",
      " [0.51660883]\n",
      " [0.55524623]\n",
      " [0.1607517 ]\n",
      " [0.568764  ]\n",
      " [0.31825298]\n",
      " [0.4139826 ]\n",
      " [0.51660883]\n",
      " [0.6069512 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_26 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_27 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_26 is normal keras bn layer\n",
      "q_activation_26      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_27 is normal keras bn layer\n",
      "q_activation_27      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 1.4694 - accuracy: 0.5016 - val_loss: 0.7300 - val_accuracy: 0.5049\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7169 - accuracy: 0.5030 - val_loss: 0.7076 - val_accuracy: 0.5065\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7049 - accuracy: 0.5039 - val_loss: 0.7011 - val_accuracy: 0.5090\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7002 - accuracy: 0.5051 - val_loss: 0.6984 - val_accuracy: 0.5102\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6978 - accuracy: 0.5057 - val_loss: 0.6968 - val_accuracy: 0.5095\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5070 - val_loss: 0.6960 - val_accuracy: 0.5095\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5082 - val_loss: 0.6953 - val_accuracy: 0.5087\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5097 - val_loss: 0.6946 - val_accuracy: 0.5123\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5100 - val_loss: 0.6946 - val_accuracy: 0.5084\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5111 - val_loss: 0.6940 - val_accuracy: 0.5097\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5123 - val_loss: 0.6935 - val_accuracy: 0.5131\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5133 - val_loss: 0.6933 - val_accuracy: 0.5148\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5137 - val_loss: 0.6933 - val_accuracy: 0.5141\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5121 - val_loss: 0.6933 - val_accuracy: 0.5135\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5136 - val_loss: 0.6930 - val_accuracy: 0.5174\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5139 - val_loss: 0.6928 - val_accuracy: 0.5108\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5141 - val_loss: 0.6926 - val_accuracy: 0.5157\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5142 - val_loss: 0.6931 - val_accuracy: 0.5169\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5144 - val_loss: 0.6927 - val_accuracy: 0.5116\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6927 - val_accuracy: 0.5169\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6927 - val_accuracy: 0.5153\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5150 - val_loss: 0.6922 - val_accuracy: 0.5178\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5155 - val_loss: 0.6925 - val_accuracy: 0.5162\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5169 - val_loss: 0.6921 - val_accuracy: 0.5137\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5165 - val_loss: 0.6916 - val_accuracy: 0.5187\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5196 - val_loss: 0.6918 - val_accuracy: 0.5158\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5213 - val_loss: 0.6913 - val_accuracy: 0.5186\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5252 - val_loss: 0.6900 - val_accuracy: 0.5297\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5291 - val_loss: 0.6893 - val_accuracy: 0.5313\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5338 - val_loss: 0.6885 - val_accuracy: 0.5350\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6863 - val_accuracy: 0.5413\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5421 - val_loss: 0.6857 - val_accuracy: 0.5421\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5464 - val_loss: 0.6828 - val_accuracy: 0.5502\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6826 - accuracy: 0.5506 - val_loss: 0.6872 - val_accuracy: 0.5383\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5569 - val_loss: 0.6783 - val_accuracy: 0.5597\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5610 - val_loss: 0.6777 - val_accuracy: 0.5611\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5645 - val_loss: 0.6742 - val_accuracy: 0.5611\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6732 - accuracy: 0.5695 - val_loss: 0.6697 - val_accuracy: 0.5760\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6719 - accuracy: 0.5719 - val_loss: 0.6701 - val_accuracy: 0.5772\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6704 - accuracy: 0.5746 - val_loss: 0.6696 - val_accuracy: 0.5782\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5769 - val_loss: 0.6712 - val_accuracy: 0.5673\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.5811 - val_loss: 0.6654 - val_accuracy: 0.5862\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5829 - val_loss: 0.6716 - val_accuracy: 0.5751\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6660 - accuracy: 0.5832 - val_loss: 0.6744 - val_accuracy: 0.5663\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5851 - val_loss: 0.6584 - val_accuracy: 0.5948\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6699 - accuracy: 0.5775 - val_loss: 0.7021 - val_accuracy: 0.5186\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6728 - accuracy: 0.5706 - val_loss: 0.6655 - val_accuracy: 0.5890\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5820 - val_loss: 0.6643 - val_accuracy: 0.5824\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6643 - accuracy: 0.5888 - val_loss: 0.6582 - val_accuracy: 0.5958\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5732 - val_loss: 0.6710 - val_accuracy: 0.5771\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6696 - accuracy: 0.5807 - val_loss: 0.6642 - val_accuracy: 0.5860\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5902 - val_loss: 0.6575 - val_accuracy: 0.6029\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6607 - accuracy: 0.5939 - val_loss: 0.6594 - val_accuracy: 0.5969\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5946 - val_loss: 0.6576 - val_accuracy: 0.5991\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5967 - val_loss: 0.6666 - val_accuracy: 0.5865\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6602 - accuracy: 0.5962 - val_loss: 0.6594 - val_accuracy: 0.5993\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.5817 - val_loss: 0.6969 - val_accuracy: 0.5386\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6790 - accuracy: 0.5600 - val_loss: 0.6676 - val_accuracy: 0.5853\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6606 - accuracy: 0.5937 - val_loss: 0.7432 - val_accuracy: 0.5738\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6611 - accuracy: 0.5969 - val_loss: 0.6544 - val_accuracy: 0.6043\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6609 - accuracy: 0.5959 - val_loss: 0.6659 - val_accuracy: 0.6065\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6592 - accuracy: 0.5990 - val_loss: 0.6565 - val_accuracy: 0.5998\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6566 - accuracy: 0.6013 - val_loss: 0.6513 - val_accuracy: 0.6110\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6563 - accuracy: 0.6030 - val_loss: 0.6514 - val_accuracy: 0.6107\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6578 - accuracy: 0.6037 - val_loss: 0.6526 - val_accuracy: 0.6078\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6723 - accuracy: 0.5792 - val_loss: 0.6629 - val_accuracy: 0.5828\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6072 - val_loss: 0.6475 - val_accuracy: 0.6185\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.6007 - val_loss: 0.6566 - val_accuracy: 0.6021\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6099 - val_loss: 0.6483 - val_accuracy: 0.6166\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6511 - accuracy: 0.6110 - val_loss: 0.6502 - val_accuracy: 0.6129\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6510 - accuracy: 0.6108 - val_loss: 0.6463 - val_accuracy: 0.6225\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6517 - accuracy: 0.6128 - val_loss: 0.6445 - val_accuracy: 0.6223\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6094 - val_loss: 0.6463 - val_accuracy: 0.6177\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6084 - val_loss: 0.6473 - val_accuracy: 0.6210\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6120 - val_loss: 0.6446 - val_accuracy: 0.6243\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6121 - val_loss: 0.6460 - val_accuracy: 0.6176\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6557 - accuracy: 0.6052 - val_loss: 0.6489 - val_accuracy: 0.6148\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6767 - accuracy: 0.5717 - val_loss: 0.6688 - val_accuracy: 0.5833\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5989 - val_loss: 0.6511 - val_accuracy: 0.6141\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6098 - val_loss: 0.6443 - val_accuracy: 0.6222\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6480 - accuracy: 0.6158 - val_loss: 0.6541 - val_accuracy: 0.6052\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6463 - accuracy: 0.6167 - val_loss: 0.6447 - val_accuracy: 0.6193\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6478 - accuracy: 0.6159 - val_loss: 0.6550 - val_accuracy: 0.6059\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5828 - val_loss: 0.6738 - val_accuracy: 0.5714\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6724 - accuracy: 0.5746 - val_loss: 0.6685 - val_accuracy: 0.5816\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6677 - accuracy: 0.5817 - val_loss: 0.6640 - val_accuracy: 0.5872\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6644 - accuracy: 0.5886 - val_loss: 0.6567 - val_accuracy: 0.5997\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6607 - accuracy: 0.5958 - val_loss: 0.6531 - val_accuracy: 0.6065\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6007 - val_loss: 0.6502 - val_accuracy: 0.6133\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6549 - accuracy: 0.6060 - val_loss: 0.6484 - val_accuracy: 0.6121\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6089 - val_loss: 0.6436 - val_accuracy: 0.6236\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6499 - accuracy: 0.6136 - val_loss: 0.6489 - val_accuracy: 0.6173\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6477 - accuracy: 0.6166 - val_loss: 0.6399 - val_accuracy: 0.6265\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6466 - accuracy: 0.6181 - val_loss: 0.6634 - val_accuracy: 0.6000\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6461 - accuracy: 0.6189 - val_loss: 0.6425 - val_accuracy: 0.6236\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6453 - accuracy: 0.6196 - val_loss: 0.6435 - val_accuracy: 0.6224\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6451 - accuracy: 0.6194 - val_loss: 0.6424 - val_accuracy: 0.6239\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6452 - accuracy: 0.6198 - val_loss: 0.6399 - val_accuracy: 0.6273\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6446 - accuracy: 0.6205 - val_loss: 0.6519 - val_accuracy: 0.6115\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6442 - accuracy: 0.6202 - val_loss: 0.6389 - val_accuracy: 0.6280\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_26\n",
      "cannot prune layer q_activation_26\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_27\n",
      "cannot prune layer q_activation_27\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 6ms/step - loss: 0.6467 - accuracy: 0.6184 - val_loss: 0.6481 - val_accuracy: 0.6135\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6456 - accuracy: 0.6180 - val_loss: 0.6644 - val_accuracy: 0.5973\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6459 - accuracy: 0.6197 - val_loss: 0.6593 - val_accuracy: 0.5989\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5271 - val_loss: 0.6972 - val_accuracy: 0.5088\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6971 - accuracy: 0.5043 - val_loss: 0.6957 - val_accuracy: 0.5077\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6954 - accuracy: 0.5043 - val_loss: 0.7022 - val_accuracy: 0.5002\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5034 - val_loss: 0.6948 - val_accuracy: 0.5045\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5039 - val_loss: 0.6946 - val_accuracy: 0.5033\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5046 - val_loss: 0.6939 - val_accuracy: 0.5065\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5049 - val_loss: 0.6935 - val_accuracy: 0.5067\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5065 - val_loss: 0.6934 - val_accuracy: 0.5049\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6935 - accuracy: 0.5075 - val_loss: 0.6935 - val_accuracy: 0.5018\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5092 - val_loss: 0.6931 - val_accuracy: 0.5037\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5111 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5147 - val_loss: 0.6924 - val_accuracy: 0.5197\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5178 - val_loss: 0.6922 - val_accuracy: 0.5194\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6903 - val_accuracy: 0.5278\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6876 - accuracy: 0.5376 - val_loss: 0.6845 - val_accuracy: 0.5498\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6811 - accuracy: 0.5590 - val_loss: 0.6793 - val_accuracy: 0.5623\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5685 - val_loss: 0.6738 - val_accuracy: 0.5785\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5779 - val_loss: 0.6839 - val_accuracy: 0.5603\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6704 - accuracy: 0.5824 - val_loss: 0.6677 - val_accuracy: 0.5880\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.5934 - val_loss: 0.6604 - val_accuracy: 0.6032\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6629 - accuracy: 0.5972 - val_loss: 0.6567 - val_accuracy: 0.6080\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6616 - accuracy: 0.5999 - val_loss: 0.6574 - val_accuracy: 0.6081\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6588 - accuracy: 0.6025 - val_loss: 0.6600 - val_accuracy: 0.6049\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6570 - accuracy: 0.6057 - val_loss: 0.6525 - val_accuracy: 0.6132\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.6081 - val_loss: 0.6506 - val_accuracy: 0.6171\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6595 - accuracy: 0.6035 - val_loss: 0.6563 - val_accuracy: 0.6050\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.6015 - val_loss: 0.7518 - val_accuracy: 0.5189\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7002 - accuracy: 0.5196 - val_loss: 0.6933 - val_accuracy: 0.5254\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5387 - val_loss: 0.6852 - val_accuracy: 0.5530\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6785 - accuracy: 0.5669 - val_loss: 0.6733 - val_accuracy: 0.5824\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6671 - accuracy: 0.5902 - val_loss: 0.6596 - val_accuracy: 0.6055\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6027 - val_loss: 0.6520 - val_accuracy: 0.6120\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6562 - accuracy: 0.6081 - val_loss: 0.6517 - val_accuracy: 0.6134\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6617 - accuracy: 0.5997 - val_loss: 0.6682 - val_accuracy: 0.5859\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6580 - accuracy: 0.6063 - val_loss: 0.6511 - val_accuracy: 0.6167\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6580 - accuracy: 0.6054 - val_loss: 0.6641 - val_accuracy: 0.5858\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.6107 - val_loss: 0.6545 - val_accuracy: 0.6111\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6558 - accuracy: 0.6080 - val_loss: 0.6522 - val_accuracy: 0.6160\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6107 - val_loss: 0.6502 - val_accuracy: 0.6098\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6112 - val_loss: 0.6454 - val_accuracy: 0.6238\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6103 - val_loss: 0.6566 - val_accuracy: 0.6122\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6104 - val_loss: 0.6470 - val_accuracy: 0.6203\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6557 - accuracy: 0.6075 - val_loss: 0.6604 - val_accuracy: 0.6052\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6702 - accuracy: 0.5844 - val_loss: 0.6658 - val_accuracy: 0.5969\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6069 - val_loss: 0.6455 - val_accuracy: 0.6175\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6129 - val_loss: 0.6429 - val_accuracy: 0.6239\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5769 - val_loss: 0.6749 - val_accuracy: 0.5786\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.56838036]\n",
      " [0.5467338 ]\n",
      " [0.39709756]\n",
      " [0.52838206]\n",
      " [0.52136457]\n",
      " [0.52010477]\n",
      " [0.511523  ]\n",
      " [0.49453235]\n",
      " [0.518509  ]\n",
      " [0.5416345 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.3, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_28 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_29 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_28 is normal keras bn layer\n",
      "q_activation_28      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_29 is normal keras bn layer\n",
      "q_activation_29      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 6s 6ms/step - loss: 0.9524 - accuracy: 0.4985 - val_loss: 0.7330 - val_accuracy: 0.5033\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7177 - accuracy: 0.5018 - val_loss: 0.7100 - val_accuracy: 0.5058\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7078 - accuracy: 0.5029 - val_loss: 0.7057 - val_accuracy: 0.5061\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7038 - accuracy: 0.5031 - val_loss: 0.7023 - val_accuracy: 0.5056\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7014 - accuracy: 0.5027 - val_loss: 0.7006 - val_accuracy: 0.5033\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6991 - accuracy: 0.5029 - val_loss: 0.6986 - val_accuracy: 0.5033\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5036 - val_loss: 0.6972 - val_accuracy: 0.5038\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6964 - accuracy: 0.5040 - val_loss: 0.6962 - val_accuracy: 0.5039\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5046 - val_loss: 0.6952 - val_accuracy: 0.5057\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5054 - val_loss: 0.6947 - val_accuracy: 0.5052\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5063\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5069 - val_loss: 0.6938 - val_accuracy: 0.5090\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5088 - val_loss: 0.6937 - val_accuracy: 0.5081\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5093 - val_loss: 0.6938 - val_accuracy: 0.5102\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5088 - val_loss: 0.6935 - val_accuracy: 0.5100\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.5077\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5099 - val_loss: 0.6934 - val_accuracy: 0.5091\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5104 - val_loss: 0.6932 - val_accuracy: 0.5112\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6931 - val_accuracy: 0.5117\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5108 - val_loss: 0.6931 - val_accuracy: 0.5117\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5118 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.5116\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5134 - val_loss: 0.6929 - val_accuracy: 0.5135\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5130 - val_loss: 0.6929 - val_accuracy: 0.5151\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5128 - val_loss: 0.6928 - val_accuracy: 0.5132\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5134 - val_loss: 0.6927 - val_accuracy: 0.5141\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5145 - val_loss: 0.6927 - val_accuracy: 0.5138\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5140 - val_loss: 0.6928 - val_accuracy: 0.5119\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5145 - val_loss: 0.6927 - val_accuracy: 0.5135\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5142 - val_loss: 0.6929 - val_accuracy: 0.5123\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5146 - val_loss: 0.6926 - val_accuracy: 0.5148\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5139\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5161 - val_loss: 0.6924 - val_accuracy: 0.5147\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6923 - val_accuracy: 0.5163\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5164 - val_loss: 0.6926 - val_accuracy: 0.5137\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5166 - val_loss: 0.6926 - val_accuracy: 0.5138\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5164 - val_loss: 0.6922 - val_accuracy: 0.5166\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5167 - val_loss: 0.6925 - val_accuracy: 0.5150\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6922 - val_accuracy: 0.5143\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5182 - val_loss: 0.6922 - val_accuracy: 0.5157\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.6923 - val_accuracy: 0.5178\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5175 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5192 - val_loss: 0.6922 - val_accuracy: 0.5156\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5184 - val_loss: 0.6923 - val_accuracy: 0.5145\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5197 - val_loss: 0.6922 - val_accuracy: 0.5159\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5188 - val_loss: 0.6922 - val_accuracy: 0.5150\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5201 - val_loss: 0.6920 - val_accuracy: 0.5158\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5204 - val_loss: 0.6920 - val_accuracy: 0.5141\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5209 - val_loss: 0.6919 - val_accuracy: 0.5171\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6910 - accuracy: 0.5208 - val_loss: 0.6920 - val_accuracy: 0.5185\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5207 - val_loss: 0.6919 - val_accuracy: 0.5150\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6919 - val_accuracy: 0.5180\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5216 - val_loss: 0.6918 - val_accuracy: 0.5163\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5218 - val_loss: 0.6917 - val_accuracy: 0.5177\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5230 - val_loss: 0.6917 - val_accuracy: 0.5172\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6906 - accuracy: 0.5232 - val_loss: 0.6914 - val_accuracy: 0.5200\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5237 - val_loss: 0.6914 - val_accuracy: 0.5189\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6904 - accuracy: 0.5248 - val_loss: 0.6913 - val_accuracy: 0.5207\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5246 - val_loss: 0.6914 - val_accuracy: 0.5215\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5243 - val_loss: 0.6910 - val_accuracy: 0.5238\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6900 - accuracy: 0.5268 - val_loss: 0.6910 - val_accuracy: 0.5201\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6898 - accuracy: 0.5267 - val_loss: 0.6905 - val_accuracy: 0.5235\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6897 - accuracy: 0.5273 - val_loss: 0.6903 - val_accuracy: 0.5229\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6895 - accuracy: 0.5292 - val_loss: 0.6904 - val_accuracy: 0.5209\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5301 - val_loss: 0.6897 - val_accuracy: 0.5272\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6890 - accuracy: 0.5314 - val_loss: 0.6897 - val_accuracy: 0.5265\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6886 - accuracy: 0.5323 - val_loss: 0.6897 - val_accuracy: 0.5276\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6884 - accuracy: 0.5335 - val_loss: 0.6893 - val_accuracy: 0.5233\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6881 - accuracy: 0.5350 - val_loss: 0.6887 - val_accuracy: 0.5341\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5362 - val_loss: 0.6886 - val_accuracy: 0.5330\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6874 - accuracy: 0.5385 - val_loss: 0.6882 - val_accuracy: 0.5352\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5386 - val_loss: 0.6874 - val_accuracy: 0.5400\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6868 - accuracy: 0.5403 - val_loss: 0.6873 - val_accuracy: 0.5351\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6865 - accuracy: 0.5407 - val_loss: 0.6872 - val_accuracy: 0.5407\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6862 - accuracy: 0.5419 - val_loss: 0.6865 - val_accuracy: 0.5433\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5423 - val_loss: 0.6863 - val_accuracy: 0.5429\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5446 - val_loss: 0.6862 - val_accuracy: 0.5449\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5447 - val_loss: 0.6871 - val_accuracy: 0.5362\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5441 - val_loss: 0.6868 - val_accuracy: 0.5457\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5439 - val_loss: 0.6854 - val_accuracy: 0.5486\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6855 - accuracy: 0.5444 - val_loss: 0.6964 - val_accuracy: 0.5067\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6853 - accuracy: 0.5444 - val_loss: 0.6851 - val_accuracy: 0.5475\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6857 - accuracy: 0.5437 - val_loss: 0.6854 - val_accuracy: 0.5470\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6856 - accuracy: 0.5441 - val_loss: 0.6848 - val_accuracy: 0.5461\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5465 - val_loss: 0.6846 - val_accuracy: 0.5483\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5478 - val_loss: 0.6844 - val_accuracy: 0.5481\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6843 - accuracy: 0.5481 - val_loss: 0.6843 - val_accuracy: 0.5498\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.5503 - val_loss: 0.6824 - val_accuracy: 0.5479\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6816 - accuracy: 0.5517 - val_loss: 0.6831 - val_accuracy: 0.5503\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5540 - val_loss: 0.6821 - val_accuracy: 0.5499\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.5538 - val_loss: 0.6818 - val_accuracy: 0.5515\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6809 - accuracy: 0.5543 - val_loss: 0.6818 - val_accuracy: 0.5551\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.5550 - val_loss: 0.6816 - val_accuracy: 0.5577\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6802 - accuracy: 0.5566 - val_loss: 0.6807 - val_accuracy: 0.5548\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5578 - val_loss: 0.6809 - val_accuracy: 0.5545\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5569 - val_loss: 0.6807 - val_accuracy: 0.5519\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5584 - val_loss: 0.6801 - val_accuracy: 0.5549\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5588 - val_loss: 0.6792 - val_accuracy: 0.5608\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6788 - accuracy: 0.5598 - val_loss: 0.6790 - val_accuracy: 0.5619\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_28\n",
      "cannot prune layer q_activation_28\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_29\n",
      "cannot prune layer q_activation_29\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6785 - accuracy: 0.5609 - val_loss: 0.6871 - val_accuracy: 0.5496\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6784 - accuracy: 0.5606 - val_loss: 0.6830 - val_accuracy: 0.5513\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6782 - accuracy: 0.5612 - val_loss: 0.6930 - val_accuracy: 0.5428\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6790 - accuracy: 0.5592 - val_loss: 0.6973 - val_accuracy: 0.5219\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6785 - accuracy: 0.5598 - val_loss: 0.6855 - val_accuracy: 0.5538\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6780 - accuracy: 0.5612 - val_loss: 0.6893 - val_accuracy: 0.5460\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6782 - accuracy: 0.5619 - val_loss: 0.6851 - val_accuracy: 0.5525\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6777 - accuracy: 0.5638 - val_loss: 0.6782 - val_accuracy: 0.5646\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6772 - accuracy: 0.5659 - val_loss: 0.6769 - val_accuracy: 0.5701\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6766 - accuracy: 0.5669 - val_loss: 0.6778 - val_accuracy: 0.5674\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6764 - accuracy: 0.5674 - val_loss: 0.6760 - val_accuracy: 0.5697\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.5690 - val_loss: 0.6800 - val_accuracy: 0.5654\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6754 - accuracy: 0.5694 - val_loss: 0.6781 - val_accuracy: 0.5679\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6753 - accuracy: 0.5698 - val_loss: 0.6755 - val_accuracy: 0.5738\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6749 - accuracy: 0.5711 - val_loss: 0.6755 - val_accuracy: 0.5747\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5713 - val_loss: 0.6749 - val_accuracy: 0.5747\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5717 - val_loss: 0.6774 - val_accuracy: 0.5681\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6740 - accuracy: 0.5732 - val_loss: 0.6740 - val_accuracy: 0.5788\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6737 - accuracy: 0.5732 - val_loss: 0.6748 - val_accuracy: 0.5746\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6734 - accuracy: 0.5737 - val_loss: 0.6731 - val_accuracy: 0.5772\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5748 - val_loss: 0.6737 - val_accuracy: 0.5758\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5751 - val_loss: 0.6734 - val_accuracy: 0.5756\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6729 - accuracy: 0.5745 - val_loss: 0.6745 - val_accuracy: 0.5736\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6726 - accuracy: 0.5754 - val_loss: 0.6742 - val_accuracy: 0.5765\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6722 - accuracy: 0.5765 - val_loss: 0.6750 - val_accuracy: 0.5739\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6721 - accuracy: 0.5768 - val_loss: 0.6721 - val_accuracy: 0.5815\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5764 - val_loss: 0.6743 - val_accuracy: 0.5732\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6715 - accuracy: 0.5781 - val_loss: 0.6723 - val_accuracy: 0.5790\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5790 - val_loss: 0.6710 - val_accuracy: 0.5806\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6711 - accuracy: 0.5782 - val_loss: 0.6709 - val_accuracy: 0.5824\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5790 - val_loss: 0.6716 - val_accuracy: 0.5795\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6704 - accuracy: 0.5796 - val_loss: 0.6700 - val_accuracy: 0.5861\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6703 - accuracy: 0.5803 - val_loss: 0.6712 - val_accuracy: 0.5799\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6700 - accuracy: 0.5810 - val_loss: 0.6698 - val_accuracy: 0.5829\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5647 - val_loss: 0.6761 - val_accuracy: 0.5688\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5740 - val_loss: 0.6719 - val_accuracy: 0.5780\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5795 - val_loss: 0.6698 - val_accuracy: 0.5840\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5812 - val_loss: 0.6697 - val_accuracy: 0.5851\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6692 - accuracy: 0.5825 - val_loss: 0.6690 - val_accuracy: 0.5863\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.5832 - val_loss: 0.6688 - val_accuracy: 0.5846\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.5831 - val_loss: 0.6684 - val_accuracy: 0.5872\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.5845 - val_loss: 0.6688 - val_accuracy: 0.5871\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6685 - accuracy: 0.5845 - val_loss: 0.6678 - val_accuracy: 0.5897\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6681 - accuracy: 0.5846 - val_loss: 0.6672 - val_accuracy: 0.5873\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6681 - accuracy: 0.5854 - val_loss: 0.6687 - val_accuracy: 0.5858\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6678 - accuracy: 0.5858 - val_loss: 0.6671 - val_accuracy: 0.5908\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6676 - accuracy: 0.5872 - val_loss: 0.6669 - val_accuracy: 0.5887\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6672 - accuracy: 0.5870 - val_loss: 0.6655 - val_accuracy: 0.5933\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6671 - accuracy: 0.5866 - val_loss: 0.6661 - val_accuracy: 0.5893\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6670 - accuracy: 0.5869 - val_loss: 0.6672 - val_accuracy: 0.5862\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.59152454]\n",
      " [0.5024879 ]\n",
      " [0.30662644]\n",
      " [0.4050146 ]\n",
      " [0.5122552 ]\n",
      " [0.5921538 ]\n",
      " [0.34931403]\n",
      " [0.394228  ]\n",
      " [0.6214111 ]\n",
      " [0.57594335]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_30 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_31 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_30 is normal keras bn layer\n",
      "q_activation_30      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_31 is normal keras bn layer\n",
      "q_activation_31      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 0.7283 - accuracy: 0.5029 - val_loss: 0.6960 - val_accuracy: 0.5071\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5050 - val_loss: 0.6945 - val_accuracy: 0.5064\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6937 - accuracy: 0.5064 - val_loss: 0.6938 - val_accuracy: 0.5086\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5069 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.5109\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6936 - val_accuracy: 0.5074\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6928 - val_accuracy: 0.5150\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6928 - val_accuracy: 0.5117\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5109 - val_loss: 0.6924 - val_accuracy: 0.5096\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6923 - val_accuracy: 0.5120\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5100 - val_loss: 0.6944 - val_accuracy: 0.5099\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5123 - val_loss: 0.6929 - val_accuracy: 0.5134\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5133 - val_loss: 0.6921 - val_accuracy: 0.5138\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5137 - val_loss: 0.6921 - val_accuracy: 0.5142\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5142 - val_loss: 0.6921 - val_accuracy: 0.5120\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5149 - val_loss: 0.6918 - val_accuracy: 0.5102\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.5147 - val_loss: 0.6916 - val_accuracy: 0.5145\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5166 - val_loss: 0.6917 - val_accuracy: 0.5180\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5162 - val_loss: 0.6911 - val_accuracy: 0.5186\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5171 - val_loss: 0.6909 - val_accuracy: 0.5198\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5158 - val_loss: 0.6903 - val_accuracy: 0.5202\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6905 - accuracy: 0.5198 - val_loss: 0.6904 - val_accuracy: 0.5221\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5214 - val_loss: 0.6940 - val_accuracy: 0.5133\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5238 - val_loss: 0.6902 - val_accuracy: 0.5221\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6891 - accuracy: 0.5265 - val_loss: 0.6887 - val_accuracy: 0.5242\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6885 - accuracy: 0.5271 - val_loss: 0.6867 - val_accuracy: 0.5337\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.5296 - val_loss: 0.6873 - val_accuracy: 0.5359\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6872 - accuracy: 0.5331 - val_loss: 0.6848 - val_accuracy: 0.5383\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6849 - accuracy: 0.5394 - val_loss: 0.6835 - val_accuracy: 0.5494\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6848 - accuracy: 0.5406 - val_loss: 0.7106 - val_accuracy: 0.5061\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5187 - val_loss: 0.6925 - val_accuracy: 0.5095\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5109 - val_loss: 0.6933 - val_accuracy: 0.5048\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5081\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5186 - val_loss: 0.6914 - val_accuracy: 0.5174\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6888 - accuracy: 0.5313 - val_loss: 0.6896 - val_accuracy: 0.5236\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6892 - accuracy: 0.5278 - val_loss: 0.6980 - val_accuracy: 0.5066\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5182 - val_loss: 0.6918 - val_accuracy: 0.5105\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5217 - val_loss: 0.6924 - val_accuracy: 0.5167\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5119 - val_loss: 0.6924 - val_accuracy: 0.5110\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5122 - val_loss: 0.6921 - val_accuracy: 0.5099\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6909 - accuracy: 0.5173 - val_loss: 0.6912 - val_accuracy: 0.5140\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5187 - val_loss: 0.6927 - val_accuracy: 0.5077\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6912 - accuracy: 0.5161 - val_loss: 0.6923 - val_accuracy: 0.5160\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6907 - accuracy: 0.5179 - val_loss: 0.6948 - val_accuracy: 0.5075\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5073 - val_loss: 0.6925 - val_accuracy: 0.5050\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5073 - val_loss: 0.6925 - val_accuracy: 0.5036\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5093 - val_loss: 0.6923 - val_accuracy: 0.5057\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5098 - val_loss: 0.6924 - val_accuracy: 0.5098\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5121 - val_loss: 0.6921 - val_accuracy: 0.5099\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_30\n",
      "cannot prune layer q_activation_30\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_31\n",
      "cannot prune layer q_activation_31\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 7s 7ms/step - loss: 0.6901 - accuracy: 0.5222 - val_loss: 0.6926 - val_accuracy: 0.5104\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6906 - accuracy: 0.5213 - val_loss: 0.6932 - val_accuracy: 0.5042\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6917 - accuracy: 0.5143 - val_loss: 0.7053 - val_accuracy: 0.5038\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6899 - accuracy: 0.5246 - val_loss: 0.7040 - val_accuracy: 0.5017\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6875 - accuracy: 0.5332 - val_loss: 0.6880 - val_accuracy: 0.5275\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6840 - accuracy: 0.5412 - val_loss: 0.7062 - val_accuracy: 0.5265\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6825 - accuracy: 0.5484 - val_loss: 0.6869 - val_accuracy: 0.5239\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5513 - val_loss: 0.6903 - val_accuracy: 0.5359\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5491 - val_loss: 0.6810 - val_accuracy: 0.5485\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6793 - accuracy: 0.5573 - val_loss: 0.6748 - val_accuracy: 0.5663\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6778 - accuracy: 0.5594 - val_loss: 0.6760 - val_accuracy: 0.5595\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6773 - accuracy: 0.5604 - val_loss: 0.6744 - val_accuracy: 0.5625\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6763 - accuracy: 0.5617 - val_loss: 0.6872 - val_accuracy: 0.5257\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6769 - accuracy: 0.5641 - val_loss: 0.6775 - val_accuracy: 0.5640\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6737 - accuracy: 0.5676 - val_loss: 0.6723 - val_accuracy: 0.5721\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6789 - accuracy: 0.5621 - val_loss: 0.6780 - val_accuracy: 0.5660\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5484 - val_loss: 0.6812 - val_accuracy: 0.5562\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6794 - accuracy: 0.5587 - val_loss: 0.6736 - val_accuracy: 0.5723\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.5675 - val_loss: 0.6723 - val_accuracy: 0.5747\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6751 - accuracy: 0.5686 - val_loss: 0.6798 - val_accuracy: 0.5547\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.5696 - val_loss: 0.6773 - val_accuracy: 0.5579\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5412 - val_loss: 0.6873 - val_accuracy: 0.5372\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5545 - val_loss: 0.6794 - val_accuracy: 0.5592\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6757 - accuracy: 0.5687 - val_loss: 0.6751 - val_accuracy: 0.5734\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6750 - accuracy: 0.5704 - val_loss: 0.6743 - val_accuracy: 0.5740\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6742 - accuracy: 0.5717 - val_loss: 0.6757 - val_accuracy: 0.5666\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5716 - val_loss: 0.6754 - val_accuracy: 0.5703\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5667 - val_loss: 0.7004 - val_accuracy: 0.5067\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5028 - val_loss: 0.6931 - val_accuracy: 0.5057\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5202 - val_loss: 0.6894 - val_accuracy: 0.5304\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5421 - val_loss: 0.6824 - val_accuracy: 0.5493\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6806 - accuracy: 0.5519 - val_loss: 0.6789 - val_accuracy: 0.5574\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6783 - accuracy: 0.5579 - val_loss: 0.6763 - val_accuracy: 0.5653\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6768 - accuracy: 0.5607 - val_loss: 0.6875 - val_accuracy: 0.5404\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6745 - accuracy: 0.5670 - val_loss: 0.6725 - val_accuracy: 0.5629\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6721 - accuracy: 0.5720 - val_loss: 0.6749 - val_accuracy: 0.5725\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6714 - accuracy: 0.5734 - val_loss: 0.6718 - val_accuracy: 0.5777\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6720 - accuracy: 0.5727 - val_loss: 0.6989 - val_accuracy: 0.5220\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6780 - accuracy: 0.5573 - val_loss: 0.6773 - val_accuracy: 0.5603\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6730 - accuracy: 0.5719 - val_loss: 0.6713 - val_accuracy: 0.5823\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.5672 - val_loss: 0.6819 - val_accuracy: 0.5460\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.5606 - val_loss: 0.6723 - val_accuracy: 0.5728\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5693 - val_loss: 0.6673 - val_accuracy: 0.5794\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6731 - accuracy: 0.5694 - val_loss: 0.7096 - val_accuracy: 0.5020\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6743 - accuracy: 0.5662 - val_loss: 0.6664 - val_accuracy: 0.5792\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6703 - accuracy: 0.5766 - val_loss: 0.6715 - val_accuracy: 0.5762\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.5787 - val_loss: 0.6640 - val_accuracy: 0.5827\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6772 - accuracy: 0.5594 - val_loss: 0.6788 - val_accuracy: 0.5493\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6733 - accuracy: 0.5675 - val_loss: 0.6799 - val_accuracy: 0.5645\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6706 - accuracy: 0.5752 - val_loss: 0.6692 - val_accuracy: 0.5807\n",
      "1714/1714 [==============================] - 3s 2ms/step\n",
      "[[0.5447618 ]\n",
      " [0.43869933]\n",
      " [0.57286525]\n",
      " [0.48713407]\n",
      " [0.4330225 ]\n",
      " [0.38866293]\n",
      " [0.40630168]\n",
      " [0.3607917 ]\n",
      " [0.5293383 ]\n",
      " [0.6608461 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.001, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_32 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_33 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_32 is normal keras bn layer\n",
      "q_activation_32      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_33 is normal keras bn layer\n",
      "q_activation_33      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 2.5501 - accuracy: 0.4980 - val_loss: 0.9253 - val_accuracy: 0.4997\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7447 - accuracy: 0.5011 - val_loss: 0.7200 - val_accuracy: 0.5001\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7127 - accuracy: 0.5012 - val_loss: 0.7094 - val_accuracy: 0.4996\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7096 - accuracy: 0.5018 - val_loss: 0.7066 - val_accuracy: 0.5020\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7067 - accuracy: 0.5019 - val_loss: 0.7087 - val_accuracy: 0.4996\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7038 - accuracy: 0.5023 - val_loss: 0.7013 - val_accuracy: 0.5034\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7019 - accuracy: 0.5017 - val_loss: 0.7023 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7006 - accuracy: 0.5020 - val_loss: 0.7012 - val_accuracy: 0.5002\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6997 - accuracy: 0.5029 - val_loss: 0.6990 - val_accuracy: 0.5046\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6990 - accuracy: 0.5027 - val_loss: 0.6986 - val_accuracy: 0.5027\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6981 - accuracy: 0.5038 - val_loss: 0.6997 - val_accuracy: 0.5064\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6974 - accuracy: 0.5049 - val_loss: 0.6994 - val_accuracy: 0.5051\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6971 - accuracy: 0.5033 - val_loss: 0.6958 - val_accuracy: 0.5025\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5040 - val_loss: 0.6951 - val_accuracy: 0.5030\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5054 - val_loss: 0.7015 - val_accuracy: 0.5036\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5048 - val_loss: 0.6942 - val_accuracy: 0.5056\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5061 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5074 - val_loss: 0.6932 - val_accuracy: 0.5085\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5085 - val_loss: 0.6933 - val_accuracy: 0.5059\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5066\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.5070\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6930 - val_accuracy: 0.5072\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6929 - val_accuracy: 0.5075\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5137 - val_loss: 0.6928 - val_accuracy: 0.5082\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5131 - val_loss: 0.6928 - val_accuracy: 0.5103\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5103\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5139 - val_loss: 0.6934 - val_accuracy: 0.5105\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5146 - val_loss: 0.6924 - val_accuracy: 0.5137\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6918 - accuracy: 0.5146 - val_loss: 0.6925 - val_accuracy: 0.5108\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5162 - val_loss: 0.6921 - val_accuracy: 0.5128\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5153 - val_loss: 0.6922 - val_accuracy: 0.5140\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5158 - val_loss: 0.6920 - val_accuracy: 0.5152\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6914 - accuracy: 0.5161 - val_loss: 0.6920 - val_accuracy: 0.5079\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.5174 - val_loss: 0.6916 - val_accuracy: 0.5154\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5175 - val_loss: 0.6912 - val_accuracy: 0.5166\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5184 - val_loss: 0.6987 - val_accuracy: 0.5162\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6915 - accuracy: 0.5210 - val_loss: 0.6914 - val_accuracy: 0.5173\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6908 - accuracy: 0.5223 - val_loss: 0.6902 - val_accuracy: 0.5223\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6903 - accuracy: 0.5255 - val_loss: 0.6892 - val_accuracy: 0.5279\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6893 - accuracy: 0.5301 - val_loss: 0.6879 - val_accuracy: 0.5336\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6862 - val_accuracy: 0.5414\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6854 - accuracy: 0.5441 - val_loss: 0.6842 - val_accuracy: 0.5485\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5488 - val_loss: 0.6848 - val_accuracy: 0.5456\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5538 - val_loss: 0.6782 - val_accuracy: 0.5609\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6792 - accuracy: 0.5580 - val_loss: 0.6769 - val_accuracy: 0.5640\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6769 - accuracy: 0.5621 - val_loss: 0.6734 - val_accuracy: 0.5664\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5659 - val_loss: 0.6759 - val_accuracy: 0.5661\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6731 - accuracy: 0.5688 - val_loss: 0.6744 - val_accuracy: 0.5641\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6710 - accuracy: 0.5735 - val_loss: 0.6795 - val_accuracy: 0.5590\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5731 - val_loss: 0.6728 - val_accuracy: 0.5707\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6691 - accuracy: 0.5767 - val_loss: 0.6651 - val_accuracy: 0.5846\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6676 - accuracy: 0.5788 - val_loss: 0.6647 - val_accuracy: 0.5825\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6775 - accuracy: 0.5617 - val_loss: 0.6767 - val_accuracy: 0.5589\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6715 - accuracy: 0.5706 - val_loss: 0.6671 - val_accuracy: 0.5725\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6674 - accuracy: 0.5779 - val_loss: 0.6604 - val_accuracy: 0.5845\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5814 - val_loss: 0.6596 - val_accuracy: 0.5910\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.5847 - val_loss: 0.6630 - val_accuracy: 0.5840\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6664 - accuracy: 0.5825 - val_loss: 0.6599 - val_accuracy: 0.5875\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.5811 - val_loss: 0.6822 - val_accuracy: 0.5513\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6663 - accuracy: 0.5824 - val_loss: 0.6627 - val_accuracy: 0.5896\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5899 - val_loss: 0.6568 - val_accuracy: 0.5962\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.5491 - val_loss: 0.6899 - val_accuracy: 0.5333\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6743 - accuracy: 0.5673 - val_loss: 0.6609 - val_accuracy: 0.5901\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6616 - accuracy: 0.5908 - val_loss: 0.6618 - val_accuracy: 0.5857\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6613 - accuracy: 0.5907 - val_loss: 0.6691 - val_accuracy: 0.5678\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6779 - accuracy: 0.5583 - val_loss: 0.6690 - val_accuracy: 0.5763\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6635 - accuracy: 0.5842 - val_loss: 0.6537 - val_accuracy: 0.5950\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.5924 - val_loss: 0.6530 - val_accuracy: 0.5993\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6591 - accuracy: 0.5910 - val_loss: 0.6524 - val_accuracy: 0.6004\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6596 - accuracy: 0.5921 - val_loss: 0.6643 - val_accuracy: 0.5833\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6727 - accuracy: 0.5733 - val_loss: 0.6697 - val_accuracy: 0.5784\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6650 - accuracy: 0.5848 - val_loss: 0.6592 - val_accuracy: 0.5948\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6603 - accuracy: 0.5939 - val_loss: 0.6570 - val_accuracy: 0.5978\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6570 - accuracy: 0.5968 - val_loss: 0.6534 - val_accuracy: 0.5994\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6551 - accuracy: 0.5985 - val_loss: 0.6526 - val_accuracy: 0.6001\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6573 - accuracy: 0.5953 - val_loss: 0.6592 - val_accuracy: 0.5965\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6013 - val_loss: 0.6523 - val_accuracy: 0.5998\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6021 - val_loss: 0.6601 - val_accuracy: 0.5940\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6012 - val_loss: 0.6487 - val_accuracy: 0.6073\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6515 - accuracy: 0.6037 - val_loss: 0.6496 - val_accuracy: 0.6099\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5729 - val_loss: 0.6888 - val_accuracy: 0.5258\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6636 - accuracy: 0.5841 - val_loss: 0.6481 - val_accuracy: 0.6069\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6019 - val_loss: 0.6444 - val_accuracy: 0.6120\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6508 - accuracy: 0.6051 - val_loss: 0.6490 - val_accuracy: 0.6072\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6498 - accuracy: 0.6070 - val_loss: 0.6664 - val_accuracy: 0.5897\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6494 - accuracy: 0.6068 - val_loss: 0.6518 - val_accuracy: 0.6051\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6495 - accuracy: 0.6074 - val_loss: 0.6418 - val_accuracy: 0.6160\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6512 - accuracy: 0.6069 - val_loss: 0.6495 - val_accuracy: 0.6093\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6114 - val_loss: 0.6420 - val_accuracy: 0.6158\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6475 - accuracy: 0.6103 - val_loss: 0.6418 - val_accuracy: 0.6179\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.5868 - val_loss: 0.6566 - val_accuracy: 0.5995\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.5995 - val_loss: 0.6529 - val_accuracy: 0.6004\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6515 - accuracy: 0.6033 - val_loss: 0.6478 - val_accuracy: 0.6065\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6503 - accuracy: 0.6047 - val_loss: 0.6502 - val_accuracy: 0.6037\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6489 - accuracy: 0.6076 - val_loss: 0.6446 - val_accuracy: 0.6114\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6488 - accuracy: 0.6072 - val_loss: 0.6437 - val_accuracy: 0.6135\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6505 - accuracy: 0.6068 - val_loss: 0.6547 - val_accuracy: 0.6015\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5168 - val_loss: 0.6966 - val_accuracy: 0.5019\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5048 - val_loss: 0.6946 - val_accuracy: 0.5003\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5085 - val_loss: 0.6941 - val_accuracy: 0.5045\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_32\n",
      "cannot prune layer q_activation_32\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_33\n",
      "cannot prune layer q_activation_33\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6912 - accuracy: 0.5232 - val_loss: 0.7012 - val_accuracy: 0.4972\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6745 - accuracy: 0.5682 - val_loss: 0.7101 - val_accuracy: 0.5083\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6962 - accuracy: 0.5116 - val_loss: 0.6981 - val_accuracy: 0.5120\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5186 - val_loss: 0.6937 - val_accuracy: 0.5208\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5176 - val_loss: 0.7109 - val_accuracy: 0.4992\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5156 - val_loss: 0.7046 - val_accuracy: 0.5093\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5185 - val_loss: 0.6920 - val_accuracy: 0.5270\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6902 - accuracy: 0.5314 - val_loss: 0.6969 - val_accuracy: 0.5100\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6864 - accuracy: 0.5431 - val_loss: 0.6846 - val_accuracy: 0.5538\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6819 - accuracy: 0.5568 - val_loss: 0.6779 - val_accuracy: 0.5663\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6758 - accuracy: 0.5705 - val_loss: 0.6725 - val_accuracy: 0.5763\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6717 - accuracy: 0.5768 - val_loss: 0.6669 - val_accuracy: 0.5869\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6688 - accuracy: 0.5820 - val_loss: 0.6631 - val_accuracy: 0.5905\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6682 - accuracy: 0.5847 - val_loss: 0.6625 - val_accuracy: 0.5897\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5861 - val_loss: 0.6605 - val_accuracy: 0.5919\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6654 - accuracy: 0.5888 - val_loss: 0.6585 - val_accuracy: 0.5947\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5722 - val_loss: 0.6607 - val_accuracy: 0.5961\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6649 - accuracy: 0.5919 - val_loss: 0.6594 - val_accuracy: 0.5964\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6659 - accuracy: 0.5893 - val_loss: 0.6702 - val_accuracy: 0.5754\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6642 - accuracy: 0.5900 - val_loss: 0.6611 - val_accuracy: 0.5920\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6652 - accuracy: 0.5911 - val_loss: 0.6612 - val_accuracy: 0.5966\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6624 - accuracy: 0.5944 - val_loss: 0.6574 - val_accuracy: 0.5993\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6631 - accuracy: 0.5925 - val_loss: 0.6563 - val_accuracy: 0.6007\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6609 - accuracy: 0.5962 - val_loss: 0.6629 - val_accuracy: 0.6024\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6622 - accuracy: 0.5957 - val_loss: 0.6556 - val_accuracy: 0.6035\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5961 - val_loss: 0.6576 - val_accuracy: 0.5987\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6683 - accuracy: 0.5856 - val_loss: 0.6654 - val_accuracy: 0.5875\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6616 - accuracy: 0.5956 - val_loss: 0.6570 - val_accuracy: 0.6001\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6615 - accuracy: 0.5968 - val_loss: 0.6560 - val_accuracy: 0.6017\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6606 - accuracy: 0.5976 - val_loss: 0.6558 - val_accuracy: 0.6024\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6617 - accuracy: 0.5957 - val_loss: 0.6563 - val_accuracy: 0.6030\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6589 - accuracy: 0.5994 - val_loss: 0.6544 - val_accuracy: 0.6064\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.6001 - val_loss: 0.6526 - val_accuracy: 0.6085\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6605 - accuracy: 0.5984 - val_loss: 0.6608 - val_accuracy: 0.5935\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6599 - accuracy: 0.5980 - val_loss: 0.6533 - val_accuracy: 0.6045\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6582 - accuracy: 0.5986 - val_loss: 0.6516 - val_accuracy: 0.6092\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6594 - accuracy: 0.5997 - val_loss: 0.6583 - val_accuracy: 0.6032\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5985 - val_loss: 0.6619 - val_accuracy: 0.6037\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6589 - accuracy: 0.5997 - val_loss: 0.6552 - val_accuracy: 0.6009\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.5998 - val_loss: 0.6515 - val_accuracy: 0.6078\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6590 - accuracy: 0.6002 - val_loss: 0.6542 - val_accuracy: 0.6039\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.6016 - val_loss: 0.6508 - val_accuracy: 0.6079\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6578 - accuracy: 0.6015 - val_loss: 0.6526 - val_accuracy: 0.6057\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6573 - accuracy: 0.6023 - val_loss: 0.6534 - val_accuracy: 0.6035\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6605 - accuracy: 0.5952 - val_loss: 0.6542 - val_accuracy: 0.6028\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6603 - accuracy: 0.5962 - val_loss: 0.6560 - val_accuracy: 0.6030\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6569 - accuracy: 0.6008 - val_loss: 0.6526 - val_accuracy: 0.6046\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6554 - accuracy: 0.6019 - val_loss: 0.6508 - val_accuracy: 0.6075\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.6026 - val_loss: 0.6511 - val_accuracy: 0.6077\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6550 - accuracy: 0.6033 - val_loss: 0.6533 - val_accuracy: 0.6093\n",
      "1714/1714 [==============================] - 4s 1ms/step\n",
      "[[0.46138406]\n",
      " [0.6879751 ]\n",
      " [0.34491414]\n",
      " [0.51884025]\n",
      " [0.25803274]\n",
      " [0.46989125]\n",
      " [0.34973258]\n",
      " [0.32250845]\n",
      " [0.44384003]\n",
      " [0.6836106 ]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n",
      "Testing hyperparameters: {'MODEL_TYPE': 'DNN', 'NUM_TIME_SLICES': 8, 'TRAIN_PT_THRESHOLD': 2, 'TEST_PT_THRESHOLD': 2, 'DNN_LAYERS': [16, 8], 'CONV_LAYER_DEPTHS': [4, 7], 'CONV_LAYER_KERNELS': [(3, 3), (3, 3)], 'CONV_LAYER_STRIDES': [(1, 1), (1, 1)], 'FLATTENED_LAYERS': [7], 'MAX_POOLING_SIZE': (2, 2), 'OUTPUT': 'SINGLE', 'WEIGHTS_BITS': 10, 'BIAS_BITS': 10, 'ACTIVATION_BITS': 15, 'INTEGER_BITS': 2, 'LEARNING_RATE': 0.0005, 'BATCH_SIZE': 1024, 'EPOCHS': 100, 'PATIENCE': 20, 'PRUNE_START_EPOCH': 0, 'NUM_PRUNE_EPOCHS': 10, 'FINAL_SPARSITY': 0.5, 'POST_PRUNE_EPOCHS': 50}\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 16)                1696      \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_34 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 8)                 32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_35 (QActivati  (None, 8)                 0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1889 (7.38 KB)\n",
      "Non-trainable params: 48 (192.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=16 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_34 is normal keras bn layer\n",
      "q_activation_34      quantized_relu(15,0)\n",
      "dense2               u=8 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "batch_normalization_35 is normal keras bn layer\n",
      "q_activation_35      quantized_relu(15,0)\n",
      "dense_output         u=1 quantized_bits(10,0,1,alpha='auto_po2') quantized_bits(10,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 1 0 1]\n",
      "Epoch 1/100\n",
      "429/429 [==============================] - 5s 6ms/step - loss: 2.7148 - accuracy: 0.4979 - val_loss: 2.2982 - val_accuracy: 0.5001\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 2.0114 - accuracy: 0.4980 - val_loss: 1.8685 - val_accuracy: 0.5004\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.7553 - accuracy: 0.4986 - val_loss: 1.6900 - val_accuracy: 0.5051\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.5454 - accuracy: 0.5001 - val_loss: 1.4310 - val_accuracy: 0.5034\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.3510 - accuracy: 0.5008 - val_loss: 1.1982 - val_accuracy: 0.5048\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 1.1064 - accuracy: 0.4995 - val_loss: 0.9319 - val_accuracy: 0.5024\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.8665 - accuracy: 0.4991 - val_loss: 0.7940 - val_accuracy: 0.5013\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7820 - accuracy: 0.4985 - val_loss: 0.7505 - val_accuracy: 0.5004\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7471 - accuracy: 0.4988 - val_loss: 0.7316 - val_accuracy: 0.5013\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7289 - accuracy: 0.4999 - val_loss: 0.7215 - val_accuracy: 0.5037\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7211 - accuracy: 0.5004 - val_loss: 0.7163 - val_accuracy: 0.5043\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7162 - accuracy: 0.5003 - val_loss: 0.7125 - val_accuracy: 0.5059\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7125 - accuracy: 0.5010 - val_loss: 0.7095 - val_accuracy: 0.5050\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7096 - accuracy: 0.5005 - val_loss: 0.7061 - val_accuracy: 0.5032\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7065 - accuracy: 0.5012 - val_loss: 0.7044 - val_accuracy: 0.5044\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7044 - accuracy: 0.5012 - val_loss: 0.7025 - val_accuracy: 0.5043\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7025 - accuracy: 0.5020 - val_loss: 0.7010 - val_accuracy: 0.5032\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7013 - accuracy: 0.5018 - val_loss: 0.7025 - val_accuracy: 0.5021\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6997 - accuracy: 0.5024 - val_loss: 0.6991 - val_accuracy: 0.5022\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6984 - accuracy: 0.5020 - val_loss: 0.6981 - val_accuracy: 0.5029\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6972 - accuracy: 0.5036 - val_loss: 0.6972 - val_accuracy: 0.5044\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6966 - accuracy: 0.5037 - val_loss: 0.6966 - val_accuracy: 0.5046\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5041 - val_loss: 0.6959 - val_accuracy: 0.5047\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5058 - val_loss: 0.6955 - val_accuracy: 0.5027\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5052 - val_loss: 0.6950 - val_accuracy: 0.5024\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6946 - accuracy: 0.5049 - val_loss: 0.6954 - val_accuracy: 0.5018\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5056 - val_loss: 0.6946 - val_accuracy: 0.5024\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6942 - accuracy: 0.5068 - val_loss: 0.6946 - val_accuracy: 0.5038\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5051 - val_loss: 0.6945 - val_accuracy: 0.5035\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5067 - val_loss: 0.6943 - val_accuracy: 0.5015\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6936 - accuracy: 0.5068 - val_loss: 0.6942 - val_accuracy: 0.5046\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5076 - val_loss: 0.6943 - val_accuracy: 0.5046\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6934 - accuracy: 0.5065 - val_loss: 0.6942 - val_accuracy: 0.5059\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5073 - val_loss: 0.6941 - val_accuracy: 0.5053\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5085 - val_loss: 0.6939 - val_accuracy: 0.5037\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5079 - val_loss: 0.6938 - val_accuracy: 0.5052\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6930 - accuracy: 0.5086 - val_loss: 0.6938 - val_accuracy: 0.5035\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5102 - val_loss: 0.6937 - val_accuracy: 0.5060\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5106 - val_loss: 0.6938 - val_accuracy: 0.5050\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5097 - val_loss: 0.6935 - val_accuracy: 0.5075\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5106 - val_loss: 0.6936 - val_accuracy: 0.5066\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5112 - val_loss: 0.6935 - val_accuracy: 0.5080\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5117 - val_loss: 0.6933 - val_accuracy: 0.5086\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5124 - val_loss: 0.6935 - val_accuracy: 0.5054\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.5081\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5119 - val_loss: 0.6934 - val_accuracy: 0.5080\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5123 - val_loss: 0.6933 - val_accuracy: 0.5080\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5131 - val_loss: 0.6933 - val_accuracy: 0.5089\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5121 - val_loss: 0.6932 - val_accuracy: 0.5088\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6927 - accuracy: 0.5124 - val_loss: 0.6932 - val_accuracy: 0.5082\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6931 - accuracy: 0.5130 - val_loss: 0.6939 - val_accuracy: 0.5093\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5135 - val_loss: 0.6934 - val_accuracy: 0.5095\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5129 - val_loss: 0.6932 - val_accuracy: 0.5094\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6945 - accuracy: 0.5130 - val_loss: 0.6936 - val_accuracy: 0.5088\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5121 - val_loss: 0.7161 - val_accuracy: 0.5103\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5126 - val_loss: 0.6932 - val_accuracy: 0.5098\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5130 - val_loss: 0.6934 - val_accuracy: 0.5105\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.5116 - val_loss: 0.6938 - val_accuracy: 0.5095\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6954 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.5106\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5109 - val_loss: 0.7097 - val_accuracy: 0.5108\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5118 - val_loss: 0.6939 - val_accuracy: 0.5106\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5131 - val_loss: 0.6946 - val_accuracy: 0.5096\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6959 - accuracy: 0.5123 - val_loss: 0.6943 - val_accuracy: 0.5116\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5123 - val_loss: 0.7075 - val_accuracy: 0.5126\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5135 - val_loss: 0.7053 - val_accuracy: 0.5134\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5132 - val_loss: 0.6945 - val_accuracy: 0.5097\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5142 - val_loss: 0.6945 - val_accuracy: 0.5126\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5151 - val_loss: 0.7048 - val_accuracy: 0.5133\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6963 - accuracy: 0.5134 - val_loss: 0.6943 - val_accuracy: 0.5135\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6961 - accuracy: 0.5150 - val_loss: 0.6943 - val_accuracy: 0.5160\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6958 - accuracy: 0.5143 - val_loss: 0.6945 - val_accuracy: 0.5141\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6955 - accuracy: 0.5173 - val_loss: 0.6943 - val_accuracy: 0.5119\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5167 - val_loss: 0.7000 - val_accuracy: 0.5189\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_34\n",
      "cannot prune layer q_activation_34\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_35\n",
      "cannot prune layer q_activation_35\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6944 - accuracy: 0.5125 - val_loss: 0.6990 - val_accuracy: 0.5012\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6961 - accuracy: 0.5080 - val_loss: 0.6988 - val_accuracy: 0.5024\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6985 - accuracy: 0.5052 - val_loss: 0.6978 - val_accuracy: 0.5039\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6993 - accuracy: 0.5052 - val_loss: 0.7042 - val_accuracy: 0.5034\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6987 - accuracy: 0.5053 - val_loss: 0.6962 - val_accuracy: 0.5050\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6979 - accuracy: 0.5044 - val_loss: 0.6948 - val_accuracy: 0.5059\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6976 - accuracy: 0.5036 - val_loss: 0.6940 - val_accuracy: 0.5054\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6967 - accuracy: 0.5052 - val_loss: 0.6946 - val_accuracy: 0.5076\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6965 - accuracy: 0.5048 - val_loss: 0.6940 - val_accuracy: 0.5076\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6960 - accuracy: 0.5060 - val_loss: 0.6937 - val_accuracy: 0.5071\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6957 - accuracy: 0.5052 - val_loss: 0.6934 - val_accuracy: 0.5079\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6953 - accuracy: 0.5066 - val_loss: 0.6934 - val_accuracy: 0.5068\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6948 - accuracy: 0.5068 - val_loss: 0.6933 - val_accuracy: 0.5080\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6947 - accuracy: 0.5071 - val_loss: 0.6932 - val_accuracy: 0.5070\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6943 - accuracy: 0.5074 - val_loss: 0.6930 - val_accuracy: 0.5090\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.5073 - val_loss: 0.6931 - val_accuracy: 0.5073\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6939 - accuracy: 0.5073 - val_loss: 0.6930 - val_accuracy: 0.5084\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6938 - accuracy: 0.5080 - val_loss: 0.6931 - val_accuracy: 0.5071\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6935 - accuracy: 0.5086 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5099 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5087\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5118 - val_loss: 0.6930 - val_accuracy: 0.5069\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6929 - val_accuracy: 0.5087\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6928 - accuracy: 0.5116 - val_loss: 0.6927 - val_accuracy: 0.5099\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6928 - val_accuracy: 0.5110\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6927 - val_accuracy: 0.5097\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6928 - val_accuracy: 0.5091\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5144 - val_loss: 0.6925 - val_accuracy: 0.5115\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6924 - val_accuracy: 0.5130\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.7097 - val_accuracy: 0.5065\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5155 - val_loss: 0.6924 - val_accuracy: 0.5120\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6925 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5129\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6926 - accuracy: 0.5144 - val_loss: 0.6924 - val_accuracy: 0.5116\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5153 - val_loss: 0.6925 - val_accuracy: 0.5125\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5142\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5165 - val_loss: 0.6922 - val_accuracy: 0.5155\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5177 - val_loss: 0.6923 - val_accuracy: 0.5150\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6922 - accuracy: 0.5182 - val_loss: 0.6924 - val_accuracy: 0.5155\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6923 - accuracy: 0.5185 - val_loss: 0.6924 - val_accuracy: 0.5173\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6925 - accuracy: 0.5190 - val_loss: 0.6924 - val_accuracy: 0.5151\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6923 - accuracy: 0.5191 - val_loss: 0.6923 - val_accuracy: 0.5157\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5190 - val_loss: 0.6922 - val_accuracy: 0.5184\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5191 - val_loss: 0.6924 - val_accuracy: 0.5169\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6922 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5186\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6924 - accuracy: 0.5197 - val_loss: 0.6921 - val_accuracy: 0.5194\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6921 - accuracy: 0.5209 - val_loss: 0.6921 - val_accuracy: 0.5189\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5215 - val_loss: 0.6921 - val_accuracy: 0.5198\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5220 - val_loss: 0.6921 - val_accuracy: 0.5194\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.5215 - val_loss: 0.6920 - val_accuracy: 0.5188\n",
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5081399 ]\n",
      " [0.5081399 ]\n",
      " [0.48060465]\n",
      " [0.5081399 ]\n",
      " [0.4789773 ]\n",
      " [0.47483745]\n",
      " [0.4809744 ]\n",
      " [0.5151807 ]\n",
      " [0.5112333 ]\n",
      " [0.47330087]]\n",
      "Hyperparameters with minimum pruned_val_loss: {\"ACTIVATION_BITS\": 15, \"BATCH_SIZE\": 1024, \"BIAS_BITS\": 10, \"CONV_LAYER_DEPTHS\": [4, 7], \"CONV_LAYER_KERNELS\": [[3, 3], [3, 3]], \"CONV_LAYER_STRIDES\": [[1, 1], [1, 1]], \"DNN_LAYERS\": [16, 8], \"EPOCHS\": 100, \"FINAL_SPARSITY\": 0.3, \"FLATTENED_LAYERS\": [7], \"INTEGER_BITS\": 2, \"LEARNING_RATE\": 0.005, \"MAX_POOLING_SIZE\": [2, 2], \"MODEL_TYPE\": \"DNN\", \"NUM_PRUNE_EPOCHS\": 10, \"NUM_TIME_SLICES\": 8, \"OUTPUT\": \"SINGLE\", \"PATIENCE\": 20, \"POST_PRUNE_EPOCHS\": 50, \"PRUNE_START_EPOCH\": 0, \"TEST_PT_THRESHOLD\": 2, \"TRAIN_PT_THRESHOLD\": 2, \"WEIGHTS_BITS\": 10}\n",
      "Minimum pruned_val_loss: 0.6512852311134338\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = hyperparameter_search(data, HYPERPARAMETERS, param_grid, result_file=SAVE_FILE)\n",
    "    send_email_notification(\"All done with hyperparameter search\", 'Done!')\n",
    "except Exception as e:\n",
    "    print(\"Error encountered:\", e)\n",
    "    send_email_notification(\"Hyperparameter search ran into an error\", 'Go fix it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE REFORMATTING\n",
    "# (RUN LATER WHEN THEY ARENT BEING WRITTEN TO)\n",
    "\n",
    "def reformat_hyperparameter_results(input_file, output_file):\n",
    "    # Read the original JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process and format the metrics\n",
    "    for key, value in data.items():\n",
    "        if \"metrics\" in value:\n",
    "            value[\"metrics\"] = format_metrics(value[\"metrics\"])\n",
    "\n",
    "    # Write the updated data to the new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Define input and output file names\n",
    "input_file = 'one_layerDNN_results.json'\n",
    "output_file = 'one_layerDNN_results.json'\n",
    "\n",
    "# Call the function to reformat the JSON data\n",
    "reformat_hyperparameter_results(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Read / Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  [(None, 105)]             0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 32)                3392      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_11 (QActivati  (None, 32)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense2 (QDense)             (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " q_activation_12 (QActivati  (None, 16)                0         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4129 (16.13 KB)\n",
      "Trainable params: 4033 (15.75 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n",
      "dense1               u=32 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "batch_normalization_11 is normal keras bn layer\n",
      "q_activation_11      quantized_relu(6,0)\n",
      "dense2               u=16 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "batch_normalization_12 is normal keras bn layer\n",
      "q_activation_12      quantized_relu(6,0)\n",
      "dense_output         u=1 quantized_bits(4,0,1,alpha='auto_po2') quantized_bits(4,0,0) \n",
      "\n",
      "Initial Sparsity: 0.00%\n",
      "shape 12323 is  (438739,) data is like [0 0 0 1]\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 5s 7ms/step - loss: 0.8549 - accuracy: 0.5019 - val_loss: 0.7027 - val_accuracy: 0.5067\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7070 - accuracy: 0.5033 - val_loss: 0.6991 - val_accuracy: 0.5085\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6967 - accuracy: 0.5044 - val_loss: 0.6954 - val_accuracy: 0.5079\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6952 - accuracy: 0.5057 - val_loss: 0.7008 - val_accuracy: 0.5020\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6946 - accuracy: 0.5070 - val_loss: 0.6943 - val_accuracy: 0.5050\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6942 - accuracy: 0.5074 - val_loss: 0.6940 - val_accuracy: 0.5110\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5087 - val_loss: 0.6958 - val_accuracy: 0.5096\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5080 - val_loss: 0.7001 - val_accuracy: 0.5098\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6951 - accuracy: 0.5084 - val_loss: 0.6939 - val_accuracy: 0.5107\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6943 - accuracy: 0.5088 - val_loss: 0.6943 - val_accuracy: 0.5068\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6940 - accuracy: 0.5095 - val_loss: 0.6931 - val_accuracy: 0.5109\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6944 - accuracy: 0.5091 - val_loss: 0.7345 - val_accuracy: 0.5004\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7014 - accuracy: 0.5031 - val_loss: 0.7074 - val_accuracy: 0.5018\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.7009 - accuracy: 0.5051 - val_loss: 0.6986 - val_accuracy: 0.5067\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.7003 - accuracy: 0.5081 - val_loss: 0.6968 - val_accuracy: 0.5084\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6994 - accuracy: 0.5070 - val_loss: 0.6946 - val_accuracy: 0.5113\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6975 - accuracy: 0.5112 - val_loss: 0.6931 - val_accuracy: 0.5159\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6970 - accuracy: 0.5129 - val_loss: 0.6934 - val_accuracy: 0.5163\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6968 - accuracy: 0.5127 - val_loss: 0.6948 - val_accuracy: 0.5117\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6933 - accuracy: 0.5157 - val_loss: 0.6924 - val_accuracy: 0.5152\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6920 - accuracy: 0.5172 - val_loss: 0.6926 - val_accuracy: 0.5153\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6921 - accuracy: 0.5156 - val_loss: 0.6921 - val_accuracy: 0.5175\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6917 - accuracy: 0.5182 - val_loss: 0.6916 - val_accuracy: 0.5116\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6919 - accuracy: 0.5181 - val_loss: 0.6910 - val_accuracy: 0.5234\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6916 - accuracy: 0.5194 - val_loss: 0.6926 - val_accuracy: 0.5164\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6914 - accuracy: 0.5216 - val_loss: 0.6914 - val_accuracy: 0.5139\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.5245 - val_loss: 0.6909 - val_accuracy: 0.5231\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6932 - accuracy: 0.5222 - val_loss: 0.6906 - val_accuracy: 0.5195\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6947 - accuracy: 0.5190 - val_loss: 0.7019 - val_accuracy: 0.5212\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6949 - accuracy: 0.5231 - val_loss: 0.6876 - val_accuracy: 0.5404\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6896 - accuracy: 0.5341 - val_loss: 0.6881 - val_accuracy: 0.5313\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6867 - accuracy: 0.5408 - val_loss: 0.7455 - val_accuracy: 0.5049\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6854 - accuracy: 0.5415 - val_loss: 0.6823 - val_accuracy: 0.5478\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6827 - accuracy: 0.5491 - val_loss: 0.6996 - val_accuracy: 0.5102\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6839 - accuracy: 0.5442 - val_loss: 0.6846 - val_accuracy: 0.5362\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6838 - accuracy: 0.5434 - val_loss: 0.6856 - val_accuracy: 0.5401\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6812 - accuracy: 0.5503 - val_loss: 0.6866 - val_accuracy: 0.5328\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5450 - val_loss: 0.7070 - val_accuracy: 0.5192\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6858 - accuracy: 0.5426 - val_loss: 0.6955 - val_accuracy: 0.5215\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5232 - val_loss: 0.6940 - val_accuracy: 0.5243\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6860 - accuracy: 0.5375 - val_loss: 0.6939 - val_accuracy: 0.5236\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6849 - accuracy: 0.5413 - val_loss: 0.6845 - val_accuracy: 0.5358\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6950 - accuracy: 0.5178 - val_loss: 0.6924 - val_accuracy: 0.5196\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6910 - accuracy: 0.5275 - val_loss: 0.7053 - val_accuracy: 0.5029\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5435 - val_loss: 0.6968 - val_accuracy: 0.5175\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6841 - accuracy: 0.5431 - val_loss: 0.7408 - val_accuracy: 0.5169\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6847 - accuracy: 0.5404 - val_loss: 0.7152 - val_accuracy: 0.5078\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6835 - accuracy: 0.5438 - val_loss: 0.6863 - val_accuracy: 0.5366\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6835 - accuracy: 0.5455 - val_loss: 0.6908 - val_accuracy: 0.5314\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5440 - val_loss: 0.7027 - val_accuracy: 0.5233\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6821 - accuracy: 0.5484 - val_loss: 0.7633 - val_accuracy: 0.5177\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.5453 - val_loss: 0.6792 - val_accuracy: 0.5482\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6823 - accuracy: 0.5466 - val_loss: 0.6765 - val_accuracy: 0.5548\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6826 - accuracy: 0.5476 - val_loss: 0.6852 - val_accuracy: 0.5319\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6802 - accuracy: 0.5504 - val_loss: 0.6734 - val_accuracy: 0.5609\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6833 - accuracy: 0.5465 - val_loss: 0.6900 - val_accuracy: 0.5265\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6798 - accuracy: 0.5525 - val_loss: 0.6866 - val_accuracy: 0.5336\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6790 - accuracy: 0.5529 - val_loss: 0.6748 - val_accuracy: 0.5559\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6810 - accuracy: 0.5490 - val_loss: 0.6778 - val_accuracy: 0.5522\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6803 - accuracy: 0.5508 - val_loss: 0.6714 - val_accuracy: 0.5663\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6782 - accuracy: 0.5559 - val_loss: 0.6899 - val_accuracy: 0.5326\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6822 - accuracy: 0.5481 - val_loss: 0.6932 - val_accuracy: 0.5315\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6795 - accuracy: 0.5550 - val_loss: 0.8419 - val_accuracy: 0.5102\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6845 - accuracy: 0.5438 - val_loss: 0.6846 - val_accuracy: 0.5400\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6750 - accuracy: 0.5614 - val_loss: 0.7219 - val_accuracy: 0.5185\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.5520 - val_loss: 0.6856 - val_accuracy: 0.5391\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6787 - accuracy: 0.5552 - val_loss: 0.7593 - val_accuracy: 0.5068\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6774 - accuracy: 0.5586 - val_loss: 0.6842 - val_accuracy: 0.5323\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6845 - accuracy: 0.5436 - val_loss: 0.7143 - val_accuracy: 0.5017\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6796 - accuracy: 0.5525 - val_loss: 0.6750 - val_accuracy: 0.5631\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.5619 - val_loss: 0.7252 - val_accuracy: 0.4961\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.5598 - val_loss: 0.6897 - val_accuracy: 0.5407\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6808 - accuracy: 0.5510 - val_loss: 0.6793 - val_accuracy: 0.5504\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6802 - accuracy: 0.5520 - val_loss: 0.6956 - val_accuracy: 0.5439\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6764 - accuracy: 0.5585 - val_loss: 0.6680 - val_accuracy: 0.5711\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6786 - accuracy: 0.5553 - val_loss: 0.7016 - val_accuracy: 0.5188\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6773 - accuracy: 0.5570 - val_loss: 0.6756 - val_accuracy: 0.5630\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6754 - accuracy: 0.5601 - val_loss: 0.6790 - val_accuracy: 0.5554\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6743 - accuracy: 0.5629 - val_loss: 0.6924 - val_accuracy: 0.5312\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6781 - accuracy: 0.5550 - val_loss: 0.6842 - val_accuracy: 0.5402\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6806 - accuracy: 0.5505 - val_loss: 0.6889 - val_accuracy: 0.5294\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6769 - accuracy: 0.5595 - val_loss: 0.6825 - val_accuracy: 0.5410\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6737 - accuracy: 0.5615 - val_loss: 0.6705 - val_accuracy: 0.5648\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.5521 - val_loss: 0.6762 - val_accuracy: 0.5547\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6743 - accuracy: 0.5619 - val_loss: 0.6930 - val_accuracy: 0.5316\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6742 - accuracy: 0.5604 - val_loss: 0.6821 - val_accuracy: 0.5449\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6725 - accuracy: 0.5643 - val_loss: 0.6792 - val_accuracy: 0.5530\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.5630 - val_loss: 0.7068 - val_accuracy: 0.5352\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6740 - accuracy: 0.5617 - val_loss: 0.6755 - val_accuracy: 0.5520\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6714 - accuracy: 0.5675 - val_loss: 0.6919 - val_accuracy: 0.5565\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6817 - accuracy: 0.5474 - val_loss: 0.6715 - val_accuracy: 0.5646\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6744 - accuracy: 0.5604 - val_loss: 0.6772 - val_accuracy: 0.5528\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6718 - accuracy: 0.5660 - val_loss: 0.6657 - val_accuracy: 0.5765\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6709 - accuracy: 0.5660 - val_loss: 0.6668 - val_accuracy: 0.5738\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6702 - accuracy: 0.5687 - val_loss: 0.7438 - val_accuracy: 0.5240\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5657 - val_loss: 0.6730 - val_accuracy: 0.5628\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5682 - val_loss: 0.6743 - val_accuracy: 0.5680\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6783 - accuracy: 0.5542 - val_loss: 0.7043 - val_accuracy: 0.5328\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6740 - accuracy: 0.5595 - val_loss: 0.6703 - val_accuracy: 0.5693\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6715 - accuracy: 0.5672 - val_loss: 0.7033 - val_accuracy: 0.5168\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6740 - accuracy: 0.5633 - val_loss: 0.6665 - val_accuracy: 0.5758\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6735 - accuracy: 0.5629 - val_loss: 0.7060 - val_accuracy: 0.5208\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6693 - accuracy: 0.5697 - val_loss: 0.6674 - val_accuracy: 0.5688\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6667 - accuracy: 0.5722 - val_loss: 0.6735 - val_accuracy: 0.5628\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6666 - accuracy: 0.5717 - val_loss: 0.6666 - val_accuracy: 0.5669\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6696 - accuracy: 0.5695 - val_loss: 0.6876 - val_accuracy: 0.5416\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6680 - accuracy: 0.5710 - val_loss: 0.8441 - val_accuracy: 0.5250\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6722 - accuracy: 0.5672 - val_loss: 0.6792 - val_accuracy: 0.5501\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.5691 - val_loss: 0.6652 - val_accuracy: 0.5783\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6730 - accuracy: 0.5644 - val_loss: 0.6951 - val_accuracy: 0.5353\n",
      "Epoch 111/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6676 - accuracy: 0.5703 - val_loss: 0.6834 - val_accuracy: 0.5473\n",
      "Epoch 112/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5716 - val_loss: 0.6626 - val_accuracy: 0.5826\n",
      "Epoch 113/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6673 - accuracy: 0.5707 - val_loss: 0.6677 - val_accuracy: 0.5659\n",
      "Epoch 114/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5712 - val_loss: 0.6908 - val_accuracy: 0.5502\n",
      "Epoch 115/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6692 - accuracy: 0.5692 - val_loss: 0.6895 - val_accuracy: 0.5424\n",
      "Epoch 116/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.5656 - val_loss: 0.6935 - val_accuracy: 0.5243\n",
      "Epoch 117/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6687 - accuracy: 0.5699 - val_loss: 0.6708 - val_accuracy: 0.5652\n",
      "Epoch 118/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6697 - accuracy: 0.5686 - val_loss: 0.6734 - val_accuracy: 0.5613\n",
      "Epoch 119/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6714 - accuracy: 0.5621 - val_loss: 0.6824 - val_accuracy: 0.5541\n",
      "Epoch 120/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5599 - val_loss: 0.6799 - val_accuracy: 0.5513\n",
      "Epoch 121/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6702 - accuracy: 0.5665 - val_loss: 0.6717 - val_accuracy: 0.5563\n",
      "Epoch 122/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6818 - accuracy: 0.5510 - val_loss: 0.6758 - val_accuracy: 0.5635\n",
      "Epoch 123/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6717 - accuracy: 0.5646 - val_loss: 0.7239 - val_accuracy: 0.5257\n",
      "Epoch 124/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6685 - accuracy: 0.5701 - val_loss: 0.7957 - val_accuracy: 0.5296\n",
      "Epoch 125/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6782 - accuracy: 0.5534 - val_loss: 0.6774 - val_accuracy: 0.5619\n",
      "Epoch 126/150\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6682 - accuracy: 0.5715 - val_loss: 0.6766 - val_accuracy: 0.5599\n",
      "Epoch 127/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6748 - accuracy: 0.5601 - val_loss: 0.6798 - val_accuracy: 0.5458\n",
      "Epoch 128/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6698 - accuracy: 0.5667 - val_loss: 0.6836 - val_accuracy: 0.5453\n",
      "Epoch 129/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6679 - accuracy: 0.5686 - val_loss: 1.0137 - val_accuracy: 0.5211\n",
      "Epoch 130/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6751 - accuracy: 0.5563 - val_loss: 0.6692 - val_accuracy: 0.5670\n",
      "Epoch 131/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6675 - accuracy: 0.5709 - val_loss: 0.6667 - val_accuracy: 0.5713\n",
      "Epoch 132/150\n",
      "429/429 [==============================] - 2s 4ms/step - loss: 0.6716 - accuracy: 0.5655 - val_loss: 1.2102 - val_accuracy: 0.4867\n",
      "pruning layer dense1\n",
      "cannot prune layer batch_normalization_11\n",
      "cannot prune layer q_activation_11\n",
      "pruning layer dense2\n",
      "cannot prune layer batch_normalization_12\n",
      "cannot prune layer q_activation_12\n",
      "pruning layer dense_output\n",
      "Epoch 1/50\n",
      "429/429 [==============================] - 6s 7ms/step - loss: 0.6700 - accuracy: 0.5675 - val_loss: 0.6644 - val_accuracy: 0.5742\n",
      "Epoch 2/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6693 - accuracy: 0.5685 - val_loss: 0.7348 - val_accuracy: 0.5124\n",
      "Epoch 3/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6686 - accuracy: 0.5705 - val_loss: 0.6916 - val_accuracy: 0.5348\n",
      "Epoch 4/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5688 - val_loss: 0.7042 - val_accuracy: 0.5163\n",
      "Epoch 5/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5732 - val_loss: 0.7893 - val_accuracy: 0.5310\n",
      "Epoch 6/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6687 - accuracy: 0.5707 - val_loss: 0.6764 - val_accuracy: 0.5678\n",
      "Epoch 7/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6672 - accuracy: 0.5742 - val_loss: 0.6733 - val_accuracy: 0.5605\n",
      "Epoch 8/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6700 - accuracy: 0.5692 - val_loss: 0.6899 - val_accuracy: 0.5449\n",
      "Epoch 9/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6677 - accuracy: 0.5735 - val_loss: 0.6754 - val_accuracy: 0.5581\n",
      "Epoch 10/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6700 - accuracy: 0.5702 - val_loss: 0.7182 - val_accuracy: 0.5216\n",
      "Epoch 11/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6729 - accuracy: 0.5603 - val_loss: 0.6713 - val_accuracy: 0.5717\n",
      "Epoch 12/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6729 - accuracy: 0.5637 - val_loss: 0.7201 - val_accuracy: 0.4972\n",
      "Epoch 13/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6684 - accuracy: 0.5698 - val_loss: 0.7025 - val_accuracy: 0.5208\n",
      "Epoch 14/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5687 - val_loss: 1.0811 - val_accuracy: 0.5310\n",
      "Epoch 15/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6728 - accuracy: 0.5655 - val_loss: 0.6805 - val_accuracy: 0.5428\n",
      "Epoch 16/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6669 - accuracy: 0.5726 - val_loss: 0.6842 - val_accuracy: 0.5482\n",
      "Epoch 17/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6673 - accuracy: 0.5726 - val_loss: 0.6709 - val_accuracy: 0.5683\n",
      "Epoch 18/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6717 - accuracy: 0.5684 - val_loss: 0.6865 - val_accuracy: 0.5446\n",
      "Epoch 19/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6694 - accuracy: 0.5694 - val_loss: 0.7981 - val_accuracy: 0.5336\n",
      "Epoch 20/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6677 - accuracy: 0.5715 - val_loss: 0.6692 - val_accuracy: 0.5698\n",
      "Epoch 21/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6694 - accuracy: 0.5708 - val_loss: 0.6703 - val_accuracy: 0.5694\n",
      "Epoch 22/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6749 - accuracy: 0.5590 - val_loss: 0.7229 - val_accuracy: 0.5103\n",
      "Epoch 23/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6706 - accuracy: 0.5665 - val_loss: 0.6693 - val_accuracy: 0.5732\n",
      "Epoch 24/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6679 - accuracy: 0.5716 - val_loss: 0.7262 - val_accuracy: 0.5513\n",
      "Epoch 25/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6805 - accuracy: 0.5505 - val_loss: 0.6800 - val_accuracy: 0.5453\n",
      "Epoch 26/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6682 - accuracy: 0.5697 - val_loss: 0.6838 - val_accuracy: 0.5443\n",
      "Epoch 27/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6696 - accuracy: 0.5664 - val_loss: 0.6713 - val_accuracy: 0.5668\n",
      "Epoch 28/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6945 - accuracy: 0.5356 - val_loss: 0.6890 - val_accuracy: 0.5321\n",
      "Epoch 29/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6763 - accuracy: 0.5509 - val_loss: 0.6693 - val_accuracy: 0.5642\n",
      "Epoch 30/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6663 - accuracy: 0.5712 - val_loss: 0.6691 - val_accuracy: 0.5633\n",
      "Epoch 31/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5679 - val_loss: 0.6926 - val_accuracy: 0.5457\n",
      "Epoch 32/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6688 - accuracy: 0.5683 - val_loss: 0.6787 - val_accuracy: 0.5604\n",
      "Epoch 33/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6659 - accuracy: 0.5737 - val_loss: 0.7290 - val_accuracy: 0.5403\n",
      "Epoch 34/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6774 - accuracy: 0.5517 - val_loss: 0.7179 - val_accuracy: 0.5313\n",
      "Epoch 35/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6665 - accuracy: 0.5724 - val_loss: 0.7045 - val_accuracy: 0.5214\n",
      "Epoch 36/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6653 - accuracy: 0.5741 - val_loss: 0.6787 - val_accuracy: 0.5681\n",
      "Epoch 37/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6936 - accuracy: 0.5225 - val_loss: 0.6913 - val_accuracy: 0.5195\n",
      "Epoch 38/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6664 - accuracy: 0.5732 - val_loss: 0.8362 - val_accuracy: 0.5283\n",
      "Epoch 39/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6673 - accuracy: 0.5722 - val_loss: 0.6632 - val_accuracy: 0.5803\n",
      "Epoch 40/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5666 - val_loss: 0.6735 - val_accuracy: 0.5611\n",
      "Epoch 41/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6668 - accuracy: 0.5722 - val_loss: 0.6693 - val_accuracy: 0.5572\n",
      "Epoch 42/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6725 - accuracy: 0.5656 - val_loss: 0.6763 - val_accuracy: 0.5575\n",
      "Epoch 43/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6641 - accuracy: 0.5750 - val_loss: 1.0427 - val_accuracy: 0.5338\n",
      "Epoch 44/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6674 - accuracy: 0.5710 - val_loss: 0.6631 - val_accuracy: 0.5758\n",
      "Epoch 45/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6645 - accuracy: 0.5743 - val_loss: 0.6684 - val_accuracy: 0.5604\n",
      "Epoch 46/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6669 - accuracy: 0.5714 - val_loss: 0.6842 - val_accuracy: 0.5493\n",
      "Epoch 47/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6764 - accuracy: 0.5574 - val_loss: 0.6868 - val_accuracy: 0.5306\n",
      "Epoch 48/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6692 - accuracy: 0.5665 - val_loss: 0.6871 - val_accuracy: 0.5515\n",
      "Epoch 49/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6661 - accuracy: 0.5715 - val_loss: 0.6899 - val_accuracy: 0.5690\n",
      "Epoch 50/50\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 0.6698 - accuracy: 0.5686 - val_loss: 0.7053 - val_accuracy: 0.5473\n"
     ]
    }
   ],
   "source": [
    "model, train_metrics = train_model(data, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714/1714 [==============================] - 3s 1ms/step\n",
      "[[0.5292969 ]\n",
      " [0.5019531 ]\n",
      " [0.51953125]\n",
      " [0.64453125]\n",
      " [0.5371094 ]\n",
      " [0.58203125]\n",
      " [0.5566406 ]\n",
      " [0.44921875]\n",
      " [0.5761719 ]\n",
      " [0.49609375]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIhCAYAAAB5deq6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB6K0lEQVR4nO3deXiM1/vH8c9kl1hjjdKIrRFLtRShtpLYaatfuliLUi2KLrR2Sqstvl20Wkq1qrqgSlJSrQhCLVFfYitBkVCUWCPL8/vDb1IjiWSSmUyW9+u6cl3myfOc5544mcw955z7mAzDMAQAAAAAyJCTowMAAAAAgLyOxAkAAAAAMkHiBAAAAACZIHECAAAAgEyQOAEAAABAJkicAAAAACATJE4AAAAAkAkSJwAAAADIBIkTAAAAAGSCxAlAjixatEgmk8niq2zZsmrVqpVWr15t13u3atVKderUses98rIqVaqoX79+mZ535/9P8eLF1bRpUy1dutTu986uuXPnatGiRWmOHzt2TCaTKd3vFVTbtm3TY489pnvvvVfu7u4qX768AgMDNXr0aIvzWrVqpVatWjkmyHRkNZ5WrVql6aPmrypVqlicu379ejVs2FBeXl4ymUxauXKlJGnZsmWqXbu2ihQpIpPJpN27d2vSpEkymUxWx92vX7809wUASXJxdAAACoaFCxfK399fhmEoLi5OH374obp06aJVq1apS5cujg6v0HviiSc0evRoGYahmJgYTZ8+XU8//bQMw9DTTz9tdXsrVqxQ8eLF7RDpLXPnzlWZMmXSJGc+Pj6KjIxUtWrV7HbvvGTNmjXq2rWrWrVqpZkzZ8rHx0exsbHasWOHvvnmG7333nup586dO9eBkeZM1apVtWTJkjTH3d3dU/9tGIZ69OihmjVratWqVfLy8tJ9992nv//+W71791b79u01d+5cubu7q2bNmho4cKDat29vdSzjx4/XiBEjcvR8ABRMJE4AbKJOnTpq2LBh6uP27durVKlSWrp0ab5OnK5duyZPT09Hh5Fj5cuXV5MmTSRJgYGBatasmapUqaJ58+ZlK3F64IEHbB1ilri7u6c+j8Jg5syZ8vPz09q1a+Xi8u+f7CeffFIzZ860ODcgICC3w7OZIkWKZPr/evr0aV24cEGPPfaY2rRpk3p88+bNSkxMVK9evdSyZcvU456enqpUqZLVsRSWpByA9ZiqB8AuPDw85ObmJldXV4vjkydPVuPGjeXt7a3ixYvrwQcf1IIFC2QYRpo2vv76awUGBqpo0aIqWrSo6tevrwULFtz1vitWrJCnp6cGDhyopKQkSdLFixc1YMAAeXt7q2jRourUqZOOHj0qk8mkSZMmpV5rntqza9cuPfHEEypVqlTqm6gbN25o7Nix8vPzk5ubm+655x698MILunjxosX972zT7M6pbeYpjr/99puef/55lSlTRqVLl9bjjz+u06dPW1ybmJioV199VRUqVJCnp6cefvhh/f7773f9OWTG19dXZcuW1ZkzZyyOx8fH6+WXX7Z4ni+99JKuXr161+djzbUpKSn64IMPVL9+fRUpUkQlS5ZUkyZNtGrVqtS29+3bp/Dw8DRTtjKaqrdp0ya1adNGxYoVk6enp5o2bao1a9ZYnGPNz/xOc+bMkclk0p9//pnme6+99prc3Nx07tw5SVJUVJQ6d+6scuXKyd3dXRUrVlSnTp108uTJu94jPefPn1eZMmUskiYzJyfLP+HpTY07efKknnjiCRUrVkwlS5bUM888o+3bt6f5Gfbr109FixbVn3/+qY4dO6po0aKqXLmyRo8erYSEBIs2rfkdtpVJkyalJkGvvfZaap/o16+fHn74YUlSz549ZTKZUn8GGU3Vy+x1Jb2peoZhaO7cual9tlSpUnriiSd09OhRi/PM04e3b9+u5s2by9PTU1WrVtVbb72llJQUi3MvXryo0aNHq2rVqnJ3d1e5cuXUsWNHHThwQIZhqEaNGmrXrl2a+K9cuaISJUrohRdesPrnCCBnSJwA2ERycrKSkpKUmJiokydPpr5hvnM049ixYxo8eLC+/fZbLV++XI8//riGDRumqVOnWpw3YcIEPfPMM6pYsaIWLVqkFStWqG/fvjp+/HiGMcyePVv/+c9/9Prrr2v+/PlycXFRSkqKunTpoq+//lqvvfaaVqxYocaNG991Cs/jjz+u6tWr67vvvtMnn3wiwzD06KOP6t1331Xv3r21Zs0ajRo1Sl988YUeeeSRNG8srTFw4EC5urrq66+/1syZM7Vhwwb16tXL4pxBgwbp3XffVZ8+ffTjjz+qe/fuevzxx/XPP/9k+76XLl3ShQsXVLNmzdRj165dU8uWLfXFF19o+PDhCg0N1WuvvaZFixapa9eud31jbM21/fr104gRI/TQQw9p2bJl+uabb9S1a1cdO3ZM0q3kt2rVqnrggQcUGRmpyMhIrVixIsN7h4eH65FHHtGlS5e0YMECLV26VMWKFVOXLl20bNmyNOdn5Wd+p169esnNzS1NwpacnKyvvvpKXbp0UZkyZXT16lUFBQXpzJkz+uijjxQWFqY5c+bo3nvv1eXLl+96j/QEBgZq27ZtGj58uLZt26bExMQsX3v16lW1bt1av/32m95++219++23Kl++vHr27Jnu+YmJieratavatGmjH3/8Uc8++6xmz56tt99+2+K8rP4OWyspKSnNlznZGDhwoJYvXy5JGjZsWGqfGD9+vD766CNJ0vTp0xUZGXnXKYvZeV2RpMGDB+ull15S27ZttXLlSs2dO1f79u1T06ZN03z4EBcXp2eeeUa9evXSqlWr1KFDB40dO1ZfffVV6jmXL1/Www8/rHnz5ql///766aef9Mknn6hmzZqKjY2VyWTSsGHDFBYWpsOHD1u0v3jxYsXHx5M4AY5gAEAOLFy40JCU5svd3d2YO3fuXa9NTk42EhMTjSlTphilS5c2UlJSDMMwjKNHjxrOzs7GM888c9frW7ZsadSuXdtITk42XnzxRcPNzc346quvLM5Zs2aNIcn4+OOPLY7PmDHDkGRMnDgx9djEiRMNScaECRMszv35558NScbMmTMtji9btsyQZHz66aepx+5s08zX19fo27dv6mPzz23o0KEW582cOdOQZMTGxhqGYRj79+83JBkjR460OG/JkiWGJIs2M2K+T2JionHz5k3j0KFDRteuXY1ixYoZO3bssPiZODk5Gdu3b7e4/vvvvzckGSEhIRk+n6xeu3HjRkOS8cYbb9w15tq1axstW7ZMczwmJsaQZCxcuDD1WJMmTYxy5coZly9fTj2WlJRk1KlTx6hUqVJqv8rqzzwjjz/+uFGpUiUjOTk59VhISIghyfjpp58MwzCMHTt2GJKMlStX3rWtrDp37pzx8MMPp/5eubq6Gk2bNjVmzJhh8XwN49bvw+0/s48++siQZISGhlqcN3jw4DQ/w759+xqSjG+//dbi3I4dOxr33XdfhvFl9DucXjwZadmyZbqvIZKMAQMGpJ5n/r9/5513LK7/7bffDEnGd999Z3Hc/PtsltXXlb59+xq+vr6pjyMjIw1JxnvvvWdx3l9//WUUKVLEePXVV9M8l23btlmcGxAQYLRr1y718ZQpUwxJRlhYWIZxxMfHG8WKFTNGjBiRpq3WrVvf9TkAsA9GnADYxOLFi7V9+3Zt375doaGh6tu3r1544QV9+OGHFuf9+uuvatu2rUqUKCFnZ2e5urpqwoQJOn/+vM6ePStJCgsLU3JycpY+Ub1x44YeffRRLVmyROvWrdMzzzxj8f3w8HBJUo8ePSyOP/XUUxm22b179zQxS0ozNe0///mPvLy8tH79+kzjzEjXrl0tHterV0+SUj8B/+233yQpzfPq0aNHutO3MjJ37ly5urrKzc1NNWvWVGhoqJYuXaoGDRqknrN69WrVqVNH9evXt/jUv127djKZTNqwYUOG7Wf12tDQUEmy2aflV69e1bZt2/TEE0+oaNGiqcednZ3Vu3dvnTx5UgcPHrS4JrOfeUb69++vkydP6pdffkk9tnDhQlWoUEEdOnSQJFWvXl2lSpXSa6+9pk8++UTR0dE5en6lS5dWRESEtm/frrfeekvdunXToUOHNHbsWNWtWzd1emB6wsPDVaxYsTSjqxn1fZPJlGY9Yr169dL8XLLyO2ytatWqpb5+3P41fvz4bLWXHmteV263evVqmUwm9erVy6JvV6hQQffff3+a34sKFSqoUaNGFsfu/DmGhoaqZs2aatu2bYb3LVasmPr3769FixalTnf99ddfFR0drRdffNGq5wDANkicANhErVq11LBhQzVs2FDt27fXvHnzFBwcrFdffTV1HdDvv/+u4OBgSdJnn32mzZs3a/v27XrjjTckSdevX5ck/f3335KUpYXdZ8+e1dq1axUYGKimTZum+f758+fl4uIib29vi+Ply5fPsE0fH5902yhbtqzFcZPJpAoVKuj8+fOZxpmR0qVLWzw2VxEz/yzMbVeoUMHiPBcXlzTX3k2PHj20fft2bdmyRfPmzVOxYsX05JNPWkwDOnPmjPbs2SNXV1eLr2LFiskwjLu+Sc/qtX///becnZ3TPJ/s+ueff2QYRpr/M0mqWLGiJKX5/8nsZ56RDh06yMfHRwsXLky996pVq9SnTx85OztLkkqUKKHw8HDVr19fr7/+umrXrq2KFStq4sSJVk2zu1PDhg312muv6bvvvtPp06c1cuRIHTt2LE2BiNudP38+3X6eUd/39PSUh4eHxTF3d3fduHEj9XFWf4et5eHhkfr6cfuXr69vttpLjzWvK7c7c+aMDMNQ+fLl0/TvrVu3pvm9SO/30t3d3eJn8/fff2cpjmHDhuny5cupFQc//PBDVapUSd26dbPqOQCwDarqAbCbevXqae3atTp06JAaNWqkb775Rq6urlq9erXFGzTzXixm5gTl5MmTqly58l3vce+992rWrFl67LHH9Pjjj+u7776zaLt06dJKSkrShQsXLJKnuLi4DNu8c0G5uY2///7bInky/r/0+kMPPZR6zN3dPd01T9lNrsxvwuLi4nTPPfekHk9KSrKqzbJly6ZWPQwMDFStWrXUsmVLjRw5MnW/rTJlyqhIkSL6/PPP022jTJkyGbaf1WvLli2r5ORkxcXFpZvsWKtUqVJycnJSbGxsmu+ZCz7cLW5rmEex3n//fV28eFFff/21EhIS1L9/f4vz6tatq2+++UaGYWjPnj1atGiRpkyZoiJFimjMmDE5jsPV1VUTJ07U7NmztXfv3gzPK126dLpFRO7W9zOT1d/hvMia15XblSlTRiaTSRERERbl0c3SO5aVWLJSLKR69erq0KGDPvroI3Xo0EGrVq3S5MmTUxN1ALmLEScAdrN7925J/75hMZlMcnFxsfijf/36dX355ZcW1wUHB8vZ2Vkff/xxlu4THBystWvXauPGjercubNFFTdzeeI7iwR88803WX4e5tLHty/ulqQffvhBV69etSiNXKVKFe3Zs8fivF9//VVXrlzJ8v1uZ64QduceN99++21q1cDsaN68ufr06aM1a9YoMjJSktS5c2cdOXJEpUuXTvfT/7ttCprVa81T2jL7v73zE/qMeHl5qXHjxlq+fLnF+SkpKfrqq69UqVIliwIYOdW/f3/duHFDS5cu1aJFixQYGCh/f/90zzWZTLr//vs1e/ZslSxZUrt27bL6fuklhJK0f/9+Sf+OqqWnZcuWunz5cur0SDNr+v6dsvo7nBdZ+7pi1rlzZxmGoVOnTqXbt+vWrWt1LB06dNChQ4dSpwHfzYgRI7Rnzx717dtXzs7OGjRokNX3A2AbjDgBsIm9e/emvpE/f/68li9frrCwMD322GPy8/OTJHXq1EmzZs3S008/reeee07nz5/Xu+++m+YT2ypVquj111/X1KlTdf36dT311FMqUaKEoqOjde7cOU2ePDnN/R9++GGtX79e7du3V3BwsEJCQlSiRAm1b99ezZo10+jRoxUfH68GDRooMjJSixcvlpS2pHN6goKC1K5dO7322muKj49Xs2bNtGfPHk2cOFEPPPCAevfunXpu7969NX78eE2YMEEtW7ZUdHS0PvzwQ5UoUSJbP9datWqpV69emjNnjlxdXdW2bVvt3btX7777bo43oJ06daqWLVum8ePH65dfftFLL72kH374QS1atNDIkSNVr149paSk6MSJE1q3bp1Gjx6txo0bp9tWVq9t3ry5evfurWnTpunMmTPq3Lmz3N3dFRUVJU9PTw0bNkzSv6M2y5YtU9WqVeXh4ZHhG9QZM2YoKChIrVu31ssvvyw3NzfNnTtXe/fu1dKlS9MtSZ1d/v7+CgwM1IwZM/TXX3/p008/tfj+6tWrNXfuXD366KOqWrWqDMPQ8uXLdfHiRQUFBaWe16ZNG4WHh2ea/LZr106VKlVSly5d5O/vr5SUFO3evVvvvfeeihYteteNWvv27avZs2erV69emjZtmqpXr67Q0FCtXbtWUtb6/p2y+jtsrevXr2vr1q3pfs9W+3Zl53VFkpo1a6bnnntO/fv3144dO9SiRQt5eXkpNjZWmzZtUt26dfX8889bFctLL72kZcuWqVu3bhozZowaNWqk69evKzw8XJ07d1br1q1Tzw0KClJAQIB+++039erVS+XKlcvRzwFADjiuLgWAgiC9qnolSpQw6tevb8yaNcu4ceOGxfmff/65cd999xnu7u5G1apVjRkzZhgLFiwwJBkxMTEW5y5evNh46KGHDA8PD6No0aLGAw88YFEJzFxV73Z79+41KlSoYDz44IPG33//bRiGYVy4cMHo37+/UbJkScPT09MICgoytm7dakgy/vvf/6Zea67CZb7udtevXzdee+01w9fX13B1dTV8fHyM559/3vjnn38szktISDBeffVVo3LlykaRIkWMli1bGrt3786wqt6dVejMFcJ+++03izZHjx5tlCtXzvDw8DCaNGliREZGpmkzI5KMF154Id3vvfLKK4YkIzw83DAMw7hy5Yoxbtw447777jPc3NyMEiVKGHXr1jVGjhxpxMXFpV7n6+tr9OvXz6KtrF6bnJxszJ4926hTp07qeYGBgamV6QzDMI4dO2YEBwcbxYoVMySlVjlLr6qeYRhGRESE8cgjjxheXl5GkSJFjCZNmli0ZxjW/czv5tNPPzUkGUWKFDEuXbpk8b0DBw4YTz31lFGtWjWjSJEiRokSJYxGjRoZixYtsjjPXH0tM8uWLTOefvppo0aNGkbRokUNV1dX49577zV69+5tREdHp2nzzip2J06cMB5//HGjaNGiRrFixYzu3bunVgL88ccfU8/r27ev4eXlleb+d1amM4ys/w7boqqeJCMxMdEwjJxX1TPL7HXlzqp6tz/vxo0bp/axatWqGX369LGoTJnea1JGbf7zzz/GiBEjjHvvvddwdXU1ypUrZ3Tq1Mk4cOBAmusnTZpkSDK2bt2a5nsAco/JMOy4Yx0A5FFff/21nnnmGW3evDndohK4O29vbz377LN69913HR0KrDR9+nSNGzdOJ06csLpQAhyjYcOGMplM2r59u6NDAQo1puoBKPCWLl2qU6dOqW7dunJyctLWrVv1zjvvqEWLFiRNVtqzZ49CQkL0zz//KDAw0NHhIBPm7QD8/f2VmJioX3/9Ve+//7569epF0pTHxcfHa+/evVq9erV27tx5102gAeQOEicABV6xYsX0zTffaNq0abp69ap8fHzUr18/TZs2zdGh5TsjRozQgQMH9PLLL+vxxx93dDjIhKenp2bPnq1jx44pISFB9957r1577TWNGzfO0aEhE7t27VLr1q1VunRpTZw4UY8++qijQwIKPabqAQAAAEAmKEcOAAAAAJkgcQIAAACATJA4AQAAAEAmCl1xiJSUFJ0+fVrFihWz6aaIAAAAAPIXwzB0+fJlVaxYMdONwQtd4nT69GlVrlzZ0WEAAAAAyCP++uuvTLdpKHSJU7FixSTd+uEUL17cwdFIiYmJWrdunYKDg+Xq6urocJAP0GdgLfoMrEWfQXbQb2CtvNBn4uPjVbly5dQc4W4KXeJknp5XvHjxPJM4eXp6qnjx4rzIIEvoM7AWfQbWos8gO+g3sFZe6jNZWcJDcQgAAAAAyASJEwAAAABkgsQJAAAAADJB4gQAAAAAmSBxAgAAAIBMkDgBAAAAQCZInAAAAAAgEyROAAAAAJAJEicAAAAAyASJEwAAAABkgsQJAAAAADJB4gQAAAAAmSBxAgAAAIBMuDg6gMKsypg1//8vJ42IXCdJer61r0a2CZCbCzktAAAAkFc49N35xo0b1aVLF1WsWFEmk0krV67M9Jrw8HA1aNBAHh4eqlq1qj755BP7B2oH/yZN0u3/DR//dlw1x4Wq+8ebdDMpJfcDAwAAAJCGQxOnq1ev6v7779eHH36YpfNjYmLUsWNHNW/eXFFRUXr99dc1fPhw/fDDD3aO1LYsk6b07Tx+STXHheq+cWv08Nvr9dFvh0ikAAAAAAdx6FS9Dh06qEOHDlk+/5NPPtG9996rOXPmSJJq1aqlHTt26N1331X37t3tFKVtZSVpul1CknTynxt6Z+1hvbP2sBr7ldKXA5owlQ8AAADIRflqjVNkZKSCg4MtjrVr104LFixQYmKiXF1d01yTkJCghISE1Mfx8fGSpMTERCUmJto3YDvYFvOPao4LVYeAcprd8345O5kcHRJymbnf5sf+C8egz8Ba9BlkB/0G1soLfcaae+erxCkuLk7ly5e3OFa+fHklJSXp3Llz8vHxSXPNjBkzNHny5DTH161bJ09PT7vFmjEn2WKGZGj0WYVOXKdyHoaalDXUsqIhBqEKl7CwMEeHgHyGPgNr0WeQHfQbWMuRfebatWtZPjdfJU6SZDJZjrAYhpHucbOxY8dq1KhRqY/j4+NVuXJlBQcHq3jx4vYLNAPm6nm2YdLZGyat+kta9ZfU2K+kPu/TkGl8BVxiYqLCwsIUFBSU7igrcCf6DKxFn0F20G9grbzQZ8yz0bIiXyVOFSpUUFxcnMWxs2fPysXFRaVLl073Gnd3d7m7u6c57urqWuB+qbfFXFTtyb+wDqqQKIh9GPZFn4G16DPIDvoNrOXIPmPNffPVO+vAwMA0Q3nr1q1Tw4YN880v6LG3Otn9HuZ1UB3/G6EFEUepxgcAAADkkEMTpytXrmj37t3avXu3pFvlxnfv3q0TJ05IujXNrk+fPqnnDxkyRMePH9eoUaO0f/9+ff7551qwYIFefvllR4SfbbmRPElSdGy8pq7Zr5rjQtVz3hYSKAAAACCbHJo47dixQw888IAeeOABSdKoUaP0wAMPaMKECZKk2NjY1CRKkvz8/BQSEqINGzaofv36mjp1qt5///18U4r8dpbJk/0TGkahAAAAgOxz6BqnVq1apRZ3SM+iRYvSHGvZsqV27dplx6hyz7G3OikxMVEhISFq0uIRPfzuBiUm2/ee0bHxil4Tr2lr9uvZh/00vnOAfW8IAAAAFAD5ao1TQeZd1E2H3+ykvZPaKahWOdUo5yUPZ/vt0WRIWrApRoHT1zP6BAAAAGQiX1XVKwyKerjos74PpT6+mZSi3vO3atuxf+xyv9j4G6o5LlSBft76YkBjKvEBAAAA6SBxyuPcXJy0bEhT3UxK0RdbjmnJ1mM6duG6ze8TGXNBNceFyte7iJrVKKNqZYqqd2AVEikAAABAJE75hpuLkwa1qKpBLarqZlKKFm2O0ZxfDulaom2n2R2/cF3Ht/0lSXozZL8GNffT2I6sgwIAAEDhxnBCPuTm4qTnWlZT9NQOGvCwr93uk2JI8zbGqOXMX/X6ij1U4wMAAEChxYhTPje+cx291j5AizbHaF10nPadjtd1RqEAAAAAm2LEqQAwj0B9/3wz7c+lUain5kUy+gQAAIBCg8SpABrfuY4OTeug1zv4q6FvSRVxtf1/s7mYBBvqAgAAoDBgql4BZR6Feq5lNUnS1NV7tWDTcZvf5/YNdZtQ0hwAAAAFFO9wCwnzKFRg1VJ2ad/Qv6NQTOMDAABAQUPiVIi4uThp6XNN7ZpASf8mUDNCou12DwAAACA3kTgVQrmVQM3bGKNnF/2uyCPnlZxi2O0+AAAAgL2xxqkQMydQN5NS9GXkMcWcu6qIw2d1/MINm93j1wN/69cDf6uYh4sevLeUWtQoo96BVVgHBQAAgHyFxAlyc3HSgOZVUx/PCInWZxExsuUg0eUbSQo/9LfCD/2taWv2q5ZPcXV/8B6SKAAAAOQLvGNFGmM7BujA1A4a36mWejW+V77eHjZt39CtanxT1+yX/3jWQgEAACDvY8QJ6cqNUSjp3w119/x1iVLmAAAAyLN4l4osuX0UqnbFYjZvn0p8AAAAyMsYcUKWmUehBjSvqptJKer7+VZFHv3HpveYtzFGEYfPs/4JAAAAeQrvSpEt9ixpbl7/dB+b6QIAACCPIHFCjtyeQI3vVEutapaxWduGmMIHAACAvIGperCJ26fx/bw3VpN/ilbsJdvtB2UuIFGjQjH5ensyjQ8AAAC5isQJNte+jo+CAiro95gLmh9xRL8e+Fu2KMYXGXNBkTEXJElvhuzXoOZ+GtsxwAYtAwAAAHdH4gS7cHYyKbBaaQVWK62bSSn6MvKYlked1L7Tl23SPmXMAQAAkJtInGB39qzGZ14DFejnzTQ+AAAA2A3vLpGrzMUkBrfwk5PJdu1GxlzQ4sjjmrpmv/zHU0wCAAAAtkXiBIew54a65ml8JE8AAACwFRInOIx5Ct+a4S00uIWfbDgAJUn6dGOM5oX/yT5QAAAAyDHWOCFPGNsxQKOD/W26/smQNCP0oN7++aAaV2ENFAAAALKPxAl5hnn9k7kK3/EL13T4THyOE6kUg1LmAAAAyBkSJ+Q55il8ZjNCovVZRIxSbLEZlP5dAxVx+Ly6P3gPI1AAAADIFO8WkeeZC0kEVi1l03ajY+OpwgcAAIAsIXFCvmCvMuYSVfgAAACQOabqIV8xF5Gw5Roos083xsjby039m1Vl6h4AAAAskDgh37lzDdTNpBQt2hyj6aEHctQuVfgAAACQERIn5HtuLk56rmU1nb+aoHkbY3LcHlX4AAAAcCc+RkeBMbZjgF3XQHX8b4QWRBxlQ10AAIBCiBEnFCj2XAMVHRuv6DXxjEABAAAUQiROKHDSWwP1ZeQxLY86qX2nL+e4ffaBAgAAKHx4t4cCz5xIrRnewqZT+dgHCgAAoPBgxAmFinkqny2q8JmZR6D2/HWJSnwAAAAFFIkTCh1bV+EzoxIfAABAwcVH4ii07FWFT/p3FOqpeZFU4QMAACgASJxQqI3tGKADUztofKda6hPoq8CqpWzafmTMBdUcxxooAACA/I6peij07F2FT1LqlECm7gEAAORPJE7AHcyJ1IDmVTUjJFqfRcQoxch5u59FxMjby02nLt6ggAQAAEA+Q+IE3MXtG+rmdAQqxZBmhB5MfUwBCQAAgPyDj7uBTNhrHygKSAAAAOQfJE6AFexRTIICEgAAAHkfU/UAK91ZTMJW66DmbYzR4bNXNKh5NTXy85azPeqkAwAAIFsYcQJyyDwKZYvRp18P/K2nPtuqh9/+VT/vjbVBdAAAALAFEifABtxcnLT0uaY2WwMVd+mGhny1S30//10LIo6yBgoAAMDBmKoH2NDtVfiOX7imSiWLaHroAavbMc/6Cz/0t8IP/U0FPgAAAAcjcQJs7M41UOevJqRugJtd5gp8EYfP69H6FVSaASgAAIBcReIE2Jl5lMgWBSSiY+MVHRsvyVkLTmxR9wcrs5EuAABALuDdFpALbFlA4haTomOvaOqa/fIfTylzAAAAeyNxAnKJrQtImJmn8ZE8AQAA2A9T9YBcdnsBiYjDf2vDoXM2afeziBh5e7np1MUb8vX2ZAofAACADZE4AQ5gLiAxoHlV/bw3VpN/ilbspRs5ajPFkGaEHkx9TCU+AAAA2+HjaMDB2tfx0abXHtHSQU3Uxr+szdo1T+F7al4k+0ABAADkEIkTkAc4O5kUWK20FvRrZPM1UJExF1RzHAUkAAAAcoKpekAec/saqOVRJ7Xv9GWbtDtvY4z2/HVJNSoUYw0UAACAlXjXBORB5jVQa4a3sOkIVGTMBS2OPE4ZcwAAACuROAF5nHkPqPGdaql2xWI2a9e8BqrjfyO0IOIo66AAAADugql6QD5wexW+q9cT9MaitSpaoYru9fbS9NADOWo7OjZe0WviqcIHAABwFyROQD7j5uKk1hUNdexYS66urjp/NUHzNsbkuF3zCFTE4fPq/uA9rIECAAC4De+KgHxubMcAm66Dio6NZw0UAADAHUicgALAvA4qsGopm7XJGigAAIB/kTgBBYSbi5OWPtfU5vtAMQIFAADAGiegwLl9H6jjF67p8Jl4RR79J8ftmkegzPcAAAAoTEicgALIXIXPbEZItD6LiFGKkfO2P4uIkbeXm05dvMFGugAAoNAgcQIKgdtHoZZHndS+05ez3VaKIc0IPZj6mDLmAACgMOBjYqCQMI9CrRnewqbroCgiAQAACgMSJ6AQMlfhG9+plmpXLGaTNikiAQAACjISJ6CQsvcIFMkTAAAoSByeOM2dO1d+fn7y8PBQgwYNFBERcdfzlyxZovvvv1+enp7y8fFR//79df78+VyKFiiY7DEC9enGGL2+Yg/T9wAAQIHg0MRp2bJleumll/TGG28oKipKzZs3V4cOHXTixIl0z9+0aZP69OmjAQMGaN++ffruu++0fft2DRw4MJcjBwoeW49AGZK+3vYX0/cAAECB4NCqerNmzdKAAQNSE585c+Zo7dq1+vjjjzVjxow052/dulVVqlTR8OHDJUl+fn4aPHiwZs6cmatxAwXdnXtBVSpZRNNDD2S7PfP0vT1/XVKNCsUoYw4AAPIdhyVON2/e1M6dOzVmzBiL48HBwdqyZUu61zRt2lRvvPGGQkJC1KFDB509e1bff/+9OnXqlOF9EhISlJCQkPo4Pj5ekpSYmKjExEQbPJOcMceQF2JB/pBbfcYkqU+TyqmPz8Zf1/zNx3PUZmTMBUXGXJB0q4z5s0199Vr7+3LUJjLH6wysRZ9BdtBvYK280GesubfJMAwbbIlpvdOnT+uee+7R5s2b1bRp09Tj06dP1xdffKGDBw+me93333+v/v3768aNG0pKSlLXrl31/fffy9XVNd3zJ02apMmTJ6c5/vXXX8vT09M2TwYoJH48ZtJvsU4yZItKErdeeh7xSVG3Kg55GQIAAIXctWvX9PTTT+vSpUsqXrz4Xc91+Aa4JpPlGzDDMNIcM4uOjtbw4cM1YcIEtWvXTrGxsXrllVc0ZMgQLViwIN1rxo4dq1GjRqU+jo+PV+XKlRUcHJzpDyc3JCYmKiwsTEFBQRkmf8DtHNlnOkq6mZSiJb+f0MqoWEXHZX8jXf1/8vVbrLPKVbpHVct46ZlG9zJ9zw54nYG16DPIDvoNrJUX+ox5NlpWOCxxKlOmjJydnRUXF2dx/OzZsypfvny618yYMUPNmjXTK6+8IkmqV6+evLy81Lx5c02bNk0+Pj5prnF3d5e7u3ua466urnnqlzqvxYO8z1F9xtVVeq5lDT3XsoZmhETrs4gYpeRgwMiQ9M32U5Kkt34+pEHN/TS2Y4BtgoUFXmdgLfoMsoN+A2s5ss9Yc1+HfbTr5uamBg0aKCwszOJ4WFiYxdS92127dk1OTpYhOzs7S7o1UgUgd91exrxX43tz3J65iETH/0ZQxhwAAOQpDp2qN2rUKPXu3VsNGzZUYGCgPv30U504cUJDhgyRdGua3alTp7R48WJJUpcuXTRo0CB9/PHHqVP1XnrpJTVq1EgVK1Z05FMBCi1zGXNJ8nJ31ryNMTluMzo2XtFr4vVmyH5GoAAAQJ7g0MSpZ8+eOn/+vKZMmaLY2FjVqVNHISEh8vX1lSTFxsZa7OnUr18/Xb58WR9++KFGjx6tkiVL6pFHHtHbb7/tqKcA4DbmBCen0/fMzCNQt7cNAADgCA4vDjF06FANHTo03e8tWrQozbFhw4Zp2LBhdo4KQHbduQfU4TPxijz6T47a/CwiRt5ebjp18QZ7QAEAAIdweOIEoOC5ffqepBwXkUgxpBmh/25RwBQ+AACQ2/jIFoDdUUQCAADkd4w4AcgVFJEAAAD5GYkTgFxnryISe/66pBoVirEOCgAA2ByJEwCHuL2IxPKok9p3+nKO24yMuaDImAuSWAcFAABsi49jATiMefremuEtNLiFn5xMtmvbPAo1IyTado0CAIBCixEnAHnCnWXMK5UsoumhB3LcLqXMAQCALZA4Acgz7ixjfv5qQo6LSFDKHAAA2AKJE4A8y9ZFJKR/p/Dd3j4AAEBmmK8CIE+7fQ+o2hWL2azdTzfGaF74n+wBBQAAsoTECUCeZ48iEoZuTeHzHx9KAQkAAJAppuoByFfuLCJx+Ey8Io/+k+32zFP3Dp+9okHNq6mRn7ecbVneDwAAFAgkTgDynTuLSMwIic7xOqhfD/ytXw/8LZ8SHprYJUDt6/jYIFIAAFBQMFUPQL53+zqoPoG+er2Df7bbirt0Q89/tUs/7421YYQAACC/Y8QJQIFgq1LmhiSTpMk/RSsooALT9gAAgCQSJwAFVE5KmRuSYi/d0O8xF9TAt1Tqeio20AUAoPAicQJQYJkLSSzaHKPpoQesvn5+xBE9M/9vi8Rr2pr9quVTXN0fvIckCgCAQoS/+AAKNDcXJz3XspoGt/Cz+tr1B/5OM1plSIqOjdfUNfspZQ4AQCHCiBOAQiEnU/cyYi5lfnv7AACgYCJxAlBo3L4HVMThv7Xh0Lk055h0a1TJGp9ujNHlhCRVK1OU6XsAABRQ/HUHUKiYq+8teraxPun1oHxKeFh8v0IJD7WsWdaqNg1JX2/7i+l7AAAUYIw4ASi02tfxUVBABf0ec0FnL99QuWIeauTnrUWbYxR+6O9stcn0PQAACiYSJwCFmrOTSYHVSlsc6x1YRW+G7M/RWiim7wEAULDwlxwA7uDm4qRBza2vwnc7pu8BAFCwMOIEAOmwZRU+pu8BAJD/kTgBQAZur8K3POqk9p2+nKP2PouIkbeXm05dvCFfb0+m8AEAkI+QOAHAXZir8A1oXlU3k1L0ZeQxxZy7qq+2nbC6rRRDmhF6MPXxmyH7Nai5H6NQAADkAyROAJBF5iRKkrzcnVOn32WXeQpfxOHz6v7gPYxAAQCQh/EXGgCyYWzHAA1u4ScnU87bio6Np4gEAAB5HCNOAJBNt6+Byu70vdtRRAIAgLyLxAkAcsDW0/ekW0UkRgf7M20PAIA8hMQJAGzEViXMUwzphSU75VOyCNX3AADII0icAMCGbp++d/zCNVUqWUTTQw9Y3U7Y/rOp/6b6HgAAjkfiBAA2dvv0PUk6fzUhR1P4qL4HAIDj8ZcXAOzMVhX4qL4HAIDjkDgBQC4Y2zFAB6Z20PhOtVS7YrEctWUegSJ5AgAg95A4AUAuMU/hWzO8hU1GoD7dGKPXV+zRgoijupmUYpsgAQBAuljjBAAOcGcRidiL1y0KQmSFIenrbX9JooAEAAD2RuIEAA5yexGJm0kp8h8fmu0y5myeCwCAfTFVDwDyADcXJw1q7pfjdj6LiGHaHgAAdsCIEwDkEbbYQDe9zXNzuJQKAACIxAkA8pTb1z4tjzqpfacvW93GnZvnPtvUV3VtGSQAAIUQiRMA5DHmtU8DmlfVjJDoHI9Azd98XK18TDqz5ZhOXkxIHYliE10AALKOxAkA8rDbR6Bizl3VV9tOZKudDbFO2hB7KPUxVfgAALCO1YnT1atX9dZbb2n9+vU6e/asUlIsFyEfPXrUZsEBACyr73m5O6dWz7OO5UonqvABAGAdqxOngQMHKjw8XL1795aPj49MJpYdA0BusUUBidt9ujFG3l5u6t+sKlP3AAC4C6sTp9DQUK1Zs0bNmjWzRzwAgEzYYvNcM0PSjNCDevvng0zdAwDgLqxOnEqVKiVvb297xAIAyCJbbp4rMXUPAIDMWD0vY+rUqZowYYKuXbtmj3gAAFay1ea5EhvoAgCQEatHnN577z0dOXJE5cuXV5UqVeTq6mrx/V27dtksOABA1thq7VOKIS3aHCNnJ5OOX7hG6XIAAP6f1YnTo48+aocwAAA5defaJ19vT52+dE0LNh3XrdVMWSvmMz30gMVjSpcDAJCNxGnixIn2iAMAYAO3r30yMxnSgs3HlN2BKPP6pz1/XVKNCsUYhQIAFErZ3gB3586d2r9/v0wmkwICAvTAAw/YMi4AgI281v4+1Uo6orMla+nttYez3U5kzAVFxlyQxCgUAKDwsTpxOnv2rJ588klt2LBBJUuWlGEYunTpklq3bq1vvvlGZcuWtUecAIAccHGSBj7sp4vXk7K5ga4lqvABAAobq+dZDBs2TPHx8dq3b58uXLigf/75R3v37lV8fLyGDx9ujxgBADYytmOABrfwk9Mdy52yu5U5VfgAAIWF1SNOP//8s3755RfVqlUr9VhAQIA++ugjBQcH2zQ4AIDtpVdEIiklRTNCD1rdVoohvbBkp3xKFmHtEwCgQLM6cUpJSUlTglySXF1dlZLCp44AkB/cWUTiZlKK3v75YLZKmYftP5v6b9Y+AQAKKqs/FnzkkUc0YsQInT59OvXYqVOnNHLkSLVp08amwQEAcoetNtE1r32aERJtg6gAAMg7rE6cPvzwQ12+fFlVqlRRtWrVVL16dfn5+eny5cv64IMP7BEjACAXZLT+KTtY+wQAKGisnqpXuXJl7dq1S2FhYTpw4IAMw1BAQIDatm1rj/gAALnozvVPh8/EK/LoP1a3k2JIX0YeU+/AKhZrqVgDBQDIr7K9j1NQUJCCgoJsGQsAIA+4c/3TjJBofRYRY/X6p+VRJ/VmyH6L61gDBQDIr7KUOL3//vt67rnn5OHhoffff/+u51KSHAAKljtHoWIvXrcoCJGRfacvpznG/k8AgPwqS4nT7Nmz9cwzz8jDw0OzZ8/O8DyTyUTiBAAF0O2jUDeTUuQ/PjRbFfjMPouI0ehgf6btAQDyjSwlTjExMen+GwBQ+Jgr8JlHjrIjxZAWbY6Rs5OJ9U8AgHzB6r9QU6ZM0bVr19Icv379uqZMmWKToAAAeVtGFficTFKAT/EstTE99ICmrtmvxZHHNXXNfvmPD6WMOQAgz7I6cZo8ebKuXLmS5vi1a9c0efJkmwQFAMj7xnYM0IGpHTS+Uy31CfTV+E61dGBqB3V/8J5stcceUACAvMzqxMkwDJlMaTf5+OOPP+Tt7W2ToAAA+YN57dOUbnU0oHlVubk4qXdglRztBcUeUACAvCjL5chLlSolk8kkk8mkmjVrWiRPycnJunLlioYMGWKXIAEA+UdO10ClGNILS3bKp2QR1j4BAPKMLCdOc+bMkWEYevbZZzV58mSVKFEi9Xtubm6qUqWKAgMD7RIkACB/MZcav3P/J5OkrBTju73cOXs/AQDygiwnTn379pUk+fn5qVmzZnJxyfbeuQCAQuDO/Z98vT2VlJKiGaEHrWqHvZ8AAHmB1XMfrl69qvXr16c5vnbtWoWGhtokKABAwXDnGqj+zapme/0Ta58AAI5kdeI0ZswYJScnpzluGIbGjBljk6AAAAWTef1TdqQY0peRx2wbEAAAWWT1fLvDhw8rICDtVAl/f3/9+eefNgkKAFBwZbT+KSuOX0i7jyAAALnB6sSpRIkSOnr0qKpUqWJx/M8//5SXl5et4gIAFGB3rn+KvXjdoiBERtyc/53ndzMpxWL9FNX3AAD2ZPVfmK5du+qll17SkSNHUo/9+eefGj16tLp27Wp1AHPnzpWfn588PDzUoEEDRURE3PX8hIQEvfHGG/L19ZW7u7uqVaumzz//3Or7AgAc6/b1Tx890yBLa5/mbzqmSav2afKqvfIfH6qpa/ZrceRxTV2zX/7jQ9k8FwBgN1YnTu+88468vLzk7+8vPz8/+fn5qVatWipdurTeffddq9patmyZXnrpJb3xxhuKiopS8+bN1aFDB504cSLDa3r06KH169drwYIFOnjwoJYuXSp/f39rnwYAIA/JytqnamVvzWpYtOWYFm45nmaan7n6HskTAMAesjVVb8uWLQoLC9Mff/yhIkWKqF69emrRooXVN581a5YGDBiggQMHSrq1V9TatWv18ccfa8aMGWnO//nnnxUeHq6jR4/K29tbktJMGQQA5E8ZrX1yMil1H6cN+8+q3xfb79rOZxExerhaWV24flPlinmokZ+3nLNbyg8AgP+Xrc2YTCaTgoOD1aJFC7m7u8tksv4P0s2bN7Vz5840lfiCg4O1ZcuWdK9ZtWqVGjZsqJkzZ+rLL7+Ul5eXunbtqqlTp6pIkSLpXpOQkKCEhITUx/Hx8ZKkxMREJSYmWh23rZljyAuxIH+gz8Ba+anPvBxUQ8NbV9OS30/oxIXrute7iJ5pdK/cXJyUmJioQ2cvZdpGiiH1Xvh76uMKxd01rqO/2tUub8/QC5T81GeQd9BvYK280GesubfViVNKSorefPNNffLJJzpz5owOHTqkqlWravz48apSpYoGDBiQpXbOnTun5ORklS9v+YesfPnyiouLS/eao0ePatOmTfLw8NCKFSt07tw5DR06VBcuXMhwndOMGTM0efLkNMfXrVsnT0/PLMWaG8LCwhwdAvIZ+gyslZ/6THlJ5Z0kXZR+Wffv1LuIGCdZO8s8Lv6GXvxmt56tmaL7S1tZxq+Qy099BnkH/QbWcmSfuXYt69VarU6cpk2bpi+++EIzZ87UoEGDUo/XrVtXs2fPznLiZHbnaJVhGBmOYKWkpMhkMmnJkiUqUaKEpFvT/Z544gl99NFH6Y46jR07VqNGjUp9HB8fr8qVKys4OFjFixe3KlZ7SExMVFhYmIKCguTq6urocJAP0GdgrYLUZ85sOaaI0ENWXmWSSVLoGU+9+kwLpu1lQUHqM8g99BtYKy/0GfNstKywOnFavHixPv30U7Vp00ZDhgxJPV6vXj0dOHAgy+2UKVNGzs7OaUaXzp49m2YUyszHx0f33HNPatIkSbVq1ZJhGDp58qRq1KiR5hp3d3e5u7unOe7q6pqnfqnzWjzI++gzsFZB6DP9mlXTWz8fsnr/J0NS7KUERZ28rMBqpe0SW0FUEPoMch/9BtZyZJ+x5r5WV9U7deqUqlevnuZ4SkqKVXME3dzc1KBBgzRDc2FhYWratGm61zRr1kynT5/WlStXUo8dOnRITk5OqlSpUpbvDQDIn7JSfe9uzl6+oZtJKVoQcVQTftyrBRFHdTMpxYYRAgAKKqsTp9q1a6e719J3332nBx54wKq2Ro0apfnz5+vzzz/X/v37NXLkSJ04cSJ1JGvs2LHq06dP6vlPP/20Spcurf79+ys6OlobN27UK6+8omeffTbD4hAAgIJlbMcADW7hl6V9n+70S3Qc+z8BALLF6ql6EydOVO/evXXq1CmlpKRo+fLlOnjwoBYvXqzVq1db1VbPnj11/vx5TZkyRbGxsapTp45CQkLk6+srSYqNjbXY06lo0aIKCwvTsGHD1LBhQ5UuXVo9evTQtGnTrH0aAIB8bGzHAI0O9teXkcd0/MI1VS7lqfkRR3T28k3dbRbfT3vSFh8y7/9kbhcAgPRYnTh16dJFy5Yt0/Tp02UymTRhwgQ9+OCD+umnnxQUFGR1AEOHDtXQoUPT/d6iRYvSHPP396daCwBAbi5OGtC8aurjyt5F9PxXu2SS7po8ZeSziBiNDvaXm4vVkzEAAIVAtv46tGvXTuHh4bpy5YquXbumTZs2KTg42NaxAQCQZe3r+OjjXg+qQgkPi+M+JTz0xIP3ZHp9iiF9GXnMTtEBAPK7bG2ACwBAXtS+jo+CAiro95gLOnv5hsoV81AjP29N/mlflq4/fuHWfh43k1JSpwH6enuqd2AVRqIAoJDLUuLk7e2tQ4cOqUyZMipVqlSG+yxJt9Yh1a5dW2+//bbq1atns0ABAMgKZydTmpLjvt5Z2/Dc19tTM0Ki9VlEjEXJ8zdD9mtQcz/WQAFAIZalxGn27NkqVqyYJGnOnDl3PTchIUEhISHq37+/du7cmeMAAQDIqd6BVfRmyP5M93+aH3FEsfE30xyngAQAIEuJU9++fdP9d0Y6dOigBg0aZD8qAABsyLz/kzn5SY9JSjdpuh0FJACg8MrWK//Fixc1f/58jR07VhcuXJAk7dq1S6dOnZIkVa5cWWfPnrVdlAAA5FBG+z85maTBLfw0tHW1TNuggAQAFF5WF4fYs2eP2rZtqxIlSujYsWMaNGiQvL29tWLFCh0/flyLFy+2R5wAAOTYnfs/3V74YcKPe7PUBgUkAKBwsjpxGjVqlPr166eZM2emrnuSbk3Pe/rpp20aHAAAtnbn/k9mFJAAANyN1R+Nbd++XYMHD05z/J577lFcXNod2QEAyA96B1ZJM40vPZ+E/6l5G2PSFJowF5CYERJtnwABAA5ldeLk4eGh+Pj4NMcPHjyosmXL2iQoAABym7mAxN04Sfr7SuJdz/ksIkY3k1JsGBkAIC+weqpet27dNGXKFH377beSJJPJpBMnTmjMmDHq3r27zQMEACC3mKfZ3TkNz8kkDWruJw9XZ/13/Z93bSPFkBZtjpGzk4n1TwBQgFidOL377rvq2LGjypUrp+vXr6tly5aKi4tTkyZN9Oabb9ojRgAAco0tCkhMDz1g8Zj1TwCQ/1mdOBUvXlybNm3Sr7/+ql27diklJUUPPvig2rZta4/4AADIdTktIHEnNtAFgPwv2/MGHnnkEb388st69dVX1bZtW+3atUudO3e2ZWwAAOQpWS0gkRHWPwFA/mVV4hQWFqZXXnlFr7/+uo4ePSpJOnDggB599FE99NBDSkpKskuQAADkBVkpIHE3bKALAPlXlhOnL774Qu3atdPChQv11ltvqUmTJvrqq6/UqFEjlSpVSn/88Yd+/vlne8YKAIDDje0YoMEt/NKMPGV1IMq8gS4AIH/J8hqn2bNna/r06RozZoy+/fZbPfnkk5o9e7aioqJUrVo1e8YIAECekl4BiaSUFM0IPZjptdldJwUAcKwsJ05HjhxRz549JUlPPPGEnJ2dNWvWLJImAEChdGcBiZtJKXr754NpNsa906lL13UlIUlF3V10Mykl3ep9AIC8J8uJ09WrV+Xl5SVJcnJykoeHhypXrmy3wAAAyE/M65/M1fMy8vmmY1qzJ1Z1KpbQbwfPWiRalC0HgLzLqnLka9euVYkSJSRJKSkpWr9+vfbutdzTomvXrraLDgCAfCSzDXSbVC2jST/t0/Hz13Qm/mya6ylbDgB5l1WJU9++fS0eDx482OKxyWRScnJyzqMCACCfutsGupLUwLeU7p+8Tneb0fdZRIxGB/szbQ8A8pAsJ04pKew7AQBAVmS0ga4kfbfjr7smTdK/Zct7B1ZhDRQA5BFWjTgBAICcyWo58uVRJ/VmyH7WQAFAHkHiBABALspqOfJ9py+nOcYaKABwHMb7AQDIRb0Dq6TZPNdan0XE6GYSU+gBIDeROAEAkIvMZctzwrwGCgCQe5iqBwBALrtb2XL/CsUVHRufaRvHL1xjA10AyEUkTgAAOEBGZcu/jDym6DWZJ06Hz8TLf3woxSMAIJdkKXEqVaqUTKasTci+cOFCjgICAKCwSK9see/AKmmq6aUn8ug/aY5RPAIA7CdLidOcOXNS/33+/HlNmzZN7dq1U2BgoCQpMjJSa9eu1fjx4+0SJAAAhYV5DZQ5AcoONtAFANvLUuLUt2/f1H93795dU6ZM0Ysvvph6bPjw4frwww/1yy+/aOTIkbaPEgCAQuRua6AaV/FWZMzdZ3eYi0dktAkvAMB6Vn8UtXbtWrVv3z7N8Xbt2umXX36xSVAAABR2YzsG6MDUDhrfqZb6BPpqfKdaOjC1g2pUKJal67O60S4AIGusLg5RunRprVixQq+88orF8ZUrV6p06dI2CwwAgMIuvTVQWd1A995S/55H9T0AyDmrE6fJkydrwIAB2rBhQ+oap61bt+rnn3/W/PnzbR4gAAD4V1aLR3y/8y8FVCyu8ENn00z5o/oeAFjP6sSpX79+qlWrlt5//30tX75chmEoICBAmzdvVuPGje0RIwAA+H9ZKR7h6mzSgTNX9PT8bel+n+p7AGC9bO3j1LhxYy1ZssTWsQAAgCy4W/GIQc39NLhldc0KO6ivtp64aztU3wOArMtW4pSSkqI///xTZ8+eVUpKisX3WrRoYZPAAABAxjLaQNecBPmV9sq0DarvAUDWWZ04bd26VU8//bSOHz8uw7CcYG0ymZScnGyz4AAAQMbSKx5hltWqeubzbi8gUamku0qnZHIhABQyVidOQ4YMUcOGDbVmzRr5+PjIZDLZIy4AAJADWa2+5+vtqRkh0Wmm/ZnkrP0uBzWuSx07RQgA+YvVidPhw4f1/fffq3r16vaIBwAA2EBWq+/Njzii2PibaY4bkuZvPi5nZycKSACAsrEBbuPGjfXnn3/aIxYAAGAj5up7d+NkUrpJ0y23ZpR8FhGjm0nM2wMAq0echg0bptGjRysuLk5169aVq6urxffr1atns+AAAED2ZVZ9z8XJSR9tOHLXNiggAQC3WJ04de/eXZL07LPPph4zmUwyDIPiEAAA5DF3q7434ce9WWoj5txVLYg4mm71PgAoLKxOnGJiMt5wDwAA5D0ZVd/LagGJr7ZZ7gf1Zsh+DWrux9onAIWK1YmTr6+vPeIAAAC5LKsFJO6UYkjzNt76IJXkCUBhYXXitHjx4rt+v0+fPtkOBgAA5B5zAQlzEmTJkLlAREY+i4jR6GB/pu0BKBSsTpxGjBhh8TgxMVHXrl2Tm5ubPD09SZwAAMhHMiogkRUUjgBQmFidOP3zzz9pjh0+fFjPP/+8XnnlFZsEBQAAcs+dBSQqlXRX+M4D2nw2803uj1+4lgsRAoDjWZ04padGjRp666231KtXLx04cMAWTQIAgFx0ewGJxMRERUfvz9J1WS0wAQD5nc0mJTs7O+v06dO2ag4AADhQ8wqGnDIfcJKri5NSrJ3jBwD5kNUjTqtWrbJ4bBiGYmNj9eGHH6pZs2Y2CwwAADiOi5P0bFNfzd98/K7nTfhxn1ZGndK0R+sqoGLxXIoOAHKf1YnTo48+avHYZDKpbNmyeuSRR/Tee+/ZKi4AAOBgr7W/T87OTmkKRziZpAEP+8mnRBHNCjukXScuqsuHm9S/aRWNDKopL3ebrAQAgDzF6le2lJQUe8QBAADyoDsLR/h6e6p3YJXUEuQd6/po6uporflfrOZvitGa/8VqYpfaale7vEymLMz1A4B8IkcfCRnGrY+feGEEAKDgur1wxJ0qlPDQR888qCcOntWEH/fqrwvXNeSrnWrjX06TutZWZYpHACggslUcYvHixapbt66KFCmiIkWKqF69evryyy9tHRsAAMgnWt9XTmEjW+rF1tXl6mzS+gNnFTQ7XHM3/KmbScxWAZD/WZ04zZo1S88//7w6duyob7/9VsuWLVP79u01ZMgQzZ492x4xAgCAfMDD1Vkvt7tPoSOaq0lVb91ITNHMnw+q0/sR2nb0vKPDA4AcsXqq3gcffKCPP/5Yffr0ST3WrVs31a5dW5MmTdLIkSNtGiAAAMhfqpcrpqWDmmhF1Cm9uWa/Dp+9op6fbtUTDSppbAd/lS7q7ugQAcBqVo84xcbGqmnTpmmON23aVLGxsTYJCgAA5G8mk0mPP1hJ60e31NON75Ukfb/zpNrMCtey7SfY+wlAvmN14lS9enV9++23aY4vW7ZMNWrUsElQAACgYCjp6abpj9XVD883lX+FYrp4LVGv/fA//WdepA7ExTs6PADIMqun6k2ePFk9e/bUxo0b1axZM5lMJm3atEnr169PN6ECAABo4FtKq4c9rEVbjmlW2CHtPP6POr2/SQMf9tOItjXk6cbeTwDyNqtHnLp3767ff/9dZcqU0cqVK7V8+XKVKVNGv//+ux577DF7xAgAAAoAF2cnDWxeVb+Maqn2tSsoOcXQvI1HFTRro9bti3N0eABwV1Z9vJOYmKjnnntO48eP11dffWWvmAAAQAFWsWQRfdK7gX49cEYTftynk/9c13Nf7lTbWuU1qWuAKpVi7ycAeY9VI06urq5asWKFvWIBAACFyCP+5RU2sqWGtqomFyeTftl/RkGzNuqT8CNKTGbvJwB5i9VT9R577DGtXLnSDqEAAIDCpoibs15t76+QEc3VyM9b1xOT9VboAXV+f5O2H7vg6PAAIJXVKzGrV6+uqVOnasuWLWrQoIG8vLwsvj98+HCbBQcAAAqHmuWLadlzTfTDrlOaHrJfB89c1n8+iVTPhpU1poO/Snm5OTpEAIWc1YnT/PnzVbJkSe3cuVM7d+60+J7JZCJxAgAA2WIymfREg0pq419Ob/98QN9s/0vLdvylddFxGtuxlv7ToJJMJpOjwwRQSFmdOMXExNgjDgAAAElSKS83vdW9np5oUEnjVu7VgbjLevX7Pfp+x0lNe6yOapYv5ugQARRCVq9xAgAAyA0Nq3jrp2EP6/WO/iri6qzfj11Qx/9G6K3QA7p2M8nR4QEoZKwecRo1alS6x00mkzw8PFS9enV169ZN3t7eOQ4OAAAUbq7OTnquRTV1qldRk1ft07roM/ok/Ih++uO0pnSrrTa1yjs6RACFhNWJU1RUlHbt2qXk5GTdd999MgxDhw8flrOzs/z9/TV37lyNHj1amzZtUkBAgD1iBgAAhcw9JYvo0z4NFRZ9RpNW7dOpi9c14Isdale7vCZ2qa2KJYs4OkQABZzVU/W6deumtm3b6vTp09q5c6d27dqlU6dOKSgoSE899ZROnTqlFi1aaOTIkfaIFwAAFGJBAeUVNqqFhrS8tffT2n1n1HZWuD7beJS9nwDYldWJ0zvvvKOpU6eqePHiqceKFy+uSZMmaebMmfL09NSECRPSVNwDAACwBU83F43p4K81w5uroW8pXbuZrDdD9qvLB5u08/g/jg4PQAFldeJ06dIlnT17Ns3xv//+W/Hx8ZKkkiVL6ubNmzmPDgAAIAP3VSimbwcHamb3eirp6aoDcZfV/eMtGrt8jy5e430IANvK1lS9Z599VitWrNDJkyd16tQprVixQgMGDNCjjz4qSfr9999Vs2ZNW8cKAABgwcnJpB4PVdavo1upR8NKkqSlv/+lR94L1w87T8owDAdHCKCgsDpxmjdvntq0aaMnn3xSvr6+uvfee/Xkk0+qTZs2+vjjjyVJ/v7+mj9/vs2DBQAASI+3l5tmPnG/vh0cqJrli+rC1Zsa/d0fevLTrfrz7GVHhwegALA6cSpatKg+++wznT9/PrXC3vnz5/Xpp5+qaNGikqT69eurfv36WWpv7ty58vPzk4eHhxo0aKCIiIgsXbd582a5uLhk+T4AAKDga+TnrdXDmuu19v7ycHXStpgL6vDfCL2z9oCu30x2dHgA8jGrE6f169dLupVA1atXT/fff39qwvThhx9a1dayZcv00ksv6Y033lBUVJSaN2+uDh066MSJE3e97tKlS+rTp4/atGljbfgAAKCAc3Nx0vOtqilsZEu18S+nxGRDH/12RMFzwvXbgbTrtAEgK6xOnLp3767t27enOT5nzhy9/vrrVrU1a9YsDRgwQAMHDlStWrU0Z84cVa5cOXXKX0YGDx6sp59+WoGBgVbdDwAAFB6VvT01v29DzevdQD4lPPTXhevqv2i7nv9qp2IvXXd0eADyGas3wJ09e7Y6duyo8PDw1A1u3333XU2dOlVr1qzJcjs3b97Uzp07NWbMGIvjwcHB2rJlS4bXLVy4UEeOHNFXX32ladOmZXqfhIQEJSQkpD42V/5LTExUYmJiluO1F3MMeSEW5A/0GViLPgNrFbQ+80jN0mo8rKk++O2IFkWeUOjeOG089LdGtKmu3o0ry8XZ6s+RkY6C1m9gf3mhz1hzb6sTp/79++v8+fMKDg7Wpk2btGzZMk2fPl2hoaFq2rRplts5d+6ckpOTVb58eYvj5cuXV1xcXLrXHD58WGPGjFFERIRcXLIW+owZMzR58uQ0x9etWydPT88sx2tvYWFhjg4B+Qx9Btaiz8BaBa3P1JM0uo707VFnHbuSrOmhB/VF+AH1qJqsKsUcHV3BUdD6DezPkX3m2rVrWT7X6sRJkl5++WWdP39eDRs2VHJystatW6fGjRtnpymZTCaLx4ZhpDkmScnJyXr66ac1efJkq0qdjx07VqNGjUp9HB8fr8qVKys4ONhiE19HSUxMVFhYmIKCguTq6urocJAP0GdgLfoMrFXQ+8yAFEPf7Tqld9Yd0qlrSZqzz0VPNqyk0UE1VKJIwXu+uaWg9xvYXl7oM+bZaFmRpcTp/fffT3PMx8dHnp6eatGihbZt26Zt27ZJkoYPH56lG5cpU0bOzs5pRpfOnj2bZhRKki5fvqwdO3YoKipKL774oiQpJSVFhmHIxcVF69at0yOPPJLmOnd3d7m7u6c57urqmqd+qfNaPMj76DOwFn0G1irIfaZXoJ861K2o6SEH9MOuk1q6/aTC9p/VG51q6dH696T7IS6ypiD3G9iHI/uMNffNUuI0e/bsdI87Oztr8+bN2rx5s6Rbo0dZTZzc3NzUoEEDhYWF6bHHHks9HhYWpm7duqU5v3jx4vrf//5ncWzu3Ln69ddf9f3338vPzy9L9wUAAJCk0kXd9V6P+/WfhpU0buVe/Xn2ikYu+0Pfbj+paY/VUbWyRR0dIoA8JEuJU0xMjF1uPmrUKPXu3VsNGzZUYGCgPv30U504cUJDhgyRdGua3alTp7R48WI5OTmpTp06FteXK1dOHh4eaY4DAABkVZOqpRUyvLk+iziq99cfVuTR8+owJ0KDW1bVC62ry8PV2dEhAsgDsrXGyVZ69uyp8+fPa8qUKYqNjVWdOnUUEhIiX19fSVJsbGymezoBAADklJuLk15oXV1d6lXUxFV79dvBv/XBr3/qx92nNfXROmpZs6yjQwTgYFbX33ziiSf01ltvpTn+zjvv6D//+Y/VAQwdOlTHjh1TQkKCdu7cqRYtWqR+b9GiRdqwYUOG106aNEm7d++2+p4AAADpube0pz7v95A+6fWgKhT30IkL19T389/1wte7dCb+hqPDA+BAVidO4eHh6tSpU5rj7du318aNG20SFAAAgKOYTCa1r+OjX0a31ICH/eTsZNKaPbFq8164Fm6OUXKK4egQATiA1YnTlStX5Obmlua4q6urVeX8AAAA8rKi7i4a3zlAq15spvqVS+pKQpIm/xStbh9t0h9/XXR0eABymdWJU506dbRs2bI0x7/55hsFBATYJCgAAIC8onbFElr+fFO9+VgdFfdw0d5T8Xp07maNX7lXl64nOjo8ALnE6uIQ48ePV/fu3XXkyJHUfZPWr1+vpUuX6rvvvrN5gAAAAI7m5GTSM419FRxQQTNC9mt51Cl9ufW4QvfGaXznWup6f0X2fgIKOKtHnLp27aqVK1fqzz//1NChQzV69GidPHlSv/zyix599FE7hAgAAJA3lC3mrlk96+vrgY1VtayXzl1J0Ihvdqv3gt8Vc+6qo8MDYEfZKkfeqVOndAtEAAAAFAZNq5dR6Ijm+jT8qD747U9t+vOc2s3eqOdbVdPzraqx9xNQAFk94gQAAADJ3cVZw9rUUNjIFmpRs6xuJqfov+sPq/2cjYo4/LejwwNgY1YnTsnJyXr33XfVqFEjVahQQd7e3hZfAAAAhYlvaS990f8hffT0gypXzF3Hzl9T7wW/a9jSKJ1l7yegwLA6cZo8ebJmzZqlHj166NKlSxo1apQef/xxOTk5adKkSXYIEQAAIG8zmUzqVM9H60e3VL+mVeRkkn7647TavBeuxZHH2PsJKACsTpyWLFmizz77TC+//LJcXFz01FNPaf78+ZowYYK2bt1qjxgBAADyhWIerprUtbZWvfiw7q9UQpcTkjThx316bO5m/e/kJUeHByAHrE6c4uLiVLduXUlS0aJFdenSrReBzp07a82aNbaNDgAAIB+qc08JLR/aTFO71VYxdxftOXlJ3T7apEmr9in+Bns/AfmR1YlTpUqVFBsbK0mqXr261q1bJ0navn273N3dbRsdAABAPuXsZFLvwCpa/3JLdatfUSmGtGjLMbV9L1w//XFahsH0PSA/sTpxeuyxx7R+/XpJ0ogRIzR+/HjVqFFDffr00bPPPmvzAAEAAPKzcsU89N8nH9BXAxrLr4yXzl5O0LClUerz+e86xt5PQL5h9T5Ob731Vuq/n3jiCVWqVElbtmxR9erV1bVrV5sGBwAAUFA8XOPW3k+fhB/R3A1HFHH4nILnbNQLraprSKuqcndh7ycgL8vWBri3a9KkiZo0aWKLWAAAAAo0D1dnvdS2prrVv0cTftyriMPnNPuXQ/px9ylNfbSOmlUv4+gQAWTA6ql658+fT/33X3/9pQkTJuiVV15RRESETQMDAAAoqPzKeGnxs430/lMPqGwxdx09d1XPzN+ml76J0t+XExwdHoB0ZDlx+t///qcqVaqoXLly8vf31+7du/XQQw9p9uzZ+vTTT9W6dWutXLnSjqECAAAUHCaTSV3vr6j1o1uqb6CvTCZp5e7TeuS9Dfpy63H2fgLymCwnTq+++qrq1q2r8PBwtWrVSp07d1bHjh116dIl/fPPPxo8eLDF+icAAABkrriHqyZ3q6MfX2imuveU0OUbSRq/cq8e/3iL9p5i7ycgr8hy4rR9+3a9+eabevjhh/Xuu+/q9OnTGjp0qJycnOTk5KRhw4bpwIED9owVAACgwKpXqaRWvtBMk7vWVlF3F/3x10V1/XCTpvwUrSsJSY4ODyj0spw4XbhwQRUqVJB0a+NbLy8veXt7p36/VKlSunz5su0jBAAAKCScnUzq27SK1o9uqc71fJRiSJ9vjlGb9zYo5H+x7P0EOJBVxSFMJtNdHwMAACDnyhf30IdPP6jFzzaSb2lPnYlP0NAlu9R/0XadOH/N0eEBhZJV5cj79esnd3d3SdKNGzc0ZMgQeXl5SZISEqgAAwAAYEstapbV2pdaaO6GI/pkwxFtOPi3gmaHa9gj1TWoBXs/Abkpy4lT3759LR736tUrzTl9+vTJeUQAAABI5eHqrFFBNdWtfkWNX7lXW46c17vrDmlF1ClNe7SuAquVdnSIQKGQ5cRp4cKF9owDAAAAd1GtbFEtGdhYq/44ramro3Xk76t66rOtevyBe/R6p1oqU9Td0SECBZrVG+ACAADAMUwmk7rVv0frR7VSryb3ymSSlkedUpv3wvX1thNKYe8nwG5InAAAAPKZEp6umvZoXa0Y2kwBPsV16XqiXl/xP3X/ZIuiT8c7OjygQCJxAgAAyKfqVy6pVS8204TOAfJyc1bUiYvq8uEmTVvN3k+ArZE4AQAA5GMuzk569mE/rR/dSp3q+ig5xdD8TTEKmhWun/fGsfcTYCMkTgAAAAVAhRIe+uiZB7Ww/0Oq7F1EsZduaMhXOzXgix366wJ7PwE5ReIEAABQgLS+r5zCRrbUi62ry9XZpF8PnFXQ7HDN3fCnbialODo8IN8icQIAAChgPFyd9XK7+xQ6ormaVPXWjcQUzfz5oDq9H6FtR887OjwgXyJxAgAAKKCqlyumpYOaaFaP+1Xay02Hz15Rz0+36uXv/tD5KwmODg/IV0icAAAACjCTyaTHH6yk9aNb6unG90qSvt95Um1mheub39n7CcgqEicAAIBCoKSnm6Y/Vlc/PN9U/hWK6eK1RI1Z/j/9Z16kDsSx9xOQGRInAACAQqSBbymtHvawxnWqJU83Z+08/o86vb9J00P26yp7PwEZInECAAAoZFycnTSweVX9Mqql2teuoOQUQ59uPKqgWeFaty/O0eEBeRKJEwAAQCFVsWQRfdK7gT7v11CVShXR6Us39NyXOzXwix06+c+/ez/dTErRgoijmvDjXi2IOEpZcxRKLo4OAAAAAI71iH95BVYtow9+PaxPNx7VL/vPaPOf5zSibQ2du5KgzzfF6PYaEtPW7FcTP299MaCx3Fz4HB6FAz0dAAAAKuLmrFfb+ytkRHM18vPW9cRkvRV6QPMjLJMmSTIkRcZcUM1xoXpqXiQjUCgUSJwAAACQqmb5Ylr2XBO91b1uls43J1Bv/3zQzpEBjkXiBAAAAAsmk0lXb1hXYW/+5uNaccxkp4gAxyNxAgAAQBrHL1zL/KQ7bIh10tPztzJ1DwUSiRMAAADS8PX2zMZVJm0/Hs/aJxRIJE4AAABIo3dgFTnlYOYdxSNQ0JA4AQAAIA03FycNau6X43bMCdTU1dE2iApwHBInAAAApGtsxwANbuEnW5R8WLApRoHT1zP6hHyLxAkAAAAZGtsxQAendVBg1VI5bis2/gbT95BvkTgBAADgrtxcnLT0uaY6NK2DGlfJeQJlnr734te7lHzn7rpAHkXiBAAAgCxxc3HSsiFNNeBhX5u0t3pPrPzHh+qnP07bpD3AnkicAAAAYJXxnevYbO1TYrKhYUujNGjxdhu0BtgPiRMAAACsZsu1T5IUFn1W7eds0IKIo6x/Qp5E4gQAAIBsSbv2KWfrlQ7EXdXUNfvlPz5UM0IoX468hcQJAAAAOeLm4qSvBjyk9xonq0Jx9xy3l2JI8zbGkDwhTyFxAgAAgE24OEkRr7S0WfGIeRtjdOVGkk3aAnKKxAkAAAA2Nb5zHR2y0fqnOpPWaupqRp7geCROAAAAsLnb1z/lNIFasClGgdPXUzQCDkXiBAAAALu5PYGqWd4r2+3Ext9QzXGhajz9F80L/5MkCrmOxAkAAAB25+bipHUjW6l/syo5audMfIJmhB5UzXGhempeJAkUcg2JEwAAAHLNxC61bVY8IjLmgmqOC2UNFHIFiRMAAABy1fjOdTS4hZ+cTLZpjzVQyA0kTgAAAMh1YzsG6MBU21Tek/5dA8XoE+yFxAkAAAAOYS4cMbiFn2w0+MToE+yGxAkAAAAONbZjgA5O6yCf4u42aY/RJ9gDiRMAAAAczs3FSZGvt7VZ4Qjp1uhTlw8ibNYeCjcSJwAAAOQZ4zvXscmmuWb/OxWvxtPCmLqHHCNxAgAAQJ5y+6a5tkigzly5qZrjQtXhv+FaEHGUJArZQuIEAACAPOn2BOr1Dv7ydM3ZW9f9sVc0dc1+3Tc+VG+uYf0TrEPiBAAAgDzNzcVJz7WspuipHWyyBsowpM8iYhQ8K5zRJ2QZiRMAAADyDfMaKFtU4Dt09grV95BlJE4AAADIV2xdgY/qe8gKEicAAADkS+bRpwo2GH2i+h4yQ+IEAACAfMvNxUlbX2+rOhWL5rgtc/U9pu4hPSROAAAAyPdWD2/J1D3YFYkTAAAACgRbFo7436l4dfpvuA2iQkHh8MRp7ty58vPzk4eHhxo0aKCIiIyz++XLlysoKEhly5ZV8eLFFRgYqLVr1+ZitAAAAMjLbFk4Yl/sFdY9IZVDE6dly5bppZde0htvvKGoqCg1b95cHTp00IkTJ9I9f+PGjQoKClJISIh27typ1q1bq0uXLoqKisrlyAEAAJCXmUefxneqpRY1ymT7TS/rnmDm0MRp1qxZGjBggAYOHKhatWppzpw5qly5sj7++ON0z58zZ45effVVPfTQQ6pRo4amT5+uGjVq6KeffsrlyAEAAJDXubk4aUDzqlo8oLEOT+8ovzKe2W5rwaYYBU7/hdGnQszFUTe+efOmdu7cqTFjxlgcDw4O1pYtW7LURkpKii5fvixvb+8Mz0lISFBCQkLq4/j4eElSYmKiEhMTsxG5bZljyAuxIH+gz8Ba9BlYiz6D7MgP/WbdiIf16Nwt2hd7JVvXx8YnqOa4UPVv6qvXO9xn4+gKn7zQZ6y5t8MSp3Pnzik5OVnly5e3OF6+fHnFxcVlqY333ntPV69eVY8ePTI8Z8aMGZo8eXKa4+vWrZOnZ/Y/dbC1sLAwR4eAfIY+A2vRZ2At+gyyI6/3m+eqSCtSTNpwxkmSKVttLNxyTL/sPqqX7zdsGlth5cg+c+3atSyf67DEycxksuywhmGkOZaepUuXatKkSfrxxx9Vrly5DM8bO3asRo0alfo4Pj5elStXVnBwsIoXL579wG0kMTFRYWFhCgoKkqurq6PDQT5An4G16DOwFn0G2ZGf+k1HSTeTUvTI7AidiU/I9Py0TPrrmrNe/V0a0ba6+japIjcXh9dcy3fyQp8xz0bLCoclTmXKlJGzs3Oa0aWzZ8+mGYW607JlyzRgwAB99913atu27V3PdXd3l7t72pKUrq6ueeqXOq/Fg7yPPgNr0WdgLfoMsiO/9BtXV2nb623V6b/h2Z66l5AszVz7p2au/VMDHvbT+M4BNo6ycHBkn7Hmvg5Ljd3c3NSgQYM0Q3NhYWFq2rRphtctXbpU/fr109dff61OnTrZO0wAAAAUYGtGtFSdikVz3A7FIwo+h44pjho1SvPnz9fnn3+u/fv3a+TIkTpx4oSGDBki6dY0uz59+qSev3TpUvXp00fvvfeemjRpori4OMXFxenSpUuOegoAAADI51YPb2mTfZ/MxSMoXV4wOTRx6tmzp+bMmaMpU6aofv362rhxo0JCQuTre6vjxsbGWuzpNG/ePCUlJemFF16Qj49P6teIESMc9RQAAABQAJj3fapQPO0SD2st2BSjLh9E2CAq5CUOLw4xdOhQDR06NN3vLVq0yOLxhg0b7B8QAAAACiU3Fydtfb2tOr8frr2ns7fuyex/p+LVeFqYIsa0oXBEAcH/IgAAAHAbW03dO3PlJlP3ChASJwAAAOAO5ql75YvnvNobU/cKBhInAAAAIB1uLk7a9nqwTUafbk3dW0fVvXyMxAkAAAC4C/PoU2DVUjlq58yVRKbu5WMkTgAAAEAm3FyctPS5pjaZvseeT/kTiRMAAACQRbaavseeT/kPiRMAAABgpdTiEcUYfSosSJwAAACAbHBzcdK2N4LVxr9Mjtph9Cl/IHECAAAAcmBBv8b68Mn6OX5jvWBTjDq/T9nyvIrECQAAAMihzvXv0eHpHVXF2yNH7ew9Ha8Hp6y1UVSwJRInAAAAwAacnUza8Gob1alYNEftXLiWJP9xa1j3lMeQOAEAAAA2tHp4yxxX3buRJNUcF6qe87aQQOURJE4AAACAjaVW3cvhnk/bYv5RzXGhmhFC4QhHI3ECAAAA7MBWez5J0ryNMSRPDkbiBAAAANiRrUaf5m2M0ZUbSTaKCtYicQIAAADszFajT3UmrdXkVftsFBWsQeIEAAAA5BLz6FMRV1O221i45ZgaTguzYVTIChInAAAAIBe5uThp/9SO8vZ0yXYb567cpGR5LiNxAgAAABxg14R2Cqjgme3rzSXLp66maERuIHECAAAAHCTkpdY5Xve0YFOMunwQYaOIkBESJwAAAMCBzOueqpXJ/ujT/07Fq/G0dUzdsyMSJwAAAMDB3FyctP7l1mpds0y22zhzJZGpe3ZE4gQAAADkEQufbay69xTLURtM3bMPEicAAAAgD/lpWIscr3v636l4dfpvuI0igkTiBAAAAOQ5ttjvaV/sFXWcQ/JkKyROAAAAQB5k3u+pVvnsF42IjruiB6astWFUhReJEwAAAJCHhY5srToVi2b7+n+uJSlgfKgNIyqcSJwAAACAPG718JY5Wvd0LTFFVcesoVx5DpA4AQAAAPmAed1T+WKu2bo+RVLNcaHqOW8LCVQ2kDgBAAAA+YSbi5O2vRGs2j7Zn7q3LeYf9nvKBhInAAAAIJ9ZM6JljtY9Sez3ZC0SJwAAACAfyum6J+nWfk+ULM8aEicAAAAgnzKve/LIwX5P0XFX5D+OwhGZIXECAAAA8jE3FycdmNpRnq7Zf2t/I0mse8oEiRMAAABQAERP7aAiLjlrY8GmGHVl3VO6SJwAAACAAmL/tE7yLuKcozb2nIpX3wVbbRRRwUHiBAAAABQguya2V9+mlXPURvjh86o3MZR1T7chcQIAAAAKmMld6+Vos1xJik9IUc1xoZq8ap8NI8u/SJwAAACAAsi8WW5O93tauOWYGkwNs1FU+ReJEwAAAFCA2WK/p/NXb6rm62tsFFH+ROIEAAAAFHDm/Z6csr/dk26mSFXGFN7kicQJAAAAKATcXJx0dEYnlfTIWQpw/8RQG0WUv5A4AQAAAIXI7kkdVKu8Z7avv5SQorbvrLdhRPkDiRMAAABQyISObJ2johF/nr+h6oVszROJEwAAAFAI5bRoRFKK5F+IkicSJwAAAKCQMheNKF80e/s93UiRahaSghEkTgAAAEAh5ubipG3jgrM9+nRThaPaHokTAAAAgNTRp+wq6MkTiRMAAAAASbdGn0ie0kfiBAAAACCVm4uTBrfwy/b1BTV5InECAAAAYGFsxwANeJjk6XYkTgAAAADSGN85QH2bZr9ceUFLnkicAAAAAKRrctc6qlepeLavrzOu4CRPJE4AAAAAMrTqxebZHnm6kiQ9MCnUxhE5BokTAAAAgLua3LVOtpOnf26kqPP74TaOKPeROAEAAADIVE6m7e09fUVXbiTZOKLcReIEAAAAIEtyMm2vzqS1No4md5E4AQAAAMiynEzbe235LhtHk3tInAAAAABYZXLXOqpT0fppe8t+j9XNpBQ7RGR/JE4AAAAArLZ6eHOV8nCx+rqa4/JnlT0SJwAAAADZEjWpnUzZuK5uPtzficQJAAAAQLbFvNXJ6msuJ0kXrty0QzT2Q+IEAAAAIEf+mBBs9TWN395g+0DsiMQJAAAAQI6U8HRV8Wysdzp3zQ7B2AmJEwAAAIAc2zEuyOprpv6RnRVSjkHiBAAAACDH3Fyc1CfQ2v2dnPTydzvsEo+tkTgBAAAAsIkp3erIw8WaFMOkH/dcyBd7O5E4AQAAALCZA9M6WH1NftjbicQJAAAAgE2FvNjc6mviLt6wQyS2Q+IEAAAAwKYCKhW3+pp2czbYPhAbInECAAAAYHPWjjpdupFsp0hsg8QJAAAAgM1lZ9QpLyNxAgAAAGAXv41q5egQbIbECQAAAIBd+JXzyvK55dzsGIgNODxxmjt3rvz8/OTh4aEGDRooIiLirueHh4erQYMG8vDwUNWqVfXJJ5/kUqQAAAAArLVrXFCWzvv51ayd5ygOTZyWLVuml156SW+88YaioqLUvHlzdejQQSdOnEj3/JiYGHXs2FHNmzdXVFSUXn/9dQ0fPlw//PBDLkcOAAAAICu8i7qpbNGMhpMMSVLZom7yzvCcvMGhidOsWbM0YMAADRw4ULVq1dKcOXNUuXJlffzxx+me/8knn+jee+/VnDlzVKtWLQ0cOFDPPvus3n333VyOHAAAAEBWbR8XlGHyVKaom7ZncVTKkVwcdeObN29q586dGjNmjMXx4OBgbdmyJd1rIiMjFRwcbHGsXbt2WrBggRITE+Xq6prmmoSEBCUkJKQ+jo+PlyQlJiYqMTExp08jx8wx5IVYkD/QZ2At+gysRZ9BdtBvkJktr7XShSs31evz33X2yk2V9XJTf994PdapmcP6jTX3dVjidO7cOSUnJ6t8+fIWx8uXL6+4uLh0r4mLi0v3/KSkJJ07d04+Pj5prpkxY4YmT56c5vi6devk6emZg2dgW2FhYY4OAfkMfQbWos/AWvQZZAf9Bpl5sbr5XzckObbPXLt2LcvnOixxMjOZTBaPDcNIcyyz89M7bjZ27FiNGjUq9XF8fLwqV66s4OBgFS/u+NryiYmJCgsLU1BQULojZsCd6DOwFn0G1qLPIDvoN7BWXugz5tloWeGwxKlMmTJydnZOM7p09uzZNKNKZhUqVEj3fBcXF5UuXTrda9zd3eXu7p7muKura576pc5r8SDvo8/AWvQZWIs+g+yg38Bajuwz1tzXYcUh3Nzc1KBBgzRDc2FhYWratGm61wQGBqY5f926dWrYsCG/oAAAAADsxqFV9UaNGqX58+fr888/1/79+zVy5EidOHFCQ4YMkXRrml2fPn1Szx8yZIiOHz+uUaNGaf/+/fr888+1YMECvfzyy456CgAAAAAKAYeucerZs6fOnz+vKVOmKDY2VnXq1FFISIh8fX0lSbGxsRZ7Ovn5+SkkJEQjR47URx99pIoVK+r9999X9+7dHfUUAAAAABQCDi8OMXToUA0dOjTd7y1atCjNsZYtW2rXrl12jgoAAAAA/uXQqXoAAAAAkB+QOAEAAABAJkicAAAAACATJE4AAAAAkAkSJwAAAADIBIkTAAAAAGSCxAkAAAAAMuHwfZxym2EYkqT4+HgHR3JLYmKirl27pvj4eLm6ujo6HOQD9BlYiz4Da9FnkB30G1grL/QZc05gzhHuptAlTpcvX5YkVa5c2cGRAAAAAMgLLl++rBIlStz1HJORlfSqAElJSdHp06dVrFgxmUwmR4ej+Ph4Va5cWX/99ZeKFy/u6HCQD9BnYC36DKxFn0F20G9grbzQZwzD0OXLl1WxYkU5Od19FVOhG3FycnJSpUqVHB1GGsWLF+dFBlahz8Ba9BlYiz6D7KDfwFqO7jOZjTSZURwCAAAAADJB4gQAAAAAmSBxcjB3d3dNnDhR7u7ujg4F+QR9Btaiz8Ba9BlkB/0G1spvfabQFYcAAAAAAGsx4gQAAAAAmSBxAgAAAIBMkDgBAAAAQCZInAAAAAAgEyROdjZ37lz5+fnJw8NDDRo0UERExF3PDw8PV4MGDeTh4aGqVavqk08+yaVIkZdY02+WL1+uoKAglS1bVsWLF1dgYKDWrl2bi9EiL7D2tcZs8+bNcnFxUf369e0bIPIca/tMQkKC3njjDfn6+srd3V3VqlXT559/nkvRIi+wts8sWbJE999/vzw9PeXj46P+/fvr/PnzuRQtHG3jxo3q0qWLKlasKJPJpJUrV2Z6TV5/H0ziZEfLli3TSy+9pDfeeENRUVFq3ry5OnTooBMnTqR7fkxMjDp27KjmzZsrKipKr7/+uoYPH64ffvghlyOHI1nbbzZu3KigoCCFhIRo586dat26tbp06aKoqKhcjhyOYm2fMbt06ZL69OmjNm3a5FKkyCuy02d69Oih9evXa8GCBTp48KCWLl0qf3//XIwajmRtn9m0aZP69OmjAQMGaN++ffruu++0fft2DRw4MJcjh6NcvXpV999/vz788MMsnZ8v3gcbsJtGjRoZQ4YMsTjm7+9vjBkzJt3zX331VcPf39/i2ODBg40mTZrYLUbkPdb2m/QEBAQYkydPtnVoyKOy22d69uxpjBs3zpg4caJx//332zFC5DXW9pnQ0FCjRIkSxvnz53MjPORB1vaZd955x6hatarFsffff9+oVKmS3WJE3iXJWLFixV3PyQ/vgxlxspObN29q586dCg4OtjgeHBysLVu2pHtNZGRkmvPbtWunHTt2KDEx0W6xIu/ITr+5U0pKii5fvixvb297hIg8Jrt9ZuHChTpy5IgmTpxo7xCRx2Snz6xatUoNGzbUzJkzdc8996hmzZp6+eWXdf369dwIGQ6WnT7TtGlTnTx5UiEhITIMQ2fOnNH333+vTp065UbIyIfyw/tgF0cHUFCdO3dOycnJKl++vMXx8uXLKy4uLt1r4uLi0j0/KSlJ586dk4+Pj93iRd6QnX5zp/fee09Xr15Vjx497BEi8pjs9JnDhw9rzJgxioiIkIsLfwYKm+z0maNHj2rTpk3y8PDQihUrdO7cOQ0dOlQXLlxgnVMhkJ0+07RpUy1ZskQ9e/bUjRs3lJSUpK5du+qDDz7IjZCRD+WH98GMONmZyWSyeGwYRppjmZ2f3nEUbNb2G7OlS5dq0qRJWrZsmcqVK2ev8JAHZbXPJCcn6+mnn9bkyZNVs2bN3AoPeZA1rzMpKSkymUxasmSJGjVqpI4dO2rWrFlatGgRo06FiDV9Jjo6WsOHD9eECRO0c+dO/fzzz4qJidGQIUNyI1TkU3n9fTAfNdpJmTJl5OzsnOaTmLNnz6bJps0qVKiQ7vkuLi4qXbq03WJF3pGdfmO2bNkyDRgwQN99953atm1rzzCRh1jbZy5fvqwdO3YoKipKL774oqRbb4oNw5CLi4vWrVunRx55JFdih2Nk53XGx8dH99xzj0qUKJF6rFatWjIMQydPnlSNGjXsGjMcKzt9ZsaMGWrWrJleeeUVSVK9evXk5eWl5s2ba9q0aXli9AB5S354H8yIk524ubmpQYMGCgsLszgeFhampk2bpntNYGBgmvPXrVunhg0bytXV1W6xIu/ITr+Rbo009evXT19//TXzxwsZa/tM8eLF9b///U+7d+9O/RoyZIjuu+8+7d69W40bN86t0OEg2XmdadasmU6fPq0rV66kHjt06JCcnJxUqVIlu8YLx8tOn7l27ZqcnCzfZjo7O0v6dxQBuF2+eB/soKIUhcI333xjuLq6GgsWLDCio6ONl156yfDy8jKOHTtmGIZhjBkzxujdu3fq+UePHjU8PT2NkSNHGtHR0caCBQsMV1dX4/vvv3fUU4ADWNtvvv76a8PFxcX46KOPjNjY2NSvixcvOuopIJdZ22fuRFW9wsfaPnP58mWjUqVKxhNPPGHs27fPCA8PN2rUqGEMHDjQUU8BuczaPrNw4ULDxcXFmDt3rnHkyBFj06ZNRsOGDY1GjRo56ikgl12+fNmIiooyoqKiDEnGrFmzjKioKOP48eOGYeTP98EkTnb20UcfGb6+voabm5vx4IMPGuHh4anf69u3r9GyZUuL8zds2GA88MADhpubm1GlShXj448/zuWIkRdY029atmxpSErz1bdv39wPHA5j7WvN7UicCidr+8z+/fuNtm3bGkWKFDEqVapkjBo1yrh27VouRw1HsrbPvP/++0ZAQIBRpEgRw8fHx3jmmWeMkydP5nLUcJTffvvtru9P8uP7YJNhMF4KAAAAAHfDGicAAAAAyASJEwAAAABkgsQJAAAAADJB4gQAAAAAmSBxAgAAAIBMkDgBAAAAQCZInAAAAAAgEyROAAAAAJAJEicAQI6YTCatXLky1+9bpUoVzZkzJ0dtXLt2Td27d1fx4sVlMpl08eLFdI9Zc69FixapZMmSOYoLAJD3kDgBADJ09uxZDR48WPfee6/c3d1VoUIFtWvXTpGRkannxMbGqkOHDg6MMn2TJk2SyWRK8+Xv7596zhdffKGIiAht2bJFsbGxKlGiRLrHtm/frueeey5L9+3Zs6cOHTpkr6cFAHAQF0cHAADIu7p3767ExER98cUXqlq1qs6cOaP169frwoULqedUqFDBgRHeXe3atfXLL79YHHNx+fdP35EjR1SrVi3VqVPnrsfKli2b5XsWKVJERYoUyUHUAIC8iBEnAEC6Ll68qE2bNuntt99W69at5evrq0aNGmns2LHq1KlT6nl3TtXbsmWL6tevLw8PDzVs2FArV66UyWTS7t27JUkbNmyQyWTS+vXr1bBhQ3l6eqpp06Y6ePBgahtHjhxRt27dVL58eRUtWlQPPfRQmgQoK1xcXFShQgWLrzJlykiSWrVqpffee08bN26UyWRSq1at0j0mpZ0WePHiRT333HMqX768PDw8VKdOHa1evVpS+lP1fvrpJzVo0EAeHh6qWrWqJk+erKSkJIuf4fz58/XYY4/J09NTNWrU0KpVqyza2Ldvnzp16qTixYurWLFiat68uY4cOaKNGzfK1dVVcXFxFuePHj1aLVq0sPpnBgBIH4kTACBdRYsWVdGiRbVy5UolJCRk6ZrLly+rS5cuqlu3rnbt2qWpU6fqtddeS/fcN954Q++995527NghFxcXPfvss6nfu3Llijp27KhffvlFUVFRateunbp06aITJ07Y5LlJ0vLlyzVo0CAFBgYqNjZWy5cvT/fYnVJSUtShQwdt2bJFX331laKjo/XWW2/J2dk53fusXbtWvXr10vDhwxUdHa158+Zp0aJFevPNNy3Omzx5snr06KE9e/aoY8eOeuaZZ1JH9k6dOqUWLVrIw8NDv/76q3bu3Klnn31WSUlJatGihapWraovv/wyta2kpCR99dVX6t+/v81+XgBQ6BkAAGTg+++/N0qVKmV4eHgYTZs2NcaOHWv88ccfFudIMlasWGEYhmF8/PHHRunSpY3r16+nfv+zzz4zJBlRUVGGYRjGb7/9Zkgyfvnll9Rz1qxZY0iyuO5OAQEBxgcffJD62NfX15g9e3aG50+cONFwcnIyvLy8LL4GDBiQes6IESOMli1bWlyX3rHb77V27VrDycnJOHjwYLr3XbhwoVGiRInUx82bNzemT59ucc6XX35p+Pj4pD6WZIwbNy718ZUrVwyTyWSEhoYahmEYY8eONfz8/IybN2+me8+3337bqFWrVurjlStXGkWLFjWuXLmS7vkAAOsx4gQAyFD37t11+vRprVq1Su3atdOGDRv04IMPatGiRemef/DgQdWrV08eHh6pxxo1apTuufXq1Uv9t4+Pj6RbxSgk6erVq3r11VcVEBCgkiVLqmjRojpw4IDVI0733Xefdu/ebfF150iPtXbv3q1KlSqpZs2aWTp/586dmjJlSuoIXtGiRTVo0CDFxsbq2rVrqefd/vPw8vJSsWLFUn8eu3fvVvPmzeXq6pruPfr166c///xTW7dulSR9/vnn6tGjh7y8vLL7NAEAd6A4BADgrjw8PBQUFKSgoCBNmDBBAwcO1MSJE9WvX7805xqGIZPJlOZYem5PAszXpKSkSJJeeeUVrV27Vu+++66qV6+uIkWK6IknntDNmzetit3NzU3Vq1e36prMWFv4ISUlRZMnT9bjjz+e5nu3J5h3JkUmkyn155HZPcuVK6cuXbpo4cKFqlq1qkJCQrRhwwar4gQA3B2JEwDAKgEBARnu2+Tv768lS5YoISFB7u7ukqQdO3ZYfY+IiAj169dPjz32mKRba56OHTuW3ZBtql69ejp58qQOHTqUpVGnBx98UAcPHsxRAlevXj198cUXSkxMzHDUaeDAgXryySdVqVIlVatWTc2aNcv2/QAAaTFVDwCQrvPnz+uRRx7RV199pT179igmJkbfffedZs6cqW7duqV7zdNPP62UlBQ999xz2r9/f+qokaQ0I1F3U716dS1fvly7d+/WH3/8kdqutZKSkhQXF2fxdebMGavbuV3Lli3VokULde/eXWFhYYqJiVFoaKh+/vnndM+fMGGCFi9erEmTJmnfvn3av3+/li1bpnHjxmX5ni+++KLi4+P15JNPaseOHTp8+LC+/PJLi0qE7dq1U4kSJTRt2jSKQgCAHZA4AQDSVbRoUTVu3FizZ89WixYtVKdOHY0fP16DBg3Shx9+mO41xYsX108//aTdu3erfv36euONNzRhwgRJltPSMjN79myVKlVKTZs2VZcuXdSuXTs9+OCDVj+Hffv2ycfHx+LL19fX6nbu9MMPP+ihhx7SU089pYCAAL366qtKTk5O99x27dpp9erVCgsL00MPPaQmTZpo1qxZVsVRunRp/frrr7py5YpatmypBg0a6LPPPrMYfXJyclK/fv2UnJysPn365Pg5AgAsmYyMJp8DAGADS5YsUf/+/XXp0iU2hrWzQYMG6cyZM2n2gAIA5BxrnAAANrV48WJVrVpV99xzj/744w+99tpr6tGjB0mTHV26dEnbt2/XkiVL9OOPPzo6HAAokEicAAA2FRcXpwkTJiguLk4+Pj76z3/+k+MS4Li7bt266ffff9fgwYMVFBTk6HAAoEBiqh4AAAAAZILiEAAAAACQCRInAAAAAMgEiRMAAAAAZILECQAAAAAyQeIEAAAAAJkgcQIAAACATJA4AQAAAEAmSJwAAAAAIBP/B6VaUxmTJfYeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8735333893782667, 0.1975694949788968), (0.8994480792426623, 0.16795226313491485), (0.9298219964179978, 0.1300756803958667), (0.96030556672393, 0.08921554358899723), (0.9801893344054973, 0.05916169407655363), (0.9899119119850872, 0.03802212196186872), (0.9949925070360759, 0.02361373890263426), (0.9975145290398041, 0.015136079173337215)]\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_batch_norm(dense_layer, bn_layer):\n",
    "    W, b = dense_layer.get_weights()\n",
    "    gamma, beta, moving_mean, moving_var = bn_layer.get_weights()\n",
    "\n",
    "    epsilon = bn_layer.epsilon\n",
    "    std = np.sqrt(moving_var + epsilon)\n",
    "    new_W = gamma / std * W\n",
    "    new_b = gamma / std * (b - moving_mean) + beta\n",
    "\n",
    "    return new_W, new_b\n",
    "\n",
    "def create_folded_model(original_model): # Fold batch normalization layers into dense layers\n",
    "    inputs = original_model.input\n",
    "    x = inputs\n",
    "    new_layers = []\n",
    "\n",
    "    for layer in original_model.layers:\n",
    "        if isinstance(layer, QDense):\n",
    "            next_layer = new_layers[-1] if new_layers else inputs\n",
    "            if isinstance(next_layer, BatchNormalization):\n",
    "                # Fold the BatchNormalization into the previous Dense layer\n",
    "                new_W, new_b = fold_batch_norm(layer, next_layer)\n",
    "                x = QDense(layer.units, weights=[new_W, new_b], kernel_quantizer=layer.kernel_quantizer, bias_quantizer=layer.bias_quantizer)(x)\n",
    "                new_layers.pop()  # Remove the BatchNormalization layer\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        elif not isinstance(layer, BatchNormalization):\n",
    "            x = layer(x)\n",
    "        new_layers.append(x)\n",
    "\n",
    "    outputs = x\n",
    "\n",
    "    new_model = Model(inputs, outputs)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDone():\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6, 6)\n",
    "    square = plt.Rectangle((0, 0), 1, 1, color='green')\n",
    "    ax.add_patch(square)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.title(\"DONE\", fontsize=20)\n",
    "\n",
    "showDone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_10\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " y_timed_input (InputLayer)  multiple                     0         ['y_timed_input[0][0]']       \n",
      "                                                                                                  \n",
      " dense1 (QDense)             (None, 32)                   3392      ['y_timed_input[1][0]']       \n",
      "                                                                                                  \n",
      " q_activation_11 (QActivati  (None, 32)                   0         ['dense1[2][0]']              \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " dense2 (QDense)             (None, 16)                   528       ['q_activation_11[3][0]']     \n",
      "                                                                                                  \n",
      " q_activation_12 (QActivati  (None, 16)                   0         ['dense2[2][0]']              \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " dense_output (QDense)       (None, 1)                    17        ['q_activation_12[3][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3937 (15.38 KB)\n",
      "Trainable params: 3937 (15.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/a/alexyue/miniconda3/envs/SmartPixel/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Create the folded model\n",
    "new_model = create_folded_model(model)\n",
    "\n",
    "# Verify the new model\n",
    "new_model.summary()\n",
    "\n",
    "new_model.save(f'./DNN_L2_S24_best_performance_single_quant_4_6.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the model when done\n",
    "model.save(f'./DNN_L3_S32_best_performance_quant.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_1\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata). They cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_1\" was not an Input tensor, it was generated by layer \"y_timed_input\".\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: KerasTensor(type_spec=TensorSpec(shape=(None, 105), dtype=tf.float32, name='y_timed_input'), name='y_timed_input', description=\"created by layer 'y_timed_input'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " y_timed_input (InputLayer)  multiple                  0         \n",
      "                                                                 \n",
      " dense1 (QDense)             (None, 8)                 848       \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_output (QDense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 857 (3.35 KB)\n",
      "Trainable params: 857 (3.35 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load in a saved model from the h5 file\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "loaded_model = load_model('./DNN_L2_S16_best_performance_single_quant_folded.h5', custom_objects=co)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m(data, loaded_model)\n\u001b[1;32m      2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m getTargetMetrics(test_results)\n\u001b[1;32m      3\u001b[0m displayPerformance(data, test_results, metrics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model(data, loaded_model, HYPERPARAMETERS)\n",
    "metrics = getTargetMetrics(test_results)\n",
    "displayPerformance(data, test_results, metrics, HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write input data to file\n",
    "with open('DNN_hp_input_features.dat', 'w') as file:\n",
    "    for row in input_train_data_combined:\n",
    "        line = ' '.join(map(str, row))  # Convert each number to string and join with space\n",
    "        file.write(line + '\\n')\n",
    "# Write target data to file\n",
    "with open('./DNN_hp_predictions_small.dat', 'w') as file:\n",
    "    for score in target_test_data_coded:\n",
    "        file.write(str(score[0]) + '\\n')  # Convert number to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set display by time slice\n",
    "def display_dataset(input_dataset, target_dataset, i, gif=False):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset[i]\n",
    "    target_datapoint = target_dataset[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if input_datapoint.shape != (20, 13, 21):\n",
    "        raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point\")\n",
    "    \n",
    "    for i in range(8):\n",
    "        for j in range(13):\n",
    "            for k in range(21):\n",
    "                print(input_datapoint[i, j, k], end=\", \")\n",
    "            print()\n",
    "        print(\"--------\", i)\n",
    "\n",
    "    # Extracting the transverse momentum (pt) from the target_data dataset\n",
    "    pt = target_datapoint[8]  # Assuming the 9th variable is at index 8\n",
    "\n",
    "    fig, ax_main = plt.subplots(figsize=(8, 6))\n",
    "    divider = make_axes_locatable(ax_main)\n",
    "\n",
    "    # Add row sum plot as an inset to the main plot\n",
    "    ax_row = divider.append_axes(\"right\", size=\"20%\", pad=0.4)\n",
    "\n",
    "    # Add column sum plot below the main plot\n",
    "    ax_column = divider.append_axes(\"bottom\", size=\"20%\", pad=0.5)\n",
    "\n",
    "    # Initial plot\n",
    "    im = ax_main.imshow(input_datapoint[0, :, :], cmap='plasma')\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[2]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "    # Function to update the animation\n",
    "    def update(t):\n",
    "        # Update main plot\n",
    "        data = input_datapoint[t, :, :]\n",
    "        im.set_data(data)\n",
    "\n",
    "        # Update row sum plot\n",
    "        ax_row.clear()\n",
    "        ax_row.barh(np.arange(data.shape[0]), np.sum(data, axis=1), color='red')\n",
    "        ax_row.set_ylim(0, data.shape[0]-1)\n",
    "        ax_row.set_yticks(np.arange(data.shape[0]))\n",
    "        ax_row.set_xlim(np.min(input_datapoint[:, :, :].sum(axis=2)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=2)) * 1.1)\n",
    "        ax_row.set_xlabel(\"Row Sum\")\n",
    "\n",
    "        # Update column sum plot\n",
    "        ax_column.clear()\n",
    "        ax_column.bar(np.arange(data.shape[1]), np.sum(data, axis=0), color='blue')\n",
    "        ax_column.set_xlim(0, data.shape[1]-1)\n",
    "        ax_column.set_xticks(np.arange(data.shape[1]))\n",
    "        ax_column.set_ylim(np.min(input_datapoint[:, :, :].sum(axis=1)) * 1.1, np.max(input_datapoint[:, :, :].sum(axis=1)) * 1.1)\n",
    "        ax_column.set_ylabel(\"Column Sum\")\n",
    "\n",
    "        # Update labels and grid\n",
    "        ax_main.set_xlabel(\"X Position\")\n",
    "        ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "        # Update title for the entire figure\n",
    "        fig.suptitle(f\"Timestep: {t+1} | Data Point: {i} | pt: {pt:.2f} GeV\")\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=20, repeat=True)\n",
    "\n",
    "    gif_path = f\"data_point.gif\"\n",
    "    if gif:\n",
    "        # Save the animation as a GIF\n",
    "        writer = PillowWriter(fps=1000 // FRAME_TIME)\n",
    "        ani.save(gif_path, writer=writer)\n",
    "\n",
    "    plt.close()\n",
    "    return display(HTML(ani.to_jshtml())), gif_path\n",
    "\n",
    "def display_model_IO(input_dataset_combined, target_dataset_coded, i):\n",
    "    # Extract the i-th data point from both datasets\n",
    "    input_datapoint = input_dataset_combined[i]\n",
    "    target_datapoint = target_dataset_coded[i]\n",
    "\n",
    "    # Check if the index is valid\n",
    "    if (MODEL_TYPE == \"DNN\"):\n",
    "        if input_datapoint.shape != (NUM_TIME_SLICES * 13 + 1,):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 2\")\n",
    "        input_datapoint = input_datapoint[:-1].reshape(NUM_TIME_SLICES, 13)\n",
    "    elif (MODEL_TYPE == \"CNN\"):\n",
    "        if input_datapoint[0].shape != (NUM_TIME_SLICES, 13):\n",
    "            raise ValueError(f\"{input_datapoint.shape } is an invalid shape for the input data point 3\")\n",
    "        input_datapoint = input_datapoint[0]\n",
    "        \n",
    "    # Extracting the label from the target datapoint\n",
    "    if target_datapoint[0] == 1:\n",
    "        label = f\"High p_t (over {TEST_PT_THRESHOLD} GeV)\"\n",
    "    elif target_datapoint[1] == 1:\n",
    "        label = f\"low p_t and negative charge\"\n",
    "    elif target_datapoint[2] == 1:\n",
    "        label = f\"low p_t and positive charge\"\n",
    "    else: \n",
    "        raise ValueError(\"Invalid labelling for the target data point\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax_main = plt.subplots(figsize=(4,4))\n",
    "    print(input_datapoint.shape)\n",
    "    print(input_datapoint)\n",
    "    im = ax_main.imshow(input_datapoint.T, cmap='coolwarm_r', vmin=-1, vmax=1)\n",
    "    ax_main.invert_yaxis()\n",
    "    ax_main.set_xticks(np.arange(input_datapoint.shape[0]))\n",
    "    ax_main.set_yticks(np.arange(input_datapoint.shape[1]))\n",
    "    ax_main.grid(True, color='gray', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Update labels and grid\n",
    "    ax_main.set_xlabel(\"Time Slice\")\n",
    "    ax_main.set_ylabel(\"Y Position\")\n",
    "\n",
    "\n",
    "    # Update title for the entire figure\n",
    "    fig.suptitle(f\"Data Point: {i} | label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_model_IO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rand_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m FRAME_TIME \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m  \u001b[38;5;66;03m# milliseconds between frames\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdisplay_model_IO\u001b[49m(input_data_combined_example, target_data_coded_example, rand_idx)\n\u001b[1;32m      5\u001b[0m animation, gif \u001b[38;5;241m=\u001b[39m display_dataset(input_data_example, target_data_example, rand_idx, gif\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_model_IO' is not defined"
     ]
    }
   ],
   "source": [
    "# DATASET DISPLAY\n",
    "rand_idx = random.randint(0, 100)\n",
    "FRAME_TIME = 120  # milliseconds between frames\n",
    "display_model_IO(input_data_combined_example, target_data_coded_example, rand_idx)\n",
    "animation, gif = display_dataset(input_data_example, target_data_example, rand_idx, gif=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import os\n",
    "os.environ['XILINX_HLS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis_HLS/2023.1'\n",
    "os.environ['XILINX_VIVADO'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vivado/2023.1'\n",
    "os.environ['XILINX_VITIS'] = '/afs/slac.stanford.edu/g/reseng/vol39/xilinx/2023.1/Vitis/2023.1'\n",
    "os.environ['XILINX_AP_INCLUDE'] = '/fs/ddn/sdf/group/atlas/d/hjia625/Smart_Pixel/HLS_arbitrary_Precision_Types/include'\n",
    "os.environ['PATH'] = os.environ['XILINX_HLS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_VITIS'] + '/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = os.environ['XILINX_AP_INCLUDE'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "strip_model = strip_pruning(qmodel_pruned)\n",
    "hls_config = hls4ml.utils.config_from_keras_model(strip_model , granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    print(Layer)\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<10,2,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "# If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "hls_config['LayerName']['output_sigmoid']['Strategy'] = 'Stable'\n",
    "hls_config['LayerName']['output_sigmoid']['Precision'] = 'ap_fixed<32,8,AP_RND_ZERO,AP_SAT>'\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vitis')\n",
    "\n",
    "cfg['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg['HLSConfig'] = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'cnn_debug/'\n",
    "cfg['XilinxPart'] = 'xcku040-ffva1156-2-e'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()\n",
    "#hls_model.profile()\n",
    "hls_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in qmodel_pruned.layers:\n",
    "    for i, w in enumerate(layer.weights):\n",
    "        try:\n",
    "            print(\"weight is\", w.numpy(), \"for layer number\", i)  # TF 2.x\n",
    "        except Exception:\n",
    "            print(\"weight is\", layer.get_weights()[i], \"for layer number\", i) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Smart_Pixel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
