{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 02:44:53.653445: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c conda-forge imbalanced-learn -y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Reshape, Concatenate, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, TimeDistributed, LSTM, Conv3D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import CSVLogger\n",
    "import psutil\n",
    "\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory usage percentage: {memory.percent}%\")\n",
    "\n",
    "def print_cpu_usage():\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 58.80 GB\n",
      "Available memory: 40.19 GB\n",
      "Used memory: 17.04 GB\n",
      "Memory usage percentage: 31.7%\n",
      "Total memory: 58.80 GB\n",
      "Available memory: 39.06 GB\n",
      "Used memory: 18.16 GB\n",
      "Memory usage percentage: 33.6%\n",
      "Total memory: 58.80 GB\n",
      "Available memory: 37.94 GB\n",
      "Used memory: 19.28 GB\n",
      "Memory usage percentage: 35.5%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_combine_shuffle_data_optimized_hdf5():\n",
    "    with h5py.File('/gpfs/slac/atlas/fs1/d/hjia625/Smart_Pixel/data.hdf5', 'r') as h5f:\n",
    "        combined_input = None\n",
    "        combined_target = None\n",
    "\n",
    "        for charge_type in ['positive-charge', 'negative-charge']:\n",
    "            for data_type in ['sig', 'bkg']:\n",
    "                # Construct dataset names\n",
    "                input_dataset_name = f'{charge_type}_{data_type}_input'\n",
    "                target_dataset_name = f'{charge_type}_{data_type}_target'\n",
    "\n",
    "                # Check if the dataset exists and load data sequentially\n",
    "                if input_dataset_name in h5f and target_dataset_name in h5f:\n",
    "                    input_data = h5f[input_dataset_name][:].astype(np.float16)\n",
    "                    target_data = h5f[target_dataset_name][:].astype(np.float16)\n",
    "\n",
    "                    if combined_input is None:\n",
    "                        combined_input = input_data\n",
    "                        combined_target = target_data\n",
    "                        # Free memory of the loaded data\n",
    "                        del input_data, target_data\n",
    "                        gc.collect()\n",
    "\n",
    "                    else:\n",
    "                        print_memory_usage()\n",
    "                        combined_input = np.vstack((combined_input, input_data))\n",
    "                        combined_target = np.vstack((combined_target, target_data))\n",
    "                        # Free memory of the loaded data\n",
    "                        del input_data, target_data\n",
    "                        gc.collect()\n",
    "\n",
    "                else:\n",
    "                    print(f\"Dataset {input_dataset_name} or {target_dataset_name} not found.\")\n",
    "\n",
    "        # Shuffling\n",
    "        indices = np.arange(combined_input.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        combined_input = combined_input[indices]\n",
    "        combined_target = combined_target[indices]\n",
    "\n",
    "        return combined_input, combined_target\n",
    "\n",
    "# Example usage\n",
    "X, y = load_combine_shuffle_data_optimized_hdf5()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 58.80 GB\n",
      "Available memory: 37.92 GB\n",
      "Used memory: 19.30 GB\n",
      "Memory usage percentage: 35.5%\n",
      "Total memory: 58.80 GB\n",
      "Available memory: 37.93 GB\n",
      "Used memory: 19.29 GB\n",
      "Memory usage percentage: 35.5%\n",
      "Total memory: 58.80 GB\n",
      "Available memory: 37.93 GB\n",
      "Used memory: 19.29 GB\n",
      "Memory usage percentage: 35.5%\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(-1, 8*13*21)\n",
    "scaler = StandardScaler()\n",
    "standard_batch_size = 10000  # Adjust based on your system's capability\n",
    "n = X.shape[0]\n",
    "# Fit the scaler incrementally\n",
    "for start in range(0, X.shape[0], standard_batch_size):\n",
    "    end = start + standard_batch_size\n",
    "    scaler.partial_fit(X[start:end, :])\n",
    "\n",
    "# Transform the data in batches\n",
    "scaled_X = np.empty_like(X, dtype=np.float16)\n",
    "for start in range(0, X.shape[0], standard_batch_size):\n",
    "    end = start + standard_batch_size\n",
    "    scaled_X[start:end, :] = scaler.transform(X[start:end, :])\n",
    "\n",
    "# Clean up\n",
    "X = scaled_X\n",
    "del scaled_X\n",
    "gc.collect()\n",
    "\n",
    "# X = X.reshape(-1, 8*13*21)\n",
    "# print_memory_usage()\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "# del X_reshaped\n",
    "# gc.collect()\n",
    "X = X.reshape(n,8,13,21)\n",
    "print_memory_usage()\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "del X\n",
    "del y\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "del X_temp\n",
    "del y_temp\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 58.80 GB\n",
      "Available memory: 42.18 GB\n",
      "Used memory: 15.04 GB\n",
      "Memory usage percentage: 28.3%\n"
     ]
    }
   ],
   "source": [
    "def to_sum(X, y):\n",
    "    if X.shape[1:] == (8, 13, 21) and y.shape[1:] == (13, ):\n",
    "        #y_expanded = np.repeat(y[:,7], 8, axis=0).reshape(-1, 8, 1)\n",
    "        X_sum = np.sum(X, axis=3)\n",
    "        X_sum.reshape(X_sum.shape[0],8,13,1)\n",
    "        n = y.shape[0]\n",
    "        one_hot = np.zeros((n, 3))\n",
    "\n",
    "        # Class 1: np.abs(y[:, 8]) > 2\n",
    "        one_hot[np.abs(y[:, 8]) >= 2, 0] = 1\n",
    "\n",
    "        # Class 2: np.abs(y[:, 8]) <= 2 and y[:, 13] == 1\n",
    "        one_hot[(np.abs(y[:, 8]) < 2) & (y[:, 8] > 0), 1] = 1\n",
    "\n",
    "        # Class 3: np.abs(y[:, 8]) <= 2 and y[:, 13] == -1\n",
    "        one_hot[(np.abs(y[:, 8]) < 2) & (y[:, 8] < 0), 2] = 1\n",
    "        return X_sum, one_hot, y[:,7].reshape(-1, 1)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong array shape!\")\n",
    "\n",
    "\n",
    "X_sum_train, y_sum_train, y0_train = to_sum(X_train, y_train)\n",
    "X_sum_val, y_sum_val, y0_val = to_sum(X_val, y_val)\n",
    "X_sum_test, y_sum_test, y0_test = to_sum(X_test, y_test)\n",
    "del X_train\n",
    "del y_train\n",
    "del X_val\n",
    "del y_val\n",
    "del X_test\n",
    "del y_test\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 02:49:03.849065: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 8, 13, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 8, 7, 8)     32          ['input_3[0][0]']                \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 8, 56)        0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 8)            2080        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 9)            0           ['lstm[0][0]',                   \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           160         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 16)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            51          ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,323\n",
      "Trainable params: 2,323\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Conv1D, TimeDistributed\n",
    "from keras.models import Model\n",
    "def LSTMmodel():\n",
    "    # Cluster y-profile input\n",
    "    y_profile_input = Input(shape=(8, 13, 1))  # Adjust the shape based on your input\n",
    "\n",
    "    # LSTM layers\n",
    "    # lstm_layer = LSTM(16, activation='relu', return_sequences=False, recurrent_dropout=0.5, input_shape=(8, 13))\n",
    "    # lstm_output = lstm_layer(y_profile_input)\n",
    "    conv1 = TimeDistributed(Conv1D(8, 3, activation='relu', strides=2, padding='same'))(y_profile_input)\n",
    "\n",
    "\n",
    "    reshaped = Reshape((8, -1))(conv1)  # Adjust the shape based on the output of Conv1D\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm_output = LSTM(8, activation='relu', return_sequences=False, recurrent_dropout=0.5)(reshaped)\n",
    "\n",
    "    # Flatten and concatenate with y0 input\n",
    "    y0_input = Input(shape=(1,))\n",
    "    concat = Concatenate()([lstm_output, y0_input])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(16, activation='relu')(concat)\n",
    "    dropout = Dropout(0.5)(dense) # best so far 0.4\n",
    "\n",
    "    # Output layer (adjust based on your classification problem)\n",
    "    output = Dense(3, activation='softmax')(dropout)  # Change number of neurons based on the number of classes\n",
    "\n",
    "    model = Model(inputs=[y_profile_input, y0_input], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and summarize the model\n",
    "model = LSTMmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"cp.ckpt\"\n",
    "earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   patience=40,\n",
    "                                   restore_best_weights=False)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n",
    "csv_logger = CSVLogger('log.csv', append=False, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_labels = np.argmax(y_sum_train, axis=1)\n",
    "\n",
    "# Now use class_labels to compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(class_labels), y=class_labels)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1718/1719 [============================>.] - ETA: 0s - loss: 1.0476 - accuracy: 0.4941"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "model = LSTMmodel()\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 200  # Number of epochs to train\n",
    "batch_size = 512  # Number of samples per gradient update\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_sum_train, y0_train], y_sum_train,  # Training data and labels\n",
    "    validation_data=([X_sum_val, y0_val], y_sum_val),  # Validation data\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[earlyStop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions_prob = model.predict([X_sum_test, y0_test])\n",
    "predictions_labels = np.argmax(predictions_prob, axis=1)\n",
    "y_test_labels = np.argmax(y_sum_test, axis=1)\n",
    "cm = confusion_matrix(y_test_labels, predictions_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='YlGnBu')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Efficiency\n",
    "signal_efficiency = cm[0, 0] / np.sum(cm[0, :])\n",
    "\n",
    "# Background Rejection Rate\n",
    "background_rejection = (cm[1, 1] + cm[1, 2]+cm[2, 1] + cm[2, 2]) / (np.sum(cm[1, :]) + np.sum(cm[2, :]))\n",
    "\n",
    "print(\"Signal Efficiency:\", signal_efficiency)\n",
    "print(\"Background Rejection Rate:\", background_rejection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class_1_true = y_sum_test[:, 0]\n",
    "class_1_scores = predictions_prob[:, 0]\n",
    "\n",
    "# Calculate the AUC for class 1\n",
    "class_1_auc = roc_auc_score(class_1_true, class_1_scores)\n",
    "\n",
    "print(\"Class 1 AUC:\", class_1_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "thresholds = np.linspace(0.2, 0.5, 500)\n",
    "signal_efficiencies = []\n",
    "background_rejections = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # predicted_class = ((predictions_prob[:, 0] + threshold > predictions_prob[:, 1]) & (predictions_prob[:, 0] + threshold > predictions_prob[:, 2])).astype(int)\n",
    "    predicted_class = (predictions_prob[:, 0] > threshold).astype(int)\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_sum_test[:, 0], predicted_class)\n",
    "    \n",
    "    # Calculate signal efficiency and background rejection\n",
    "    signal_efficiency = cm[1, 1] / np.sum(cm[1, :])\n",
    "    background_rejection = cm[0, 0] / np.sum(cm[0, :])\n",
    "    \n",
    "    # Store metrics\n",
    "    signal_efficiencies.append(signal_efficiency)\n",
    "    background_rejections.append(background_rejection)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, signal_efficiencies, label='Signal Efficiency')\n",
    "plt.plot(thresholds, background_rejections, label='Background Rejection')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Effect of Threshold on Signal Efficiency and Background Rejection')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(signal_efficiencies, background_rejections, marker='o')\n",
    "plt.xlabel('Signal Efficiency')\n",
    "plt.ylabel('Background Rejection')\n",
    "plt.title('Background Rejection vs. Signal Efficiency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_sum_test, predictions_prob)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(sorted_array, value):\n",
    "    # Ensure the array is a NumPy array\n",
    "    sorted_array = np.array(sorted_array)\n",
    "\n",
    "    # Compute the absolute difference\n",
    "    abs_diff = np.abs(sorted_array - value)\n",
    "\n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = np.argmin(abs_diff)\n",
    "\n",
    "    return closest_index\n",
    "\n",
    "\n",
    "index_848 = find_closest(signal_efficiencies, 0.848)\n",
    "index_933 = find_closest(signal_efficiencies, 0.933)\n",
    "index_976 = find_closest(signal_efficiencies, 0.976)\n",
    "print(f\"Signal Efficiency:{signal_efficiencies[index_848]*100:.1f}%\",f\"Background Rejections:{background_rejections[index_848]*100:.1f}%\")\n",
    "print(f\"Signal Efficiency:{signal_efficiencies[index_933]*100:.1f}%\",f\"Background Rejections:{background_rejections[index_933]*100:.1f}%\")\n",
    "print(f\"Signal Efficiency:{signal_efficiencies[index_976]*100:.1f}%\",f\"Background Rejections:{background_rejections[index_976]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_accuracy, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y_sum_train[:,0]))\n",
    "print(y_sum_train.shape)\n",
    "print(np.sum(y_sum_test[:,0]))\n",
    "print(y_sum_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracy, val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Smart_Pixel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
